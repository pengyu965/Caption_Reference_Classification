[
    {
        "Text": "Table 4: Mutual information between feature subset and class label with f req based feature ranking. ",
        "Entity": "Caption"
    },
    {
        "Text": "As can be seen in figure 2, wing \"part of a bird\" is closely related to tail, as is wing \"part of a plane\"",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: The screenshot of our web-based system shows a simple quantitative analysis of the frequency of two terms in news articles over time. While in the 90s the term Friedensmission (peace operation) was predominant a reverse tendency can be observed since 2001 with Auslandseinsatz (foreign intervention) being now frequently used. ",
        "Entity": "Caption"
    },
    {
        "Text": "We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Semantic expansion example. Note that the expanded queries that were generated in the first two retrieved texts (listed under \u2018matched query\u2019) do not contain the original query. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Descriptive statistics for Web scores and BNC scores for other-anaphora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Language families in our data set. The Other category includes 9 language isolates and 21 language family singletons. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Comparison our closed results with the top three in all test sets",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 gives the mean values for the comparison of troughs placed by the segmentation algorithm to the segmentation points identified by the test subjects for all the texts",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Scores for CityU corpus",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Meaning of diacritics indicating statistical sig- nificance (\u03c72 tests) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Average word accuracy for transduced sentences.",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 2. Schematic \ufb02owchart of the work\ufb02ow we followed, regarding the datasets, the training techniques and the operations.",
        "Entity": "Caption"
    },
    {
        "Text": "MENE has only been tested on MUC7.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 20 Feature templates for the phrase-structure parser. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Morph Examples and Motivations.",
        "Entity": "Caption"
    },
    {
        "Text": "The translations of 6 of the 43 words are words in the dictionary (denoted as \u201ccomm.\u201d in Table 3) and 4 of the 43 words appear less than 10 times in the English part of the corpus (denoted as \u201cinsuff\u201d).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1. Correlation between cohesion-driven functions.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Results for ODP system using various sources of DA tags ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 12 MWE identification F1 of the parsing models vs. the mwetoolkit baseline (test set, sentences \u2264 40 words). FactLex\u2217 uses gold morphological analyses at test time. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 10 Number of processed arcs for the pseudotranslation task as a function of the input sentence length J (y-axis is given in log scale). The complexity for the four different reordering constraints MON, GE, EG, and S3 is given. The complexity of the S3 constraint is close to J4 . ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7",
        "Entity": "Reference"
    },
    {
        "Text": "igure 1 gives an example.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 16 Accuracy comparisons between various dependency parsers on English data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Translation Performance (%).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example of semantic trees",
        "Entity": "Caption"
    },
    {
        "Text": "Table 18: Arabic idafa Construct",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Bootstrapping new heuristics.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 10: Comparison of our approach with the state-of-art systems",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Examples of disagreement in segmentation guidelines",
        "Entity": "Caption"
    },
    {
        "Text": "We compare in Table 2 the performance of Unified Parse and Semantic Trees with different kinds of Entity Semantic Tree setups using standard convolution tree kernel, while the SPT and DSPT with only entity-type information are listed for reference.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: BLEU scores on the test08 and news08 test data obtained by models trained by MERT and SVM. ",
        "Entity": "Caption"
    },
    {
        "Text": "We choose t = 1, 5, and 30 for the fertility HMM",
        "Entity": "Reference"
    },
    {
        "Text": "See Fig ure 6 for some examples.",
        "Entity": "Reference"
    },
    {
        "Text": "As an example, the probability of accepting the prediction in figure 1 is about .25.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: The regular expressions available in Foma from highest to lower precedence. Horizontal lines separate precedence classes. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 21 The standard split of CTB2 data for phrase-structure parsing. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10: Performance of Two Categories",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 Illustration of the paraphrase degree calculation.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Architecture of the translation approach based on Bayes\u2019 decision rule. ",
        "Entity": "Caption"
    },
    {
        "Text": "  Figure 2. Automatically detected posture points (H = headDepth, M = midTorsoDepth, L = lowerTorsoDepth) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Dependency Parsing: MWE results",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 14: Correlation between manual and automatic scores for English-French",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 shows the training and test corpus statistics.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 12 Large-scale clustering on D2. ",
        "Entity": "Caption"
    },
    {
        "Text": "At the morp heme level, stems are divid ed from their affixe s. For exam ple, altho ugh both naga no (Naga no) and shi (city) can appea r as indivi dual words , nagano shi (Nag ano city) is brack eted as [[naga no][s hi]], since here shi Figure 3: Determining word boundaries.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4: Examples of signature caseframes found in Study 2. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: the influence of agenda size.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Optimized Edit Costs",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Examples of SMT errors due to MWEs.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 shows the influence of the four parameters.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 8: Contribution of feature sets (material).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: F-Score of the RF and SVM, GIZA++ and Levenshtein distance-based classifier on the second order dataset ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6 Frequency distribution of the MWE types in the ATB and FTB training sets. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2. The pseudo code of Algorithm 1.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Incremental dev set results for the manually annotated grammar (sentences of length \u2264 70).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Variation in performance, by number of sentence boundaries (n), and by training corpus size.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Corpus.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 14 Summary of results for unknown-boundary condition. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Relation Feature Spaces of the Example Sentence \u201c\u2026\u2026 to stop the merger of an estimated",
        "Entity": "Caption"
    },
    {
        "Text": "Tables 4 and 5 show the effect of the pruning parameter tp with the histogram pruning parameter Np = 50,000",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5 Results when the development set is not used to estimate \u03bb and K. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 16 Overview of the results for all BNC algorithms for coreference. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Results of 1000 sentences",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Accuracy of our system in each period (M = 10) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10 %BLEU on tune and test sets when comparing parsers for ZH\u2192EN translation. QPD uses all features, including T GT T REE and T REE T O T REE. The table first pairs supervised English parsing with supervised, unsupervised, and random Chinese parsing, then pairs unsupervised English parsing with supervised and unsupervised Chinese parsing. \u2020 = significantly better than sup/sup, \u2217 = significantly worse than sup/sup. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: BLEU scores after discriminative hypergraph- reranking. Only the language model (LM) or the transla- tion model (TM) or both (LM+TM) may be discrimina- tively trained to prefer the oracle-best hypotheses. ",
        "Entity": "Caption"
    },
    {
        "Text": "The second factor of Equation (13) is estimated from the Poisson distribution whose parameter",
        "Entity": "Reference"
    },
    {
        "Text": "In MUC6, the best result is achieved by SRA (Krupka, 1995).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5 Performance on morphological analysis.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The Stanford parser (Klein and Manning, 2002) is unable to recover the verbal reading of the unvocalized surface form 0 an (Table 1)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Size of Seed Lexicons",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Example of a word lattice",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Performance of different relation types and major subtypes in the test data",
        "Entity": "Caption"
    },
    {
        "Text": "To simplify the description, we assume in Figure 2 that a bigram language model is used and all the TTS templates are binarized.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Segmentation recall relative to gold word frequency. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Accuracy for all cases, all excluding sen- tences with quotes, and only sentences with quotes. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. The scored results of our CWS in the MSR_C track (OOV is 0.034) for 3rd bakeoff. ",
        "Entity": "Caption"
    },
    {
        "Text": "But it conflates the coordinating and discourse separator functions of wa (<..4.b \ufffd \ufffd) into one analysis: conjunction(Table 3).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2. Percentage of examples of major syntactic classes.",
        "Entity": "Caption"
    },
    {
        "Text": "Nc Nl HGFC uncons trained A G G N MI F N M I F 13 0 13 3 57 .3 1 36 .6 5 54 .2 2 32 .6 2 11 4 11 7 54 .6 7 37 .9 6 51 .3 5 32 .4 4 50 51 37 .7 5 40 .0 0 32 .6 1 32 .7 8 Table 2: Performance on T2 using a predefined tree structure.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: MAP(%), under the \u201850 rules, All\u2019 setup, when adding component match scores to Precision (P) or prior- only MAP baselines, and when ranking with allCP or allCP+pr methods but ignoring that component scores. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 shows empirical search timings for various values of M , for the MEMD model described in the next section.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: Translation results for English-German",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Recall on different types of empty categories. YX = (Yang and Xue, 2010), Ours = split 6\u00d7. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Most frequent Brown cluster phrase dependencies extracted from DE\u2192EN data, shown with their counts. As in Table 4, we only show those in which one phrase has at least two tokens and neither phrase is entirely punctuation. Each cluster is shown as a set of words large enough to cover 95% of the token counts in the cluster, up to a maximum of four words. It is characteristic of Brown clustering that very frequent tokens (e.g., function words) often receive their own clusters. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Possible Relations between ARG-1 and ARG-2",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Distribution of reasons for false negatives (missed argument mentions) by BInc at K=20. ",
        "Entity": "Caption"
    },
    {
        "Text": "We compared the ATB5 to tree- banks for Chinese (CTB6), German (Negra), and English (WSJ) (Table 4)",
        "Entity": "Reference"
    },
    {
        "Text": "However, if we remove the mouse-node from its local graph illustrated in figure 1, the graph decomposes into two parts, one representing the electronic device meaning of mouse and the other one representing its animal sense.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5: Our approach for detecting parallel fragments. The lower part of the figure shows the source and target sentence together with their alignment. Above are displayed the initial signal and the filtered signal. The circles indicate which fragments of the target sentence are selected by the procedure. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Average results of Reweighting among all 7 speakers when the amount of speaker specific data is 0, 500, 2000 ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7 The adjusted frequencies of character sequences. ",
        "Entity": "Caption"
    },
    {
        "Text": "(2011), we confirmed that omission of the look-ahead features results in a 0.26% decrease in the parsing accuracy on CTB5d (dev).Figure 2: F1 scores (in %) of SegTagDep on CTB 5c1 w.r.t. the training epoch (x-axis) and parsing feature weights (in legend).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Accuracies (%) for Coarse and Fine-Grained PSD, Using MALT and Heuristics. Sorted by preposition. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 shows the correctness evaluation results.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2 Example of a (symmetrized) word alignment (Verbmobil task). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Outline of word segmentation process",
        "Entity": "Caption"
    },
    {
        "Text": "Table 22 Cross-system comparison results. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: OT grammar for devoicing compiled into an FST. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Coverage of caseframes in summaries with respect to the source text. The model aver- age is statistically significantly different from all the other conditions p < 10\u22128 (Study 3). ",
        "Entity": "Caption"
    },
    {
        "Text": "p(z = (FJ , EI , A ) J f ) = 1 1 | N(C( f )) (10",
        "Entity": "Caption"
    },
    {
        "Text": "We see from Table 5, that the improvement in overall parse results is mainly in terms of dependency labeling, reflected in the LAS score.",
        "Entity": "Reference"
    },
    {
        "Text": ", C(e )) (18) 1 1 1 1 i=1 i 1",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Plate diagram representation of the trigram HMM. The indexes i and j range over the set of tags and k ranges over the set of characters. Hyper-parameters have been omitted from the figure for clarity. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Evaluation results within sets",
        "Entity": "Caption"
    },
    {
        "Text": "In our segmentation system, a hybrid strategy is applied (Figure 1): First, forward maximum matching (Chen and Liu, 1992), which is a dictionary-based method, is used to generate a segmentation result.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2. Mapping algorithm \u2013 refining step",
        "Entity": "Caption"
    },
    {
        "Text": "Table 19 Precision of location name recognition on the MSR test set, using Viterbi iterative training, initialized by four seed sets with different sizes. ",
        "Entity": "Caption"
    },
    {
        "Text": "(2004) in figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: The contribution of MMVC in a rank-based classi- fier combination on S ENSEVAL -1 and S ENSEVAL -2 English as computed by 5-fold cross validation over training data ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 IV and OOV recall in   (Zhang et al., 2006a) ",
        "Entity": "Caption"
    },
    {
        "Text": "If it starts with a lower case letter, and contains both upper and lower case letters, then (mixedCaps, zone) is set to 1.",
        "Entity": "Reference"
    },
    {
        "Text": "The list of the features used in our joint model is presented in Table 1, where S01 S05, W01 W21, and T01 05 are taken from Zhang and Clark (2010), and P01 P28 are taken from Huang and Sagae (2010).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: a) An example of a document from Tu\u0308Ba-D/Z, b) an abbreviated entity grid representation of it, and c) the feature vector representation of the abbreviated entity grid for transitions of length two. Mentions of the entity Frauen are underlined. nom: nominative, acc: accusative, oth: dative, oblique, and other arguments ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Bahktin\u2019s characterization of dialogue: Bahktin (1986) describes a discourse along the three major properties style, situation and topic. Current information retrieval systems focus on the topical as- pect which might be crucial in written documents. Furthermore, since throughout text analysis is still a hard problem, information retrieval has mostly used keywords to characterize topic. Many features that could be extracted are therefore ignored in a tradi- tional keyword based approach. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 15 Reduplication \u2013 general case. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9: Comparison of our approach with using only the Gigaword corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: The constituent Restoring of its constructive and effective role parsed by the three different models (gold segmen- tation). The ATB annotation distinguishes between verbal and nominal readings of maSdar process nominals. Like verbs, maSdar takes arguments and assigns case to its objects, whereas it also demonstrates nominal characteristics by, e.g., taking determiners and heading iDafa (Fassi Fehri, 1993). In the ATB, +2  3   asta\u2019adah is tagged 48 times as a noun and 9 times as verbal noun. Consequently, all three parsers prefer the nominal reading. Table 8b shows that verbal nouns are the hardest pre-terminal categories to identify. None of the models attach the attributive adjectives correctly. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Diff results tested against gs-swaco",
        "Entity": "Caption"
    },
    {
        "Text": "K): J k = fj k 1 +1 , ..",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Topic #141",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 12 Interdigitation FSRA \u2013 general. ",
        "Entity": "Caption"
    },
    {
        "Text": ">10 nouns (a) (b) classified as 222 125 (a) class animate 49 3390 (b) class inanimate Table 4: Confusion matrix for the MBLclassifier with a general feature space on the >10 data set on Talbanken05 nouns.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4 shows a constituent headed by a process nominal with an embedded adjective phrase.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Graphical model of HM-BiTAM",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: The response of the rhyme search engine.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Dependency between amount of training data for syntactic parser and quality of morphological prediction.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The clausal and topological field structure of a German sentence. Notice that the subordinate clause receives its own topology. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Performance of Altavista counts and BNC counts for compound bracketing (data from Lauer 1995) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results for baseline (BAS) system (standard multiclass SVM) ",
        "Entity": "Caption"
    },
    {
        "Text": "The bottom-up decoding algorithm for the TTS transducer is sketched in Figure 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8b shows that verbal nouns are the hardest pre-terminal categories to identify.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 12 Overview of the results for all baselines for coreference. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Two dendrograms for the graph in Figure 3.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 22: Arabic Diglossia",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Syntactic tree kernel (STK).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 15: Experimental results of test corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7: Corpus size vs. KL-divergence",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Precision at rank for the different sys- tems on the Athletes class. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: BLEU scores on the News-Commentary development test data ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Syntactic frames for VerbNet classes",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Sources of Dictionaries",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: (a) An undirected graph G representing the similarity matrix; (b) The bipartite graph showing three clusters on G; (c) The induced clusters U ; (d) The new graph G1 over clusters U ; (e) The new bipartite graph over G1 p |vi ) = Vl 1 ...",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1. Result for microblog classification",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Chinese character usage in 3 corpora. The   numbers in brackets indicate the percentage of  characters that are shared by at least 2 corpora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Missing argument examples of biological interactions ",
        "Entity": "Caption"
    },
    {
        "Text": "An abridged version of the grammatical representation produced by the implemented grammar for this sentence is presented in Figure 1, where the feature structures below the tree correspond to partial grammatical representations of the constituents 16 See Kamp and Reyle (1993) for a comprehensive rendering of DRT, and Branco (2000, Chapter 5) for an.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4 Word sense disambiguation accuracy for \u201cNP1 V NP2 to NP3\u201d frame. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Testing phase: baseline vs. LR[0.20,0.95].",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Statistical Information of Corpora",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Corpus statistics in Sighan Bakeoff 2005",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: The Effects of Social Features.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4. Step-by-step Growing Algorithm",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 1 GETARUNS AR algorithm",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Illustration of Pareto Frontier. Ten hypotheses are plotted by their scores in two metrics. Hypotheses indicated by a circle (o) are pareto-optimal, while those indicated by a plus (+) are not. The line shows the convex hull, which attains only a subset of pareto-optimal points. The triangle (4) is a point that is weakly pareto-optimal but not pareto-optimal. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Parsing and word segmentation F- measures vs. the experiment numbers. Lines with triangles: segmentation; Lines with circles: label; Dotted-lines: language-independent features only; Solid lines: plus lexical features. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Frequencies and scores for each resolution class.",
        "Entity": "Caption"
    },
    {
        "Text": "This approach can be seen as a generalization of the originally suggested source channel modeling framework for statistical machine translation",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3. Results obtained in the detection of zero-pronouns.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11 Demonstration of the combination of the two pruning thresholds tC = 5.0 and tc = 12.5 to speed up the search process for the two reordering constraints GE and S3 (no = 50). The translation performance is shown in terms of mWER on the TEST-331 test set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Development results for POS tagging. Given are training times in minutes (TT) and accuracies (ACC). Best baseline results are underlined and the overall best results bold. * indicates a significant difference (positive or negative) between the best baseline and a PCRF model. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9: Labeled and Unlabeled TedEval Results for raw Scenarios, Trained on 5k sentences and tested on 5k terminals. The upper part refers to constituency parsing and the lower part refers to dependency parsing. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Feature templates for the full joint model.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: A text segment from MUC-6 data set",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8 FSRA-2 for Arabic nominative definite and indefinite nouns. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 shows the effect of the role-based preference on our data.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 7: Vocabulary size of NIST task (40K)",
        "Entity": "Caption"
    },
    {
        "Text": "This list of 43 words is shown in Table 3.",
        "Entity": "Reference"
    },
    {
        "Text": "A comparison of the lines for grammatical roles and for surface order in Table 1 shows that the same is true in German.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: The increase in performance for successive variants of Bayes and Mixture Model as evaluated by 5-fold cross vali- dation on S ENSEVAL -2 English data ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 Imbalance Training Class Problem",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Overall results Coverage/Accuracy",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Automatically determined mixture component num-",
        "Entity": "Caption"
    },
    {
        "Text": "Examples of the deletion features can be found in Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Four synchronous rules with topic distributions. Each sub-graph shows a rule with its topic distribution, where the X-axis means topic index and the Y-axis means the topic probability. Notably, the rule (b) and rule (c) shares the same source Chinese string, but they have different topic distributions due to the different English translations. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: A CRF as a multi-sentential parsing model.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Categories of Message Speech Act.",
        "Entity": "Caption"
    },
    {
        "Text": "The unknown parameters are determined by maximizing the likelihood on the parallel training corpus:",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Architecture of Name-aware Machine Translation System.",
        "Entity": "Caption"
    },
    {
        "Text": "In these experiments, the input lacks segmentation markers, hence the slightly different dev set baseline than in Table 6.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Features and functions used in clustering algorithm",
        "Entity": "Caption"
    },
    {
        "Text": "The distinctions in the ATB are linguistically justified, but complicate parsing.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Word perplexity with different \u03b2 using manual reference or ASR hypotheses on CCTV. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Numbers of relations on the ACE RDC 2004: break down by relation types and subtypes ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Learning curves of word prediction accu- racies of IGT REE trained on TRAIN - REUTERS, and tested on REUTERS, ALICE, and BROWN. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Rhetorical pattern of C-Exclamation",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Space comparison between FSAs and FSRAs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Boxer output for Shared Task Text 2",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 9 shows that in fact both contribute to producing good segmentations.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Effectiveness of post-processing rules",
        "Entity": "Caption"
    },
    {
        "Text": "We obtain the following decision rule: eI = argmax Pr(eI | f J ) 1 1 1 I 1 M ) = argmax m hm (eI , f J ) 1 1 I m=1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Results of different systems for pronoun resolution on MUC-6 and MUC-7 (*Here we only list backward feature assigner for pronominal candidates. In RealResolve-1 to RealResolve-4, the backward features for non-pronominal candidates are all found by DTnon\u2212pron .) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Frequency of Relation SubTypes in the ACE training and devtest corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3. Results for \u03b1 value setting.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Question tracking interface to a summa- rization system. ",
        "Entity": "Caption"
    },
    {
        "Text": "5http://cactus.aistnara.ac.jp/lab/nltlchasen.html 6http://pine.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html Word accuracy 90 CHASEN JUMAN opllnizt oplnuo recall opiJTozt F Figure 4: Word accuracy.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Sample of experimental items for the meta alternation anmfod.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Dirichlet-Tree prior of depth two.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Rules for simplifying the morphological complexity for RU. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Gibbs sampling for Bayesian inference outperforms both EM and Variational Bayesian EM. \u2217 The output of EM alignment was used as the gold standard. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: NIL expression forms based on POS attribute.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10: English to German Final System Re- sults. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Relation extraction results on the JDPA Corpus test set, broken down by document source.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 is, in fact, a weighted sum of these two distributions.",
        "Entity": "Reference"
    },
    {
        "Text": "Shortest-enclosed Path Tree from P/R/F (81.1/6.7/73.2) of Dynamic Context-Sensitive Shortest- enclosed Path Tree according to Table 2 (Zhou et al., 2007)",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4: A chain-structured DCRF as our intra- sentential parsing model. ",
        "Entity": "Caption"
    },
    {
        "Text": "If M = 10, 15 Chinese words (i.e., the first 19 Chinese words in Table 3 except \u53f6\u739b\u65af,\u5df4\u4f50\u4e9a,\u5769\u57da,\u666e\u5229\u6cd5) have their correct English translations at rank one position.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: SyntSem tagged corpus extract.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 suggests that the best setup is dependent on the specific meta sense or meta alternation being MACRO MICRO repM gram gramlex lex space type MACRO MICRO repA",
        "Entity": "Reference"
    },
    {
        "Text": "As the search space increases expo nentially, it is not possible to explicitly represent it.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 9: A* (E+) Success Rate for 12- and 14-word sentences [%].",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Overview of the WEBRE algorithm (Illustrated with examples sampled from experiment results). The tables and rec- tangles with a database sign show knowledge sources, shaded rectangles show the 2 phases, and the dotted shapes show the sys- tem output, a set of Type A relations and a set of Type B relations. The orange arrows denote resources used in phase 1 and the green arrows show the resources used in phase 2. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Baseline + Word Clustering by Relation +            Re-ranking by Coreference ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Distribution of Pronoun Mentions and Fre- quency of c-command Features ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Tradeoff between Margin Threshold and name recognition performance ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Dev set frequencies for the two most significant discourse markers in Arabic are skewed toward analysis as a conjunction.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Results tested against gs-so",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Integration of semantic relations \u2013 news corpus \u2013 best F1-measures. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Structure of the out-of-vocabulary word \u623d\u4282 \u483d\u543c \u2018English People\u2019. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Effect of Arabic stemming features on coreference resolution.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: TreeTagger and RFTagger outputs. Starred word forms are modified during preprocessing.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Coarse overview: From multilingual in- put to typed relations and instances ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Density of signature caseframes (Study 2).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Frequency of speech acts.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Some training events for the English word \u201cwhich\u201d. The symbol \u201c \u201d is the placeholder of the English word \u201cwhich\u201d in the English context. In the German part the placeholder (\u201c \u201d) corresponds to the word aligned to \u201cwhich\u201d, in the first example the German word \u201cdie\u201d, the word \u201cdas\u201d in the second and the word \u201cwas\u201d in the third. The considered English and German contexts are separated by the double bar \u201c  p  \u201d.The last number in the rightmost position is the number of occurrences of the event in the whole corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5: Our parsing model applied to the sequences at different levels of a sentence-level DT. (a) Only possible se- quence at the first level, (b) Three possible sequences at the second level, (c) Three possible sequences at the third level. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Topics are meaningful within languages but di-",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results on Chinese Semantic Similarity",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6 Effect of pruning parameter Np and heuristic function on search efficiency for direct-translation model (tp = 10 12 )",
        "Entity": "Caption"
    },
    {
        "Text": "W can be encoded by an undirected graph G (Figure 2(a)), where the nouns are mapped to vertices and Wij is the edge weight between vertices i and j",
        "Entity": "Reference"
    },
    {
        "Text": "Table 15 Overview of the results for all Web algorithms for coreference. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Correctness evaluation result",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Data used for training SMT models (all counts in millions). Parallel data sets refer to the bitexts aligned to English and their token counts include both languages. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: structured-features for the instance i{\u201chim\u201d, \u201cthe man\u201d}",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: The coreference systems that achieved the highest F-measure scores for each test set and scorer combination. The average rank of the candidate partitions produced by each system for the corresponding test set is also shown. ",
        "Entity": "Caption"
    },
    {
        "Text": "To reduce the memory requirement of the alignment templates, we compute these probabilities only for phrases up to a certain maximal length in the source language.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Adding new features with MIRA significantly improves translation accuracy. Scores are case-insensitive IBM B\uf76c\uf765\uf775 scores. \u2217 or \u2217\u2217 = significantly better than MERT baseline (p < 0.05 or 0.01, respectively). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Feature templates used for CRF in our system",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Part of a sample headline cluster, with sub-clusters ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: The framework of our system. We first enumerate all possible candidate states, and then filter out low probability states by using a light-weight classifier, and represent them by using feature forest. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Contribution of different features over 43          relation subtypes in the test data ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3. Impact of Data Size (English)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3 Word sense disambiguation accuracy for \u201cNP1 V NP2 NP3\u201d frame. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results on the STS video dataset.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Topic transfer in bilingual LSA model.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 Experiments on the word length\u2013precision relationship of the large corpus with threshold nine. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Distribution of the sentences where the semantic role features give no/positive/negative impact to the sentence fluency in terms of the completeness and ordering of the semantic roles. ",
        "Entity": "Caption"
    },
    {
        "Text": "Pr(f J , aJ | eI ) = p (f J , aJ | eI ) (6) 1 1 1 1 1 1",
        "Entity": "Caption"
    },
    {
        "Text": "This is because the lefthand side of Equation (7) represents the probability of the string c1",
        "Entity": "Reference"
    },
    {
        "Text": "Fig. 5 The semantic graph of an English sentence and the semantic features extracted from it for an SMT phrase ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Mixed-case TER and BLEU, and lower- case METEOR scores on Chinese NIST MT05. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 shows the results of both unconstrained and constrained versions of HGFC and those of AGG on the test set T3 (where singular classes are removed to enable proper evaluation of the constrained method).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4: Two words that differ only in one character, but have different internal structures. The character \u543c \u2018people\u2019 is part of a personal name in tree (a), but is a suffix in (b). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Translation results in terms of BLEU score and translation edit rate (TER) estimated on newstest2010 with the NIST scoring script. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: The average performance of TL-comb with different \u03bbT\u00b5 . (\u03bbk\u00b5 = 104 and \u03bb\u03bd = 1.) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 shows the NA M N O M PR O NA M 34 13 (21 %) 67 (6 6 %) 11 (4 6 %) N O M 43 (67 %) 21 48 (4 9 %) 9 (8 9 %) PR O 86 8 (32 %) 17 71 (5 5 %) 53 08 (2 4 %) Table 2: Number of clustering decisions made according to mention type (rows anaphor, columns antecedent) and percentage of wrong decisions.",
        "Entity": "Reference"
    },
    {
        "Text": "Fig. 11 English-to-Dutch Learning curves (left-hand side graphs) and difference curves (right-hand side graphs) comparing the Moses baseline against four context-informed models (CCG\u00b11, LTAG\u00b11, PR, PS-AL, POS\u00b12 and Word\u00b12). These curves are plotted with scores obtained using three evaluation met- rics: BLEU (top), METEOR (centre) and TER (bottom) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: V-Measure and paired FScore results for different partitionings of the dendrogram. The dashed vertical line indicates SP D ",
        "Entity": "Caption"
    },
    {
        "Text": "The distribution of errors is displayed in Table 4.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Monolingual and Crosslingual Baseline Slot Filling Pipelines ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Parameter estimation from Q/A pairs.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Ironic tweets that received every party and their election results. The \ufb02uctuation describes the difference between the May 2012 election results and the previous. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Distributions of Morph Examples",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 8 Quasi-synchronous tree-to-tree configurations from Smith and Eisner (2006). There are additional configurations involving NULL alignments and an \u201cother\u201d category for those that do not fit into any of the named categories. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6 Comparing clustering initializations on D1. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Translation outputs for the EN con- nective as, which was translated more correctly by SYSTEM 2 thanks to the disambiguating sense tags compared to the BASELINE that often just produces the prepositional as \u2013 jako. The erro- neous translations are marked in bold. The PDTB sense tags indicate the meaning of the CZ trans- lations and are encoded as follows: Synchrony (Sy), Asynchrony (Asy), Contingency (Co), Cause (Ca). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Translation results for English\u2192French",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: POS Tagging of Unknown Word using Contextual and Lexical features in a Sequential Model. The input for capitalized classifier has 2 values and therefore 2 ways to create confusion                                      k\u0097 sets. There are at most \u0089\u0095\u0094 &F\u0096 \u0081 \u0081 +!\u0098 different in- puts for the suffix classifier (26 character + 10 digits + 5 other symbols), therefore suffix may                      k\u0097 emit up to \u0089 \u0094 &R\u0096 \u0081 \u0081 +R\u0098 confusion sets. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: BLEUscores for the Chinese test set (     de- coding) in dependence of maximum swap distance and maximum swap segment size. ",
        "Entity": "Caption"
    },
    {
        "Text": "The entity features can be attached under the top node, the entity nodes, or directly combined with the entity nodes as in Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Results of task#17",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Examples of positive and negative words. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7 Algorithm for breadth-first search with pruning",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Oracle lower-case BLEU",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 10: Evaluation of translation from English on out-of-domain test data",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Distribution of elicited ratings for High and Low similarity items ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 13 Comparison scores for PK open and CTB open. ",
        "Entity": "Caption"
    },
    {
        "Text": "see table 3) as the impact of soft constraints is the weakest for the constrained method at this level.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4: Inferred Dirichlet transition hyperparameters for bigram CLUST on three-way classification task with four latent clusters. Row gives starting state, column gives target state. Size of red blobs are proportional to magnitude of corresponding hyperparameters. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Results are shown in Table 2; we see that better word alignment results do not lead to better translations.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6: Final results on CTB-6 and CTB-7",
        "Entity": "Caption"
    },
    {
        "Text": "eI = argmax {Pr(eI | f J )} (1)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Rank of correct translation for period Dec 01 \u2013 Dec 15 and Dec 16 \u2013 Dec 31. \u2018Cont. rank\u2019 is the context rank, \u2018Trans. Rank\u2019 is the transliteration rank. \u2018NA\u2019 means the word cannot be transliterated. \u2018insuff\u2019 means the correct translation appears less than 10 times in the English part of the comparable corpus. \u2018comm\u2019 means the correct translation is a word appearing in the dictionary we used or is a stop word. \u2018phrase\u2019 means the correct translation contains multiple English words.",
        "Entity": "Caption"
    },
    {
        "Text": "Examples are given in Table 4.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 6: Cooccurence between frequent gold (y-axis) and predicted (x-axis) tags, comparing mkcls (top) and PYP-1HMM-LM (bottom). Both axes are sorted in terms of frequency. Darker shades indicate more frequent cooc- curence and columns represent the induced tags. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 16: Arabic Order-Free Structure",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 shows the results.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Comparison with previous work.",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 3. Procedure to find semantic types for antecedent candidates",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Number of clustering decisions made ac- cording to mention type (rows anaphor, columns antecedent) and percentage of wrong decisions. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Best observed performance of RF, SVM and GIZA++ and Levenshtein Distance",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Feature counts for Ling and Seed feature sets.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Disambiguation examples (\u2217 : using morpho-syntactic analysis).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Test set results. Maamouri et al. (2009b) evaluated the Bikel parser using the same ATB split, but only reported dev set results with gold POS tags for sentences of length \u2264 40. The Bikel GoldPOS configuration only supplies the gold POS tags; it does not force the parser to use them. We are unaware of prior results for the Stanford parser.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Lexical features for relation extraction.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4. Performance with SVM trained on a fraction of adj. It shows 5 fold cross validation results.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Results of 2000 sentences",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Precision, recall and F-measure for non-projective arcs",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: DP algorithm for statistical machine translation.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: BS on IWSLT 2006 task",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results of different systems on the CoNLL\u201912 English data sets.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Stemmed results on 3,138-utterance test set. Asterisked results are significantly better than the baseline (p \u2264 0.05) using 1,000 iterations of paired bootstrap re-sampling (Koehn, 2004). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: F-measure(%) Breakdown by Mention Type: NAM(e), NOM(inal), PRE(modifier) and PRO(noun). Chinese data does not have the PRE type. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: An underspecified discourse structure and its five configurations",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Overview of the complete processing chain. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Precision @ top N (with 3 seeds, and window size w = 3)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Part of speech tagging accuracy of unknown words (the last column represents the percentage of correctly tagged unknown words in the correctly segmented unknown words",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Performance of SYSTEM 2 (simplified PDTB tags) when manually counting for improved, equal and degraded translations compared to the BASELINE, in samples from the PDTB section 23 test set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2 shows the F1 scores of the proposed model (SegTagDep) on CTB5c1 with respect to the training epoch and different parsing feature weights, where Seg , Tag , and Dep respectively denote the F1 scores of word segmentation, POS tagging, and dependency parsing.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6 Results for Single Document System",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Discourse parsing framework.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. OOV term translation examples.",
        "Entity": "Caption"
    },
    {
        "Text": "In fact, from the last column of Figure 8 we see that even if our algorithm has access to only five anno tated sequences when Juman has access to ten times as many, we still achieve better precision and better F measure.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Result of our topic similarity model in terms of BLEU and speed (words per second), comparing with the traditional hierarchical system (\u201cBaseline\u201d) and the topic-specific lexicon translation method (\u201cTopicLex\u201d). \u201cSimSrc\u201d and \u201cSimTgt\u201d denote similarity by source-side and target-side rule-distribution respectively, while \u201cSim+Sen\u201d acti- vates the two similarity and two sensitivity features. \u201cAvg\u201d is the average B LEU score on the two test sets. Scores marked in bold mean significantly (Koehn, 2004) better than Baseline (p < 0.01). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 portrays how the states are aligned using the proposed scheme, where a subtree is denoted as a rectangle with its partial index shown inside it.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 9: Entries indicate whether best performance is achieved using the local maximum condition (M), the threshold condition (T), or both.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 11 Results for hierarchical lexicon model Nespole! \u201cRestructuring\u201d entails treatment of question inversion and separated verb prefixes as well as merging of phrases in both languages. The same conventional dictionary was used as in the experiments the Verbmobil. The language model was trained on a combination of the English parts of the Nespole! corpus and the Verbmobil corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Datasets for the two experimental conditions.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: A Parallel Fragment Extraction System",
        "Entity": "Caption"
    },
    {
        "Text": "In general, as shown in this figure, there may be additional transformations to make the translation task simpler for the algorithm.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "This approach has been suggested by Papineni, Roukos, and Ward (1997, 1998) for a natural language understanding task.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Overview of the ACE 2005 data.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Experiment results (as F1 scores) where IM is identification of mentions and S - Setting. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Accuracy scores for the CoNLL 2009 shared task development sets as a function of the number of tags k and the score threshold \u03b1. Beam parameters fixed at b1 = 40, b2 = 4. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: space distribution by part-of-speech of",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Glue Semantics proof for (83), English Way Construction (manner interpretation)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Urdu-English Results (% BLEU).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: A WSD example that shows the influence of syntactic, collocational and long-distance context features, the                       \u00ae probability estimates used by Na\u00efve Bayes and MM and their associated weights ( ), and the posterior probabilities of the true sense as computed by the two models. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Sources of the training data",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Calculated Values of \u03bbi",
        "Entity": "Caption"
    },
    {
        "Text": "For example, if x in figure 1 were evenir aux choses, then x14 would map to v1 = evenir, w2 = aux, and u3 = cho.",
        "Entity": "Reference"
    },
    {
        "Text": "In this section, we try to compare our results with those obtained by IdentiFinder ' 97 (Bikel et al., 1997), IdentiFinder ' 99 (Bikel et al., 1999), and MENE (Borthwick, 1999).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2 Summariser and VPA Architecture",
        "Entity": "Caption"
    },
    {
        "Text": "Table 17 Comparison of performance of MSRSeg: The versions that are trained using (semi-)supervised iterative training with different initial training sets (Rows 1 to 8) versus the version that is trained on annotated corpus of 20 million words (Row 9). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Results of the mapping algorithm",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: A sample CCG parse.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Example problem from the FraCaS suite.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 shows a confusion matrix for the classification of the nouns.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Results of the first run (without postprocessing)",
        "Entity": "Caption"
    },
    {
        "Text": "In Table 5 we present results from small test cor pora for the productive affixes handled by the current version of the system; as with names, the segmentation of morphologically derived words is generally either right or wrong.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 lists new conceptsthat CAM introduces to manipulate vector represen tations.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Relation types for ACE 05 corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7: Computing the partition function of the conditional probability P r(S|T ).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Learning curves using different sam- pling strategies. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 shows the effect of the length of the language model history on translation quality.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: The total costs for the three MTurk subtasks in- volved with the creation of our Dialectal Arabic-English parallel corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3. Table Construction",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 Comparing selectional preference slot definitions. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Outline of word segmentation process",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Accuracy and frequency of the top 5% for each iteration ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Mention Detection Results",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 1. Integration of confidence measures \u2013 recall/precision curves (figures in the legend correspond to resp. \u03b41 and \u03b42 ).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Statistics from 1993 Japanese newswire (NIKKEI), 79,326,406 characters total.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4. Comparison of Unsupervised Learning                   Methods ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Tuple extraction from a sentence pair.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 Effect of the length of the language model history (Unigram/Bigram/Trigram: word-based; CLM: class-based 5-gram).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Result of synthetic data learning experiment for MERT and PRO, with and without added noise. As the dimensionality increases MERT is unable to learn the original weights but PRO still performs adequately. ",
        "Entity": "Caption"
    },
    {
        "Text": "The results for French to English and for English to French are shown in Table 10",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 Na\u0131\u0308ve FSA with duplicated paths. ",
        "Entity": "Caption"
    },
    {
        "Text": "Evaluation results are listed in Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Quantitative Evaluation of Common Topic Finding (\u201ccross-collection\u201d log-likelihood) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11: Results of the corpus-based model",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Structure of a term in the original documents",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: (A) current configuration for internal node Dk and its associated subtrees (B) first alternative configuration, (C) second alternative configuration. Note that swapping st1, st2 in (A) results in an equivalent tree. Hence, this configuration is excluded. ",
        "Entity": "Caption"
    },
    {
        "Text": "The accuracy of a baseline tagger which chooses the most probable tag9 ignoring the context is 67.3% without and 69.4% with the supple 92.3 92.2 92.1 92 91.9 91.8 91.7 91.6 91.5 91.4 2 3 4 5 6 7 8 9 10 mentary lexicon.",
        "Entity": "Reference"
    },
    {
        "Text": "The corresponding translation quality improves from an mWER of 45.9% to an mWER of 31.8%.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 shows the absolute frequencies of sen tence recency values when only the most recent antecedent (in the order just stated) is considered.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Hypegraph size measured by the average number of hyperedges (h = 1 for CF). \u201clattice\u201d is the average number of edges in the original CN. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Illustration of similarities in POS tag statistics across languages. (a) The unigram frequency statistics on five tags for two close languages, English and German. (b) Sample sentences in English and German. Verbs are shown in blue, prepositions in red and noun phrases in green. It can be seen that noun phrases follow prepositions. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: The performance of different resolution systems",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Distribution of Error Types",
        "Entity": "Caption"
    },
    {
        "Text": "The baseline system in Table 3 refers to the maximum entropy system that uses only local features.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: Performance of our proposed method (Spectral- based clustering) compared with other unsupervised methods: ((Hasegawa et al., 2004))\u2019s clustering method and K-means clustering. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Statistics about the training/tuning/test datasets used in our experiments. The token counts are calculated before MADA segmentation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Chinese-English Results (% BLEU).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Framework overview.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Accuracies (%) for Word-Extraction Us- Litkowski and Hargraves (2007) selected exam- ing MALT Parser or Heuristics.                  ples based on a search for governors8 , most anno- ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Plate diagram of the basic model with a single feature per token (the observed variable f ). M , Z, and nj are the number of word types, syntactic classes z, and features (= tokens) per word type, respectively. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4. Accuracy of 5-fold cross-validation with self-              extracted semantic features ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: From DRS to DRG: labelling.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. GETARUNS pronouns collapsed at structural level",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Features based on the token string",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Lexicon-based phrase labeling",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: The top-ranking feature for each group of features and the classifier of a slot",
        "Entity": "Caption"
    },
    {
        "Text": "The joint probability of the two sequences is defined as the product of context probabilities and lexical probabilities over all POS tags:",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Summary table on the various methods investigated for POS tagging ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 11 The four types of changes. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4. Examples of the top-3 candidates in the       transliteration of English \u2013 Chinese ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Accuracy with different sizes of unlabeled data from WordNet relation",
        "Entity": "Caption"
    },
    {
        "Text": "even after removal of the wing-node, the two areas of meaning are still linked via tail",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Statistics of datasets.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: Examples of ambiguous words that are trans- lated incorrectly by the MSA-English system, but cor- rectly by the Dialectal Arabic-English system. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 shows the record for the headword orange followed by its collocates",
        "Entity": "Reference"
    },
    {
        "Text": "As shown in Figure 8, relative word performance was not degraded and sometimes even slightly better.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Example sentence and extracted features from the SENSEVAL 2 word church ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Comparing F-measure, precision, and recall of different voting schemes for Chinese relation extraction. ",
        "Entity": "Caption"
    },
    {
        "Text": "Cik Figure 1: Local graph of the word mouse",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 contains results for two different translation models.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Classifier combination accuracy over 5 base classifiers: NB, BR, TBL, DL, MMVC. Best perform- ing methods are shown in bold. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Effectiveness of Extracting Common Topics",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Some words extracted from the large corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "The algorithm takes into account possibly unaligned words at the boundaries of the source or target language phrases.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: GC examples.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Network of relations. Edges indicate that the relations have a non-empty support inter- section, and edge labels show the size of the inter- section. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Types of features extracted for edge e from h to n",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Pseudo-code to extract UW",
        "Entity": "Caption"
    },
    {
        "Text": "BP(f J , eI , A) = f j+m , ei+n 1 1 j i : (i , j ) A : j j j + m i i i + n (9) (i , j ) A : j j j + m i i i + n",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Comparison to onlySL and onlyGraph.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6 A paraphrase generation lattice for the sentence He ate lunch at a cafe near Paris. Alternate paths between various nodes represent phrasal replacements. The probability values associated with each edge are not shown for the sake of clarity. ",
        "Entity": "Caption"
    },
    {
        "Text": "where N is role features when combining two children states, and ex amples can be found in Figure 3.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 7: Accuracy of underlying segment hypotheses.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6. Thread Classification Results",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Precision for each phrase type (Ev.Ling).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Count of relationships in 77 gold standard documents.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Subtypes of the ArgM modifier tag. ",
        "Entity": "Caption"
    },
    {
        "Text": "n our experimentations, SVMlight (Joachims, 1998) with the tree kernel function (75.0) (53.7) (62.6) Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Dependency accuracy on 13 languages. Unlabeled (UA) and Labeled Accuracy (LA). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Rule types in SSTb and HeiST",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Statistics for the ACE corpus.",
        "Entity": "Caption"
    },
    {
        "Text": "Then, every phrase f produces its translation e (using the corresponding alignment template z).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 evaluates the contributions of different kinds of constituent dependencies to extraction performance on the 7 relation types of the ACE RDC 2004 corpus",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5. Precision at top 50",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Size of the vocabularies for the \u201cNo LP\u201d and \u201cWith LP\u201d models for which we can impose constraints. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7. Statistics of the ACE corpus.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results on MUC-4 entity extraction. C&J 2011 +granularity refers to their experiment in which they mapped one of their templates to five learned clusters rather than one. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Network after pair-wise TER alignment.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Parameters used in our system.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The Ensemble Semantics framework for information extraction.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 9: Chunk - Length and count of glue rules used decoding test set ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1. Basic Statistics of DUC2007 Update Data Set",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Effect of observation pruning on the translation quality (average over all test sets).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Multiple Bayesian learning runs (using averag- ing) for POS tagging. Each point represents one run; the y-axis is tagging accuracy and the x-axis is the average \u2212 log P(derivation) over all samples after burn-in. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Impact of Syntactic Features on English Sys- tem After Taking out Distance Features. Numbers are F-measures(%). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 17 Training, development, and test data for Chinese dependency parsing. ",
        "Entity": "Caption"
    },
    {
        "Text": "As can be seen in Table 4, our training data is a lot less than those used by MENE and IdentiFinder3.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Translation Model (IBM Model 4)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results of segmentation of entry titles (F-score (precision/recall)).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Statistics for the ACE 2005 corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results for all experiments",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Statistics for Verbmobil task: training corpus (Train), conventional dictionary (Lex), development corpus (Dev), test corpus (Test) (Words*: words without punctuation marks). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 Japanese-to-English Display of NICT- ATR Speech-to-Speech Translation System ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Context Clustering with Spectral-based Clustering technique. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Accuracy on development data depend ing on context size",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Human Assessment of Errors",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: A multiword expression in HeiST",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results of different systems on the CoNLL 12 English data sets.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 Sources of con\ufb02ict in cross-lingual subjectivity transfer. Definitions and synonyms of the fourth sense of the noun argument, the fourth sense of verb decide, and the first sense of adjective free as provided by the English and Romanian WordNets; for Romanian we also provide the manual translation into English. ",
        "Entity": "Caption"
    },
    {
        "Text": "This class-based model gives reasonable results: for six radical classes, Table 1 gives the estimated cost for an unseen hanzi in the class occurring as the second hanzi in a double GIVEN name.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: Average I NV R for 300 headwords",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Meta-evaluation results at document and system level for submitted metrics",
        "Entity": "Caption"
    },
    {
        "Text": "and 3",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: Results of an iterative approach.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 French standard parsing experiments (test set, sentences \u2264 40 words). FactLex uses basic POS tags predicted by the parser and morphological analyses from Morfette. FactLex* uses gold morphological analyses. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Performance on T3 using a predefined tree structure.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6: Translation results for English-French",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 Corpus statistics for Hansards task (Words*: words without punctuation marks). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Examples of sentences x, domain-independent underspecified logical forms l0 , fully specified logical forms y, and answers a drawn from the Freebase domain. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Performance of WSD system over individual ab- breviations in three reduced corpora ",
        "Entity": "Caption"
    },
    {
        "Text": "As column 5 (SVM) in table 2 shows, the classification results are very similar to the results obtained with MBL.12 We furthermore find a very similar set of errors, and in particular, we find that 51.0 % of the errors for the inanimate class are nouns with the gradient animacy properties presented in (9)-(13) above.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4 shows the performance on the test data.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8 Results for Mutiple Document System         with additional retrieved texts ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Evaluation results for ReWoS variants and baselines.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Examples for the effect of equivalence classes resulting from dropping morpho-syntactic tags not relevant for translation. First the translation using the original representation, then the new representation, its reduced form and the resulting translation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Structure of a three-pass machine translation system with the new regeneration pass. The original N-best translations list (N- best1) is expanded to generate a new N-best translations list (N-best2) before the rescoring pass. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: The demographics of RWSData. No, RW, RA, SbL, WbL, TS, and TD are labeled as (N)umber (o)f, (R)elated (W)orks, (R)eferenced (A)rticles, (S)entence-(b)ased (L)ength of, (W)ord- (b)ased (L)ength of, (T)ree (S)ize, and (T)ree (D)epth, respectively. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: BLEU results",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5. Accuracy of 5-fold cross-validation with self-  extracted semantic features based on different levels             of syntactic/semantic relations ",
        "Entity": "Caption"
    },
    {
        "Text": "tables 2",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Comparison of the effect of morphological segmentation when translating MSA web text and Dialectal Arabic web text. The morphological segmentation uniformly improves translation quality, but the improvements are more dramatic for MSA than for Dialectal Arabic when comparing similarly-sized training corpora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Additional feature templates used in C- phase. Example used is \u201c32 ddd\u201d with tagging results after R-phase as \u201cSSLMR\u201d. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Derivation with \u03bb-DRSs, including \u03b2-conversion, for \u201cA record date\u201d. Com- binatory rules are indicated by solid lines, semantic rules by dotted lines. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Feature templates for the word segmentor. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 13 Distribution of Category (VI) error classes (type-insensitive). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Diacritized particles and pseudo-verbs that, after orthographic normalization, have the equivalent surface form 0 an.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. The Riv over the bakeoff-2 data.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Kendall\u2019s (\u03c4 ) correlation over WMT 2013 (all- en), for the full dataset and also the subset of the data containing a noun compound in both the reference and the MT output ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 9: After the Application of Compile- Replace ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 shows these similarity measures.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Results of SVM and Mincuts with different settings of feature",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: V-measure (VM) and many-to-one (M-1) results on the languages in the MULTEXT-East corpus using the gold standard number of classes shown in Table 4. BASE results use \u00b11-word context features alone or with morphology. ALIGNMENTS adds alignment features, reporting the average score across all possible choices of paired language and the scores under the best performing paired language (in parens), alone or with morphology features. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. MT system combination. Each 1-best outputs are aligned to create as many Confusion Networks which are connected together to form a lattice. This lattice is then decoded with a token-pass decoder using a Language Model to produce 1-best and/or                                 n-best hypotheses. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Examples given in the description of Task 2.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 13: Zero-Copula in Russian",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Results for Positive and Negative Classes.",
        "Entity": "Caption"
    },
    {
        "Text": "In the following, we describe the criterion that defines the set of phrases that is consistent with the word alignment matrix",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 The scales of corpora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Experimental results over the 120 evalu- ation sentences. Alignment error rates in both di- rections are provided here. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Evaluation of the three anaphoric resolvers discussed by Ng and Cardie. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 Split constituents: In this case, a single semantic role label points to multiple nodes in the original treebank tree. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 4. Integration of confidence measures and interpolation \u2013 recall/precision curves.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Results for nouns",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Impact of the topic cache size",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Vocabulary size of IWSLT task (40K)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Perplexity measured on nt08 with the baseline LM (std), with the LM estimated on the sampled texts (generated texts), and with the inter- polation of both. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Results of Uryupina\u2019s discourse new clas- sifier ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 11 Participle-forming combinations in German. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 Performance evaluation on MSRVDC Dataset 2. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Cluster features ordered by importance.",
        "Entity": "Caption"
    },
    {
        "Text": "Strube (1998) s centeri ng appro ach (whos e senten ce orderi ng is designate d as SR2 in Table 2) also deals with and even prefer s intrase ntenti al anaph ora, which raises the upper limit to a more accept able 80.2% .",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Example of a MUC-4 template",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Boundary information is added to states to cal- culate the bracket scores in the face of word segmentation errors. Left: the original parse tree, Right: the converted parse tree. The numbers in the brackets are the indices of the character boundaries based on word segmentation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Learning curve of SuperSense on SE2",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Comparing F-measure, precision, and recall of different voting schemes for Arabic relation ex- traction. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: The amount of training and test sets The first factor in the righthand side of Equa tion (13) is estimated from the relative frequency of the corresponding events in the training corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5 Comparing similarity measures on D1 and D2. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Total number of synsets classified by sentiment.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Example sentences in the synthetic languages. Words in Category 1 are made of characters a-d, Cate- gory 2 e-h, Category 3 m-p, Category 4 r-u. Suffixes in Language B are separated with periods (.) for illustrative purposes only. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Topics, along with associated regression coefficient \u03b7 from a learned 25-topic model on German-English (left) and German-Chinese (right) documents. Notice that theme-related topics have regression parameter near zero, topics discussing the number of pages have negative regression parameters, topics with \u201cgood,\u201d \u201cgreat,\u201d \u201cha\u030co\u201d (good) and \u201cu\u0308berzeugt\u201d (convinced) have positive regression parameters. For the German-Chinese corpus, note the presence of \u201cgut\u201d (good) in one of the negative sentiment topics, showing the difficulty of learning collocations. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Experimental Results",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Consistently formatted term translation                       pairs ",
        "Entity": "Caption"
    },
    {
        "Text": "This group consists of 10 features based on the string , as listed in Table 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 7 shows a structogram of the algorithm",
        "Entity": "Reference"
    },
    {
        "Text": "Table 14 %BLEU on tune and test sets for UR\u2192EN translation, comparing several settings for maximum dependency lengths in the decoder (\u03c9x is for the source side and \u03c9y is for the target side). The upper table shows Moses BLEU scores for comparison. The lower table compares two max dependency length settings during tuning, and several for decoding on the test sets, showing both BLEU scores and average decoding times per sentence. See text for discussion. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Example translations from the different methods. Boldface indicates correct translations. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Percentage of major punctuation       marks in the Chinese corpus 4 ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Related research integrating context into word-based SMT (WB-SMT) models",
        "Entity": "Caption"
    },
    {
        "Text": "e suggest the use of a log-linear model to incorporate the various knowledge sources into an overall translation system and to perform discriminative training of the free model parameters",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4 The accuracies of various word segmentors over the first SIGHAN bakeoff data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Decoding algorithm for the standard Tree-to-String transducer. lef tw/rightw denote the left/right boundary word of s. c1 , c2 denote the descendants of v, ordered based on RHS of t. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 Performance of our method for paraphrase acquisition.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The System architecture1",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Test corpora statistics.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Quasi-morphological operations",
        "Entity": "Caption"
    },
    {
        "Text": "We set Table 6: Word segmentation accuracy of all words r e c pr ec F Po iss on +b igr a m W T +P oi ss on +b igr a m P O S + Po iss on +b igr a m P O S + W T + Po iss on + bi gr a m 94 .5 94 .4 94 .4 94 .6 93 .1 93 .8 93 .6 93 .7 93 .8 94 .1 94 .0 94 .1",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Precision and recall for prepositions.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: 10 most used verbs (lemma) in indirect speech. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 19 The combined segmentation, POS-tagging, and dependency parsing F-scores using different pipelined systems. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 BNC results for other-anaphora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: NIST BLEU scores on the German-English (de- en) and French-English (fr-en) Europarl test2008 set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 17 Type-insensitive improvement for Chinese/English NER. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10 Simplified prevalence score, evaluation on SemCor, polysemous words only. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4. The differences of F-measure and ROOV between near-by steps of our CWS. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Segmentation, tagging and parsing results on the Standard dev/train Split, for all Sentences",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Overview of Morph Resolution",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: AER comparison (cn \u2192en)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 7 Word reordering for the translation direction English to German: The reordering is restricted to the English verb group. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 Generative patterns of ONA, where sij denotes the j-th character of the i-th word of ON (Sun, Zhou and Gao 2003). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Accuracy in Lexical Sample Tasks",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Most frequent sense analysis for Senseval-2 and -3 polysemous lemmas occurring more than once in a document (adverb data is only from Senseval-2). ",
        "Entity": "Caption"
    },
    {
        "Text": "Comparison of segmentation algorithm using different linguistic features.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Results for the various experiments (Exp) for both the development and test portions of the data, including per- token clitic separation (tokenization) accuracy, part-of-speech tagging F1, affix boundary detection F1, affix labeling F1, and both unlabeled and labeled attachment scores. ",
        "Entity": "Caption"
    },
    {
        "Text": "The model is described using a log-linear modeling approach, which is a generalization of the often used source channel approach",
        "Entity": "Reference"
    },
    {
        "Text": "The coupling between B and is removed by setting H = B 1: n min (W, H H T ), s.t. hip = 1 (1) H, i=1 BT Dl Bl according to equation 4 l end for return BL , BL 1 ...B1 Additional steps need to be performed in order to extract a tree from the hierarchical graph.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "The graphical structure depicted in Figure 1 models these relations between the four mentions Leaders, Paris, recent developments and They.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 Overview of the results for all baselines for other-anaphora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6 The initial frequencies of character sequences. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Abbreviations",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 presents the distributions of some examples of morphs and their targets in English Twitter and Chinese Sina Weibo.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4: Using different amounts of annotated training data for the article meta-classifier. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: BLEU scores on the Europarl development test data ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Rules of the baseline system.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: The semantic roles of cases beside C-1 verb cluster ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Confusion Sets",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8 FSRA-2 for Arabic nominative definite and indefinite nouns. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Accuracy (recall) of systems on the two bench- marks. The systems are divided into three groups. Group 1 uses 10-fold cross-validation; groups 2 and 3 use the in- dependent test set. Groups 1 and 2 measure accuracy of logical form; group 3 measures accuracy of the answer; but there is very small difference between the two as seen from the Kwiatkowski et al. (2010) numbers. Our best system improves substantially over past work, despite us- ing no logical forms as training data. ",
        "Entity": "Caption"
    },
    {
        "Text": "The results are shown in the corr row of table 2, for exact character-probability estimates.",
        "Entity": "Reference"
    },
    {
        "Text": "3",
        "Entity": "Reference"
    },
    {
        "Text": "Table 9: Accuracy of Target Candidate Detection",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Syntactic Seeding Heuristics",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Experiments on the threshold\u2013partial recall relationship of the small corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Our probabilistic model: a question x is mapped to a latent logical form z, which is then evaluated with respect to a world w (database of facts), producing an answer y. We represent logical forms z as labeled trees, induced automatically from (x, y) pairs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Graph of words for the target word paper. Numbers inside vertices correspond to their degree. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Average Metric Rank in NIST Metrics MATR 2008 Official Results",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Translation results for French-English",
        "Entity": "Caption"
    },
    {
        "Text": "For instance, the gain for the prediction in figure 1 would be 2 7 8 = 6.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "Typically, the translation probability Pr(eI | f J ) is decomposed via additional hid 1 1 den variables",
        "Entity": "Reference"
    },
    {
        "Text": "Fig.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 10: MUC-5: Level Distribution of the Five Facts Combined",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Parser projection with target trees. Using the true or 1-best parse trees in the source language is equivalent to having twice as much data in the target language. Note that the penalty for using automatic alignments instead of gold alignments is negligible; in fact, using Source text alone is often higher than +Gold alignments. Using gold source trees, however, significantly outperforms using 1-best source trees. ",
        "Entity": "Caption"
    },
    {
        "Text": "range free green lemon peel red state yellow",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: The System Performance Based on Com- binations of Surface and Semantic Features. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7. MRRs of the phonetic transliteration",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Graphical model of synonym pair gen- erative process ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9:      Top ranked sentences using the LR[0.20,0.95] system on the question \u201cWhat caused the Kursk to sink?\u201d ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Example of the Character Tagging Method: Word boundaries are indicated by vertical lines (\u2018|\u2019).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3 The lattice for the Hebrew sequence !\u202b( \u05d1\u05e6\u200c\u05dc\u05dd \u05d4\u05e0\u200c\u05e2\u05d9\u05dd\u202csee footnote 19). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Performance of Final Translation (BLEU-4).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5: Frequencies of patterns in the evaluation data (causation). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: oscillating states in matrix CW for an unweighted graph ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Conditioning features for the probabilistic CFG used in the reported empirical trials ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Comparison of a confusion network and a lat- tice. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Modification of \u201cO\u201d (other labels) to transfer information on a preceding named entity. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Experiment 3: Results by training set size, \u03b8 = 1.0 ",
        "Entity": "Caption"
    },
    {
        "Text": "The test 1:ART.Nom checks if the preceding word is a nominative article.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Weights learned for generating syntactic nodes of various types anywhere in the English translation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 A focused entailment graph. For clarity, edges that can be inferred by transitivity are omitted. The single strongly connected component is surrounded by a dashed line. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 shows type- and token-level error rates for each corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Examples for disagreement between the two judges.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7: An Employment Chain. Dotted lines indicate incorrect before relations. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Dependency tree for the sentence \u201cPROT1 contains a sequence motif binds to PROT2.\u201d ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: The set of types and subtypes of relations used in the 2004 ACE evaluation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Language Model Pruning Algorithm",
        "Entity": "Caption"
    },
    {
        "Text": "The renormalization needed in equation (3) requires a sum over manypossible sentences, for which we do not know of an efficient algorithm",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Sample Minipar parse and extracted gram- matical function features ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Translation results for German-English",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: The correlation between the non terminals per sentence ratio and Leaf Accuracy (macro) scores. x: #non terminal/ #sentence ; y: Acc.(%) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Effect on %BLEU of varying number of non-terminals ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 The generic beam-search algorithm. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: An example showing how to compute the target side position of a semantic role by using the median of its aligning points.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 13: Performance comparison with the literature for noun countability detection ",
        "Entity": "Caption"
    },
    {
        "Text": "The second condition is necessary to allow for single-character words (see Figure 3).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Performance of Algorithms switch under different input data.",
        "Entity": "Reference"
    },
    {
        "Text": "If it is made up of all capital letters, then (allCaps, zone) is set to 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: N/P classifier with learned SWSD integration",
        "Entity": "Caption"
    },
    {
        "Text": "This sentence should be tagged as shown in table 1.",
        "Entity": "Reference"
    },
    {
        "Text": "and 8 show word accuracy for Chasen, Juman, and our algorithm for parameter settings optimizing word precision, recall, and F-measure rates.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 7: Effect of Training Corpus Size (2)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3 Accuracy for words with high confidence measure. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2. Incremental alignment with TERp resulting in a confusion network.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Automatic evaluation with 50% of Freebase relation data held out and 50% used in training on the 102 largest relations we use. Precision for three different feature sets (lexical features, syntactic features, and both) is reported at recall levels from 10 to 100,000. At the 100,000 recall level, we classify most of the instances into three relations: 60% as location-contains, 13% as person-place-of-birth, and 10% as person-nationality. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: The sizes of dictionaries as automata",
        "Entity": "Caption"
    },
    {
        "Text": "We see that the language model perplexity improves from 4,781 for a unigram model to 29.9 for a trigram model.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 16 FSRA* for Arabic nominative definite nouns. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: The parse tree for This car is not blue, highlighting the limited scope of the negation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Analysis of the number of relevant documents out of the top 10 and the total number of retrieved documents (up to 100) for a sample of queries. ",
        "Entity": "Caption"
    },
    {
        "Text": "In Table 7 we give results for several evaluation metrics.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 18 Test accuracies of various dependency parsers on CTB5 data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: A comparison of translation quality of Egyptian, Levantine, and MSA web text, using various training corpora. The highest BLEU scores are achieved using the full set of dialectal data (which combines Levantine and Egyptian), since the Egyptian alone is sparse. For Levantine, adding Egyptian has no effect. In both cases, adding MSA to the dialectal data results in marginally worse translations. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6 Dependencies in the alignment template mode",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Results on three query categories.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Relation types and subtypes in the ACE                   training data ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Official Bakeoff Outcome",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 breaks down the performance of the best CAM model by meta alternation.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 shows the manual evaluation results based on the entire test set, and the improvement from SRF is significant at p < 0.005 based on a t-test.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 6",
        "Entity": "Reference"
    },
    {
        "Text": "Table 14. Rules of Referral",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Opinion PageRank Performance with varying parameter \u03bb (\u00b5 = 0.5) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: NIST scores",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4. Summary of results for random and first sense baselines and supersense tagger, \u03c3 is the standard error computed on the five trials results. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: A toy instance of lattice construction",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11 Feature ablation experiments for UR\u2192EN translation with string-to-tree features, showing the drop in BLEU when separately removing word (W ORD), cluster (C LUST), and configuration (C FG) feature sets. \u2217 = significantly worse than T GT T REE. Removing word features causes no significant difference. Removing cluster features results in a significant difference on both test sets, and removing configuration features results in a significant difference on test 2 only. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Entity detection and tracking system flow. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: BLEU difference of 1000 bootstrap sam- ples. 95% confidence interval is [.15, .90] The proposed approach therefore seems to be a stable method. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "In all four tables, we provide theresults for using no heuristic functions and three variants of an increasingly infor mative heuristic function.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Training Data",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: The templates for generating potentially deter- ministic constraints of English POS tagging. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 11 An example Chinese lexicalized phrase-structure parse tree. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: BLEU scores for SparseHRM features. Notes in Table 2 also apply here.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 The cost as a novel given name (second position) for hanzi from various radical classes.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Example of a prediction for English to French translation. s is the source sentence, h is the part of its translation that has already been typed, x\u2217 is what the translator wants to type, and x is the prediction. ",
        "Entity": "Caption"
    },
    {
        "Text": "alignment models Pr(f J , aJ | eI ),",
        "Entity": "Reference"
    },
    {
        "Text": "I+1 hCLM(eI , f J , K , zK ) = log n p(C(ei ) | C(ei 4 ), ..",
        "Entity": "Caption"
    },
    {
        "Text": "A standard criterion on a parallel training corpus consisting of S sentence pairs {(fs , es ): s = 1, .",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: Results obtained by a combination of the best statistical and knowledge-based configuration. \u2018Best- Single\u2019 is the best precision or recall obtained by a sin- gle measure. \u2018Union\u2019 merges the detections of both approaches. \u2018Intersection\u2019 only detects an error if both methods agree on a detection. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Confusion matrix for the MBL-classifier with a general feature space on the >10 data set on Talbanken05 nouns. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Features Used for Initial Distribution",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7 An example showing the generalization of the word lattice (a) into a slotted lattice (b). The word lattice is produced by aligning seven sentences. Nodes having in-degrees > 1 occur in more than one sentence. Nodes with thick incoming edges occur in all sentences. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2",
        "Entity": "Reference"
    },
    {
        "Text": "hWRD(eI , f J , K , zK ) = log n p(ei | {fj | (i, j) A}, Ei ) (14) 1 1 1 1 i=1",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 1. (a) An example in which an English sentence is parsed into a tree structure with 12 PCFG rules; (b) an instance in which a Chinese sentence (both Chinese characters and Chinese Pinyin are provided, and note that we will use Chinese Pinyin throughout the paper) is converted into an English tree using 6 STSG rules. The symbol to the upper right of a node indicates that this node is constructed using rule . ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: AER comparison (en\u2192cn)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 shows that by changing the word spelling model from zerogram to bigram, character perplex ity is greatly reduced.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3 Contributions of features",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Tagging accuracy on development data depending on context size ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Relation clusters and a few individual relations. Edge labels show the size of the inter- section. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 Example translations for the translation direction German to English using three different reordering constraints: MON, GE, and S3. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: One possible breakdown of spoken Arabic into dialect groups: Maghrebi, Egyptian, Levantine, Gulf and Iraqi. Habash (2010) gives a breakdown along mostly the same lines. We used this map as an illustration for annotators in our dialect classification task (Section 3.1), with Arabic names for the dialects instead of English. ",
        "Entity": "Caption"
    },
    {
        "Text": "Finally, Table 4 shows the results for the unconstrained HGFC on T2 and and T3 when the tree structure is not predefined but inferred automatically as described in section 3.2.3.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Example of telescopic compound (a) and sepa- rable word (b). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Numbers of expressions of all the differ- ent types from the DISCO and Reddy datasets. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9: The effect of gender detection schemes",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5. Performance Comparison of Different                Pruning Methods ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Correlation between F-score and BLEU",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 Examples of word alignment patterns in German\u2013English that require the increased expressive power of synchronous tree adjoining grammar. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Annotation statistics",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Effect of Factors antecedent is found in the previous context, subsequent sentences are inspected (cataphora), also ordered by proximity to the pronoun.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Empirically-derived classifier similarity",
        "Entity": "Caption"
    },
    {
        "Text": "We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4, 5, and 6.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 6: Metaphors tagged by the system (in bold) whereby the main source of disagreement was the presence of lexicalized metaphors, e.g. verbs such as impose, decline etc.",
        "Entity": "Reference"
    },
    {
        "Text": "The performance of these systems is shown in Table 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: Examples of DCS trees that use the aggregate relation (\u03a3) to (a) compute the cardinality of a set and (b) take the average over a set. ",
        "Entity": "Caption"
    },
    {
        "Text": "2",
        "Entity": "Reference"
    },
    {
        "Text": "E.g. rat and printer are very different in meaning, but they are both closely related to different meanings of mouse",
        "Entity": "Reference"
    },
    {
        "Text": "Table 17 Results on large-scale Dutch-to-English translation",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Individuals for accusative and sin- gular in the TIGER Annotation Model ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 shows that this reimplementation almost reproduces the accuracy of their implementation.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Counts of matches between MUC and Soderland data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Top-7 Chinese long-form candidates for the En- glish acronym TAA, according to the LH score. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Corpora",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 Percentage of correct entailments within the top 40 candidate pairs of each of the methods, LIN and Bootstrapped LIN (denoted as LINB in the \ufb01gure), when using varying numbers of top-ranked features in the feature vector. The value of \u201cAll\u201d corresponds to the full size of vectors and is typically in the range of 300\u2013400 features. ",
        "Entity": "Caption"
    },
    {
        "Text": ", fjk (11)",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 4. Simpli\ufb01ed Lesk algorithm [21].",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Dependency parse tree for the sentence (in the ACE corpus): \u201c[Toujan Faisal], 54, {said} [she] was {informed} of the refusal by an [Interior Min- istry committee] overseeing election preparations.\u201d ",
        "Entity": "Caption"
    },
    {
        "Text": "Tree setups P(%) R(%) F SPT 76.3 59.8 67.1 DSPT 77.4 65.4 70.9 UPST (BOF) 80.4 69.7 74.7 UPST (FPT) 80.1 70.7 75.1 UPST (EPT) 79.9 70.2 74.8 Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: Translations output by system RESC2 and COMB on IWSLT task (case-insensitive).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Frequency distribution for sentence lengths in the WSJ (sections 2\u201323) and the ATB (p1\u20133).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Constituent Parsing: Leaf-Ancestor scores for full and 5k training sets and for gold and predicted input.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Evaluation of the Turkish n-gram model.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Example coreferent paths: Italicized entities generally corefer.",
        "Entity": "Caption"
    },
    {
        "Text": "Probabilities We find that Equation (7) assigns too little proba bilities to long words (5 or more characters).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6 Effect of pruning parameter Np and heuristic function on search efficiency for direct-translation model (tp = 10\u221212 ). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Average min/max hypothesis length pro- ducible by each method (h = 1 for CF). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7. Impacts of the mined semantic lexicons and the use of PubMed",
        "Entity": "Caption"
    },
    {
        "Text": "The dataset (table 1) consists of the main text of 28 articles selected from the topical domains of history, sports, science, and technology",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 6: Examples of word, morpheme, and compatible-bracket errors.",
        "Entity": "Reference"
    },
    {
        "Text": "#Cor is the number of correct English translations output.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: An ATB sample from the human evaluation. The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a). But the city name Sharm Al- Sheikh is also iDafa, hence the possibility for the incorrect annotation in (b). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Experiment 2: Results for label unknown sense, NN-based outlier detection, \u03b8 = 1.0. \u03c3: stan- dard deviation ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2 \u201cTYPE\u201d precision on finding the predominant sense for the Senseval-2 English all-words test data for nouns having a frequency less than or equal to various thresholds. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Extraction of Raw Pattern",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5. Another example of some discovered paraphrases.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Identified metaphorical expressions for the mappings FEELING IS FIRE and CRIME IS A DISEASE ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example context for WSD S ENSEVAL-2 target word bar (inventory of 21 senses) and extracted features ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: The feature set for coreference resolution. Non-relational features describe a mention and in most cases take on a value of YES or NO. Relational features describe the relationship between the two mentions and indicate whether they are COMPATIBLE, INCOMPATIBLE or NOT APPLICABLE. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: A Portion of the Syntactic Tree.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3 Illustration of the transitions in the regular and in the inverted alignment model. The regular alignment model (left figure) is used to generate the sentence from left to right; the inverted alignment model (right figure) is used to generate the sentence from bottom to top. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fi gure 7 depic ts the comp atible brack ets and all comp atible brack ets rates.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Average Precision and Coherence (\u03ba) for each meta alternation. Correlation: r = 0.743 (p < 0.001) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Example non-coreferent paths: Italicized entities do not generally corefer",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 shows four words",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1(d) shows a sentence maintain rental property he owns in the state , where the ART.User-or-Owner relation holds between the entities property and he",
        "Entity": "Reference"
    },
    {
        "Text": "where ECD|S,T (fi), the expected count of a feature over all derivations given a pair of tree and string, can be computed using the modified inside- outside algorithm described in Section 3.2, and ECS |T (fi), the expected count of a feature over all possible target strings given the source tree, can be computed in a similar way to the partition function described in Figure 7.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6: ROUGE-SU measures in EM learning",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Synthetic Data Set from Xinhua News",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Comparison of Min-, Simple-, Full-and Dynamic-Expansions: More Examples",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: A relative comparison of running a se- lection of regular expressions and scripts against other finite-state toolkits. The first and second en- tries are short regular expressions that exhibit ex- ponential behavior. The second results in a FSM with 221 states and 222 arcs. The others are scripts that can be run on both Xerox/PARC and Foma. The file lexicon.lex is a LEXC format English dic- tionary with 38418 entries. North Sami is a large lexicon (lexc file) for the North Sami language available from http://divvun.no. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Rhetorical pattern of C-Dash",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1:   CORE 12  with inflectional features, predicted input. Top: Adding all nine features to CORE 12. Second part: Adding each feature separately, comparing difference from CORE 12. Third part: Greedily adding best features from second part. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8: The actual output of our parser trained with a fully annotated treebank. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Local graph of the word wing of a graph",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Substitution/deletion/insertion costs for /g/.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Experimental Procedure",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Overview of the tasks investigated in this paper (n: size of n-gram; POS: parts of speech; Ling: linguistic knowledge; Type: type of task) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Single systems (English) in cross- validation, sorted by recall. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Segmentation accuracy of different seg- menters. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: POS tagging accuracy on texts without punctuation and capitalization, for tagging on the original data, the gold-standard normalization, and automatic normalizations using the first n tokens as training data ",
        "Entity": "Caption"
    },
    {
        "Text": "E.g., the city Fez, Mo rocco (figure 1) was tagged as a single LOCATION by one annotator and as two by the other.",
        "Entity": "Reference"
    },
    {
        "Text": "Tables 6 and 7 show the effect of the pruning pa rameter Np with the pruning parameter tp = 10 12",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4 Patterns and instantiations for other-anaphora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Performance of Altavista counts and BNC counts for adjective ordering (data from Malouf 2000) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5. Sample poster scores.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: The System Performance Based on Each Single Feature Set. ",
        "Entity": "Caption"
    },
    {
        "Text": "equation 4",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6: Precision on full RTE data",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5",
        "Entity": "Reference"
    },
    {
        "Text": " Figure 1.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Tagging accuracy on development data depending on context size",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Screenshot of ConAno",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 9 An example of syntactically motivated paraphrastic patterns that can be extracted from the paraphrase corpus constructed by Cohn, Callison-Burch, and Lapata (2008). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 Model accuracy using equal distribution of verb frequencies for the estimation of P(c). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4 Examples of alignment templates obtained in training. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 12: The Malay fst After the Application of Compile-Replace to the Lower-Side Language",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 9 Word sense disambiguation accuracy for \u201cNP1 V for NP2 NP3\u201d frame. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Performance of our system on the evalu- ation set ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results of UniGraph, BiGraph, and Bi- Graph*. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Etrees and Derivation Trees for (2abc).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Parsing tree",
        "Entity": "Caption"
    },
    {
        "Text": "I exp[ m=1 m hm (e 1 , f1 )]",
        "Entity": "Caption"
    },
    {
        "Text": "First, the source sentence words f J are grouped into phrases f K . For each phrase f an 1 1 alignment template z is chosen and the sequence of chosen alignment templates is reordered (according to K ).",
        "Entity": "Reference"
    },
    {
        "Text": "Contribution of constituent dependencies in respective mode (inside parentheses) and accumulative mode (outside parentheses) The table shows that the final DSPT achieves the best performance of 77.4%/65.4%/70.9 in precision/recall/F-measure respectively after applying all the dependencies, with the increase of F-measure by 8.2 units compared to the baseline MCT.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6: Example Translations for the Verbmobil task.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The predicates of two sentences (white: \u201cThe company has said it plans to restate its earnings for 2000 through 2002.\u201d; grey: \u201cThe company had announced in January that it would have to restate earnings (. . . )\u201d) from the Microsoft Research Paragraph Corpus are aligned by computing clusters with minimum cuts. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Clustering-based stratified seed sampling",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. System performance on the is-a relation on the CHEM dataset. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Proposed discourse structures for Ex. 4: (a) In terms of informational relations; (b) in terms of inten- tional relations ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Illustrative tableau for a simple constraint sys- tem not capturable as a regular relation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Length Distribution In word segmentation, one of the major problems of the word length model of Equation (6) is the decom position of unknown words.",
        "Entity": "Reference"
    },
    {
        "Text": "By introducing the distinction of word type to the model of Equation(12),we can derive a more sophis ticated unknown word model that reflects both word 3 When a Chinese character is used to represent a seman tically equivalent Japanese verb, its root is written in the Chinese character and its inflectional suffix is written in hi ragana.",
        "Entity": "Reference"
    },
    {
        "Text": "We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4, 5, and 6.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2 shows an example of a symmetrized alignment",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: Improvement in (gold mention) RE.",
        "Entity": "Caption"
    },
    {
        "Text": "exp[ M m hm (eI , f J )] m=1 1 1 M I J (3)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Average Precision and Coherence ( ) for each meta alternation.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Case restoration performance using an MD-trie, English. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Results on the NIST MATR 2008 test set for several variations of paraphrase usage.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 17 Training the Easy-First Parser on gold and predicted tags, accuracy by gold attachment type (selected): subject, object, modification (of a verb or a noun) by a noun, modification (of a verb or a noun) by a preposition, idafa, and overall results (repeated). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Distributions of Morph Examples",
        "Entity": "Caption"
    },
    {
        "Text": "Table 23 Comparisons against other segmenters: In Column 1, SXX indicates participating sites in the 1st SIGHAN International Chinese Word Segmentation Bakeoff, and CRFs indicates the word segmenter reported in (Peng et al. 2004). In Columns 2 to 5, entries contain the F-measure of each segmenter on different open runs, with the best performance in bold. Column Site-Avg is the average F-measure over the data sets on which a segmenter reported results of open runs, where a bolded entry indicates the segmenter outperforms MSRSeg. Column Our-Avg is the average F-measure of MSRSeg over the same data sets, where a bolded entry indicates that MSRSeg outperforms the other segmenter. ",
        "Entity": "Caption"
    },
    {
        "Text": "U = {up}m represent the hidden m struct a new graph G1 (Figure 1(d)) with the clusters U as vertices.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Sample of subjective features appearing in the top 100 discriminant attributes selected with Information Gain on the 3rd fold training data at iteration 10. The words in italics in the multilingual features represent equivalent translations in English and Romanian. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Equivalent Left LM State Computation.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Examples of high-weight features for several relations. Key: SYN = syntactic feature; LEX = lexical feature; x = reversed; NE# = named entity tag of entity. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Training instances obtained from Verb- Net (upper) and VerbNet+SemLink (lower) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 shows the results of an evaluation based on the plain STTS tagset.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Final results on 25 corpora in 20 languages, with the number of induced classes equal to the number of gold standard tags in all cases. k-means and SVD2 models could not produce a clustering in the Czech CoNLL corpus due its size. Best published results are from \u2217 Christodoulopoulos et al. (2010), \u2020 Berg-Kirkpatrick et al. (2010) and \u2021 Lee et al. (2010). The latter two papers do not report VM scores. No best published results are shown for the MULTEXT languages; Christodoulopoulos et al. (2010) report results based on 45 tags suggesting that clark performs best on these corpora. ",
        "Entity": "Caption"
    },
    {
        "Text": "For fair comparison, we have tabulated all results with the size of training data used (Table 5",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Processing time for POS tagging of known words using contextual features (In CPU seconds). Train: training time over     + sentences. Brill\u2019s learner was interrupted after 12 days of train- ing (default threshold was used). Test: average number of seconds to evaluate a single sentence. All runs were done on the same machine. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Results for basic DAC system (per-class feature optimization followed by maximum confidence based choice); \u201cER\u201d refers to error reduction in percent over standard multiclass SVM (Table 2) ",
        "Entity": "Caption"
    },
    {
        "Text": "This leaves us with 60 meta alternations, shown in Table 5.",
        "Entity": "Reference"
    },
    {
        "Text": "equation",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Memory-based learner results",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Character type configuration of infrequent words in the EDR corpus",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Reachability of 1000 training sentences: can they be translated with the model? ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 presents an overview of the animacy data Clas s Ani mat e Type s Tok 6 4 4 ens cover ed 6 0 1 0 Inan imat e 691 0 3 4 8 2 2 Tota l 755 4 4 0 8 3 2 Table 1: The animacy data set from Talbanken05; number of noun lemmas (Types) and tokens in each class.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1. Segmentation algorithm.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Sources of Dictionaries",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Dev set learning curves for sentence lengths \u2264 70. All three curves remain steep at the maximum training set size of 18818 trees. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: NIST scores per Levenshtein distance",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Performance of knowledge-based approach using different relatedness measures. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Performance comparison, the numbers in parentheses report the performance over the 24 ACE subtypes while the numbers outside paren- theses is for the 5 ACE major types ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 4. Examples of rules used during decoding.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7. System Performance Comparison.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Decoding algorithm for the standard Tree-to-String transducer.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6: Selected entries from the confusion matrix for parts of speech in English with F-scores for the left-hand- side category. DT = determiner; IN = preposition or sub- ordinating conjunction; JJ = adjective; JJR = compara- tive adjective; NN = singular or mass noun; NNS = plural noun; POS = possessive clitic; RB = adverb; RBR = com- parative adverb; RP = particle; UH = interjection; VB = base form verb; VBD = past tense verb; VBG = gerund or present participle; VBN = past participle; VBP = present tense verb, not 3rd person singular; VBZ = present tense verb, 3rd person singular. We use \u03b1* to denote the set of categories with \u03b1 as a prefix. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Compilation of a Regular Expression into an fst that Maps between Two Regular Languages ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 An example of some discovered paraphrases.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Syntactic dependency scheme used in this work. Labels that aren\u2019t self-explanatory or similar to the labels used by Tratz and Hovy (2011) for English or CATiB for Arabic (Habash and Roth, 2009) are in bold (for completely new relations) or italics (for similarly named but semantically different relations) ",
        "Entity": "Caption"
    },
    {
        "Text": "(Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Supersense evaluation results. Values are the percentage of correctly assigned supersenses. k indicates the number of nearest neighbours considered. ",
        "Entity": "Caption"
    },
    {
        "Text": "If the token starts with a capital letter (initCaps), then an additional feature (init- Caps, zone) is set to 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8 The accuracies of joint segmentation and POS-tagging by 10-fold cross validation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Closed test, in percentages (%)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: a) A related work section extracted from (Wu and Oard, 2008); b) An associated topic hierar- chy tree of a); c) An associated topic tree, annotated with key words/phrases. ",
        "Entity": "Caption"
    },
    {
        "Text": "Based on this experiment, we set the beam size of SegTagDep to 64 throughout the exper 64 96.28 92.37 74.96 0.48 Table 3: F1 scores and speed (in sentences per sec.)",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Dataset Statistics",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: POS Tagging of known words using con- textual features (accuracy in percent). one-vs-all denotes training where example ` serves as positive example to the true tag and as negative example to all the other tags. SM| \u00a8R\u00a9 denotes training where                            2  example ` serves as positive example to the true tag ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The relationship extraction system.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Comparison of size of k-best list for cube decoding with various feature sets. ",
        "Entity": "Caption"
    },
    {
        "Text": "(2004) makes use of a coding manual designed for a project studying genitive modification (Garretson et al., 2004) and presents an explicit annotation scheme for an _ Samma _ PO _ KP erfarenhet NN _ gjorde VV PT engelsmannen NN DD|HH imacy, illustrated by figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: The density of the F1 -scores with the three approaches. The prior used is a symmetric Dirichlet with \u03b1 = 0.1. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Inuktitut: Parimunngaujumaniralauqsimanngittunga = \u201cI never said I wanted to go to Paris\u201d ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: ROUGE-2 in empirical approach",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: ROUGE-2 measures in k-means learning",
        "Entity": "Caption"
    },
    {
        "Text": "Our experimental data was drawn from 150 megabytes of 1993 Nikkei newswire (see Figure I).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2. Modified Viterbi search \u2013 stop-word treatment",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 \u2013 Pk for Le Monde corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Effect of our method comparing with MERT and perceptron in terms of B LEU. We also compare our fast generation method with different data (only reachable or full data). #Data is the size of data for training the feature weights. * means significantly (Koehn, 2004) better than MERT (p < 0.01). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: An example showing the combination of the se- mantic role sequences of the states. Above/middle is the state information before/after applying the TTS template, and bot- tom is the used TTS template and the triggered SRFs during the combination. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 3. Interpolation \u2013 recall/precision curves.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 DP-based algorithm for solving traveling-salesman problems due to Held and Karp. The outermost loop is over the cardinality of subsets of already visited cities. ",
        "Entity": "Caption"
    },
    {
        "Text": "We illustrate its use with an example (see Then, we average the contributions of each n-gram order: Figure 2).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 Varying the number of clusters (evaluation: Randadj ). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: RST Spanish Treebank statistics",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: good#a#15 gloss and examples.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Examples of templates suggested by DIRT and TEASE as having an entailment relation, in some direction, with the input template \u2018X change Y \u2019. The entailment direction arrows were judged manually and added for readability. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Probability of # of boundaries f10 (m\u2032 ; 3).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Examples of non-phonetic translations.",
        "Entity": "Caption"
    },
    {
        "Text": "Tables 6 and 7 show the effect of the pruning pa rameter Np with the pruning parameter tp = 10 12",
        "Entity": "Reference"
    },
    {
        "Text": "The last is exhibited for the first mention in figure 1, where one annotator chose ARTIFACT (referring tothe physical book) while the other chose COMMUNICATION (the content)",
        "Entity": "Reference"
    },
    {
        "Text": "Table 9: Arabic Clitics - Example 1",
        "Entity": "Caption"
    },
    {
        "Text": "In Figure 4 we show an example of variation between the parsing models.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 7: Average search time [s] per sentence.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 14 Comparison scores for HK open and AS open. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Graph of Word Senses",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag. Across all languages, high performance can be attained by selecting a single tag per word type. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Evaluation of the Russian n-gram model.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Results with varying sizes of training data. Year 2003 is not explicitly shown because it has an unusually small number of documents compared to other years. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Disambiguation results in % dependent on word class (nouns, verbs, adjectives) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Contribution of combining the dynamic and",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Number of recall errors according to mention type (rows anaphor, columns antecedent).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6 shows the word segmentation accuracy of four unknown word models over test set-2.",
        "Entity": "Reference"
    },
    {
        "Text": "Comparison of different systems on the ACE RDC 2004 corpus In Table 3 we summarize the improvements of different tree setups over SPT.",
        "Entity": "Reference"
    },
    {
        "Text": "whose unvocalized surface forms 0 an are indistinguishable.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Additional notation and signatures for CAM",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Results of the baseline model: best 5 guesses",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Log-likelihood of samples plotted against iter- ations. Dark lines show the average over five runs, grey lines in the back show the real samples. ",
        "Entity": "Caption"
    },
    {
        "Text": "However, the learning curves in Figure 3 show that the Berkeley parser does not exceed our manual grammar by as wide a margin as has been shown for other languages (Petrov, 2009).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 shows a probability estimation tree for the prediction of the probability of the nominative attribute of nouns.",
        "Entity": "Reference"
    },
    {
        "Text": "Fig. 7. Example of fuzzy divisive clustering.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: (a) An example of a DCS tree (written in both the mathematical and graphical notation). Each node is labeled with a predicate, and each edge is labeled with a relation. (b) A DCS tree z with only join relations en- codes a constraint satisfaction problem. (c) The denota- tion of z is the set of consistent values for the root node. ",
        "Entity": "Caption"
    },
    {
        "Text": "hLEX(eI , f J , K , zK ) = #CO-OCCURRENCES (LEX, eI , f J ) (20) 1 1 1 1 1 1",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Example of word-dependent substitution costs.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Illustration of bottom-to-top search.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 NW 11 identification results on PK test set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5 Dev-set results when using lattice parsing on top of an external lexicon/analyzer. ",
        "Entity": "Caption"
    },
    {
        "Text": "The Stanford parser (Klein and Manning, 2002) is unable to recover the verbal reading of the unvocalized surface form 0 an (Table 1).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Statistics on the Italian EVALITA 2009       and English CoNLL 2003 corpora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 9. Intrinsic and extrinsic evaluation of alignments in the small data experiments. (a) Alignment dictionary size normalized by the average of source and target vocabulary sizes. (b) Average alignment fertility of aligned singletons. (c) Percentage of unaligned singletons. (d) Number of symmetric alignments normalized by the average of source and target tokens. (e) Percentage of training set vocabulary covered by single-word phrases in the phrase table. (f) Decode-time rate of input words that are in the training vocabulary but without a translation in the phrase table. (g) Phrase table size normalized by the average of source and target tokens. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3. Committee-Based Unsupervised Learning",
        "Entity": "Caption"
    },
    {
        "Text": "For example, looking at Figure 1(b), V on G can be grouped into three clusters u1, u2 and u3.",
        "Entity": "Reference"
    },
    {
        "Text": "K hAT(eI , f J , K , zK ) = log n p(zk | f j k ) (13) 1 1 1 1 k=1 j k 1 +1",
        "Entity": "Caption"
    },
    {
        "Text": "The probabilities P( <U-t>lwi_I) can be esti mated from the relative frequencies in the training corpus whose infrequent words are replaced with their corresponding unknown word tags based on their part of speeches 2 Table 1 shows examples of word bigrams including unknown word tags.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 12: MUC-6: Level Distribution of the Six Facts Combined",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Chinese parse tree with empty elements marked. The meaning of the sentence is, \u201cImplementation of the law is temporarily suspended.\u201d ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5",
        "Entity": "Reference"
    },
    {
        "Text": "Fig. 6. Algorithm for fuzzy divisive clustering based on nouns.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 13: Correlation between manual and automatic scores for German-English",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Segmentation precision/recall relative to gold word length in training data.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Sentence error rates of end-to-end evalua- tion (speech recognizer with WER=25%; corpus of 5069 and 4136 dialogue turns for translation Ger- man to English and English to German, respec- tively). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 13 Accuracy of semantic-role prediction for unknown boundaries (the system must identify the correct constituents as arguments and give them the correct roles). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Size of co-occurrence databases",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: The results for three systems associ- ated with the project for the NP bracketing task, the shared task at CoNLL-99. The baseline re- sults have been obtained by finding NP chunks in the text with an algorithm which selects the most frequent chunk tag associated with each part-of- speech tag. The best results at CoNLL-99 was obtained with a bottom-up memory-based learner. An improved version of that system (MBL) deliv- ered the best project result. The MDL results have been obtained on a different data set and therefore combination of the three systems was not feasible. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Tagging accuracies on development data in percent.",
        "Entity": "Caption"
    },
    {
        "Text": "In addition to the basic regular expression operators shown in table 1, the formalism is extended in various ways.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Examples of features and associated costs. Pseudofeatures are shown in boldface. Exceptional denotes a situation such as the semivowel [j] substituting for the affricate [dZ]. Substitutions between these two sounds actually occur frequently in second-language error data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Clustering evaluation for the experiment with Named Entities ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Weights learned for word-context features, which fire when English word e is generated aligned to Chinese word f , with Chinese word f\u22121 to the left or f+1 to the right. Glosses for Chinese words are not part of features. ",
        "Entity": "Caption"
    },
    {
        "Text": "table",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: The fraction of verb pairs clustered together, as a function of the number of shared senses (results of the NN algo- rithm) ",
        "Entity": "Caption"
    },
    {
        "Text": "To differentiate between the coordinating and discourse separator functions of conjunctions (Table 3), we mark each CC with the label of its right sister (splitCC).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4: Improvements in F-measure on MUC-7 plotted against amount of selected unlabeled data used ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The Lattice for the Hebrew Phrase bclm hneim",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: AER comparison (en\u2192cn)",
        "Entity": "Caption"
    },
    {
        "Text": "For example, the pairwise words orange and peel form a collocation.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6: Distribution of errors",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1. Results of system combination on Dev7 (development) corpus and Test09,             the o\ufb03cial test corpus of IWSLT\u201909 evaluation campaign. ",
        "Entity": "Caption"
    },
    {
        "Text": "This corresponds to maximizing the equivocation or maximizing the likelihood of the direct-translation model",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: Evaluation of 100 randomly sampled variation nuclei types.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Corpus statistics of Chinese side, where Sent., Avg., Lon., and Len. are short for sentence, longest, average, and length respectively. RT RAIN denotes the reachable (given rule table without added rules) subset of T RAIN data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4 A bootstrapping algorithm to extract phrasal paraphrase pairs from monolingual parallel corpora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 Polysemous word types in the Senseval-2 and -3 English all-words tasks test documents with no data in SemCor (0 columns), or with very little data (\u2264 1 and \u2264 5 occurrences). Note that there are no annotations for adverbs in the Senseval-3 documents. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Verb classes (see Section 3.1), their Levin class numbers, and the number of experimental verbs in each (see Section 3.2). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Example of topic-to-topic correspondence. The last line shows the correspondence probability. Each col- umn means a topic represented by its top-10 topical word- s. The first column is a target-side topic, while the rest three columns are source-side topics. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4. Some of the top selected features by Infor-                    mation Gain ",
        "Entity": "Caption"
    },
    {
        "Text": "= argmax M 1 S s=1 ) log p M (es | fs ) (4)",
        "Entity": "Caption"
    },
    {
        "Text": "As a result, Arabic sentences are usually long relative to English, especially after",
        "Entity": "Reference"
    },
    {
        "Text": "The TnT tagger achieves 86.3% accuracy on the default tagset.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5: Avg. runtime per sentence of FindPareto",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2 shows that the tagging accuracy tends to increase with the context size.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5: Topics sorted by number of words assigned.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2. Consistently formatted sentence trans-                   lation pairs ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Topic words extracted from target-side doc-                       uments ",
        "Entity": "Caption"
    },
    {
        "Text": "A relative comparison of running a selection of regular expressions and scripts against other finite-state toolkits.",
        "Entity": "Caption"
    },
    {
        "Text": "aJ = argmax p (f J , aJ | eI ) (8) 1 1 1 1 J 1",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: TV show types: The distribution of show types in a large database of TV shows (1067 shows) that has been recorded over the period of a couple of months until April 2000 in Pittsburgh, PA ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Weights set by maximum entropy.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: An excerpt from the graph for Italian. Three of the Italian vertices are connected to an automatically la- beled English vertex. Label propagation is used to propa- gate these tags inwards and results in tag distributions for the middle word of each Italian trigram. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: F-score on development data",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4. Results for initial ranking manner.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Architecture of the translation approach based on Bayes\u2019 decision rule. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2 Examples of dependency trees with word alignment. Arrows are drawn from children to parents. A child word is a modifier of its parent. Each word has exactly one parent and $ is a special \u201cwall\u201d symbol that serves as the parent of all root words in the tree (i.e., those with no other parent). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: The BLEU score of self-trained cascaded trans- lation model under five initial training sets. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Contribution of constituent dependen- cies in respective mode (inside parentheses) and accumulative mode (outside parentheses) ",
        "Entity": "Caption"
    },
    {
        "Text": "and 3",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Performance of the mention detection system including all ACE 04 subtasks",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3 FSA for the pattern hit a e . ",
        "Entity": "Caption"
    },
    {
        "Text": "The overall architecture of the log-linear modeling approach is summarized in Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 11: MUC-6: Level Distribution of Each of the Six Facts",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 9 Illustration of the IBM-style reordering constraint. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Comparison of raw input and constrained input.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Comparison with other approaches",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Average three-fold cross-validation accuracies, in percent. Boldface: best performance for a given setting (row). Recall that our baseline results ranged from 50% to 69%. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Screenshot of Annis Linguistic Database",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Results for feature ablation experiments.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2. Representation Size vs. Training               Corpus Size ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: DRS and corresponding DRG (in tuples and in graph format) for \u201cA customer did not pay.\u201d",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Two examples from the all-en dataset. Each example shows a reference translation, and the outputs of two machine translation systems. In each case, the output of MT system 1 is annotated as the better translation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Sentence Recency",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: F1s of some individual FN role classifiers and the overall multiclassifier accuracy (454 roles).",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 2. A real-world situation",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results for the pronoun resolution",
        "Entity": "Caption"
    },
    {
        "Text": "Table 12 Semantic preferences for verbs with the double-object frame. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Gibbs sampling for word alignment.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: An example of the label consistency problem excerpted from a document in the CoNLL 2003 English dataset.",
        "Entity": "Caption"
    },
    {
        "Text": "So we estimate that English translations are present in the English part of the corpus for Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": " Table 1. OBI vs. BI; where the lost of F > 1%,   such as SC-B, is caused by incorrect English segments that will be discussed in the section 4. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 The top 20 most similar words for country (and their ranks) in the similarity list of LIN, followed by the next four words in the similarity list that were judged as entailing at least in one direction. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results \u2014 Evaluation A.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: An example of annotation projection for relation detection of a bitext in English and Korean",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Three types of transitivity constraint violations.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5 Overall architecture of MSRSeg. ",
        "Entity": "Caption"
    },
    {
        "Text": "his optimization can be performed using the expectation maximization (EM) algorithm (Dempster, Laird, and Rubin 1977).",
        "Entity": "Reference"
    },
    {
        "Text": "n all four tables, we provide theresults for using no heuristic functions and three variants of an increasingly infor mative heuristic function.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Selected morphosyntactic categories in the OLiA Reference Model ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Seeds with the Highest Weight",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Turning distributional similarity into a weighted inference rule ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Statistical results",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4. Error Distribution",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: B3 results for baselines and lexicalized feature sets on the broad-coverage ACE 2004 data set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: The effect of syntactic features when predicting morphology using lexicons. * mark statistically signifi- cantly better models compared to our baseline (sentence- based t-test with \u03b1 = 0.05). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 Number of learned splits per POS category after five split-merge cycles. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Results on WMT-2013 (blindtest)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure",
        "Entity": "Reference"
    },
    {
        "Text": "Fig. 1. The polarity classification (positive and negative) based on product aspect framework",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 9 BLEU difference curves of four context-informed models using TRIBL",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Number of training examples",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example of a long jump alignment grid. All possible deletion, insertion, identity and substitution op- erations are depicted. Only long jump edges from the best path are drawn. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: MTO is fairly stable as long as the Z\u0303 constant 5.4 Morphological and orthographic features is within an order of magnitude of the real Z value. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 Thesaurus coverage of polysemous words (excluding multiwords) in WordNet 1.6. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table (1) and Eq.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1. System overview",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The Maytag interface",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Parsing results of different models using manual (gold) segmentation. Performances significantly superior to HILDA (with p<7.1e-05) are denoted by *. Significant differences between TSP 1-1 and TSP SW (with p<0.01) are denoted by \u2020. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 8 Average number of target phrase distribution sizes for source phrases for TRIBL and IGTree com- pared to the Moses baseline ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Output of word sense clustering.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 7 Word sense disambiguation accuracy for \u201cNP1 V NP2 NP3\u201d frame. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1, Figure 1, and Figure 2 shows the AER results for different models.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: BLEU scores in translation experiments. E: En- glish, T: Turkish, C: Czech, A: Arabic. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Cross-domain B3 (Bagga and Baldwin, 1998) results for Reconcile with its general feature set. The Paired Permutation test (Pesarin, 2001) was used for statistical significance testing and gray cells represent results that are not significantly different from the best result. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Dependency representation of example (2) from Talbanken05.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: precision decrease when omitting non-",
        "Entity": "Caption"
    },
    {
        "Text": "It should be emphasized that this constraint to consecutive phrases limits the expressive power.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4 Related research integrating context into alternative SMT models",
        "Entity": "Caption"
    },
    {
        "Text": "equation (3)",
        "Entity": "Reference"
    },
    {
        "Text": "Table 21: Arabic Vocalization Problem",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Official BakeOff2005 results. Keys: F - Regular Tagging only, all training data are used P1 - Regular Tagging only, 90% of training data are used P2 - Regular Tagging only, 70% of training data are used S - Regular and Correctional Tagging, Separated Mode I - Regular and Correctional Tagging, Integrated Mode ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Example nouns and their supersenses",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Results of Multiple Trials and Compari- son to Simulated Annealing ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Context-sensitive spelling correction (* denotes also using 60% WSJ, 5% corrupted) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Training, tuning, and test conditions",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: BLEU results",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10: Cross-Language Evaluation: Unlabeled TedEval Results in gold input scenario, On a 5k-sentences set set and a 5k-terminals test set. The upper part refers to constituency parsing and the lower part refers to dependency parsing. For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics. ",
        "Entity": "Caption"
    },
    {
        "Text": "Among all possible target sentences, we will choose the sentence with the highest probability",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 6: Results on the FQ dataset.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Unbalanced vs. balanced combining. All runs ignored the context. Evaluated on the Test data set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3 Back-off lattice with more specific distributions towards the top. ",
        "Entity": "Caption"
    },
    {
        "Text": "If we compare the error rates in Table 7, which correspond to about 55 search errors in Table 6, we obtain an mWER of 36.7% (53 search errors) using no heuristic function and an mWER of 32.6% (57 search errors) using the combined heuristic function.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4. Performance comparison on the ACE 2004 data over both 7 major types (the numbers outside parentheses) and 23 subtypes (the num- bers in parentheses) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: The number of vocabularies in the 10k, 50k and 100k data sets. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 Grammatical contexts used for acquiring the BNC thesaurus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Effect of Factors",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Test results for POS+MORPH tagging. Best baseline results are underlined and the overall best results bold. * indicates a significant difference between the best baseline and a PCRF model. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: F-measures for every kernel in (Khayyamian et al., 2009) and MEDLDA",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Positive and negative examples for entailment in the training set. The direction of entailment is from the left template to the right template. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Word length distribution of unknown words and its estimate by Poisson distribution",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Influence of the n-gram model on the perfor- mance of the statistical approach. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Three most distinctive topics are displayed. The English words for each topic are ranked according to p(e|z) estimated from the topic-specific English sentences weighted by {\u03c6dnk }. 33 functional words were removed to highlight the main content of each topic. Topic A is about Us-China economic relationships; Topic B relates to Chinese companies\u2019 merging; Topic C shows the sports of handicapped people. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2. A derivation for Mary likes Susan",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Example questions correctly answered by CCG-Distributional.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Approximate times in seconds to generate predictions of maximum word sequence length M , on a 1.2GHz processor, for the MEMD model.",
        "Entity": "Reference"
    },
    {
        "Text": "If we compare the error rates in Table 7, which correspond to about 55 search errors in Table 6, we obtain an mWER of 36.7% (53 search errors) using no heuristic function and an mWER of 32.6% (57 search errors) using the combined heuristic function",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Above: The complete supersense tagset for nouns; each tag is briefly described by its symbol, NAME, short description, and examples.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 7 The empirical formulae for the prediction (linear model). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Precision, recall and F-scores for the two classes in MBL-experiments with a general feature space. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Glue Semantics proof for (86), English Way Construction (means interpretation)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Performance of the mention detection sys- tem including all ACE\u201904 subtasks ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Interannotator agreement. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 The results of setting 4 (Punctuation and other encoding information are used; the maximum length is 30). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The first set of features in our model. All of them are binary. The final feature set includes two sets: the set here, and a set obtained by its conjunction with the verb\u2019s lemma. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Parse likelihood",
        "Entity": "Caption"
    },
    {
        "Text": "These semantic features Figure 8: Examples of the MT outputs with and without SRFs",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Relationship types and their argument type con- straints. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Character classes",
        "Entity": "Caption"
    },
    {
        "Text": "In all four tables, we provide theresults for using no heuristic functions and three variants of an increasingly infor mative heuristic function",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Accuracy scores for WSJ-PTB converted with head rules of Yamada and Matsumoto (2003) and labeling rules of Nivre (2006). Best dev setting: k = 3, \u03b1 = 0.4. Results marked with \u2020 use additional information sources and are not directly comparable to the others. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: BLEU-4 scores of different systems",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Comparison figures on subsets of the Stanford Sentiment Treebank ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Comparison of the performance of the bootstrapped SVM method from (Zhang, 2004) and LP method with 100 seed labeled examples for relation type classification task. ",
        "Entity": "Caption"
    },
    {
        "Text": "tion (13) is estimated from the relative frequency of the corresponding events in the training corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 13 Experiments applying individual features in English-to-Hindi translation",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Selected entries from the confusion matrix for parts of speech in German with F-scores for the left-hand- side category. ADJ* (ADJD or ADJA) = adjective; ADV = adverb; ART = determiner; APPR = preposition; NE = proper noun; NN = common noun; PRELS = relative pronoun; VVFIN = finite verb; VVINF = non-finite verb; VAFIN = finite auxiliary verb; VAINF = non-finite auxil- iary verb; VVPP = participle; XY = not a word. We use \u03b1* to denote the set of categories with \u03b1 as a prefix. ",
        "Entity": "Caption"
    },
    {
        "Text": "As illus trated in Figure 1(e), the NP coordination in the Qian et al.",
        "Entity": "Reference"
    },
    {
        "Text": "Recall is somewhat difficult to estimate because we do not know whether the English translation of a Chinese word appears in the English part of the corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 Training, development, and test data for word segmentation on CTB5. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Expanding a partial hypothesis via a matching n-gram. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 13 Example translations for Chinese\u2013English MT. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Dynamic-Expansion Tree Span Scheme",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 shows examples of the feature SRR.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: LO Dice configuration scores",
        "Entity": "Caption"
    },
    {
        "Text": "Another interesting example is shown in Figure 1(b), where the base-NP of the second entity town is a possessive NP and there is no relationship between the entities one and town defined in the ACE corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: An example confusion network construc- tion ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 Web results for other-anaphora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Word Sense clusters Class-label arms knees trousers feet biceps hips elbows backs wings body part breasts shoulders thighs bones buttocks ankles legs inches wrists shoes necks horses muskets charges weapons methods firearms weapon knives explosives bombs bases mines projectiles drugs missiles uniforms jersey israel colomho guernsey luxeinhourg denmark maim European greece belgium swede, turkey gibraltar portugal ire- country land mauritius britain cyprus netherlands norway aus tralia italy japan canada kingdom spain austria zealand england france germany switzerland finland poland a merica usa iceland holland scotland uk crucifix bow apron sweater tie anorak hose bracelet garment helmet waistcoat jacket pullover equipment cap collar suit fleece tunic shirt scarf belt head voice torso back chest face abdomen side belly groin body part spine breast bill rump midhair hat collar waist tail stomach skin throat neck speculum ceo treasurer justice chancellor principal founder pres- person ident commander deputy administrator constable li brarian secretary governor captain premier executive chief curator assistant committee patron ruler oil heat coal power water gas food wood fuel steam tax object heating kerosene fire petroleum dust sand light steel telephone timber supply drainage diesel electricity acid air insurance petrol tempera gouache watercolour poster pastel collage paint acrylic lemon bread cheese [flint butter jam cream pudding yogurt foodstuff sprinkling honey jelly toast ham chocolate pie syrup milk meat beef cake yoghurt grain hazel elder holly family virgin hawthorn shrub cherry cedar larch mahogany water sycamore lime teak ash wood hornbeam oak walnut hazel pine beech alder thorn poplar birch chestnut blackthorn spruce holly yew lau rel maple elm fir hawthorn willow bacon cream honey pie grape blackcurrant cake ha- foodstuff mama",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Three representations of NP modifications: (a) the original treebank representation; (b) Selective left-corner representation; and (c) a flat structure that is unambiguously equivalent to (b) ",
        "Entity": "Caption"
    },
    {
        "Text": "We also investigated the effect of varying M . The results are shown in Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: The parts of taxonomic names",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 11: Two Paths in the Initial Malay Transducer Defined via Concatenation",
        "Entity": "Caption"
    },
    {
        "Text": "The points labelled smoothed in figure 2 were obtained using a sliding-average smoother, and the model curve was obtained using two-component Gaussian mixtures to fit the smoothed empirical likelihoods p(gain|a = 0) and p(gain|a = 1).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: LexRank example: sentence similarity graph with a cosine threshold of 0.15. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 9: MUC-5: Level Distribution of Each of the Five Facts",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Effect of Arabic stemming features on coref- erence resolution. The row marked with \u201cTruth\u201d represents the results with \u201ctrue\u201d mentions while the row marked with \u201cSystem\u201d represents that mentions are detected by the system. Numbers under \u201cECM- F\u201d are Entity-Constrained-Mention F-measure and numbers under \u201cACE-Val\u201d are ACE-values. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4. Results of 3000 sentences",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 shows the bilingual phrases containing between two and seven words that result from the application of this algorithm to the alignment of Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "                            C2 Figure 3: An underspecified d ",
        "Entity": "Caption"
    },
    {
        "Text": "eik (12",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Summary of LINGUA performance",
        "Entity": "Caption"
    },
    {
        "Text": "Table 13 Macro-average recall, precision, and F1 on the development set and test set using the parameters that maximize F1 of the learned edges over the development set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Translated fragments, according to the lexicon.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Hinton diagram comparing most frequent tags and clusters.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Training time comparison. The training time for each model is calculated from scratch. For example, the training time of IBM Model 4 includes the training time of IBM Model 1, the HMM, and IBM Model 3. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Approximate times in seconds to generate predictions of maximum word sequence length M , on a 1.2GHz processor, for the MEMD model. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Impact of the use of sampled texts.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Dataset statistics: development (dev) and test.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A pair of comparable, non-parallel documents",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Performance of different settings",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2. Extracted NE pair instances and context",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: good#a#15 SentiWN scores.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: AER results. IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM. We choose t = 1, 5, and 30 for the fertility HMM. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Training Data Sizes for Common ESL Confused Words ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Accuracy of part of speech estimation each part of speech and word type (POS + WT + Poisson + bigram).",
        "Entity": "Reference"
    },
    {
        "Text": "In the table, Nc indicates the number of clusters in the inferred tree, while Nl indicates the closest match to the number of classes in the gold standard.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3. System Comparison",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8 A hierarchical summary of propositions involving nausea as an argument, such as headache is related to nausea, acupuncture helps with nausea, and Lorazepam treats nausea. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Evaluation results",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Overlaid bilingual embeddings: English words are plotted in yellow boxes, and Chinese words in green; reference translations to English are provided in boxes with green borders directly below the original word. ",
        "Entity": "Caption"
    },
    {
        "Text": "figure.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6: Comparison of the existing efforts on ACE RDC task.",
        "Entity": "Caption"
    },
    {
        "Text": "The breakdown of the different types of words found by ST in the test corpus is given in Table 3.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Evaluation of topic segmentation for the English corpus (Pk and WD as percentages) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6. Precision at top 100",
        "Entity": "Caption"
    },
    {
        "Text": "In Table 1, period 1 is Jul 01 \u2013 Jul 15, period 2 is Jul 16 \u2013 Jul 31, \u2026, period 12 is Dec 16 \u2013 Dec 31.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Word Alignment Accuracy (F-measure) and Machine Translation Quality for BiTAM Models, comparing with IBM Models, and HMMs with a training scheme of 18 h7 43 on the Treebank data listed in Table 1. For each column, the highlighted alignment (the best one under that model setting) is picked up to further evaluate the translation quality. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Dependency graph for Czech sentence from the Prague Dependency Treebank1",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: ROUGE-W in empirical approach",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: BLEU scores as a function of development data size. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Effect of pruning parameter tp and heuristic function on search efficiency for direct-translation model (Np = 50,000). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results of paraphrases evaluation for 100 sentences in French using English as the pivot lan- guage. Comparison between the baseline system MOSES and our algorithm MCPG. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 14 Reduplication for n = 4. ",
        "Entity": "Caption"
    },
    {
        "Text": "This is especially true in the case of quotations\u2014which are common in the ATB\u2014where (1) will follow a verb like (2) (Figure 1).",
        "Entity": "Reference"
    },
    {
        "Text": "and 2",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Graphical depiction of our model and summary of latent variables and parameters. The type-level tag assignments T generate features associated with word types W . The tag assignments constrain the HMM emission parameters \u03b8. The tokens w are generated by token-level tags t from an HMM parameterized by the lexicon structure. The hyperparameters \u03b1 and \u03b2 represent the concentration parameters of the token- and type-level components of the model respectively. They are set to fixed constants. ",
        "Entity": "Caption"
    },
    {
        "Text": "Results for 2 and for 10 preceding POS tags as context are reported for our tagger.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Accuracy for MBL and SVM classifiers on Talbanken05 nouns in accumulated frequency bins by Parole frequency.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: Encoding local word order.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Solution of the multiple choice exercise",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Salience grading for candidate antecedents",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Comparison results with TAC 2008 Three Top Ranked Systems (system 1-3 demonstrate top 3 systems in TAC) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Number of candidates for each target                 language. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Initial type-sensitive Chinese/English NER performance. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Precision of acquired relations (prevention). L and S denote lenient and strict evaluation. ",
        "Entity": "Caption"
    },
    {
        "Text": "This table also shows that: (1) Both modification within base-NPs and modification to NPs contribute much to performance improvement, acquiring the increase of F- measure by 4.4/2.4 units in mode M1 and 4.4/2.3 units in mode M2 respectively.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Sample pairs of similar caseframes by relation type, and the similarity score assigned to them by our distributional model. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Learning curves of bootstrapping meth- ods for semantic classification on TS1. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: BLEU scores for different configuration of factored translation models. The big prefix de- notes experiments with the larger context for n- gram translation models. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: MUC-7: Level Distribution of Each of the Facts",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Performance of the knowledge-based ap- proach using the JiangConrath semantic relatedness measure. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 Words (excluding multiwords) in WordNet 1.7.1 and the BNC without any data in SemCor. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Effect of language and error models to speed (time in seconds per 10,000 word forms) ",
        "Entity": "Caption"
    },
    {
        "Text": "able 1 shows the bilingual phrases containing between two and seven words that result from the application of this algorithm to the alignment of Figure 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Example of a prediction for English to French translation.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 11 Results of the error analysis for the sample of 80 words. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A sentence from the article Islamic GoldenAge, with the supersense tagging from one of two anno tators.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Various features used for computing edge weights between foreign trigram types. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Experimental results with individual features, compared against Moses and the moses-chart baseline. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Evolution of \u03c4A means relative to the length of the n-best sequence ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Recall and precision of the patterns.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Unigram, bigram and trigram counts of                the word corpus ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7. System performance on the reaction relation on the CHEM dataset. ",
        "Entity": "Caption"
    },
    {
        "Text": "able 2 shows the corpus statistics for this task.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 Penn Arabic Treebank part 3 v3.1 data split. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results on the MT02 and MT05 test sets",
        "Entity": "Caption"
    },
    {
        "Text": "Although kanji sequences are difficult to seg ment, they can comprise a significant portion of Japanese text, as shown in Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1. A brief description of the tested parsers. Note that the Tune data is not the data used to train the individual parsers. Higher numbers in the right column reflect just the fact that the Test part is slightly easier to parse. ",
        "Entity": "Caption"
    },
    {
        "Text": "Preliminary observations show that the different neighbours in Table 1 can be used to indicate with great accuracy which of the senses is being used",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: The decision tree (Nwire) for the system using the single semantic relatedness feature ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Results - Evaluation B",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: List of results in Sighan Bakeoff 2005",
        "Entity": "Caption"
    },
    {
        "Text": "Thus,we have crafted more specific explanations, sum marized for nouns in figure 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: Search Success Rate (1 million hypothe- ses) [%]. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Association overlap for target verbs.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7: Ablation Results",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Word segmentation on IWSLT data sets",
        "Entity": "Caption"
    },
    {
        "Text": "IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6: Comparison of results for MUC7",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Priority Order for Second Person ADs",
        "Entity": "Caption"
    },
    {
        "Text": "As for the unknown word model, word-based char acter bigrams are computed from the words with Table 5: Cross entropy (CE) per word and character perplexity (PP) of each unknown word model Part of Speech Estimation Accuracy 0.95 0.9 frequency one (49,653 words).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 17 WSD using predominant senses, training, and testing on all domain combinations (automatically classified corpora). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Comparison for Head and Tail datasets",
        "Entity": "Caption"
    },
    {
        "Text": "Only tokens with initCaps not found in commonWords are tested against each list in Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 8 Extracting consistent bilingual phrasal correspondences from the shown sentence pairs. (i1 , j1 ) \u00d7 (i2 , j2 ) denotes the correspondence   fi1 . . . fj1 , ei2 . . . ej2  . Not all extracted correspondences are shown. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Experiment Results",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11 Model accuracy using unequal distribution of verb frequencies for the estimation of P(c). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Information Flow in the Xerox Arabic Demo. Input words from the user interface are transmitted across the Internet (dotted lines) and analyzed by a server, typically producing multi- ple analysis strings. Each analysis string is then generated in fully voweled form, combined with English glosses and then reformatted as HTML before being sent back across the Internet to the user\u2019s browser for display. The analyzer and gen- erator finite-state transducers (FSTs) are identical except that the lower side language of the genera- tor is limited to contain only fully-voweled words. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: SemCor results",
        "Entity": "Caption"
    },
    {
        "Text": "A token that is allCaps will also be initCaps.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Gross statistics for several different treebanks. Test set OOV rate is computed using the following splits: ATB (Chiang et al., 2006); CTB6 (Huang and Harper, 2009); Ne- gra (Dubey and Keller, 2003); English, sections 2-21 (train) and section 23 (test). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 25 Comparison of dependency accuracies between phrase-structure parsing and dependency parsing using CTB5 data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Example annotation from the BioNLP Shared Task 2011 Epigenetics and Post-translational Modifications event extraction task. ",
        "Entity": "Caption"
    },
    {
        "Text": "this error is not counted, the tagging accuracy on the development data rises from 92.17% to 94.27%.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Properties of the training and test sets used in the shared task. The training data is the Europarl cor- pus, from which also the in-domain test set is taken. There is twice as much language modelling data, since training data for the machine translation system is filtered against sentences of length larger than 40 words. Out-of-domain test data is from the Project Syndicate web site, a compendium of political commentary. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Tagging and segmentation results on Estonian Multext-East corpus (Learned seg and Learned tag) com- pared to the semisupervised setting where segmentations are fixed to gold standard (Fixed seg) and tags are fixed to gold standard (Fixed tag). Finally the segmentatation results from Morfessor system for comparison are pre- sented. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: An underspecified discourse structure and its five configurations",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: BLEU4 scores of different systems Source Launching1 New2 Diplomatic3 Offensive4 SRF On 1 2 3 4 SRF Off 2 3 4 It1 is2 therefore3 necessary4 to5 speed6 up7 the8 equal better worse With SRF vs. W/O SRF 72% 20.2% 7.8% Source transformation9 of10 traditional11 industries12 with13 high14 technologies15",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: Time to read and accept or reject proposals versus their length tion, because the empirical probability of acceptance is very low when it is less than zero and rises rapidly as it increases.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: Word type coverage by normalized frequency: words are grouped by word count / highest word count ratio: low [0, 0.01), medium [0.01, 0.1), high [0.1, 1]. ",
        "Entity": "Caption"
    },
    {
        "Text": "segmentation (Table 2).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 7: Two possible DTs for three sentences.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Comparison of the news and reports corpora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Network after incremental TER alignment.",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 1. System architecture overview",
        "Entity": "Caption"
    },
    {
        "Text": "Pr(f J | eI ) = Pr(f J , aJ | eI ) (5) 1 1 1 1 1 J 1",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. F-scores of UML-DOP compared to      previous models on the same data ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Statistics on a travel conversation corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Number of extracted paraphrases.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 Smoothed estimates. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 NER type-insensitive (type-sensitive) performance of different English NE recognizers. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Symmetry of window size",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Comparison to Related Approaches",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Number and ratio of statistically signifi- cant distinction between system performance. Au- tomatic scores are computed on a larger tested than manual scores (3064 sentences vs. 300\u2013400 sen- tences). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 presents the wide range of cases that are used to create the morphs.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: Effectiveness of score propagation.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Baseline Pipeline Results",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: A partially scaled and inverted identity matrix J\u00b5 . Such a matrix can be used to trans- form a vector storing a domain and value repre- sentation into one containing the same domain but a partially inverted value, such as W and \u00acW de- scribed in Figure 1. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Type-level Results: Each cell report the type- level accuracy computed against the most frequent tag of each word type. The state-to-tag mapping is obtained from the best hyperparameter setting for 1-1 mapping shown in Table 3. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8 Algorithm min-jumps to compute the minimum number of needed jumps D(cJ , j) to complete the translation",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Evidence cardinality in the corpora.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Rule type distribution of a sample of 200 rules that extracted incorrect mentions. The corre- sponding numbers of incorrect mentions extracted by the sampled rules is shown in parentheses. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2 shows the word length distribution of words consists of only kanji characters and words consists of only katakana characters.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 Proportion of OOV words in some corpora used for real world applications. (Numbers in parentheses exclude words whose first letters are capitalized because they are likely to refer to named entities.) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 Architecture of the translation approach based on a log-linear modeling approach",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: CoreLex s basic types with their corresponding WordNet anchors.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4: Devoicing transducer compiled through a rule.",
        "Entity": "Caption"
    },
    {
        "Text": "As each global feature group is added to the list of features, we see improvements to both MUC6 and",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5: A filter RTG corresponding to Ex. 2",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Effect of two-level lexicon combination. For the baseline we used the conventional one-level full form lexicon. ",
        "Entity": "Caption"
    },
    {
        "Text": "If we directly translate the EM algorithm into the log- linear model, the problem becomes maximizing 0 X P r(S, T , D) = X @ Y P r(t) Y 1 P r(f )A the data likelihood represented by feature weights instead of feature probabilities: D D t D f F (S,T .role,D) Though the above formulation, which makes the P r(S, T ) = D exp i i fi (S, T , D) total probability of all the pairs of trees and strings P P exp P f (S , T , D ) St ,T t Dt i i i less than 1, is not a strict generative model, we can still use the EM algorithm (Dempster et al., 1977) to estimate the probability of the TTS templates and the semantic features, as shown in Figure 6.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: NMI of unconstrained HGFC when trees for T2 and T3 are inferred automatically. ",
        "Entity": "Caption"
    },
    {
        "Text": "As shown in figure 3, read times are much higher for predictions that get accepted, re B(x, k, a) = R1(x) + T (x, k) E(x, k), a = 1 R0(x), a = 0 flecting both a more careful perusal by the translator and the fact the rejected predictions are often simplywhere Ra(x) is the cost of reading x when it ulignored.2 In both cases there is a weak linear rela timately gets accepted (a = 1) or rejected (a = 0), T (x, k) is the cost of manually typing xk , and E(x, k) is the edit cost of accepting x and erasing to the end of its first k characters.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Examples of context- free and context-sensitive sub- trees related with Figure 1(b). Note: the bold node is the root for a sub-tree. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8 Utility Score Comparison",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Macro-accuracy for multilingual bootstrapping (versus cross-lingual framework) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: FDG Analyser\u2019s output example",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6 Word sense disambiguation accuracy for \u201cNP1 V NP2\u201d frame. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3. Comparison of two augmentation strategies over different sampling strategies in selecting the initial seed set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1shows the word length distribution of in frequent words in the EDR corpus, and the estimate of word length distribution by Equation (6) whose parameter (.A = 4.8) is the average word length of infrequent words.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: All the parameters of WSMs described in Section 2 used in all our experiments. Semicolon denotes OR. All the examined combinations of parameters are implied from reading the diagram from left to right. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: An example of alignment for Japanese and English sentences",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results on Named Entity Recognition",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Subparts and features",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Feature set for our pronoun resolution system(*ed feature is only for the single-candidate model while **ed feature is only for the twin-candidate mode) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Baseline performance and n-best oracle scores (UAS/LAS) on the development sets. mate\u2019 uses the prepro- cessing provided by the organizers, the other parsers use the preprocessing described in Section 2. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6 lists a sample of targets for the five meta alternations involved.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 shows a comparison with some existing toolkits that build deterministic, minimized automata/transducers.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Optimality of window size",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Effects of using CRF. The separator \u201c/\u201d divides the results of s1, and s3.",
        "Entity": "Caption"
    },
    {
        "Text": "As Table 1 shows, word bigrams whose infrequent word bigram",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Part-of-speech tags of the Penn Chinese   Treebank that are referenced in this paper.      Please see (Xia, 2000) for the full list. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2 In this example, the path from the predicate ate to the argument NP He can be represented as VBjVPjS,NP, with j indicating upward movement in the parse tree and , downward movement. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: In-domain (first column) and out-of-domain performance (columns two to four) on ACE 2005. PET and BOW are abbreviated by P and B, respectively. If not specified BOW is marked. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 DP-TSG notation. For consistency, we largely follow the notation of Liang, Jordan, and Klein (2010). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Functional features: gender, number, rationality.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4",
        "Entity": "Reference"
    },
    {
        "Text": "The annotation manual (Teleman, 1974) states that a markable should be tagged as human (H H) if it may be replaced by the interrogative pronoun vem who and be referred to by the personal pronouns han he or hon she .There are clear similarities between the anno tation for human reference found in Talbanken05 and the annotation scheme for animacy discussed HUM Other animate Inanimate ORG ANIM CONC NCONC TIME PLACE Figure 1: Animacy classification scheme (Zaenen et al., 2004)",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 6: Average scores for different language pairs. Manual scoring is done by different judges, resulting in a not very meaningful comparison. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6 shows example sentences annotated by HGFC.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 7: Results for OOV-processing and MBR, German\u2192English. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Automatically generated training set examples.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Log probability of the sampler state over 1000 iterations on Languages A and B. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11. A Cascade of Compositions",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Resampling probabilities for alternations, after 1000 iterations. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 11 Large-scale clustering on D1. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5 The binary tree of Selection. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8a shows that the best model recovers SBAR at only 71.0% F1.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: The conditioning structure of the hierarchical PYP with an embedded character language models. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: New training and testing procedures",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 %BLEU on tune and test sets for ZH\u2192EN translation, showing the contribution of feature sets in our QPD model. Both QPD models are significantly better than the best Moses numbers on test sets 1 and 2, but not on test set 3. The full QPD model is significantly better than the version with only T GT T REE features on test set 1 but statistically indistinguishable on the other two test sets. Hiero is significantly better than the full QPD model on test set 2 but not on the other two. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Number of entries in 3 corpora",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 1 Examples of TERp alignment output. In each example, R, H and H  denote the reference, the original hypothesis and the hypothesis after shifting respectively. Shifted words are bolded and other edits are in [brackets]. Number of edits shown: TERp (TER) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Segmentation performance presented in previous work and of our combination model. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Resolution accuracy (%)",
        "Entity": "Caption"
    },
    {
        "Text": "Except our own and MENE + reference resolution, the results in Table 6 are all official MUC7 results.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 Coreference factors for name recognition",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Sample targets for meta alternations with high AP and mid-coherence values.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 7: Percentage of obtaining two clusters when applying CW on n-bipartite cliques ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Accuracy(%) of \u2018obscure\u2019 name recognition",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Contribution of individual features to overall performance.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results for different predictor configurations.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4: A RTG integrating the attachment constraint for Contrast from Ex. 2 into Fig. 3",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Workflow for NIL knowledge engineering component. NILE refers to NIL expression, which is identified and annotated by human annotator. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Baseline Results.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results on G EO with 250 training and 250 test examples. Our results are averaged over 10 random 250+250 splits taken from our 600 training examples. Of the three systems that do not use logical forms, our two systems yield significant improvements. Our better sys- tem even outperforms the system that uses logical forms. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 12: Results of the corpus-based model on words with different frequency ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Corpus statistics for Chinese (Zh) character segmentation and English (En)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5 gives an example of the word alignment and phrase alignment of a German English sentence pair.We describe our model using a log-linear modeling approach",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Precision statistics for pronouns.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Composition of two FSTs maintaining separate transitions. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Rank trajectories of 4 LDA inferred topics, with incremental topic inference. The x-axis indicates the utterance number. The y-axis indicates a topic\u2019s rank at each utterance. ",
        "Entity": "Caption"
    },
    {
        "Text": "The corresponding figures for the test data are. 89.53% for our tagger and 88.88% for the TnT tag- ger.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 15 The training, development, and test data for English dependency parsing. ",
        "Entity": "Caption"
    },
    {
        "Text": "Training time comparison.",
        "Entity": "Caption"
    },
    {
        "Text": "The result is shown in Table 4: the baseline numbers without stem features are listed under Base, and the results of the coreference system with stem features are listed under Base+Stem.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Graphical model for the Bayesian Query-Focused Summarization Model. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Illustration of dictionary based segmenta tion finite state transducer 3.1 Bootstrapping.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6: WS: word-segmentation.           Baseline: language-independent features. LexFeat: plus lex- ical features. Numbers are averaged over the 10 ex- periments in Figure 2. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Sample of experimental items for the meta alternation anm-fod. (Abbreviations are listed in Table 2.)",
        "Entity": "Caption"
    },
    {
        "Text": "On the other hand, using our method of combining both sources of information and setting M = \u221e, 19 Chinese words (i.e., the first 22 Chinese words in Table 3 except \u5df4\u4f50\u4e9a,\u5769\u57da,\u666e\u5229\u6cd5) have their correct English translations at rank one position.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: Estimated precision on human-evaluation experiments of the highest-ranked 100 and 1000 results per relation, using stratified samples. \u2018Average\u2019 gives the mean precision of the 10 relations. Key: Syn = syntactic features only. Lex = lexical features only. We use stratified samples because of the overabundance of location-contains instances among our high-confidence results. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 illustrates the effects of different components of the user model by showing results for simulated users who read infinitely fast and accept only predictions having positive benefit (superman); who read normally but accept like superman (rational); and who match the standard user model (real).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2. A Morphophonological Rule",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Examples of unigram and bigram features extracted from Figure 1. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Bilingual training size vs. BLEU score (mid- dle line, left axis) and phrase table composition (top line, right axis) on Arabic Development Set. The baseline BLEU score (bottom line) is included for comparison. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Some examples for MEDLINE tagset: Number of lex. entries per tag and sample words. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Experimental data sets",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: F1 scores (in %) of SegTagDep on CTB- 5c-1 w.r.t. the training epoch (x-axis) and parsing feature weights (in legend). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Different Context Window Size Setting",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Results of Uryupina\u2019s uniqueness classifier",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results for morphological processing, German\u2192English ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Statistics of three test sets.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Frequency distribution for sentence lengths in the WSJ (sections 2\u201323) and the ATB (p1\u20133). English parsing evaluations usually report results on sentences up to length 40. Arabic sentences of up to length 63 would need to be evaluated to account for the same fraction of the data. We propose a limit of 70 words for Arabic parsing evaluations. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 shows the tagging accuracy of unknown words.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Coverage/precision with various rule collections",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4 shows examples of alignment templates",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Results for baseline using introspection and simple statistics of the data (including test data).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6 Evaluation of Feature and Their Combinations",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Optimization & Test Set Pearson Correlation Results",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Statistics of datasets.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 15 Experimental results on the WMT 2009 test set",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Illustration of the alignment of steps.",
        "Entity": "Caption"
    },
    {
        "Text": "of the 43 words are translated to English multi-word phrases (denoted as \u201cphrase\u201d in Table 3).",
        "Entity": "Reference"
    },
    {
        "Text": "By far the most frequent tagging error was the confusion of nominative and accusative case.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Average sentence cover size: the average number of sentences needed to generate the case- frames in a summary sentence (Study 1). Model summaries are shown in darker bars. Peer system numbers that we focus on are in bold. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Word prediction from a partial parse",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 9: Example word structure annotation. We add an \u2018f\u2019 to the POS tags of words with no further structures. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: PARSEVAL scores on the development sets.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5 Recall by Named Entity Class",
        "Entity": "Caption"
    },
    {
        "Text": "The model 1 The total 74,597 sentence pairs used in experiments are those in the FBIS corpus whose English part can be parsed using Charniak (2000)",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Tagging accuracies on test data.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Scores for UPUC corpusFrom those tables, we can see that a simple ma jority voting algorithm produces accuracy that is higher than each individual system and reasonably high F-scores overall.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: Individual Performance of KSs for Disasters",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: LLDA Fmeausres for 3 feature conditions",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Performance of our system versus a baseline",
        "Entity": "Caption"
    },
    {
        "Text": "using the convolution parse tree kernel as depicted in Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 Similarity matrix for segmentation judgments.",
        "Entity": "Reference"
    },
    {
        "Text": "eI K 1 = e1 , ek = eik 1 +1 , .",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Illustration of features f8-12.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 shows the experimental results with and without the stem n-grams features.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3 Algorithm phrase-extract for extracting phrases from a word-aligned sentence pair. Here quasi-consecutive(TP) is a predicate that tests whether the set of words TP is consecutive, with the possible exception of words that are not aligned. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 shows the effect of constraining the maximum length of the alignment templates in the source language.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: The 10 best languages for the particle compo- nent of BANNARD using LCS. ",
        "Entity": "Caption"
    },
    {
        "Text": "As shown in figure 1, a similarity matrix W models one-hop transitions that follow the links from vertices to neighbors.",
        "Entity": "Reference"
    },
    {
        "Text": "(e | f , i, j): p(ei | fj , i 1 i =1 [(i , j) A], j 1 j =1 [(i, j ) A]) (15)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Features based on the token string that are based on the probability of each name class during training.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The value of the penalized loss based on the number of iterations: DPLVMs vs. CRFs on the MSR data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Gender detection accuracies (%) using a 4-gram language model for the letter sequence of          the source name in Latin script. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Example confounders for \u201cfestival\u201d and \u201claws\u201d and their similarities ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3:",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: RM update with margin and bounding con- straints. The diagonal dotted line depicts cost\u2013margin equi- librium. The vertical gray dotted line depicts the bound B. White arrows indicate updates triggered by constraint viola- tions. Squares are data points in the k-best list not selected for update in this round. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Results of POS Guessing of Unknown Words",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 6. German English BLEU scores of various al EM(Co), GS(Co), EM(Co)+GS(Co), and VB(Co). ",
        "Entity": "Caption"
    },
    {
        "Text": "For MUC6, the reduction in error due to global features is 27%, and for MUC7,14%.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Learning curves of systems with different features",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9: Performance comparison with the literature for compound bracketing ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: F-measure after successive addition of each global feature group",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Examples of translations",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11: Performance comparison with the literature for compound interpretation ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Simple parser vs full parser \u2013 syntactic quality. Trained on first 5,000 sentences of the training set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 reports experimental results using lexical features only; we observe that the stemming n-gram features boost the performance by one point (64.7 vs. 65.8).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5 Effect of pruning parameter tp and heuristic function on error rate for direct-translation model (Np = 50,000).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 The accuracies of various word segmentors over the second SIGHAN bakeoff data. ",
        "Entity": "Caption"
    },
    {
        "Text": "The sources of our dictionaries are listed in Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "The statistics of 96 these splits are shown in Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "In order to effectively capture entity-related semantic features, and their combined features as well, especially bi-gram or tri-gram features, we build an Entity-related Semantic Tree (EST) in three ways as illustrated in Figure 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 7: Word segmentation accuracy of unknown words r e c pr ec F Po iss on + bi gr a m W T + P oi ss o n + b i g r a m P O S + P o is s o n + b i g r a m P O S + W T + P o is s o n + bi g ra m 31 .8 45 .5 39 .7 42.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 19 English NE recognition on test data after semi-supervised learning. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Macro-accuracy for cross-lingual bootstrapping",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: BLEU scores achieved with different sets of parallel corpora. All systems are base- line n-code with POS factor models. The follow- ing shorthands are used to denote corpora, : \u201dN\u201d stands for News-Commentary, \u201dE\u201d for Europarl, \u201dC\u201d for CommonCrawl, \u201dU\u201d for UN and (nf) for non filtered corpora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Topics with MWEs",
        "Entity": "Caption"
    },
    {
        "Text": "Results: Table I gives the results for the comparison of the troughs placed by the segmentation algorithm to the known subject change points.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Examples of the semantic role features assuming that the semantic roles have been tagged for the source sentences.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5 shows the decoding algorithm incorporating the SRR features.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1. Upper triangle of the sentence-similarity matrix.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5. Bootstrapping time for different p values ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1. Complementarity",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: The solid line shows recall-at-1220 when com- bining the k best-performing bilingual statistics and three monolingual statistics. The dotted line shows the indi- vidual performance of the kth best-performing bilingual statistic, when applied in isolation to rank candidates. ",
        "Entity": "Caption"
    },
    {
        "Text": "For example, no synset covers any combinations of the main words in Figure 2, namely buy , acquire and merger",
        "Entity": "Reference"
    },
    {
        "Text": "vecI : IL Rk instance vector computation C : Rk m Rk centroid computation vecL : L Rk lemma (type) vector computation repM : M Rk meta sense representation Table 3: Additional notation and signatures for CAM explicit sense disambiguation, CAM represents lemmas by their type vectors, i.e., the centroid of their instances, and compares their vectors (attributes) to those of the meta alternation hence the name.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Comparison of different methods on ACE 2004 data set. P, R and F stand for precision, recall and F1, respectively. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Per category performance of the Berkeley parser on sentence lengths \u2264 70 (dev set, gold segmentation). (a) Of the high frequency phrasal categories, ADJP and SBAR are the hardest to parse. We showed in \u00a72 that lexical ambiguity explains the underperformance of these categories. (b) POS tagging accuracy is lowest for maSdar verbal nouns (VBG,VN) and adjectives (e.g., JJ). Richer tag sets have been suggested for modeling morphologically complex distinctions (Diab, 2007), but we find that linguistically rich tag sets do not help parsing. (c) Coordination ambiguity is shown in dependency scores by e.g., \u2217SSS R) and \u2217NP NP NP R). \u2217NP NP PP R) and \u2217NP NP ADJP R) are both iDafa attachment.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Recall (R), Precision (P) and Mean Average Pre- cision (MAP) when also using rules for matching. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: The deductive system for Earley\u2019s genera- tion algorithm ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8: Relative word accuracy as a function of training set size.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Results on Penn (English) Treebank, Wall Street Journal, sentences with 100 words or fewer.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Clustering performance on the predominant senses, with and without prepositions. The last entry presents the per- formance of random clustering with K = 25, which yielded the best results among the three values K=25, 35 and 42. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: Meta alternations and their average precision values for the task. The random baseline performs at 0.313 while the frequency baseline ranges from 0.255 to 0.369 with a mean of 0.291. Alternations for which the model outperforms the frequency baseline are in boldface (mean AP: 0.399, standard deviation: 0.119). ",
        "Entity": "Caption"
    },
    {
        "Text": "We include a list of per-category results for selected phrasal labels, POS tags, and dependencies in Table 8.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Experiment 1: Results for label unknown sense, WSD confidence level approach. \u03b8: confi- dence threshold. \u03c3: std. dev. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10 The comparison of overall accuracies of various joint segmentor and POS-taggers by 10-fold cross validation using CTB. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: The STTS tags PDAT and ART, their rep- resentation in the Annotation Model and linking with the Reference Model. ",
        "Entity": "Caption"
    },
    {
        "Text": "figure)",
        "Entity": "Reference"
    },
    {
        "Text": "Table 9: Dev set results for sentences of length \u2264 70.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Examples of two- to seven-word bilingual phrases obtained by applying the algorithm phrase-extract to the alignment of Figure 2",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 2 Example of CCG supertags. CCG supertags are combined under the operations of forward and backward applications into a parse tree ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: BLEU scores of English to Russian ma- chine translation system evaluated on tst2012 us- ing baseline GIZA++ alignment and translitera- tion augmented-GIZA++. OOV-TI presents the score of the system trained using TA-GIZA++ af- ter transliterating OOVs ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Our boosted ranker combining monolingual and bilingual features (bottom) compared to three base- lines (top) gives comparable performance to the human- curated upper bound. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: MRR performance of phonetic translit- eration for 3 corpora using unigram and bigram                 language models. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Accuracy with different sizes of labeled data",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Features used by the polyglot ranking system.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Accuracy of our system in each period (M = 10)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Comparison of MERT, PRO, and MIRA on tuning Urdu-English SBMT systems, and test results at every iteration. PRO performs comparably to MERT and MIRA. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Distribution of inferred alignment fertilities. The four blocks of rows from top to bottom correspond to (in order) the total number of source tokens, source tokens with fertilities in the range 4\u20137, source tokens with fertil- ities higher than 7, and the maximum observed fertility. The first language listed is the source in alignment (Sec- tion 2). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Individual Performance of KSs for Terrorism",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Some of the words extracted from the small corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 Architecture of the statistical translation approach based on Bayes decision rule.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 Classes of words found by ST for the test corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4 Examples of alignment templates obtained in training",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Labeled precision and recall for the three types of labels. The line labeled \u2018Flat*\u2019 is for unlabeled met- rics of flat words, which is effectively the ordinary word segmentation accuracy. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Estimation of model parameters. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Evaluation of topic segmentation for the French corpus (Pk and WD as percentages) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3 show the training time for different models.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: the largest clusters from partitioning the second order graph with CW. ",
        "Entity": "Caption"
    },
    {
        "Text": "Ta SegTag 97.66 93.61 SegTagDep 97.73 94.46 SegTag(d) 98.18 94.08 SegTagDep(d) 98.26 94.64 Table 5: Final results on CTB5j 76 75 74 ble 4 shows the segmentation, POS tagging, and dependency parsing F1 scores of these models on CTB5c.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Different tree span categories with SPT (dotted circle) and an ex- ample of the dynamic context-sensitive tree span (solid circle) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: AER results",
        "Entity": "Caption"
    },
    {
        "Text": "The probability of using an alignment template to translate a specific source language phrase f is estimated by means of relative frequency",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2. Learning Curves for Confusable Disambiguation",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Rhetorical pattern of C-Semicolon",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Parsing accuracy (AS = attachment score, EM = exact match; U = unlabeled, L = labeled)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: This log-log plot shows that there are many rare features and few common features. The probability that a feature occurs in x number of N- best lists behaves according to the power-law x\u2212\u03b1 , where \u03b1 = 2.28. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Accuracy (%) of permutation detection experiment with various entity representations us- ing manual and automatic annotations of topolog- ical fields and grammatical roles on subset of cor- pus used by Filippova and Strube (2007a). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: target word frequency (F), average",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Average rank of correct translation according to average source term frequency ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2 The effect of varying the number of samples (k) on accuracy. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Values obtained for Precision, Recall and F- score with method 1 by changing the threshold frequency of the correspondences and applying a post-filter. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: System Architecture.",
        "Entity": "Caption"
    },
    {
        "Text": "As shown in Table 3, using just context information alone, 10 Chinese words (the first 10) have their correct English translations at rank one position.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Example of Lattice Used in the Markov Model-Based Method",
        "Entity": "Caption"
    },
    {
        "Text": "Improvements of different tree setups over SPT on the ACE RDC 2004 corpus Finally, Table 4 compares our system with other state-of-the-art kernel-based systems on the 7 relation types of the ACE RDC 2004 corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Ungrammatical Arabic output of Google Trans- late for the English input The car goes quickly. The subject should agree with the verb in both gender and number, but the verb has masculine inflection. For clarity, the Arabic tokens are arranged left-to-right. ",
        "Entity": "Caption"
    },
    {
        "Text": "The overall architecture of the statistical translation approach is summarized in Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Effect of model parameters on performance.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5: MM and MMVC performance by performing 5- fold cross validation on S ENSEVAL -2 data for 4 languages ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8: Surface composition of embedded structures.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Numbers of projected instances",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Final results on CTB6 and CTB7 accuracies of POS tagging and dependency parsing were remarkably improved by 0.6% and 2.4%, respectively corresponding to 8.3% and 10.2% error reduction.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Results from WSD system applied to various sections of the NLM-WSD data set using a variety of fea- tures and machine learning algorithms. Results from baseline and previously published approaches are included for comparison. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: With the English tree and alignment provided by a parser and aligner at test time, the Chinese parser finds the correct dependencies (see \u00a76). A monolingual parser\u2019s incor- rect edges are shown with dashed lines. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. The initial performance of applying various sampling strategies to selecting the initial ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Evaluation scores for in-domain and out- of-domain test sets, averaged over all systems ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Evaluating Word Alignment Accuracies and Machine Translation Qualities for BiTAM Models, IBM Models, HMMs, and boosted BiTAMs using all the training data listed in Table. 1. Other experimental conditions are similar to Table. 4. ",
        "Entity": "Caption"
    },
    {
        "Text": "As we will see from Table 3, not much improvement is derived from this feature.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Accuracy results for binary decisions.",
        "Entity": "Caption"
    },
    {
        "Text": "For MUC7, there are also no published results on systems trained on only the official training data of 200 aviation disaster articles.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: (a) An undirected graph G representing the similarity matrix; (b) The bipartite graph showing three clusters on G; (c) The induced clusters U ; (d) The new graph G1 over clusters U ; (e) The new bipartite graph over G1 ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 29: Different Arabic Transliterations of \"Los Angeles\"",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 shows the number of sentences, words, and characters of the training and test sets.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1, Figure 1, and Figure 2 shows the AER results for different models.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Confusion matrix of acquired nouns.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8. System performance on the production relation on the CHEM dataset. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9: Example patterns of nominal interaction keywords ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: De\ufb01nition of NE in IREX.",
        "Entity": "Caption"
    },
    {
        "Text": "The alignment aJ that has the highest probability (under a certain model) is also called the Viterbi alignment (of that model)",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Examples of common character bigrams for each part of speech in the infrequent words pa rt of sp ee ch ch ar ac ter bi gr a m fre qu en cy no un nu m be r a dj e ct iv al v er b v er b ad je cti ve ad ve rb < e o w > <b o w > 1 S \" J < e o w > I t < e o w > L < e o w > < e o w > 13 43 4 8 4 3 2 7 2 1 3 69 63 resented all unknown words by one length model.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Comparison of average per-document ter- comTER with invWER on the EVAL07 GALE Newswire (\u201cNW\u201d) and Weblogs (\u201cWB\u201d) data sets. ",
        "Entity": "Caption"
    },
    {
        "Text": "MUC7 test accuracy.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 12 Experiments combining dependency relations, words and part-of-speech",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Relation Extraction Results. Models using hidden constituency syntax provide significant gains over the syntactically-uniformed baseline model in both languages, but the advantages of the latent syntax were mitigated on the smaller Chinese data set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Again, Table 2 shows that using stem n-grams features gave a small boost to the whole main-type classification system4.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Distribution of generated paraphrases per Lev- enshtein distance ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Classification results with XLE starredness, parser exceptions and zero parses (Method 1) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A fragment of an entailment graph (a), its SCC graph (b) and its reduced graph (c). Nodes are predicates with typed variables (see Section 5), which are omitted in (b) and (c) for compactness. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2 A latent layered POS tag representation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2. F1-measure with \uf062 in [0,1]",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Translation results for English-French",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2 plots AP by for all meta alternations.",
        "Entity": "Reference"
    },
    {
        "Text": "Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006) and the CRF model using minimum subword-based tagging, both of which are statistical methods, are used individually to solve the Figure 1: Outline of the segmentation process 2.1 Forward Maximum Matching.",
        "Entity": "Reference"
    },
    {
        "Text": "The third model is Equation (11), which is a set of word models trained for each word type (WT +Poisson+ bigram).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Illustration on temporality",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5 A comparison between ILP-Global and Greedy-Global. Parts A1\u2013A3 depict the incremental progress of Greedy Global for a fragment of the headache graph. Part B depicts the corresponding fragment in ILP-Global. Nodes surrounded by a bold oval shape are strongly connected components. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Example of inference rules needed in RTE ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Bibliome system scores at Bacteria Biotope          Task in BioNLP shared tasks 2011. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Instructions for judging of unsharpened factoids.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5: Procedure for scoring agreement for each hy- pothesis generated during the search algorithm of Fig. 4. In the extended hypothesis eI1 , the index n + 1 indicates the start of the new attachment. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Types of message speech acts in corpus.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Example of DIRT algorithm output. Most confident paraphrases of X put emphasis on Y ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Activity detection: Activities are detected on the Santa Barbara Corpus (SBC) and the meet- ing database (meet) either without clustering the activities (all) or clustering them according to their interactivity (interactive) (see Sec. 2 for details). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Relation extraction rules used by Gamallo et.al.[36]",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 A layered POS tag representation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5 NP agreement violations that were caught by the agreement filter system. (a) Noun-compound case that was correctly handled. (b) Case involving conjunction that was correctly handled. (c) A case where fixing the agreement violation introduces a PP-attachment mistake. ",
        "Entity": "Caption"
    },
    {
        "Text": "W can be encoded by a undi rected graph G (Figure 1(a)), where the verbs are mapped to vertices and the Wij is the edge weight between vertices i and j.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4: Definition of the operations used to transform the structure of the underspecified logical form l0 to match the ontology O. The function type(c) calculates a constant c\u2019s type. The function freev(lf ) returns the set of variables that are free in lf (not bound by a lambda term or quantifier). The function subexps(lf ) generates the set of all subexpressions of the lambda calculus expression lf . ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Self-bootstrapping algorithm",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 14 Reduplication for n = 4. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: The effect of language and gender in-",
        "Entity": "Caption"
    },
    {
        "Text": "we directly model the posterior probability Pr(eI| f J )",
        "Entity": "Reference"
    },
    {
        "Text": "Fig. 6. F-measure for the objective and subjective classes for multilingual bootstrapping (versus cross-lingual framework).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Effect of semantic alignments",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Dependency structure of text. Tree skeleton in bold ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11: Effects of Popularity of Morphs",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Results of combining the character-category association and rule-based models: best guess ",
        "Entity": "Caption"
    },
    {
        "Text": "As Figure 3 shows, word type information improves the prediction accuracy significantly.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Logical form graph.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: S/O classifier with and without SWSD.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Comparison of our system with other best-reported systems on the ACE corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Parser performance on WSJ;23, supervised adaptation. All models use Brown;T,H as the out-of-domain treebank. Baseline models are built from the fractions of WSJ;2-21, with no out-of-domain treebank. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Performance on Ar-En with basic (left) and sparse (right) feature sets on MT05 and MT08.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Comparison to SVM.",
        "Entity": "Caption"
    },
    {
        "Text": "and Table 6)",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: Performance of baseline and joint models w.r.t. the average processing time (in sec.) per sen- tence. Each point corresponds to the beam size of 4, 8, 16, 32, (64). The beam size of 16 is used for SegTag in SegTag+Dep and SegTag+TagDep. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: The performances of the transliteration models and their comparison on EMatch. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 12 Interdigitation FSRA \u2013 general. ",
        "Entity": "Caption"
    },
    {
        "Text": "Column four (MBL) in table 2 shows the accuracy obtained with all features in the general feature space.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Lexical variations creating new rules based on DIRT rule X face threat of Y \u2192 X at risk of Y ",
        "Entity": "Caption"
    },
    {
        "Text": "However, the spelling model, especially the character bigrams in Equation (17) are hard to es timate because of the data sparseness.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Precision and recall for articles.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A Generic Morphological Analyzer as a Black Box ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Source span lengths",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 Arabic standard parsing experiments (test set, sentences \u2264 40 words). SplitPCFG is the same grammar used in the Stanford parser, but without the dependency model. FactLex uses basic POS tags predicted by the parser and morphological analyses from MADA. FactLex* uses gold morphological analyses. Berkeley and DP-TSG results are the average of three independent runs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Comparative precision values for the top 20 similarity lists of the three selected similarity measures, with MI and Bootstrapped feature weighting for each. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Performance of TL-comb and TL-auto as H changes. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11: Term variation examples",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5. Chinese system performance with   system mentions and system relations ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results on Penn (Chinese) Treebank.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 k-means experiment baseline and upper bound. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Key notation. Feature factorings are elaborated in Tab. 2.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Average of Weights Results.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: A sentence decomposed into its depen- dency edges, and the caseframes derived from those edges that we consider (in black). ",
        "Entity": "Caption"
    },
    {
        "Text": "the time-consuming renormalization in equation (3) is not needed in search",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: The NLM-WSD test set and some of its sub- sets. Note that the test set used by (Joshi et al., 2005) comprises the set union of the terms used by (Liu et al., 2004) and (Leroy and Rindflesch, 2005) while the \u201ccom- mon subset\u201d is formed from their intersection. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: BLEU scores for the French-to-English translation task measured on nt10 with systems tuned on development sets selected according to their original language (adapted tuning). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Space comparison between FSAs and FSRAs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: PTB vs. Wiktionary type coverage across sec- tions of the Brown corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Optimized TERp Edit Costs",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8a shows that the best model recovers SBAR at only 71.0% F1.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4 Differences in performance between our system and Wang, Li, and Chang (1992).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Particles and prepositions allowed in phrasal verbs gathered from Wiktionary. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "Fig. 5. Macro-accuracy for multilingual bootstrapping (versus cross-lingual framework).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Performance of the mention detection system using lexical features only.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Pearson\u2019s r and Kendall\u2019s \u03c4 (absolute) between adequacy and automatic evaluation measures on different levels of the MATR MT06 data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 24: Splitting Compounds in Russian",
        "Entity": "Caption"
    },
    {
        "Text": "equation (1)",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Word length distribution of kanji words and katakana words length model does not reflect the variation of the word length distribution resulting from the Japanese orthography.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Mixed Membership MEDLDA",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Experimental results using different phrase ta- bles. OutBp: the out-of-domain phrase table. AdapBp: the adapted phrase table. ",
        "Entity": "Caption"
    },
    {
        "Text": "In all four tables, we provide theresults for using no heuristic functions and three variants of an increasingly infor mative heuristic function.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6: Positive and negative examples",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 3. An example of the mutual reinforcement between P r(para(ti , tj )) and P r(coord(ek , eg )). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Effect of supplementing recasing model training data with the test set source. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Maximally Accurate Assignment",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. MAP of different IR systems with differ- ent segmenters. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The trigram version of our language model rep- resented as a graphical model. G1w is the unigram model of \u00a72.2. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 \u2013 Precision/recall for Le Monde corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: DRS for the sentence \u201cI saw nothing suspi- cious\u201d ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: An example where syntactic features help to link the PRO mention \u00d1\u00eb (hm) with its antecedent, the NAM                   ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Candidates for equivalence classes.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Perplexity results for the immediate- bihead model ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: Dev set learning curves for sentence lengths \u2264 70. All three curves remain steep at the maximum training set size of 18818 trees.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Examples of word bigrams including un known word tags example",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1, Figure 1, and Figure 2 shows the AER results for different models.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 6: Results with varying sizes of training data. ",
        "Entity": "Caption"
    },
    {
        "Text": "= argmax S I n s=1 a l) p (fs , a | es ) (7)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Correlations of resolution class scores with respect to the average. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Proposed method: data flow.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Results using 5-fold cross validation on S ENSEVAL- 1 training data (English) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Character- and word-based features of a possi- ble word wi over the input character sequence c. Suppose that wi = ci0 ci1 ci2 , and its preceding and following char- acters are cl and cr respectively. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: The middle node gets the grey or the black class. Small numbers denote edge weights. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8: GeoQuery Results",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five trials on CTB5c.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 7 Algorithm for breadth-first search with pruning. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5 Example output of our model for Chinese\u2192English translation. The word-segmented Chinese sentence and dependency tree are inputs. Our model\u2019s outputs include the English translation, phrase segmentations for each sentence (a box surrounds each phrase), a one-to-one alignment between the English and Chinese phrases, and a projective dependency tree on the English phrases. Note that the Chinese dependency tree is on words whereas the English dependency tree is on phrases. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: F1 per coarse relation type (ACE 2005). SYS is the final model, i.e. last row (PET+PET WC+PET LSA) of Table 5. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 Procedural description to compute the set Succ of successor hypotheses by which to extend a partial hypothesis (S, C, j). ",
        "Entity": "Caption"
    },
    {
        "Text": "Pr(eI | f J ) = p M (eI | f J )",
        "Entity": "Caption"
    },
    {
        "Text": "Equation (12), which is a set of word models trained for each part of speech (POS + Poisson + bigram).",
        "Entity": "Reference"
    },
    {
        "Text": "Fig. 2 Metric correlations with adequacy on the MetricsMATR 2008 development set. Correlations are significantly different if the center point of one correlation does not lie within the confidence interval of the other correlation ",
        "Entity": "Caption"
    },
    {
        "Text": "(equation 2",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: English word perplexity (PPL) on the RT04 test set using a unigram LM. ",
        "Entity": "Caption"
    },
    {
        "Text": "For the graph depicted in Figure 1 this algorithm computes the clusters {They, Leaders}, {Paris} and {recent developments}.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Lexicalize and generalize operators over t1 (part) in Figure 1. Although here only shows the nodes, we also need to change relative edges actually. (1) Applying lexicalize operator on the non-terminal node X0,1 in (a) results a new derivation shown in (b). (2) When visiting bei in (b), the generalize operator changes the derivation into (c). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Precision statistics for pronouns. Rows are pronoun surfaces, columns number of cluster- ing decisions and percentage of wrong decisions for all and only anaphoric pronouns respectively. ",
        "Entity": "Caption"
    },
    {
        "Text": "Collocations were automatically located in a text by looking up pairwise words in this lexicon",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Results on the Arabic Treebank (ATB) data set: We compare our models against Poon et al. (2009) (PCT09) and the Morfessor system (Morfessor-CAT). For our full model (+T OKEN -S EG) and its simplifica- tions (BASIC, +POS, +T OKEN -POS), we perform five random restarts and show the mean scores. The sample standard deviations are shown in brackets. The last col- umn shows results of a paired t-test against the preceding model: ++ (significant at 1%), + (significant at 5%), \u223c (not significant), - (test not applicable). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Distribution of annotated data.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Three narrative events and the six most likely events to include in the same chain. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 shows that tagging precision is im proved from 88.2% to 96.6%.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Final system results (as F1 scores) where IM is identification of mentions and S - Setting. For more details cf. (Recasens et al., 2010). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Accuracy of the proposed system on a set which contains misspelled queries that the users had reformulated ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 26: Arabic Tokenization Schemes",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2 shows the labeled dependency graph of example (2), taken from Talbanken05.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8 Numbers of parse-tree nodes in the 1-best parses of the development set that triggered gender or number agreement checks, and the results of these checks. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Comparison of difference systems on the      performs the state-of-the-art Collins and Duffy\u2019s con- ACE RDC 2003 corpus over both 5 types (outside the    volution tree kernel. It also shows that feature-based parentheses) and 24 subtypes (inside the parentheses) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Learning curves: word-segmentation F- measure and parsing label F-measure vs. percentage of training data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The Buckwalter Arabic Morphological Analyzer\u2019s lookup process exemplified for the word lilkitAbi.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: MUC-7: Level Distribution of Each of the Facts",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5. Some subtrees from trees in figure 4",
        "Entity": "Caption"
    },
    {
        "Text": "We establish a corresponding feature function by multiplying the probability of all used alignment templates and taking the logarithm",
        "Entity": "Reference"
    },
    {
        "Text": "equation 2) says that the prob ability of starting a new entity, given the current mention m and the previous entities e1, e2, , et, is simply 1 minus the maximum link probability between the current mention and one of the previous entities.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Comparison of three statistical translation approaches (test on text input: 251 sentences = 2197 words + 430 punctuation marks). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: F-score of two segmenters, with (\u2212) and without (+) word token/type features. ",
        "Entity": "Caption"
    },
    {
        "Text": "The use of the language model feature in equation (18) helps take long-range dependencies better into account",
        "Entity": "Reference"
    },
    {
        "Text": "Table 15 Effect of adjacent contextual (non-NE) bigrams on the test set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Statistical Information of Corpora",
        "Entity": "Caption"
    },
    {
        "Text": "The bipartite graph K also induces a similarityagain (Figure 1(e)).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: An example dependency parse tree rep- resentation. The subgraph represents a dependency relation feature between arg 1 \u201cPalestinians\u201d and \u201cof\u201d. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Recall (R), Precision (P) and Mean Average Pre- cision (MAP) when only matching template hypotheses directly. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 Results for Mutiple Document System",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 NEA type-insensitive (type-sensitive) performance with a different English NE recognizer and another Chinese NE recognizer. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 3: NEs after agents-based modification",
        "Entity": "Caption"
    },
    {
        "Text": "and ap plied to an Arabic sentence in figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3:    Extend feature templates: G fA&@         k : : :\u0082l - is the root constituent label of th ",
        "Entity": "Caption"
    },
    {
        "Text": "For example, in the sentence bought one of town s two meat- packing plants as illustrated in Figure 1(a), the constituents before the headword plants can be removed from the parse tree.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4: Weather Text Evaluations.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Step 2",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Characteristics of the parallel corpus used for experiments. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: InvR scores ranked by difference, Giga- word to Web Corpus ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4. System performance on the part-of relation on the TREC-9 dataset. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Signature caseframe densities for differ- ent sets of summarizers, for the initial and update guided summarization tasks (Study 2). \u2217 : p < 0.005. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Evaluation setup",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Evaluation results for links",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Performance comparison on the ACE 2003/2003 data over both 5 major types (the numbers outside parentheses) and 24 subtypes (the numbers in parentheses) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: cumulative distribution of frequency (CDF) of the relative ranking of model-predicted probability of being positive for false negatives in a pool mixed of false negatives and true negatives; and the CDF of the relative ranking of model-predicted probability of being negative for false positives in a pool mixed of false positives and true positives. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: During training, a classified instance (in this case for the confusible pair {then , than }) are generated from a sentence. During testing, a similar instance is generated. The classifier decides what the corresponding class, and hence, which word should be the focus word. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A Motivating Example",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Tagging accuracy on the gold-standard normalizations (OrigP = original punctuation, ModP = modern punctuation, NoP = no punctu- ation) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Contribution of the static cache on the first            sentence of each test document           (i.e. with empty dynamic cache) ",
        "Entity": "Caption"
    },
    {
        "Text": "The argmax operation denotes the search problem, that is, the generation of the output sentence in the target language",
        "Entity": "Reference"
    },
    {
        "Text": "To illustrate the complete user model, in the figure 1 example the benefit of accepting would be7 2 4.2 = .8 keystrokes and the benefit of reject ing would be .2 keystrokes.",
        "Entity": "Reference"
    },
    {
        "Text": "the target Chen Guangcheng only appears once in Weibo",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Two STs composing a STN",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8. EDT and mention detection results.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Examples of DTs and their ICD-codes.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A correct tree (tree1) and an incorrect tree (tree2) for \u201cBCLM HNEIM\u201d, indexed by terminal boundaries. Erroneous nodes in the parse hypothesis are marked in italics. Missing nodes from the hypothesis are marked in bold. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: POS tagging experiments with pruned and unpruned CRFs with different orders n. For every language the training time in minutes (TT) and the POS accuracy (ACC) are given. * indicates models significantly better than CRF (first line). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: (a) A Chinese sentence. Slashes indicate word boundaries. (b) An output of our word segmentation system. Square brackets indicate word boundaries. + indicates a morpheme boundary. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: CCG and LTAG supertag sequences.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Features used in our parsing models.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Notation used in this paper. The convention eIi indicates a subsequence of a length I sequence. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Verb Stem Alternation",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Filters applied to candidate pair (H, S)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Example input and best output found",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Sample seeds used for each semantic relation and sample outputs from Espresso. The number in the parentheses for each relation denotes the total number of seeds. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5. Impact of Data Selection (Chinese)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 shows the performance and speed of the full joint model (with no dictionaries) on CTB5c1 with respect to the beam size.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 8 Word sense disambiguation accuracy for \u201cNP1 V to NP2 NP3\u201d frame. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Arabic Nominal Inflection - Broken Plural",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Distribution of the sentences where the semantic role features give no/positive/negative impact to the sentence fluency in terms of the completeness and ordering of the semantic roles.classes in VerbNet (Dang et al., 1998).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Russian to English machine translation system evaluated on tst2012 and tst2013. Human evaluation in WMT13 is performed on the system trained using the original corpus with TA-GIZA++ for alignment (marked with *) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Probabilistic Approaches",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Step 4",
        "Entity": "Caption"
    },
    {
        "Text": "An extract of the results is listed in table 1",
        "Entity": "Reference"
    },
    {
        "Text": "14http://www.cis.upenn.edu/ dbikel/software.html Gold standard Automatic UAS LAS UAS LAS Baseline 89.87 84.92 89.87 84.92 Anim 89.81 84.94 89.87 84.99 Table 5: Overall results in experiments with automatic features compared to gold standard features, expressed as unlabeled and labeled attachment scores.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: Examples for the effect of the combined lexica.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 shows the cross entropy per word and char acter perplexity of three unknown word model.",
        "Entity": "Reference"
    },
    {
        "Text": "The tagset refinement increases the accuracy by about 0.6%, and the external lexicon by another 3.5%.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Scores for MSRA corpus",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6: Results when using unsupervised dependency parsers. Cells contain averaged % BLEU on the three test sets and % BLEU on tuning data (MT03) in parentheses. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "Fig. 4 The dependency parse tree of the English sentence Can you play my favourite old record? and the dependency features extracted from it for the SMT phrase play my favourite ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Feature set comparison (BLEU).",
        "Entity": "Caption"
    },
    {
        "Text": "Figures 4",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Results",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 The prevalence ranking process for the noun star. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Evaluation closed results on all data sets",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Derivation with Hierarchical model",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Comparison our results with the best ones from Sighan Bakeoff 2005 in terms of F-score ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Speed in lookups per microsecond by data structure and number of 64-bit entries. Performance dips as each data structure outgrows the processor\u2019s 12 MB L2 cache. Among hash tables, indicated by shapes, probing is initially slower but converges to 43% faster than un- ordered or hash set. Interpolation search has a more ex- pensive pivot function but does less reads and iterations, so it is initially slower than binary search and set, but be- comes faster above 4096 entries. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A simplified version in Foma source code of the regular expressions and transducers used to bypass several sources of distortion within the DTs in order to parse variations of unseen input DTs. ",
        "Entity": "Caption"
    },
    {
        "Text": "This is usually straightforward, with the exception of the case where the words that are aligned to a particular role s span in the source side are not continuous in the target side, as shown in Figure 4.",
        "Entity": "Reference"
    },
    {
        "Text": "                                                                               \u00a9\u00b0          \u00a92\u00c1 Figure 4: WSD example showing the utility of the MVC method. A sense with a high variational coefficient is preferred to the mode     of the MM distribution (the fields corresponding to the true sense are highlighted) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Number of search iterations (left) and total number of alignments considered (right) during search in depen- dence of input length. The data is taken from the translation of the Chinese testset from the TIDES MT evaluation in June 2002. Translations were performed with a maximum swap distance of 2 and a maximum swap segment size of 5. ",
        "Entity": "Caption"
    },
    {
        "Text": "PRO (c) Entity-Paired Tree(EPT)Figure 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6: Results on a truly independent test set, consisting of data harvested from Egyptian Facebook pages that are entirely distinct from the our dialectal training set. The improvements over the MSA baseline are still considerable: +2.9 BLEU points when no Facebook data is available for tuning and +2.7 with a Facebook tuning set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Results of different feature groups under the TC model for N-pron resolution ",
        "Entity": "Caption"
    },
    {
        "Text": "Collocation: Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio (Church and Hanks, 1990) and outputted to a lexicon.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Distribution of Class Labels in the WSJ Section of the Penn TreeBank. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7: A comparison between QA semantic parsing approaches[12]",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 describes the components and how this system works.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Percentage of non-projective arcs recovered correctly (number of labels in parentheses)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4. All binary trees for NNS VBD JJ NNS         (Investors suffered heavy losses) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: precision with and without feature",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: An example CCG parse obtained from [60]",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Taxonomy of Chinese words used in developing MSRSeg. ",
        "Entity": "Caption"
    },
    {
        "Text": "The results when we set M = 10 are shown in Table 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Composition Gold Standards",
        "Entity": "Caption"
    },
    {
        "Text": "Table 12 Precision and partial recall of word lengths two to seven of the second experiment on IT and AV. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Snapshot of the supersense-annotated data.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Histogram of token movement size ver- sus its occurrences performed by the model Neu- big on the source english data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Rules for morphological simplification.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10. MRRs for the phonetic transliteration 2",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3 shows the performance and processing time comparison of various models and their combinations.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5: M-1 accuracy vs. number of samples.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Results on the standard 14 CSSC data sets",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Translation Candidates for \u8e81\u9b31\u75c5 (manic- depression) ",
        "Entity": "Caption"
    },
    {
        "Text": "On the contrary, in the above training stage, although the samples are not accurate enough to represent the distribution defined by Equation 7 for each alignment aj , it is accurate enough for computing the expected counts, which are defined at the corpus level.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Feature sets used for learning relationships. The size of a set is the number of features in that set.",
        "Entity": "Caption"
    },
    {
        "Text": "Prec. is the precision.",
        "Entity": "Reference"
    },
    {
        "Text": "The second model is Equa tion (13), which is a set of word models trained for",
        "Entity": "Reference"
    },
    {
        "Text": "Table 10 Accuracy of semantic-role prediction (in percentages) for known boundaries (the system is given the constituents to classify). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Filtering results using the naive Bayes classifier. The number of entity candidates for the training set was 4179662, and that of the develop- ment set was 418628. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Syntagmatic vs. paradigmatic axes for words in a simple sentence (Chandler, 2007). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Parallel topics extracted by the bLSA model. Top words on the Chinese side are translated into English for illustration purpose. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5:",
        "Entity": "Reference"
    },
    {
        "Text": "Table 27: Performance comparison between original and universal tagsets",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Visualisation examples. Top: named en- tity recognition, middle: dependency syntax, bot- tom: verb frames. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: The pool of features for all languages.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4. Pseudo-code to extract UW",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 Effect of pruning parameter Np and heuristic function on error rate for direct-translation model (tp = 10 12 ).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8: Examples of the MT outputs with and without SRFs. The first and second example shows that SRFs improve the completeness and the ordering of the MT outputs respectively, the third example shows that SRFs improve both properties. The subscripts of each Chinese phrase show their aligned words in English. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: An example of words and their bit string representations obtained in this paper. Words in bold are head words that appeared in Table 1. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 A framework for jointly identifying and aligning bilingual NEs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Effect of text type on error models to speed (in seconds per 10,000 word-forms) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Definitions for the top four senses of \u201claw\u201d according to WordNet ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2. The procedure of TBL entity track- ing/coreference model ",
        "Entity": "Caption"
    },
    {
        "Text": "L set of lemmas IL set of (lemma-wise) instances SL set of (lemma-wise) senses inst : L (IL ) mapping lemma instances sns : L (SL ) mapping lemma senses M set of meta senses meta : SL M mapping senses meta senses A M M set of meta alternations (MAs) A set of MA representations score : A S2 R scoring function for MAs repA : A A MA representation function comp : A S2 R compatibility function Table 1: Notation and signatures for our framework.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1. Removal and reduction of constituents using dependencies",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 shows the distribution of character type sequences that constitute the infrequent words in the EDR corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "hAL(eI , f J , K , zK ) = |j 1 j | (16) 1 1 1 1 k k=1 k 1",
        "Entity": "Caption"
    },
    {
        "Text": "The graph displayed in Figure 1 is the graph constructed for the mentions Leaders, Paris, recent developments and They from the example sentence at the beginning of this Section, where R = {P AnaPron, P Subject, N Number}.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Pseudo-word discrimination performance",
        "Entity": "Caption"
    },
    {
        "Text": "Considering that the way the semantic where all(T ) denotes all the possible target strings which can be generated from the source tree T . Given a set of TTS templates, the new partition function can be efficiently computed using the dynamic programming algorithm shown in Figure 7.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Distribution of activity types: Both databases contain a lot of discussing, informing and story-telling activities however the meeting data contains a lot more planning and advising. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Numbers of rules in Hiero or phrase-pairs in Moses. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Translation performance of baseline and bLSA-Adapted Chinese-English SMT systems on manual transcriptions and 1-best ASR hypotheses ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Results by relation",
        "Entity": "Caption"
    },
    {
        "Text": "The direct translation probability is given by",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Question-specific popular topic words and opinion words generated by Opinion HITS ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Performance analysis of HRGs, CWU, CWW & HAC for different parameter combinations (Table 2). (A) All combinations of p1 , p2 and p3 = 0.05. (B) All combinations of p1 , p2 and p3 = 0.09. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3. Another derivation yielding same tree",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 9 Transition-based feature context for the dependency parser. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: BLC for WN1.6 using all or hyponym relations",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Distribution of dialogue acts in our dataset.",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 2. Collecting paraphrases using a Paraphrase Recognizer.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: German\u2013English translation results. Results are cumulative. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Mixed-case TER and BLEU, and lower- case METEOR scores on Arabic NIST MT05. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Learning curve of the hierarchical strategy and its comparison with the flat strategy for some      major relation subtypes (Note: FS for the flat strategy and HS for the hierarchical strategy) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 The similarity score features used to represent pairs of templates. The columns specify the corpus over which the similarity score was computed, the template representation, the similarity measure employed, and the feature representation (as described in Section 4.1). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Sense-tagged corpus for the example in Figure 3",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Examples of correct (above) and incorrect (below) alignments ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Total documents",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Posterior likelihood at each of the first 100 iter- ations, from 4 runs (with different random seeds) on 10% of the Morphochallenge dataset (\u03b1i6=j = 0.001, \u03b1i=j = 100, \u03b2 = 0.1), indicating convergence within the first 15 iterations. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 lists both the success rate maximally achievable (broken down according to different types of pronouns) and the average number of antecedents remaining after applying each factor.",
        "Entity": "Reference"
    },
    {
        "Text": "(Abbreviations are listed in Table 2.)",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: R-iv and R-oov varing as the confidence threshold, t.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Performance of Unsupervised Name Mining",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3 Positional sentence weight for varying",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 displays the performance of our model and of the systems that obtained the best (Fernandes et al., 2012) and the median performance in the MUC B3 CEAFe average R P F1 R P F1 R P F1 CoNLL 12 English development data be st 64.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Tagging accuracy on the combined TIGER/Tu\u0308ba corpus, using 10-fold CV, evaluated with and without capitalization, punctuation, and sentence boundaries (SB) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Spelling correction accuracy (%), impact of combining word co-occurrence CLASSIFIER: Logistic Regression trained on 1G words of news text, tested on 9-months NYT data. COMBINED SYSTEM: CLASSIFER plus system based on first-order word co-occurrence. &: Relative increase or decrease in error rate compared to CLASSIFIER #: As in Bergsma et al. (2009; 2010), no morphological variants of the words are used in evaluation ",
        "Entity": "Caption"
    },
    {
        "Text": "A simple lexicalized PCFG with second order Markovization gives relatively poor performance: 75.95% F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline (Table 7).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Counts of the number of times multiple occurrences of a token sequence is labeled as different entity types in the same document. Taken from the CoNLL training set. ",
        "Entity": "Caption"
    },
    {
        "Text": "and Table 6 show a comparison of the segmentation and POS tagging accuracies with other state-of-the-art models.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Feature set for the baseline pronoun res- olution system ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 12 Common values (in percentages) for parse tree path in PropBank data, using gold-standard parses. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 Training and test with hierarchical lexicon. \u201c(Inverse) restructuring,\u201d \u201canalyze,\u201d and \u201cannotation\u201d all require morpho-syntactic analysis of the transformed sentences. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 14 Experiments applying combinations of features in English-to-Hindi translation",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example of morphological analyses.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: POS tagging with deterministic constraints. The maximum in each column is bold. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10 Alternatives to training on gold-only feature values. Top: Select MaltParser CORE 12+. . . models re-trained on predicted or gold + predicted feature values. Bottom: Similar models to the top half, with the Easy-First Parser. Statistical significance tested only for CORE 12+. . . models on predicted input: significance of the MaltParser models from the MaltParser CORE 12 baseline model, and significance of the Easy-First Parser models from the Easy-First Parser CORE 12 baseline. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Meaning of different feature categories where s represents a specific target word and t repre- sents a specific source word. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 6: Model accuracy across the Brown cor- pus sections. ST: Stanford tagger, Wik: Wiktionary- tag-set-trained SHMM-ME, PTBD: PTB-tag-set-trained SHMM-ME, PTB: Supervised SHMM-ME. Wik outper- forms PTB and PTBD overall. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Results of the inflection exercise",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Distribution of antecedent NP types in the other-anaphora data set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: An example parse tree for the \u2018second head word\u2019 feature. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Propagation: All items",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Data examined by the two systems for the ATB",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11 French MWE identification per category and overall results (test set, sentences \u2264 40 words). MWI and MWCL do not occur in the test set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 Effect of alignment template length on translation quality. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6 Baseline + Word Clustering by Relation +           Re-ranking by Coreference +              Re-ranking by Relation ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5 Word sense disambiguation accuracy for \u201cNP1 V NP2 for NP3\u201d frame. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1), i.e., the degree to which a sense pair (s1, s2) matches a meta alternation a.",
        "Entity": "Reference"
    },
    {
        "Text": "bothmentions are in a parallel construction in adja Figure 1: An example graph modeling relations between mentions.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Results on wide-coverage Question Answer- ing task. CCG-Distributional ranks question/answer pairs by confidence\u2014@250 means we evaluate the top 250 of these. It is not possible to give a recall figure, as the total number of correct answers in the corpus is unknown. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Sentiment lexicon description",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Average feature values across best translations of sentences in the MT03 tuning set, both before MERT (column 2) and after (column 3). \u201cSame\u201d versions of tree- to-tree configuration features are shown; the rarer \u201cswap\u201d features showed a similar trend. ",
        "Entity": "Caption"
    },
    {
        "Text": "igure 2 Example of a (symmetrized) word alignment (Verbmobil task).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 presents the performance in terms of precision, recall, and F- measure of the whole system.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: Constituent Parsing: ParsEval F-scores for full and 5k training sets and for gold and predicted input. Results in bold show the best results per language and setting. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Two STs composing a STN",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Rhetorical pattern of C-Question",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Average scores by cluster: baseline versus LR[0.20,0.95]. ",
        "Entity": "Caption"
    },
    {
        "Text": "With an absolute frequency threshold of 10, we obtain an accuracy of 95.4%, which constitutes a 50% reduction of error rate.Table 3 presents the experimental results rela tive to class.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 9: Example error cases, with associated frequencies, illustrating system output and gold standard references. 5% of the cases were miscellaneous or otherwise difficult to categorize. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Probability estimation tree for the nomi native case of nouns.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6 Top 30 features of town by bootstrapped weighting based on LIN, WJ, and COS as initial similarities. The three sets of words are almost identical, with relatively minor ranking differences. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Schematic of our proposed method",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 14: Intermediate Result.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Error analysis on the latent variable seg- menter. The errors are grouped into four types: over- generalization, errors on named entities, errors on idioms and errors from data-inconsistency. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: List of keywords used in WordNet search for generating WN CLASS features. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: An excerpt from the text, with core- ferring noun phrases annotated. English trans- lation in italics. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: LO sentence configuration scores",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5 Example of segmentation of German sentence and its English translation into alignment templates",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Priority Order for Third Person ADs",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Performance on Bilingual Lexicon Extraction",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Cross entorpy and accuracy of each model.",
        "Entity": "Caption"
    },
    {
        "Text": "See Figure 3 for examples.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: An example of alignment units",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Number of features used according to different cut-off threshold. In the second column of the table are shown the number of features used when only the English context is considered. The third column correspond to English, German and Word-Classes contexts. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Performance of baseline and joint models w.r.t. the average processing time (in sec.)",
        "Entity": "Reference"
    },
    {
        "Text": "To compute the third factor of Equation (13), we have to estimate the character bigram probabilities that are classified by word type and part of speech.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4 Distribution of domain labels of predominant senses for 38 polysemous nouns ranked using the SPORTS and FINANCE corpora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 9 The difference between the results of four settings. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Selected morphological features in the OLiA Reference Model ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Results of the experiment.",
        "Entity": "Caption"
    },
    {
        "Text": "We also propose to use the features U01 U03, which we found are effective to adjust the character Figure 1: Illustration of the alignment of steps.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4 shows an example of calculating the target side SRS based on a complicated TTS template.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: Distribution of supersense mentions by domain (left), and counts for tags occurring over 800 times (below).",
        "Entity": "Reference"
    },
    {
        "Text": "The reordering of the semantic roles from source to target is computed for each TTS template as part of the template extraction process, using the word-level alignments between the LHS/RHS of the TTS template (e.g., Figure 3).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5 Another example of some discovered paraphrases.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: POS tagging of unknown words using contextual features (accuracy in percent).   \u008d is a classifier that uses only contextual features,   \u008d + baseline is the same classifier with the addition of the baseline feature (\u201cNNP\u201d or \u201cNN\u201d). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Distribution over number of hits",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Tagging accuracies on test data.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Per category performance of the Berkeley parser on sentence lengths \u2264 70 (dev set, gold segmentation). (a) Of the high frequency phrasal categories, ADJP and SBAR are the hardest to parse. We showed in \u00a72 that lexical ambiguity explains the underperformance of these categories. (b) POS tagging accuracy is lowest for maSdar verbal nouns (VBG,VN) and adjectives (e.g., JJ). Richer tag sets have been suggested for modeling morphologically complex distinctions (Diab, 2007), but we find that linguistically rich tag sets do not help parsing. (c) Coordination ambiguity is shown in dependency scores by e.g.,  S S S R  and  NP NP NP R .  NP NP PP R  and  NP NP ADJP R  are both iDafa attachment. ",
        "Entity": "Caption"
    },
    {
        "Text": "This sum can be computed efficiently using the algorithm shown in Figure 8",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2 Regular alignment example for the translation direction German to English. For each German source word there is exactly one English target word on the alignment path. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: An ATB sample from the human evaluation. The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a). But the city name Sharm Al- Sheikh is also iDafa, hence the possibility for the incorrect annotation in (b).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Acceptance rates for a noun phrase in the course of iteration. All models were with back-off mix- ing (+BM). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6 gives an overview of the decisions made in the alignment template model.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Runtimes for sentences of length 10\u201380. The                                          graph shows the average runtimes ( ) of 10 different sample sentences of the respective length with swap op- erations restricted to a maximum swap segment size of 5 and a maximum swap distance of 2. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 9: Evaluation of translation to English on out-of-domain test data",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Letting u1 be the prefix of the word that ends in v1 (eg, r in figure 1), w1 = u1v1, and h = htu1:is less accurate because it ignores the alignment rela tion between s and h, which is captured by even the simplest noisy-channel models.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Collecting evidence for a word boundary - are the non-straddling n-grams 8 1 and 82 more frequent than the straddling n-grams t 1, t2, and t3?",
        "Entity": "Reference"
    },
    {
        "Text": "Table 7 Some of the possible Spanish translations of the English phrase make with their memory-based con- text-dependent translation probabilities (rightmost column) compared against context-independent transla- tion probabilities of the baseline system ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4 Illustration of the algorithm by Held and Karp for a traveling salesman problem with J = 5 cities. Not all permutations of cities have to be evaluated explicitly. For a given subset of cities the order in which the cities have been visited can be ignored. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: RM gain over other optimizers averaged over all test sets. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Features used in the naive Bayes Classi- fier for the entity candidate: ws , ws+1 , ..., we . spi is the result of shallow parsing at wi . ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10 Translation results on the Hansards task",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 shows that by using word type and part of speech information, recall is improved from 28.1% to 40.6% and precision is improved from 57.3% to 64.1%.",
        "Entity": "Reference"
    },
    {
        "Text": "The regular expressions available in Foma from highest to lower precedence.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 Example levels of generalization for different values of \u03b1. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: EM Algorithm For Estimating TTS Templates and Semantic Features framework (May and Knight, 2007).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Segmentation results on different languages. Results are calculated based on word types. For each language we report precision, recall and F1 measure, number of word types in the corpus and number of word types with gold standard segmentation available. For each language we report the segmentation result without and with emission likelihood scaling (without LLS and with LLS respectively). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Sequence of POS-tagged units used to estimate the bilingual n-gram LM. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Training and test set sources, genres, sizes in terms of numbers of tokens, and unigram and bi- gram coverage (%) of the training set on the test sets. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A verse written in the BAD web application.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Normalization accuracy after training on n tokens and evaluating on 1,000 tokens (average of 10 random training and evaluation sets), compared to the \u201cbaseline\u201d score of the full text without any normalization ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Time comparison between FSAs and FSRAs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Type-level (top) and token-level (bottom) cov- erage for the nine languages in three versions of the Wik- tionary. ",
        "Entity": "Caption"
    },
    {
        "Text": "composite kernel 83.0 72.0 77.1 Zhou et al., (2007): composite kernel 82.2 70.2 75.8 Zhang et al., (2006): composite kernel 76.1 68.4 72.1 Zhao and Grishman, (2005):4 composite kernel 69.2 70.5 70.4 Ours: CTK with UPST 80.1 70.7 75.1Zhou et al., (2007): context sensitive CTK with CS-SPT 81.1 66.7 73.2 Zhang et al., (2006): CTK with SPT 74.1 62.4 67.7 Table 4.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Character Types",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A typed narrative chain. The four top arguments are given. The ordering O is not shown. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Average precisions over the 10 corpora of different window size (3 seeds)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Arabic Verbal Inflection",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Experimental Results. C5.0 is supervised accuracy; Base         is      on random clusters.  set; Ling is manually selected subset; Seed is seed-verb-selected set. See text for further description. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8: Evaluation of translation from English on in-domain test data",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. 5-fold cross-validation results. All are trained on fp1 (except the last row showing the unchanged algorithm trained on adj for comparison), and tested on adj. McNemar's test show that the improvement from +purify to +tSVM, and from +tSVM to ADJ are statistically significant (with p<0.05). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 Confusion matrix for argument labels, with ArgM labels collapsed into one category. Entries are a fraction of total annotations; true zeros are omitted, while other entries are rounded to zero. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: MAP values on corresponding test set ob- tained by each method. Figures in parentheses in- dicate optimal number of LDA topics. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: The Wiktionary vs. tree bank tag sets. Around 90% of the Wiktionary tag sets are identical or subsume tree bank tag sets. See text for details. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Directed dependency accuracy of our model and the baselines using gold POS tags for the target language. The first section of the table is for the direct transfer of the MST parser (McDonald et al., 2011). The second section is for the weighted mixture parsing model (Cohen et al., 2011). The first two columns (Random and Greedy) of each section present the parsing performance with a random or a greedy mapping. The third column (Petrov) shows the results when the mapping of Petrov et al. (2011) is used. The fourth column (Model) shows the results when our mapping is used and the fifth column in the first section (Best Pair) shows the performance of our model when the best source language is selected for every target language. The last column (Tag Diff.) presents the difference between our mapping and the mapping of Petrov et al. (2011) by showing the percentage of target language tokens for which the two mappings select a different universal tag. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Translation results for German\u2192English",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Finite-state cascades for five natural language problems.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Examining the learned hidden representation for SRL. In this example the syntactic dependency arcs derived from gold standard syntactic annotations (left) are entirely disjoint from the correct predicate/arguments pairs (shown in the heatmaps by the squares outlined in black), and the observed syntax model fails to recover any of the correct predictions. In contrast, the hidden model structure (right) learns a representation that closely parallels the desired end task predictions, helping it recover three of the four correct SRL predictions (shaded arcs: red corresponds to a correct prediction, with true labels GA, KARA, etc.), and providing some evidence towards the fourth. The dependency tree corresponding to the hidden structure is derived by edge-factored decoding: dependency variables whose beliefs > 0.5 are classified as true (though some arcs not relevant to the SRL predictions are omitted for clarity). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5 shows our morpheme accuracy results.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Illustration of entity-relationship graphs",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Corpus statistics",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Most frequent phrase dependencies in DE\u2192EN data, shown with their counts and attachment directions. Child phrases point to their parents. To focus on interesting phrase dependencies, we only show those in which one phrase has at least two tokens and neither phrase is entirely punctuation. The words forming the longest lexical dependency in each extracted phrase dependency are shown in bold; these are used for back-off features. ",
        "Entity": "Caption"
    },
    {
        "Text": "The first model is Equation (5), which is the combina . tion of Poisson distribution and character zerogram",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: HeiST baseline, cross-lingual projection, SVM.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Performance of the mention detection sys- tem using lexical, syntactic, gazetteer features as well as features obtained by running other named-entity classifiers ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Sizes of rule application test set for each learned rule-set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Performance of the mention detection system using lexical, syntactic, gazetteer features as well as features obtained by running other named-entity classifiers named-entity classifiers (with different semantic tag sets).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 summarizes the results obtained with different taggers and tagsets on the development data.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1; capital letters designate sets and small letters elements of sets).2 For a lemma l like lamb, we want to knowhow well a meta alternation (such as ANIMAL FOOD) explains a pair of its senses (such as the animal and food senses of lamb).3 This is formalized through the function score, which maps a meta alternation and two senses onto a score.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Corpus statistics of the MATR MT06 corpus that was used for experimental evaluation of the proposed measures. ",
        "Entity": "Caption"
    },
    {
        "Text": "#o is the total number of output English translations.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Algorithm description",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Clusters for transitive, unaccusative, and ditransitive",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10: Performance of Altavista counts and BNC counts for compound interpretation (data from Lauer 1995) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Intercoder agreement for activities: The meeting dialogues and Santa Barbara corpus have been annotated by a semi-naive coder and the first author of the paper. The \u03ba-coefficient is determined as in Carletta et al. (1997) and mutual information measures how much one label \u201cinforms\u201d the other (see Sec. 3). For CallHome Spanish 3 dialogues were coded for activities by two coders and the result seems to indicate that the task was easier. ",
        "Entity": "Caption"
    },
    {
        "Text": "#c is the total number of new Chinese source words in the period",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Gross statistics for several different treebanks.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Comparison with other systems",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 Words in the MSR gold test set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Stages of the proposed method.",
        "Entity": "Caption"
    },
    {
        "Text": "Tables 4 and 5 show the effect of the pruning parameter tp with the histogram pruning parameter Np = 50,000.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5: Example outputs of matching implementation of Finnish OT. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Selected document statistics for three JDPA Corpus document sources.",
        "Entity": "Caption"
    },
    {
        "Text": "                                                                   ings of the 13th International Conference, pages 182\u2013190. Table 6: Accuracy on S ENSEVAL-1 and S ENSEVAL-2 En-                                                                 A. R. Golding and D. Roth. 1999. A winnow-based appro glish test data (only the supervised systems with a coverage of    to context-sensitive spelling correction. Machine Learni at least 97% were used to compute the mean and variance)           34(1-3):107\u2013130. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Effect of discriminatively learned penalties for OOV words. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: POS tagging accuracy using one-at-a- time, word-based POS tagger ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Screenshot of the main BRAT user-interface, showing a connection being made between the annotations for \u201cmoving\u201d and \u201cCitibank\u201d. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11 NWI results on PK and CTB corpora, NWI as post-processor versus unified approach. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7 Accuracy of different methods in predicting OOV words polarity. ",
        "Entity": "Caption"
    },
    {
        "Text": "For example, if a token starts with a capital letter and ends with a period (such as Mr.), then the feature InitCapPeriod is set to 1, etc",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Results for different user simulations.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: MRRs on the augmented candidate list.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Example derivation for the query \u2018how many people visit the public library of new york annu- ally.\u2019 Underspecified constants are labelled with the words from the query that they are associated with for readability. Constants from O, written in typeset, are introduced in step (c). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 15 Most frequent SFC labels for all senses of polysemous words in WordNet, by part of speech. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example of the non-relation Same-Unit",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Results on unseen test set for models which performed best on dev set \u2013 predicted input. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example dependency tree.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: The Performance of SVM and LP algorithm with different sizes of labeled data for relation detection on relation subtypes. The LP algorithm is run with two similarity measures: cosine similarity and JS divergence. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Comparison of paraphrase generators. Top: the MOSES baseline; middle and bold: the \u201ctrue-score\u201d MCPG; down: the \u201ctranslator\u201d MCPG. The use of \u201ctrue-score\u201d improves the MCPG per- formances. MCPG reaches MOSES performance level. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 French grammar development. Incremental effects on grammar size and labeled F1 for each of the manual grammar features (development set, sentences \u2264 40 words). The baseline is a parent-annotated grammar. The features tradeoff between maximizing two objectives: overall parsing F1 and MWE F1. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: NMI of unconstrained HGFC when trees for T2 and T3 are inferred automatically.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Number of recall errors according to mention type (rows anaphor, columns antecedent). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Feature growth rate: For N-best list i in the table, we have (#NewFt = number of new fea- tures introduced since N-best i \u2212 1) ; (#SoFar = Total number of features defined so far); and (#Ac- tive = number of active features for N-best i). E.g., we extracted 7535 new features from N-best 2; combined with the 3900 from N-best 1, the total features so far is 11435. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Performance of various clustering-based seed sampling strategies on the held-out test data with the optimal cluster number for each clustering algorithm ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "The first machine, illustrated in Figure 1 encodes the prefix and suffix expansion rules, producing a lattice of possible segmentations.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Organisation of the hierarchical graph of concepts Following previous semantic noun classification experiments (Pantel and Lin, 2002; Bergsma et al., 2008), we use the grammatical relations (GRs) as features for clustering.",
        "Entity": "Reference"
    },
    {
        "Text": "However, this information is hard to extract reliably from the available data; and even if were obtainable, many of the 0.3 0.2 0.1 0 60 50 40 30 20 10 0 10 20 30 40 50 60 gain (length of correct prefix length of incorrect suffix) Figure 2: Probability that a prediction will be accepted versus its gain.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: Example of the Hybrid Method",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Results of MET2 under different configurations",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2 shows examples of the discovered patterns for the merger and acquisition topic.",
        "Entity": "Reference"
    },
    {
        "Text": "For example, look ing at Figure 2(b), V on G can be grouped into three clusters u1, u2 and u3.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: Example of a word alignment and of ex- tracted alignment templates. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Development testing evaluation.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11 gives an overview on the training and test data.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Performance vs. log start penalty",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Corpus characteristics for translation task. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Overall accuracy of maximum entropy sys- tem using different subsets of features for People\u2019s Daily News words (automatically segmented, part- of-speech-tagged, parsed). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Comparison of training log likelihood of English LSA models bootstrapped from a Chinese LSA and from a flat monolingual English LSA. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Spelling correction precision (%), impact of adding parse features SVM trained on 1G words of news text, tested on 9-months of NYT data. *: Improvement of (NG+)LEX+PAR vs. (NG+)LEX is statistically significant. \u03b1: Improvement of NG+LEX+PAR vs. NG is statistically significant. &: Relative increase or decrease of error rate compared to \u201dNG+LEX\u201d #: As in Bergsma et al. (2009; 2010) no morphological variants of the words are used in evaluation ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: PubMed Results. The curve represents the Pareto Frontier of all results collected after multiple runs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters. Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner using increasing numbers of training sentences. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 Experiments on the word length\u2013precision relationship of the small corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "In Figure 1(c) we show a sentence one of about 500 people nominated for , where there exists a DISC relationship between the entities one and people",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Parsing scores of the various systems",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Single-threaded speed and memory use on the perplexity task. The P ROBING model is fastest by a sub- stantial margin but generally uses more memory. T RIE is faster than competing packages and uses less memory than non-lossy competitors. The timing basis for Queries/ms in- cludes kernel and user time but excludes loading time; we also subtracted time to run a program that just reads the query file. Peak virtual memory is reported; final resident memory is similar except for BerkeleyLM. We tried both aggressive reading and lazy memory mapping where appli- cable, but results were much the same. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7: Computing the partition function of the conditional probability P r(S|T ). Sema(s1 , s2 , t) denotes all the seman- tic role features generated by combining s1 and s2 using t. ",
        "Entity": "Caption"
    },
    {
        "Text": "A natural unit for B(x, k, a) is the number of keystrokes saved, so all elements of the above equation are converted to this measure.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3. Semantic Role learning curve",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 5. Two real translation examples,",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: Effects of combination using the confidence measure. Here we used \u03b1 = 0.8 and confidence threshold t = 0.7. The separator \u201c/\u201d divides the results of s1, s2, and s3. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 Number of learned splits per NT-category after five split-merge cycles. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2 Taxonomy of morphologically derived words (MDWs) in MSRSeg. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Multiple Analyses for suis",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7: Compatible brackets and all-compatible bracket rates when word accuracy is optimized.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Average precision of discovered senses      for English in relation with WordNet ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Resolution accuracies for the ACE test set.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5. Templates for feedback.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: EM input for our example sentence. j-values follow each lexical candidate. ",
        "Entity": "Caption"
    },
    {
        "Text": "For example, the expressions in Figure 2 are identified as paraphrases by this method; so these three patterns will be placed in the same pattern set.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Possible relations appearing on the edges of a DCS tree. Here, j, j 0 \u2208 {1, 2, . . . } and i \u2208 {1, 2, . . . }\u2217 . ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Lexical features. Top part: Adding each feature separately; difference from CORE 12 (predicted). Bottom part: Greedily adding best features from previous part. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Performance in F1-score over different cluster numbers with intra-stratum sampling on the develop- ment data ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Error analysis of confidence measure with and without EIV tag",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Ten relation instances extracted by our system that did not appear in Freebase.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Evaluation results of the methods.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Evaluation of context-sensitive convolution tree kernels using SPT on the major relation types of the ACE RDC 2003 (inside the parentheses) and 2004 (outside the parentheses) corpora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: The effect of language detection",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: The 10 best languages for R EDDY using LCS.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The greedy binding problem. (a) The correct binding, (b) the greedy binding, (c) the result.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4 The effect of varying the number of seeds on accuracy. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4. Gold standard length distribution.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 shows the result of varying the number of samplers and iterations for all",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Possibility combination of neighboring        tokens within the corpus for PER ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9. Comparison with voted cache",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 shows examples of common char acter bigrams for each part of speech in the infre quent words of the EDR corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2. PAT-Tree Instantiation for Figure 1. In the extraction process, the PAT-tree is ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2 shows empirical estimates of p(a = 1|2k l) from the TransType data.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6: Results using different parsers",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Error analysis",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Six of the top 20 scored Narrative Schemas. Events and arguments in italics were marked misaligned by FrameNet definitions. * indicates verbs not in FrameNet. - indicates verb senses not in FameNet. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1.<",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: Contribution of features",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Error analysis of parser components av- eraged over Arabic, Bulgarian, Danish, Dutch, Japanese, Portuguese, Slovene, Spanish, Swedish and Turkish. N/P: Allow non-projective/Force pro- jective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. ",
        "Entity": "Caption"
    },
    {
        "Text": "The second model is the combination of Poisson distribution (Equation (6)) and character bigram",
        "Entity": "Reference"
    },
    {
        "Text": "Throughout in this paper, we used Equation (9) to compute the word spelling probabilities.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4 Evaluation of Two Detection and Classification Modes",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Dependency representation of example (2) from Talbanken05. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Post-hoc analysis on the models built by the DAC system: some of the top features with corresponding feature weights in parentheses, for each individual tagger. (POS tags are capitalized; BOS stands for Beginning Of Sentence) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Scores for UPUC corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Weather Sentence Evaluations.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Overall accuracy of maximum entropy sys- tem using different subsets of features for Penn Chi- nese Treebank words (manually segmented, part-of- speech-tagged, parsed). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Accuracy by sentence length for Method 5 measured on separate grammatical and ungrammat- ical data: Gr = Grammatical, AG = Agreement, RW = Real-Word, EW = Extra Word, MW = Missing Word ",
        "Entity": "Caption"
    },
    {
        "Text": "In the last two lines of Equation 3, \u03c6\u01eb and each P (f |e) = \"\u00a3s c (f |e; f (s), e(s)) (4) \u03c6i are not free variables, but are determined by f s c(f |e; f (s), e(s))the alignments.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Comparison against Stevenson and Joanis (2003) s result on T1 (using similar features).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Accuracy for induced verb classes.",
        "Entity": "Caption"
    },
    {
        "Text": "For the Verbmobil task, we train the model parameters M according to the maximum class posterior probability criterion (equation (4)).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "equation (2)",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Sizes of bilingual dictionaries induced by differ- ent alignment methods. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Bilingual training corpus, recognition lex- icon and translation lexicon (PM = punctuation mark). ",
        "Entity": "Caption"
    },
    {
        "Text": "The figure schematically shows a small portion of the graph describing the concepts of mechanism (concrete), political system and relationship (abstract) at two levels of generality.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: Final Performance (Frozen Systems) on SENSEVAL Lexical Sample WSD Test Data",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: DP corpus comparison for OPUS features based on frequent vs. domain-relevant verbs ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Distribution of the lexical items",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results on the MT06 and MT08 test sets",
        "Entity": "Caption"
    },
    {
        "Text": "#e is the total number of English translation candidates in the period.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Weights learned for discount features. Nega- tive weights indicate bonuses; positive weights indicate penalties. ",
        "Entity": "Caption"
    },
    {
        "Text": "(prec2 in Table 8)",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Voting under hand-invented schemes.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Performance of Algorithms",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Perplexity results for the immediate- trihead model ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Multi-threaded time and memory consumption of Moses translating 3003 sentences on eight cores. Our code supports lazy memory mapping (-L) and prefault- ing (-P) with MAP POPULATE, the default. IRST is not threadsafe. Time for Moses itself to load, including load- ing the language model and phrase table, is included. Along with locking and background kernel operations such as prefaulting, this explains why wall time is not one-eighth that of the single-threaded case. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "Table 13 Large-scale clustering on D3 with n/na/nd/nad/ns-dass. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Most frequent BLC\u201320 semantic classes on WordNet 3.0",
        "Entity": "Caption"
    },
    {
        "Text": "We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4, 5, and 6.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5: MEDLDA Fmeausres for 3 feature conditions",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Glue Semantics proof for (83), English Way Construction (means interpretation)",
        "Entity": "Caption"
    },
    {
        "Text": "The first example in Table 3 shows that words ending in ' -' are likely to be nouns.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Classification results with decision tree on XLE output (Method 3) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6 Dependencies in the alignment template model. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Weights learned for inserting target English words with rules that lack Chinese words. ",
        "Entity": "Caption"
    },
    {
        "Text": "To illustrate how SRF impacts the translation results, Figure 8 gives 3 examples of the MT outputs with and without the SRFs",
        "Entity": "Reference"
    },
    {
        "Text": "Consider the example in Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Mapping algorithm - refining step.",
        "Entity": "Caption"
    },
    {
        "Text": "(2)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Precision curves of paraphrase extraction.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Adapting a parser to a new annotation style. We learn to parse in a \u201ctarget\u201d style (wide column label) given some number (narrow column label) of supervised target-style training sentences. As a font of additional features, all training and test sentences have already been augmented with parses in some \u201csource\u201d style (row label): either gold-standard parses (an oracle experiment) or else the output of a parser trained on 18k source trees (more realistic). If we have 0 training sentences, we simply output the source-style parse. But with 10 or 100 target-style training sentences, each off-diagonal block learns to adapt, mostly closing the gap with the diagonal block in the same column. In the diagonal blocks, source and target styles match, and the QG parser degrades performance when acting as a \u201cstacked\u201d parser. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Dendrogram of the participants cluster based on their feedback profile ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Diacritized particles and pseudo-verbs that, after orthographic normalization, have the equivalent surface form    an. The distinctions in the ATB are linguistically justified, but complicate parsing. Table 8a shows that the best model recovers SBAR at only 71.0% F1. ",
        "Entity": "Caption"
    },
    {
        "Text": "See Table 1 for details.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Results for the coreference resolution",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Inflectional+lexical features together.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: space distribution of most reliable",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Our segmentation results by the dictionary- based approach for the closed test of Bakeoff 2005, very low R-oov rates due to no OOV recognition applied. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 compares the results of the unconstrained version of HGFC against those of AGG on our largest test set T2.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8 Top 10 features of country by the Bootstrapped feature weighting. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results for morphological processing, English\u2192German ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 14: An example result of BioAR",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Features used in baseline system",
        "Entity": "Caption"
    },
    {
        "Text": "Table 16 WSD using predominant senses, training, and testing on all domain combinations (hand-classified corpora). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Performance of WSD system using various combinations of learning algorithms and features.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: BLEU scores for each translation direction trained on (\u2190) directional (condition on target and generate source) and (\u2194) symmetrised alignments (grow-diag-final-and). Observe that the plots are on different scales. This means that results cannot directly be compared across plots. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Precision-recall curve for rescoring",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 shows the usefulness evaluation result.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Tags produced by the different models along with the reference set of tags for a part of a sentence from the Italian test set. Italicized tags denote incorrect labels. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The role of the standard Basque (Batua) ana- lyzer in filtering out unwanted output candidates created by the induced rule set produced by method 1. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: # of features.",
        "Entity": "Caption"
    },
    {
        "Text": "The results are displayed in Table 3.",
        "Entity": "Reference"
    },
    {
        "Text": "Reduction of conjuncts for NP coordination Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 10 Examples of the effect of the hierarchical lexicon. ",
        "Entity": "Caption"
    },
    {
        "Text": "he largest effect seems to come from taking into account the bigram dependence, which achieves an mWER of 32.9%",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 9: Word-aligned DRG for the sentence \u201cMichelle thinks that Obama smokes.\u201d ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: AER comparison (cn \u2192en)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3:",
        "Entity": "Reference"
    },
    {
        "Text": "Fig 3",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: LL results tested against gs-swaco",
        "Entity": "Caption"
    },
    {
        "Text": "I+1 hLM(eI , f J , K , zK ) = log n p(ei | ei 2 , e ) (17) 1 1 1 1 i=1 i 1",
        "Entity": "Caption"
    },
    {
        "Text": "Thetheoretical upper bound of the decoding complex Figure 5: Decoding algorithm using semantic role features.",
        "Entity": "Reference"
    },
    {
        "Text": "More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; Paa and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8: Parser performance on WSJ;23, unsupervised adaptation. For all trials, the base training is Brown;T, the held out is Brown;H plus the parser output for WSJ;24, and the mixing parameter \u03c4A is 0.20e c(A). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Standards and corpora. ",
        "Entity": "Caption"
    },
    {
        "Text": "(Equation (7)) (Poisson + hi gram).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Graphical model for PLTM.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: New Verb Classes",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Performance of RDC trained on fp1/fp2/adj, and tested on adj.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3 shows the distribution of SSTs in the corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Plate diagram of our model.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Topic-specific translation lexicons are learned by a 3-topic BiTAM-1. The third lexicon (Topic-3) prefers to translate the word Korean into ChaoXian (\u008am:North Korean). The co-occurrence (Cooc), IBM-1&4 and HMM only prefer to translate into HanGuo (\u00b8I:South Korean). The two candidate translations may both fade out in the learned translation lexicons. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Multilingual bootstrapping",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Contribution of combining the dynamic",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8: a) Parsing of input sentence[78]",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Evaluation of coarse-grained POS tagging on test data ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 A general architecture for paraphrasing approaches leveraging the distributional similarity hypothesis. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: File formats. Trees (a) and (b) are aligned constituency and dependency trees for a mockup English example. Boxed labels are shared across the treebanks. Figure (c) shows an ambiguous lattice. The red part represents the yield of the gold tree. For brevity, we use empty feature columns, but of course lattice arcs may carry any morphological features, in the FEATS CoNLL format. ",
        "Entity": "Caption"
    },
    {
        "Text": "Excerpt from the collocation lexicon",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Event descriptions spread across two sentences ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: An example showing the combination of the semantic role sequences of the states.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Dependency parse with dependency path from \u2018Edwin Hubble\u2019 to \u2018Marshfield\u2019 highlighted in boldface. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Sources of Dictionaries",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Empirical results for the position-based model, the KL-based models and BAYE S UM, with different inputs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Comparison of results for MUC6",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 10 Dutch-to-English Learning curves (left-hand side graphs) and difference curves (right-hand side graphs) comparing the Moses baseline against four context-informed models (PR, OE, POS\u00b12 and Word\u00b12). These curves are plotted with scores obtained using three evaluation metrics: BLEU (top), METEOR (centre) and TER (bottom) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Comparison of various groups of parsers. All percentages refer to the share of the total words in test data, attached correctly. The \u201csingle parser\u201d part shows shares of the data where a single parser is the only one to know how to parse them. The sizes of the shares should correlate with the uniqueness of the individual parsers\u2019 strategies and with their contributions to the overall success. The \u201cat least\u201d rows give clues about what can be got by majority voting (if the number represents over 50 % of parsers compared) or by hypothetical oracle selection (if the number represents 50 % of the parsers or less, an oracle would generally be needed to point to the parsers that know the correct attachment). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 underscores the virtues of Sentence Recency: In the most recent sentence with antecedents satisfying the filters, there are on aver ble.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Architecture of NILER system.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Baseline vs. Submitted Results",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3 shows the part of speech prediction accu racy of two unknown word model without context.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Performance comparison with the literature for candidate selection for MT ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 Models with functional features: GENDER, NUMBER, rationality (RAT). F N * = functional feature(s) based on Alkuhlani and Habash (2011); GN = GENDER + NUMBER ; GNR = GENDER + NUMBER + RAT . Statistical significance tested only for CORE 12+. . . models on predicted input, against the CORE 12 baseline. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 Corpus statistics for Hansards task (Words*: words without punctuation marks",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Three confusion networks with prior prob- abilities. ",
        "Entity": "Caption"
    },
    {
        "Text": "CoreLex defines a layer of abstraction above WordNet consisting of 39 basic types, coarse- grained ontological classes (Table 2).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: BLEU scores of two open test sets obtained when training by MERT, S-slack-SVM and 1-slack-SVM using four development sets containing 400 sentences randomly se- lecting from WMT-08 dev2006. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Outline of the segmentation process",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Example of the context of \u201c\u6c34\u201d in \u201c\u5403\u6c34 \u679c (Eat fruits)\u201d and the context of \u201c\u7bee\u201d in \u201c\u6253\u7bee\u7403 (Play basketball)\u201d ",
        "Entity": "Caption"
    },
    {
        "Text": "Tree setups P(%) R(%) F CS-SPT over SPT3 1.5 1.1 1.3 DSPT over SPT 1.1 5.6 3.8 UPST (FPT) over SPT 3.8 10.9 8.0 Table 3.",
        "Entity": "Reference"
    },
    {
        "Text": "                   \u00fd Figure 7 (a) A Chinese OAS      . (b) Two sentences in the training set, which contain t whose OASs have been replaced with the single tokens <OAS>. (Li et al. 2003). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Similarity graph after its sparsification",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Compound Splitting Results.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Simple parser vs full parser \u2013 morphological quality. The parsing models were trained on the first 5,000 sentences of the training data, the morphological tagger was trained on the full training set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Factoring of global feature collections g into f . xji denotes hxi , . . . xj i in sequence x = hx1 , . . .i. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 21 MSRSeg system results for the MSR test set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results of the filtering experiments",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Tradeoffs of precision and recall values in the experiments with method 1 using various different pa- rameters. When the unigram filter is applied the precision is much better, but the recall drops. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6 Dev-set results of using the agreement-filter on top of the lexicon-enhanced parser (starting from gold segmentation). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Semantic role learning curve.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Meta alternations and their average precision values for the task.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 6 Distribution of probabilities given by the classi\ufb01er over all node pairs of the test-set graphs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Opinion PageRank",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 The comparison between NPYLM and ESA. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Features used by paraphrase classifier.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 14: Arabic Equational Sentences",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Pronouns as Opinion Targets",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8: Rate of obtaining two clusters for mix- tures of SW-graphs dependent on merge rate r. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Active sparse feature templates",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6 Results of different approach used in our experiments (White background lines are the results we repeat Zhang\u201fs methods and they have some trivial difference with Table 1.) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example consensus network with votes on word arcs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Example of BLC selection",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Examples for correct templates that were learned by TEASE for input templates. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Accuracy on seen and unseen tokens.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Subgraph of tuned-LP output for \u201cheadache\u201d",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Caseframe Network Examples",
        "Entity": "Caption"
    },
    {
        "Text": "equation (2)",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Evaluation of translation of phrasal verbs in test set.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Distribution of reasons for false positives (incorrect argument extractions) by BInc at K=20. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Parse Feature Example for the sentence: \u201cGM says the addition of OnStar, which includes a system that automatically notifies an OnStar operator if the vehicle is involved in a collision, complements the Vue\u2019s top five-star safety rating for the driver and front passenger in both front- and side-impact crash tests.\u201d ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Possessive pronoun resolution examples",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Test results",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3 gives the algorithm phrase-extract that computes the phrases",
        "Entity": "Reference"
    }
]