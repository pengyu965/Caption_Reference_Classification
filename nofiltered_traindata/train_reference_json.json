[
    {
        "Text": "Table 2 compares the results of the unconstrained version of HGFC against those of AGG on our largest test set T2.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five trials on CTB5c.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Character type configuration of infrequent words in the EDR corpus",
        "Entity": "Reference"
    },
    {
        "Text": "As column 5 (SVM) in table 2 shows, the classification results are very similar to the results obtained with MBL.12 We furthermore find a very similar set of errors, and in particular, we find that 51.0 % of the errors for the inanimate class are nouns with the gradient animacy properties presented in (9)-(13) above.",
        "Entity": "Reference"
    },
    {
        "Text": "W can be encoded by an undirected graph G (Figure 2(a)), where the nouns are mapped to vertices and Wij is the edge weight between vertices i and j",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: The Stanford parser (Klein and Manning, 2002) is unable to recover the verbal reading of the unvocalized surface form 0 an (Table 1)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "equation",
        "Entity": "Reference"
    },
    {
        "Text": "We obtain the following decision rule: eI = argmax Pr(eI | f J ) 1 1 1 I 1 M ) = argmax m hm (eI , f J ) 1 1 I m=1",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2 shows empirical estimates of p(a = 1|2k l) from the TransType data.",
        "Entity": "Reference"
    },
    {
        "Text": "For example, look ing at Figure 2(b), V on G can be grouped into three clusters u1, u2 and u3.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: Evaluation of 100 randomly sampled variation nuclei types.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Tagging accuracies on development data in percent.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 shows that by using word type and part of speech information, recall is improved from 28.1% to 40.6% and precision is improved from 57.3% to 64.1%.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 presents the performance in terms of precision, recall, and F- measure of the whole system.",
        "Entity": "Reference"
    },
    {
        "Text": "We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4, 5, and 6.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4",
        "Entity": "Reference"
    },
    {
        "Text": ", fjk (11)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Performance of baseline and joint models w.r.t. the average processing time (in sec.)",
        "Entity": "Reference"
    },
    {
        "Text": "alignment models Pr(f J , aJ | eI ),",
        "Entity": "Reference"
    },
    {
        "Text": "These semantic features Figure 8: Examples of the MT outputs with and without SRFs",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 shows examples of the feature SRR.",
        "Entity": "Reference"
    },
    {
        "Text": "We also propose to use the features U01 U03, which we found are effective to adjust the character Figure 1: Illustration of the alignment of steps.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Gross statistics for several different treebanks.",
        "Entity": "Caption"
    },
    {
        "Text": "If we directly translate the EM algorithm into the log- linear model, the problem becomes maximizing 0 X P r(S, T , D) = X @ Y P r(t) Y 1 P r(f )A the data likelihood represented by feature weights instead of feature probabilities: D D t D f F (S,T .role,D) Though the above formulation, which makes the P r(S, T ) = D exp i i fi (S, T , D) total probability of all the pairs of trees and strings P P exp P f (S , T , D ) St ,T t Dt i i i less than 1, is not a strict generative model, we can still use the EM algorithm (Dempster et al., 1977) to estimate the probability of the TTS templates and the semantic features, as shown in Figure 6.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8 shows that by using word type and part of speech information, recall is improved from 28.1% to 40.6% and precision is improved from 57.3% to 64.1%.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: Comparison of results for MUC6",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 Effect of pruning parameter Np and heuristic function on error rate for direct-translation model (tp = 10 12 ).",
        "Entity": "Caption"
    },
    {
        "Text": "As the search space increases expo nentially, it is not possible to explicitly represent it.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2 shows the labeled dependency graph of example (2), taken from Talbanken05.",
        "Entity": "Reference"
    },
    {
        "Text": "Contribution of constituent dependencies in respective mode (inside parentheses) and accumulative mode (outside parentheses) The table shows that the final DSPT achieves the best performance of 77.4%/65.4%/70.9 in precision/recall/F-measure respectively after applying all the dependencies, with the increase of F-measure by 8.2 units compared to the baseline MCT.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Dev set frequencies for the two most significant discourse markers in Arabic are skewed toward analysis as a conjunction.",
        "Entity": "Caption"
    },
    {
        "Text": "In the table, Nc indicates the number of clusters in the inferred tree, while Nl indicates the closest match to the number of classes in the gold standard.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6: Incremental dev set results for the manually annotated grammar (sentences of length \u2264 70).",
        "Entity": "Caption"
    },
    {
        "Text": "p(z = (FJ , EI , A ) J f ) = 1 1 | N(C( f )) (10",
        "Entity": "Caption"
    },
    {
        "Text": "At the morp heme level, stems are divid ed from their affixe s. For exam ple, altho ugh both naga no (Naga no) and shi (city) can appea r as indivi dual words , nagano shi (Nag ano city) is brack eted as [[naga no][s hi]], since here shi Figure 3: Determining word boundaries.",
        "Entity": "Reference"
    },
    {
        "Text": "In the last two lines of Equation 3, \u03c6\u01eb and each P (f |e) = \"\u00a3s c (f |e; f (s), e(s)) (4) \u03c6i are not free variables, but are determined by f s c(f |e; f (s), e(s))the alignments.",
        "Entity": "Reference"
    },
    {
        "Text": "Results are shown in Table 2; we see that better word alignment results do not lead to better translations.",
        "Entity": "Reference"
    },
    {
        "Text": "The results are shown in the corr row of table 2, for exact character-probability estimates.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4",
        "Entity": "Reference"
    },
    {
        "Text": "Figures 4",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4",
        "Entity": "Reference"
    },
    {
        "Text": "On the other hand, using our method of combining both sources of information and setting M = \u221e, 19 Chinese words (i.e., the first 22 Chinese words in Table 3 except \u5df4\u4f50\u4e9a,\u5769\u57da,\u666e\u5229\u6cd5) have their correct English translations at rank one position.",
        "Entity": "Reference"
    },
    {
        "Text": "E.g. rat and printer are very different in meaning, but they are both closely related to different meanings of mouse",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "So we estimate that English translations are present in the English part of the corpus for Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1.<",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 shows the record for the headword orange followed by its collocates",
        "Entity": "Reference"
    },
    {
        "Text": "Table 11 gives an overview on the training and test data.",
        "Entity": "Reference"
    },
    {
        "Text": "vecI : IL Rk instance vector computation C : Rk m Rk centroid computation vecL : L Rk lemma (type) vector computation repM : M Rk meta sense representation Table 3: Additional notation and signatures for CAM explicit sense disambiguation, CAM represents lemmas by their type vectors, i.e., the centroid of their instances, and compares their vectors (attributes) to those of the meta alternation hence the name.",
        "Entity": "Reference"
    },
    {
        "Text": "In the following, we describe the criterion that defines the set of phrases that is consistent with the word alignment matrix",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 is, in fact, a weighted sum of these two distributions.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five trials on CTB5c.",
        "Entity": "Reference"
    },
    {
        "Text": "range free green lemon peel red state yellow",
        "Entity": "Reference"
    },
    {
        "Text": "If it starts with a lower case letter, and contains both upper and lower case letters, then (mixedCaps, zone) is set to 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: Meta alternations and their average precision values for the task.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Performance of the mention detection system using lexical features only.",
        "Entity": "Reference"
    },
    {
        "Text": "K): J k = fj k 1 +1 , ..",
        "Entity": "Caption"
    },
    {
        "Text": "The unknown parameters are determined by maximizing the likelihood on the parallel training corpus:",
        "Entity": "Reference"
    },
    {
        "Text": "whose unvocalized surface forms 0 an are indistinguishable.",
        "Entity": "Reference"
    },
    {
        "Text": "Excerpt from the collocation lexicon",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 describes the components and how this system works.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: NMI of unconstrained HGFC when trees for T2 and T3 are inferred automatically.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "(2)",
        "Entity": "Caption"
    },
    {
        "Text": "In Figure 4 we show an example of variation between the parsing models.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8 shows the tagging accuracy of unknown words.",
        "Entity": "Reference"
    },
    {
        "Text": "Among all possible target sentences, we will choose the sentence with the highest probability",
        "Entity": "Reference"
    },
    {
        "Text": "I+1 hCLM(eI , f J , K , zK ) = log n p(C(ei ) | C(ei 4 ), ..",
        "Entity": "Caption"
    },
    {
        "Text": "In Table 7 we give results for several evaluation metrics.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3 show the training time for different models.",
        "Entity": "Reference"
    },
    {
        "Text": "For MUC6, the reduction in error due to global features is 27%, and for MUC7,14%.",
        "Entity": "Reference"
    },
    {
        "Text": "I exp[ m=1 m hm (e 1 , f1 )]",
        "Entity": "Caption"
    },
    {
        "Text": "Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006) and the CRF model using minimum subword-based tagging, both of which are statistical methods, are used individually to solve the Figure 1: Outline of the segmentation process 2.1 Forward Maximum Matching.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 shows a probability estimation tree for the prediction of the probability of the nominative attribute of nouns.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: Dev set learning curves for sentence lengths \u2264 70. All three curves remain steep at the maximum training set size of 18818 trees.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Tagging accuracies on test data.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Performance of Algorithms switch under different input data.",
        "Entity": "Reference"
    },
    {
        "Text": "If the token starts with a capital letter (initCaps), then an additional feature (init- Caps, zone) is set to 1",
        "Entity": "Reference"
    },
    {
        "Text": "In Table 7 we give results for several evaluation metrics.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "and 8 show word accuracy for Chasen, Juman, and our algorithm for parameter settings optimizing word precision, recall, and F-measure rates.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1, Figure 1, and Figure 2 shows the AER results for different models.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Performance of the mention detection system using lexical, syntactic, gazetteer features as well as features obtained by running other named-entity classifiers named-entity classifiers (with different semantic tag sets).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4 shows examples of alignment templates",
        "Entity": "Reference"
    },
    {
        "Text": "The probabilities P( <U-t>lwi_I) can be esti mated from the relative frequencies in the training corpus whose infrequent words are replaced with their corresponding unknown word tags based on their part of speeches 2 Table 1 shows examples of word bigrams including unknown word tags.",
        "Entity": "Reference"
    },
    {
        "Text": ", C(e )) (18) 1 1 1 1 i=1 i 1",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Dev set frequencies for the two most significant discourse markers in Arabic are skewed toward analysis as a conjunction.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4",
        "Entity": "Reference"
    },
    {
        "Text": "As shown in Figure 8, relative word performance was not degraded and sometimes even slightly better.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Dependency representation of example (2) from Talbanken05.",
        "Entity": "Reference"
    },
    {
        "Text": "To simplify the description, we assume in Figure 2 that a bigram language model is used and all the TTS templates are binarized.",
        "Entity": "Reference"
    },
    {
        "Text": "This corresponds to maximizing the equivocation or maximizing the likelihood of the direct-translation model",
        "Entity": "Reference"
    },
    {
        "Text": "The model is described using a log-linear modeling approach, which is a generalization of the often used source channel approach",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6: Sample targets for meta alternations with high AP and mid-coherence values.",
        "Entity": "Reference"
    },
    {
        "Text": "We compared the ATB5 to tree- banks for Chinese (CTB6), German (Negra), and English (WSJ) (Table 4)",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4: An example showing how to compute the target side position of a semantic role by using the median of its aligning points.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6 shows the word segmentation accuracy of four unknown word models over test set-2.",
        "Entity": "Reference"
    },
    {
        "Text": "It should be emphasized that this constraint to consecutive phrases limits the expressive power.",
        "Entity": "Reference"
    },
    {
        "Text": "The model is described using a log-linear modeling approach, which is a generalization of the often used source channel approach",
        "Entity": "Reference"
    },
    {
        "Text": "equation (1)",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Diacritized particles and pseudo-verbs that, after orthographic normalization, have the equivalent surface form 0 an.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 shows empirical search timings for various values of M , for the MEMD model described in the next section.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 shows the influence of the four parameters.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Features based on the token string that are based on the probability of each name class during training.",
        "Entity": "Caption"
    },
    {
        "Text": "Tables 4 and 5 show the effect of the pruning parameter tp with the histogram pruning parameter Np = 50,000.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "The second model is Equa tion (13), which is a set of word models trained for",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8 shows the effect of the length of the language model history on translation quality.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 9: Dev set results for sentences of length \u2264 70.",
        "Entity": "Caption"
    },
    {
        "Text": "equation (2)",
        "Entity": "Reference"
    },
    {
        "Text": "We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4, 5, and 6.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Performance of the mention detection system using lexical, syntactic, gazetteer features as well as features obtained by running other named-entity classifiers named-entity classifiers (with different semantic tag sets).",
        "Entity": "Reference"
    },
    {
        "Text": "U = {up}m represent the hidden m struct a new graph G1 (Figure 1(d)) with the clusters U as vertices.",
        "Entity": "Reference"
    },
    {
        "Text": "The distribution of errors is displayed in Table 4.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "The entity features can be attached under the top node, the entity nodes, or directly combined with the entity nodes as in Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2 plots AP by for all meta alternations.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3:",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Tagging accuracies on development data in percent.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Accuracy for MBL and SVM classifiers on Talbanken05 nouns in accumulated frequency bins by Parole frequency.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1), i.e., the degree to which a sense pair (s1, s2) matches a meta alternation a.",
        "Entity": "Reference"
    },
    {
        "Text": "L set of lemmas IL set of (lemma-wise) instances SL set of (lemma-wise) senses inst : L (IL ) mapping lemma instances sns : L (SL ) mapping lemma senses M set of meta senses meta : SL M mapping senses meta senses A M M set of meta alternations (MAs) A set of MA representations score : A S2 R scoring function for MAs repA : A A MA representation function comp : A S2 R compatibility function Table 1: Notation and signatures for our framework.",
        "Entity": "Reference"
    },
    {
        "Text": "Word Sense clusters Class-label arms knees trousers feet biceps hips elbows backs wings body part breasts shoulders thighs bones buttocks ankles legs inches wrists shoes necks horses muskets charges weapons methods firearms weapon knives explosives bombs bases mines projectiles drugs missiles uniforms jersey israel colomho guernsey luxeinhourg denmark maim European greece belgium swede, turkey gibraltar portugal ire- country land mauritius britain cyprus netherlands norway aus tralia italy japan canada kingdom spain austria zealand england france germany switzerland finland poland a merica usa iceland holland scotland uk crucifix bow apron sweater tie anorak hose bracelet garment helmet waistcoat jacket pullover equipment cap collar suit fleece tunic shirt scarf belt head voice torso back chest face abdomen side belly groin body part spine breast bill rump midhair hat collar waist tail stomach skin throat neck speculum ceo treasurer justice chancellor principal founder pres- person ident commander deputy administrator constable li brarian secretary governor captain premier executive chief curator assistant committee patron ruler oil heat coal power water gas food wood fuel steam tax object heating kerosene fire petroleum dust sand light steel telephone timber supply drainage diesel electricity acid air insurance petrol tempera gouache watercolour poster pastel collage paint acrylic lemon bread cheese [flint butter jam cream pudding yogurt foodstuff sprinkling honey jelly toast ham chocolate pie syrup milk meat beef cake yoghurt grain hazel elder holly family virgin hawthorn shrub cherry cedar larch mahogany water sycamore lime teak ash wood hornbeam oak walnut hazel pine beech alder thorn poplar birch chestnut blackthorn spruce holly yew lau rel maple elm fir hawthorn willow bacon cream honey pie grape blackcurrant cake ha- foodstuff mama",
        "Entity": "Caption"
    },
    {
        "Text": "In fact, from the last column of Figure 8 we see that even if our algorithm has access to only five anno tated sequences when Juman has access to ten times as many, we still achieve better precision and better F measure.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Gross statistics for several different treebanks.",
        "Entity": "Caption"
    },
    {
        "Text": "L set of lemmas IL set of (lemma-wise) instances SL set of (lemma-wise) senses inst : L (IL ) mapping lemma instances sns : L (SL ) mapping lemma senses M set of meta senses meta : SL M mapping senses meta senses A M M set of meta alternations (MAs) A set of MA representations score : A S2 R scoring function for MAs repA : A A MA representation function comp : A S2 R compatibility function Table 1: Notation and signatures for our framework.",
        "Entity": "Reference"
    },
    {
        "Text": "Finally, Table 4 shows the results for the unconstrained HGFC on T2 and and T3 when the tree structure is not predefined but inferred automatically as described in section 3.2.3.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Accuracy for MBL and SVM classifiers on Talbanken05 nouns in accumulated frequency bins by Parole frequency.",
        "Entity": "Reference"
    },
    {
        "Text": "Contribution of constituent dependencies in respective mode (inside parentheses) and accumulative mode (outside parentheses) The table shows that the final DSPT achieves the best performance of 77.4%/65.4%/70.9 in precision/recall/F-measure respectively after applying all the dependencies, with the increase of F-measure by 8.2 units compared to the baseline MCT.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2",
        "Entity": "Reference"
    },
    {
        "Text": "If we directly translate the EM algorithm into the log- linear model, the problem becomes maximizing 0 X P r(S, T , D) = X @ Y P r(t) Y 1 P r(f )A the data likelihood represented by feature weights instead of feature probabilities: D D t D f F (S,T .role,D) Though the above formulation, which makes the P r(S, T ) = D exp i i fi (S, T , D) total probability of all the pairs of trees and strings P P exp P f (S , T , D ) St ,T t Dt i i i less than 1, is not a strict generative model, we can still use the EM algorithm (Dempster et al., 1977) to estimate the probability of the TTS templates and the semantic features, as shown in Figure 6.",
        "Entity": "Reference"
    },
    {
        "Text": "The corresponding figures for the test data are. 89.53% for our tagger and 88.88% for the TnT tag- ger.",
        "Entity": "Reference"
    },
    {
        "Text": "The dataset (table 1) consists of the main text of 28 articles selected from the topical domains of history, sports, science, and technology",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Another interesting example is shown in Figure 1(b), where the base-NP of the second entity town is a possessive NP and there is no relationship between the entities one and town defined in the ACE corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "Throughout in this paper, we used Equation (9) to compute the word spelling probabilities.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Results of different systems on the CoNLL 12 English data sets.",
        "Entity": "Reference"
    },
    {
        "Text": "This list of 43 words is shown in Table 3.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4 shows the number of sentences, words, and characters of the training and test sets.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Performance of the mention detection system using lexical features only.",
        "Entity": "Reference"
    },
    {
        "Text": "equation 2) says that the prob ability of starting a new entity, given the current mention m and the previous entities e1, e2, , et, is simply 1 minus the maximum link probability between the current mention and one of the previous entities.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4",
        "Entity": "Reference"
    },
    {
        "Text": "As shown in figure 3, read times are much higher for predictions that get accepted, re B(x, k, a) = R1(x) + T (x, k) E(x, k), a = 1 R0(x), a = 0 flecting both a more careful perusal by the translator and the fact the rejected predictions are often simplywhere Ra(x) is the cost of reading x when it ulignored.2 In both cases there is a weak linear rela timately gets accepted (a = 1) or rejected (a = 0), T (x, k) is the cost of manually typing xk , and E(x, k) is the edit cost of accepting x and erasing to the end of its first k characters.",
        "Entity": "Reference"
    },
    {
        "Text": "If we directly translate the EM algorithm into the log- linear model, the problem becomes maximizing 0 X P r(S, T , D) = X @ Y P r(t) Y 1 P r(f )A the data likelihood represented by feature weights instead of feature probabilities: D D t D f F (S,T .role,D) Though the above formulation, which makes the P r(S, T ) = D exp i i fi (S, T , D) total probability of all the pairs of trees and strings P P exp P f (S , T , D ) St ,T t Dt i i i less than 1, is not a strict generative model, we can still use the EM algorithm (Dempster et al., 1977) to estimate the probability of the TTS templates and the semantic features, as shown in Figure 6.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 shows these similarity measures.",
        "Entity": "Reference"
    },
    {
        "Text": "So we estimate that English translations are present in the English part of the corpus for Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Effect of Arabic stemming features on coreference resolution.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Results for different predictor configurations.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5 Effect of pruning parameter tp and heuristic function on error rate for direct-translation model (Np = 50,000).",
        "Entity": "Caption"
    },
    {
        "Text": "CoreLex defines a layer of abstraction above WordNet consisting of 39 basic types, coarse- grained ontological classes (Table 2).",
        "Entity": "Reference"
    },
    {
        "Text": "We obtain the following decision rule: eI = argmax Pr(eI | f J ) 1 1 1 I 1 M ) = argmax m hm (eI , f J ) 1 1 I m=1",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "(2004) makes use of a coding manual designed for a project studying genitive modification (Garretson et al., 2004) and presents an explicit annotation scheme for an _ Samma _ PO _ KP erfarenhet NN _ gjorde VV PT engelsmannen NN DD|HH imacy, illustrated by figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 underscores the virtues of Sentence Recency: In the most recent sentence with antecedents satisfying the filters, there are on aver ble.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3 shows the performance and processing time comparison of various models and their combinations.",
        "Entity": "Reference"
    },
    {
        "Text": "and 2",
        "Entity": "Reference"
    },
    {
        "Text": "The overall architecture of the statistical translation approach is summarized in Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 Classes of words found by ST for the test corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "As can be seen in Table 4, our training data is a lot less than those used by MENE and IdentiFinder3.",
        "Entity": "Reference"
    },
    {
        "Text": "and ap plied to an Arabic sentence in figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4",
        "Entity": "Reference"
    },
    {
        "Text": "even after removal of the wing-node, the two areas of meaning are still linked via tail",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8b shows that verbal nouns are the hardest pre-terminal categories to identify.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: An example showing the combination of the semantic role sequences of the states.",
        "Entity": "Reference"
    },
    {
        "Text": "and Table 6)",
        "Entity": "Reference"
    },
    {
        "Text": "(2011), we confirmed that omission of the look-ahead features results in a 0.26% decrease in the parsing accuracy on CTB5d (dev).Figure 2: F1 scores (in %) of SegTagDep on CTB 5c1 w.r.t. the training epoch (x-axis) and parsing feature weights (in legend).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 gives the mean values for the comparison of troughs placed by the segmentation algorithm to the segmentation points identified by the test subjects for all the texts",
        "Entity": "Reference"
    },
    {
        "Text": "Probabilities We find that Equation (7) assigns too little proba bilities to long words (5 or more characters).",
        "Entity": "Reference"
    },
    {
        "Text": "In this section, we try to compare our results with those obtained by IdentiFinder ' 97 (Bikel et al., 1997), IdentiFinder ' 99 (Bikel et al., 1999), and MENE (Borthwick, 1999).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "able 2 shows the corpus statistics for this task.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 shows a comparison with some existing toolkits that build deterministic, minimized automata/transducers.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 shows the NA M N O M PR O NA M 34 13 (21 %) 67 (6 6 %) 11 (4 6 %) N O M 43 (67 %) 21 48 (4 9 %) 9 (8 9 %) PR O 86 8 (32 %) 17 71 (5 5 %) 53 08 (2 4 %) Table 2: Number of clustering decisions made according to mention type (rows anaphor, columns antecedent) and percentage of wrong decisions.",
        "Entity": "Reference"
    },
    {
        "Text": "As can be seen in Table 4, our training data is a lot less than those used by MENE and IdentiFinder3.",
        "Entity": "Reference"
    },
    {
        "Text": "The first machine, illustrated in Figure 1 encodes the prefix and suffix expansion rules, producing a lattice of possible segmentations.",
        "Entity": "Reference"
    },
    {
        "Text": "The distinctions in the ATB are linguistically justified, but complicate parsing.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6 lists a sample of targets for the five meta alternations involved.",
        "Entity": "Reference"
    },
    {
        "Text": "As each global feature group is added to the list of features, we see improvements to both MUC6 and",
        "Entity": "Reference"
    },
    {
        "Text": "Preliminary observations show that the different neighbours in Table 1 can be used to indicate with great accuracy which of the senses is being used",
        "Entity": "Reference"
    },
    {
        "Text": "If we compare the error rates in Table 7, which correspond to about 55 search errors in Table 6, we obtain an mWER of 36.7% (53 search errors) using no heuristic function and an mWER of 32.6% (57 search errors) using the combined heuristic function.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Distributions of Morph Examples",
        "Entity": "Reference"
    },
    {
        "Text": "(2004) in figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "#c is the total number of new Chinese source words in the period",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Approximate times in seconds to generate predictions of maximum word sequence length M , on a 1.2GHz processor, for the MEMD model.",
        "Entity": "Reference"
    },
    {
        "Text": "E.g. rat and printer are very different in meaning, but they are both closely related to different meanings of mouse",
        "Entity": "Reference"
    },
    {
        "Text": "= argmax S I n s=1 a l) p (fs , a | es ) (7)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: The amount of training and test sets The first factor in the righthand side of Equa tion (13) is estimated from the relative frequency of the corresponding events in the training corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 summarizes the results obtained with different taggers and tagsets on the development data.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 presents the distributions of some examples of morphs and their targets in English Twitter and Chinese Sina Weibo.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 Similarity matrix for segmentation judgments.",
        "Entity": "Reference"
    },
    {
        "Text": "Tables 6 and 7 show the effect of the pruning pa rameter Np with the pruning parameter tp = 10 12",
        "Entity": "Reference"
    },
    {
        "Text": "Our experimental data was drawn from 150 megabytes of 1993 Nikkei newswire (see Figure I).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 lists both the success rate maximally achievable (broken down according to different types of pronouns) and the average number of antecedents remaining after applying each factor.",
        "Entity": "Reference"
    },
    {
        "Text": "W can be encoded by an undirected graph G (Figure 2(a)), where the nouns are mapped to vertices and Wij is the edge weight between vertices i and j",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "Shortest-enclosed Path Tree from P/R/F (81.1/6.7/73.2) of Dynamic Context-Sensitive Shortest- enclosed Path Tree according to Table 2 (Zhou et al., 2007)",
        "Entity": "Reference"
    },
    {
        "Text": "and Table 6 show a comparison of the segmentation and POS tagging accuracies with other state-of-the-art models.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2 shows the word length distribution of words consists of only kanji characters and words consists of only katakana characters.",
        "Entity": "Reference"
    },
    {
        "Text": "In all four tables, we provide theresults for using no heuristic functions and three variants of an increasingly infor mative heuristic function.",
        "Entity": "Reference"
    },
    {
        "Text": "Based on this experiment, we set the beam size of SegTagDep to 64 throughout the exper 64 96.28 92.37 74.96 0.48 Table 3: F1 scores and speed (in sentences per sec.)",
        "Entity": "Reference"
    },
    {
        "Text": "figure.",
        "Entity": "Reference"
    },
    {
        "Text": "A standard criterion on a parallel training corpus consisting of S sentence pairs {(fs , es ): s = 1, .",
        "Entity": "Reference"
    },
    {
        "Text": "To reduce the memory requirement of the alignment templates, we compute these probabilities only for phrases up to a certain maximal length in the source language.",
        "Entity": "Reference"
    },
    {
        "Text": "On the contrary, in the above training stage, although the samples are not accurate enough to represent the distribution defined by Equation 7 for each alignment aj , it is accurate enough for computing the expected counts, which are defined at the corpus level.",
        "Entity": "Reference"
    },
    {
        "Text": ">10 nouns (a) (b) classified as 222 125 (a) class animate 49 3390 (b) class inanimate Table 4: Confusion matrix for the MBLclassifier with a general feature space on the >10 data set on Talbanken05 nouns.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5 breaks down the performance of the best CAM model by meta alternation.",
        "Entity": "Reference"
    },
    {
        "Text": "Results: Table I gives the results for the comparison of the troughs placed by the segmentation algorithm to the known subject change points.",
        "Entity": "Reference"
    },
    {
        "Text": "where N is role features when combining two children states, and ex amples can be found in Figure 3.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Approximate times in seconds to generate predictions of maximum word sequence length M , on a 1.2GHz processor, for the MEMD model.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1. Accuracy of our system in each period (M = 10)",
        "Entity": "Caption"
    },
    {
        "Text": "As an example, the probability of accepting the prediction in figure 1 is about .25.",
        "Entity": "Reference"
    },
    {
        "Text": "equation 2) says that the prob ability of starting a new entity, given the current mention m and the previous entities e1, e2, , et, is simply 1 minus the maximum link probability between the current mention and one of the previous entities.",
        "Entity": "Reference"
    },
    {
        "Text": "The second model is the combination of Poisson distribution (Equation (6)) and character bigram",
        "Entity": "Reference"
    },
    {
        "Text": "equation (2)",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 6 shows example sentences annotated by HGFC.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4 Examples of alignment templates obtained in training",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8: Relative word accuracy as a function of training set size.",
        "Entity": "Reference"
    },
    {
        "Text": "BP(f J , eI , A) = f j+m , ei+n 1 1 j i : (i , j ) A : j j j + m i i i + n (9) (i , j ) A : j j j + m i i i + n",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "An extract of the results is listed in table 1",
        "Entity": "Reference"
    },
    {
        "Text": "tables 2",
        "Entity": "Reference"
    },
    {
        "Text": "Table 9: Dev set results for sentences of length \u2264 70.",
        "Entity": "Caption"
    },
    {
        "Text": "The alignment aJ that has the highest probability (under a certain model) is also called the Viterbi alignment (of that model)",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: Dev set learning curves for sentence lengths \u2264 70. All three curves remain steep at the maximum training set size of 18818 trees.",
        "Entity": "Caption"
    },
    {
        "Text": "For example, no synset covers any combinations of the main words in Figure 2, namely buy , acquire and merger",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 shows the bilingual phrases containing between two and seven words that result from the application of this algorithm to the alignment of Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Organisation of the hierarchical graph of concepts Following previous semantic noun classification experiments (Pantel and Lin, 2002; Bergsma et al., 2008), we use the grammatical relations (GRs) as features for clustering.",
        "Entity": "Reference"
    },
    {
        "Text": "Typically, the translation probability Pr(eI | f J ) is decomposed via additional hid 1 1 den variables",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "The baseline system in Table 3 refers to the maximum entropy system that uses only local features.",
        "Entity": "Reference"
    },
    {
        "Text": "If we compare the error rates in Table 7, which correspond to about 55 search errors in Table 6, we obtain an mWER of 36.7% (53 search errors) using no heuristic function and an mWER of 32.6% (57 search errors) using the combined heuristic function.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1.",
        "Entity": "Reference"
    },
    {
        "Text": "PRO (c) Entity-Paired Tree(EPT)Figure 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 10 Translation results on the Hansards task",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Sources of Dictionaries",
        "Entity": "Caption"
    },
    {
        "Text": "Strube (1998) s centeri ng appro ach (whos e senten ce orderi ng is designate d as SR2 in Table 2) also deals with and even prefer s intrase ntenti al anaph ora, which raises the upper limit to a more accept able 80.2% .",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 shows four words",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Comparison against Stevenson and Joanis (2003) s result on T1 (using similar features).",
        "Entity": "Reference"
    },
    {
        "Text": "A standard criterion on a parallel training corpus consisting of S sentence pairs {(fs , es ): s = 1, .",
        "Entity": "Reference"
    },
    {
        "Text": "table",
        "Entity": "Reference"
    },
    {
        "Text": "Comparison of segmentation algorithm using different linguistic features.",
        "Entity": "Reference"
    },
    {
        "Text": "We compare in Table 2 the performance of Unified Parse and Semantic Trees with different kinds of Entity Semantic Tree setups using standard convolution tree kernel, while the SPT and DSPT with only entity-type information are listed for reference.",
        "Entity": "Reference"
    },
    {
        "Text": "In the table, Nc indicates the number of clusters in the inferred tree, while Nl indicates the closest match to the number of classes in the gold standard.",
        "Entity": "Reference"
    },
    {
        "Text": "The tagset refinement increases the accuracy by about 0.6%, and the external lexicon by another 3.5%.",
        "Entity": "Reference"
    },
    {
        "Text": "For MUC6, the reduction in error due to global features is 27%, and for MUC7,14%.",
        "Entity": "Reference"
    },
    {
        "Text": "where N is role features when combining two children states, and ex amples can be found in Figure 3.",
        "Entity": "Reference"
    },
    {
        "Text": "hWRD(eI , f J , K , zK ) = log n p(ei | {fj | (i, j) A}, Ei ) (14) 1 1 1 1 i=1",
        "Entity": "Caption"
    },
    {
        "Text": "See Fig ure 6 for some examples.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Performance of the mention detection system using lexical, syntactic, gazetteer features as well as features obtained by running other named-entity classifiers named-entity classifiers (with different semantic tag sets).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "As Table 1 shows, word bigrams whose infrequent word bigram",
        "Entity": "Reference"
    },
    {
        "Text": "A relative comparison of running a selection of regular expressions and scripts against other finite-state toolkits.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 9: Entries indicate whether best performance is achieved using the local maximum condition (M), the threshold condition (T), or both.",
        "Entity": "Reference"
    },
    {
        "Text": "segmentation (Table 2).",
        "Entity": "Reference"
    },
    {
        "Text": "Equation (12), which is a set of word models trained for each part of speech (POS + Poisson + bigram).",
        "Entity": "Reference"
    },
    {
        "Text": "The TnT tagger achieves 86.3% accuracy on the default tagset.",
        "Entity": "Reference"
    },
    {
        "Text": "In all four tables, we provide theresults for using no heuristic functions and three variants of an increasingly infor mative heuristic function.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1, Figure 1, and Figure 2 shows the AER results for different models.",
        "Entity": "Reference"
    },
    {
        "Text": "The second condition is necessary to allow for single-character words (see Figure 3).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 The cost as a novel given name (second position) for hanzi from various radical classes.",
        "Entity": "Reference"
    },
    {
        "Text": "Figures 4",
        "Entity": "Reference"
    },
    {
        "Text": "See Fig ure 6 for some examples.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5 Performance on morphological analysis.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 shows the NA M N O M PR O NA M 34 13 (21 %) 67 (6 6 %) 11 (4 6 %) N O M 43 (67 %) 21 48 (4 9 %) 9 (8 9 %) PR O 86 8 (32 %) 17 71 (5 5 %) 53 08 (2 4 %) Table 2: Number of clustering decisions made according to mention type (rows anaphor, columns antecedent) and percentage of wrong decisions.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 is, in fact, a weighted sum of these two distributions.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: Accuracy on development data depend ing on context size",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8 shows that tagging precision is im proved from 88.2% to 96.6%.",
        "Entity": "Reference"
    },
    {
        "Text": "The probabilities P( <U-t>lwi_I) can be esti mated from the relative frequencies in the training corpus whose infrequent words are replaced with their corresponding unknown word tags based on their part of speeches 2 Table 1 shows examples of word bigrams including unknown word tags.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 6: EM Algorithm For Estimating TTS Templates and Semantic Features framework (May and Knight, 2007).",
        "Entity": "Reference"
    },
    {
        "Text": "For instance, the gain for the prediction in figure 1 would be 2 7 8 = 6.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: A sentence from the article Islamic GoldenAge, with the supersense tagging from one of two anno tators.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Feature templates for the full joint model.",
        "Entity": "Reference"
    },
    {
        "Text": "For example, the pairwise words orange and peel form a collocation.",
        "Entity": "Reference"
    },
    {
        "Text": "we directly model the posterior probability Pr(eI| f J )",
        "Entity": "Reference"
    },
    {
        "Text": "As column 5 (SVM) in table 2 shows, the classification results are very similar to the results obtained with MBL.12 We furthermore find a very similar set of errors, and in particular, we find that 51.0 % of the errors for the inanimate class are nouns with the gradient animacy properties presented in (9)-(13) above.",
        "Entity": "Reference"
    },
    {
        "Text": "However, this information is hard to extract reliably from the available data; and even if were obtainable, many of the 0.3 0.2 0.1 0 60 50 40 30 20 10 0 10 20 30 40 50 60 gain (length of correct prefix length of incorrect suffix) Figure 2: Probability that a prediction will be accepted versus its gain.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2",
        "Entity": "Reference"
    },
    {
        "Text": "The list of the features used in our joint model is presented in Table 1, where S01 S05, W01 W21, and T01 05 are taken from Zhang and Clark (2010), and P01 P28 are taken from Huang and Sagae (2010).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 shows the absolute frequencies of sen tence recency values when only the most recent antecedent (in the order just stated) is considered.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2",
        "Entity": "Reference"
    },
    {
        "Text": "This sum can be computed efficiently using the algorithm shown in Figure 8",
        "Entity": "Reference"
    },
    {
        "Text": "As the search space increases expo nentially, it is not possible to explicitly represent it.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Decoding algorithm for the standard Tree-to-String transducer.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Statistics from 1993 Japanese newswire (NIKKEI), 79,326,406 characters total.",
        "Entity": "Reference"
    },
    {
        "Text": "#c is the total number of new Chinese source words in the period",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Prec. is the precision.",
        "Entity": "Reference"
    },
    {
        "Text": "Collocations were automatically located in a text by looking up pairwise words in this lexicon",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "Tables 6 and 7 show the effect of the pruning pa rameter Np with the pruning parameter tp = 10 12",
        "Entity": "Reference"
    },
    {
        "Text": "The probability of using an alignment template to translate a specific source language phrase f is estimated by means of relative frequency",
        "Entity": "Reference"
    },
    {
        "Text": "But it conflates the coordinating and discourse separator functions of wa (<..4.b \ufffd \ufffd) into one analysis: conjunction(Table 3).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 shows the distribution of character type sequences that constitute the infrequent words in the EDR corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "The first machine, illustrated in Figure 1 encodes the prefix and suffix expansion rules, producing a lattice of possible segmentations.",
        "Entity": "Reference"
    },
    {
        "Text": "table",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4: An example showing how to compute the target side position of a semantic role by using the median of its aligning points.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Scores for CityU corpus",
        "Entity": "Reference"
    },
    {
        "Text": "exp[ M m hm (eI , f J )] m=1 1 1 M I J (3)",
        "Entity": "Caption"
    },
    {
        "Text": "For instance, the gain for the prediction in figure 1 would be 2 7 8 = 6.",
        "Entity": "Reference"
    },
    {
        "Text": "Only tokens with initCaps not found in commonWords are tested against each list in Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Results for different predictor configurations.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2 shows empirical estimates of p(a = 1|2k l) from the TransType data.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Probability estimation tree for the nomi native case of nouns.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8 Algorithm min-jumps to compute the minimum number of needed jumps D(cJ , j) to complete the translation",
        "Entity": "Caption"
    },
    {
        "Text": "We illustrate its use with an example (see Then, we average the contributions of each n-gram order: Figure 2).",
        "Entity": "Reference"
    },
    {
        "Text": "Training time comparison.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Performance of Algorithms switch under different input data.",
        "Entity": "Reference"
    },
    {
        "Text": "Except our own and MENE + reference resolution, the results in Table 6 are all official MUC7 results.",
        "Entity": "Reference"
    },
    {
        "Text": "As column 5 (SVM) in table 2 shows, the classification results are very similar to the results obtained with MBL.12 We furthermore find a very similar set of errors, and in particular, we find that 51.0 % of the errors for the inanimate class are nouns with the gradient animacy properties presented in (9)-(13) above.",
        "Entity": "Reference"
    },
    {
        "Text": "As shown in Table 3, using just context information alone, 10 Chinese words (the first 10) have their correct English translations at rank one position.",
        "Entity": "Reference"
    },
    {
        "Text": "In Table 7 we give results for several evaluation metrics.",
        "Entity": "Reference"
    },
    {
        "Text": "Examples are given in Table 4.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Performance on T3 using a predefined tree structure.",
        "Entity": "Reference"
    },
    {
        "Text": "Only tokens with initCaps not found in commonWords are tested against each list in Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5 breaks down the performance of the best CAM model by meta alternation.",
        "Entity": "Reference"
    },
    {
        "Text": "Ta SegTag 97.66 93.61 SegTagDep 97.73 94.46 SegTag(d) 98.18 94.08 SegTagDep(d) 98.26 94.64 Table 5: Final results on CTB5j 76 75 74 ble 4 shows the segmentation, POS tagging, and dependency parsing F1 scores of these models on CTB5c.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "The TnT tagger achieves 86.3% accuracy on the default tagset.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 shows the usefulness evaluation result.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1).",
        "Entity": "Reference"
    },
    {
        "Text": "n our experimentations, SVMlight (Joachims, 1998) with the tree kernel function (75.0) (53.7) (62.6) Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "able 1 shows the bilingual phrases containing between two and seven words that result from the application of this algorithm to the alignment of Figure 2.",
        "Entity": "Reference"
    },
    {
        "Text": "This is especially true in the case of quotations\u2014which are common in the ATB\u2014where (1) will follow a verb like (2) (Figure 1).",
        "Entity": "Reference"
    },
    {
        "Text": "Again, Table 2 shows that using stem n-grams features gave a small boost to the whole main-type classification system4.",
        "Entity": "Reference"
    },
    {
        "Text": "In this section, we try to compare our results with those obtained by IdentiFinder ' 97 (Bikel et al., 1997), IdentiFinder ' 99 (Bikel et al., 1999), and MENE (Borthwick, 1999).",
        "Entity": "Reference"
    },
    {
        "Text": "Then, every phrase f produces its translation e (using the corresponding alignment template z).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Training Data",
        "Entity": "Caption"
    },
    {
        "Text": "tion (13) is estimated from the relative frequency of the corresponding events in the training corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3.",
        "Entity": "Reference"
    },
    {
        "Text": "A token that is allCaps will also be initCaps.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "To simplify the description, we assume in Figure 2 that a bigram language model is used and all the TTS templates are binarized.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4.",
        "Entity": "Reference"
    },
    {
        "Text": "At the morp heme level, stems are divid ed from their affixe s. For exam ple, altho ugh both naga no (Naga no) and shi (city) can appea r as indivi dual words , nagano shi (Nag ano city) is brack eted as [[naga no][s hi]], since here shi Figure 3: Determining word boundaries.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Performance of the mention detection system including all ACE 04 subtasks",
        "Entity": "Reference"
    },
    {
        "Text": "In order to effectively capture entity-related semantic features, and their combined features as well, especially bi-gram or tri-gram features, we build an Entity-related Semantic Tree (EST) in three ways as illustrated in Figure 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Thus,we have crafted more specific explanations, sum marized for nouns in figure 2.",
        "Entity": "Reference"
    },
    {
        "Text": "The results when we set M = 10 are shown in Table 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 shows four words",
        "Entity": "Reference"
    },
    {
        "Text": "In all four tables, we provide theresults for using no heuristic functions and three variants of an increasingly infor mative heuristic function.",
        "Entity": "Reference"
    },
    {
        "Text": "See Figure 3 for examples.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4 shows a constituent headed by a process nominal with an embedded adjective phrase.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4",
        "Entity": "Reference"
    },
    {
        "Text": "#o is the total number of output English translations.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6 lists a sample of targets for the five meta alternations involved.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3 shows the part of speech prediction accu racy of two unknown word model without context.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Results for different predictor configurations.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8 shows the effect of the length of the language model history on translation quality.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 7: Computing the partition function of the conditional probability P r(S|T ).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2 shows examples of the discovered patterns for the merger and acquisition topic.",
        "Entity": "Reference"
    },
    {
        "Text": "On the contrary, in the above training stage, although the samples are not accurate enough to represent the distribution defined by Equation 7 for each alignment aj , it is accurate enough for computing the expected counts, which are defined at the corpus level.",
        "Entity": "Reference"
    },
    {
        "Text": "range free green lemon peel red state yellow",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 reports experimental results using lexical features only; we observe that the stemming n-gram features boost the performance by one point (64.7 vs. 65.8).",
        "Entity": "Reference"
    },
    {
        "Text": "As we will see from Table 3, not much improvement is derived from this feature.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 evaluates the contributions of different kinds of constituent dependencies to extraction performance on the 7 relation types of the ACE RDC 2004 corpus",
        "Entity": "Reference"
    },
    {
        "Text": ", fjk (11)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: BLEU results",
        "Entity": "Caption"
    },
    {
        "Text": "Pr(eI | f J ) = p M (eI | f J )",
        "Entity": "Caption"
    },
    {
        "Text": "Considering that the way the semantic where all(T ) denotes all the possible target strings which can be generated from the source tree T . Given a set of TTS templates, the new partition function can be efficiently computed using the dynamic programming algorithm shown in Figure 7.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 displays the performance of our model and of the systems that obtained the best (Fernandes et al., 2012) and the median performance in the MUC B3 CEAFe average R P F1 R P F1 R P F1 CoNLL 12 English development data be st 64.",
        "Entity": "Reference"
    },
    {
        "Text": "As shown in Table 3, using just context information alone, 10 Chinese words (the first 10) have their correct English translations at rank one position.",
        "Entity": "Reference"
    },
    {
        "Text": "The points labelled smoothed in figure 2 were obtained using a sliding-average smoother, and the model curve was obtained using two-component Gaussian mixtures to fit the smoothed empirical likelihoods p(gain|a = 0) and p(gain|a = 1).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 6: Metaphors tagged by the system (in bold) whereby the main source of disagreement was the presence of lexicalized metaphors, e.g. verbs such as impose, decline etc.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 9 Corpus statistics for Hansards task (Words*: words without punctuation marks",
        "Entity": "Caption"
    },
    {
        "Text": "(Equation (7)) (Poisson + hi gram).",
        "Entity": "Reference"
    },
    {
        "Text": "The second model is the combination of Poisson distribution (Equation (6)) and character bigram",
        "Entity": "Reference"
    },
    {
        "Text": "We compared the ATB5 to tree- banks for Chinese (CTB6), German (Negra), and English (WSJ) (Table 4)",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3 shows the distribution of SSTs in the corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Dev set frequencies for the two most significant discourse markers in Arabic are skewed toward analysis as a conjunction.",
        "Entity": "Caption"
    },
    {
        "Text": "Excerpt from the collocation lexicon",
        "Entity": "Caption"
    },
    {
        "Text": "The performance of these systems is shown in Table 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "Table 7 Effect of pruning parameter Np and heuristic function on error rate for direct-translation model (tp = 10 12 ).",
        "Entity": "Caption"
    },
    {
        "Text": "The use of the language model feature in equation (18) helps take long-range dependencies better into account",
        "Entity": "Reference"
    },
    {
        "Text": "In all four tables, we provide theresults for using no heuristic functions and three variants of an increasingly infor mative heuristic function.",
        "Entity": "Reference"
    },
    {
        "Text": "By introducing the distinction of word type to the model of Equation(12),we can derive a more sophis ticated unknown word model that reflects both word 3 When a Chinese character is used to represent a seman tically equivalent Japanese verb, its root is written in the Chinese character and its inflectional suffix is written in hi ragana.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 8: Relative word accuracy as a function of training set size.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4 Examples of alignment templates obtained in training",
        "Entity": "Caption"
    },
    {
        "Text": "Tables 6 and 7 show the effect of the pruning pa rameter Np with the pruning parameter tp = 10 12",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8 shows the tagging accuracy of unknown words.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: Dev set learning curves for sentence lengths \u2264 70. All three curves remain steep at the maximum training set size of 18818 trees.",
        "Entity": "Caption"
    },
    {
        "Text": "However, if we remove the mouse-node from its local graph illustrated in figure 1, the graph decomposes into two parts, one representing the electronic device meaning of mouse and the other one representing its animal sense.",
        "Entity": "Reference"
    },
    {
        "Text": ">10 nouns (a) (b) classified as 222 125 (a) class animate 49 3390 (b) class inanimate Table 4: Confusion matrix for the MBLclassifier with a general feature space on the >10 data set on Talbanken05 nouns.",
        "Entity": "Reference"
    },
    {
        "Text": "W can be encoded by a undi rected graph G (Figure 1(a)), where the verbs are mapped to vertices and the Wij is the edge weight between vertices i and j.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1. Accuracy of our system in each period (M = 10)",
        "Entity": "Caption"
    },
    {
        "Text": "segmentation (Table 2).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Average Precision and Coherence ( ) for each meta alternation.",
        "Entity": "Reference"
    },
    {
        "Text": "The algorithm takes into account possibly unaligned words at the boundaries of the source or target language phrases.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Number of recall errors according to mention type (rows anaphor, columns antecedent).",
        "Entity": "Reference"
    },
    {
        "Text": "With an absolute frequency threshold of 10, we obtain an accuracy of 95.4%, which constitutes a 50% reduction of error rate.Table 3 presents the experimental results rela tive to class.",
        "Entity": "Reference"
    },
    {
        "Text": "This is usually straightforward, with the exception of the case where the words that are aligned to a particular role s span in the source side are not continuous in the target side, as shown in Figure 4.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "The argmax operation denotes the search problem, that is, the generation of the output sentence in the target language",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 presents the wide range of cases that are used to create the morphs.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5 shows that this reimplementation almost reproduces the accuracy of their implementation.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: NMI of unconstrained HGFC when trees for T2 and T3 are inferred automatically.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Effect of model parameters on performance.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Frequency distribution for sentence lengths in the WSJ (sections 2\u201323) and the ATB (p1\u20133).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3.",
        "Entity": "Reference"
    },
    {
        "Text": "5http://cactus.aistnara.ac.jp/lab/nltlchasen.html 6http://pine.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html Word accuracy 90 CHASEN JUMAN opllnizt oplnuo recall opiJTozt F Figure 4: Word accuracy.",
        "Entity": "Reference"
    },
    {
        "Text": "However, if we remove the mouse-node from its local graph illustrated in figure 1, the graph decomposes into two parts, one representing the electronic device meaning of mouse and the other one representing its animal sense.",
        "Entity": "Reference"
    },
    {
        "Text": "The probabilities P( <U-t>lwi_I) can be esti mated from the relative frequencies in the training corpus whose infrequent words are replaced with their corresponding unknown word tags based on their part of speeches 2 Table 1 shows examples of word bigrams including unknown word tags.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5",
        "Entity": "Reference"
    },
    {
        "Text": "hWRD(eI , f J , K , zK ) = log n p(ei | {fj | (i, j) A}, Ei ) (14) 1 1 1 1 i=1",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Sample targets for meta alternations with high AP and mid-coherence values.",
        "Entity": "Reference"
    },
    {
        "Text": "The regular expressions available in Foma from highest to lower precedence.",
        "Entity": "Caption"
    },
    {
        "Text": "figure)",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "This group consists of 10 features based on the string , as listed in Table 1.",
        "Entity": "Reference"
    },
    {
        "Text": "(equation 2",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Tagging accuracy on development data depending on context size",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 shows the cross entropy per word and char acter perplexity of three unknown word model.",
        "Entity": "Reference"
    },
    {
        "Text": "For the graph depicted in Figure 1 this algorithm computes the clusters {They, Leaders}, {Paris} and {recent developments}.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 6 Dependencies in the alignment template mode",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8 Effect of the length of the language model history (Unigram/Bigram/Trigram: word-based; CLM: class-based 5-gram).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: AER comparison (cn \u2192en)",
        "Entity": "Caption"
    },
    {
        "Text": "For MUC7, there are also no published results on systems trained on only the official training data of 200 aviation disaster articles.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8: Part of speech tagging accuracy of unknown words (the last column represents the percentage of correctly tagged unknown words in the correctly segmented unknown words",
        "Entity": "Reference"
    },
    {
        "Text": "This class-based model gives reasonable results: for six radical classes, Table 1 gives the estimated cost for an unseen hanzi in the class occurring as the second hanzi in a double GIVEN name.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Number of recall errors according to mention type (rows anaphor, columns antecedent).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Feature templates for the full joint model.",
        "Entity": "Reference"
    },
    {
        "Text": "Collocation: Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio (Church and Hanks, 1990) and outputted to a lexicon.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Number of recall errors according to mention type (rows anaphor, columns antecedent).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Collecting evidence for a word boundary - are the non-straddling n-grams 8 1 and 82 more frequent than the straddling n-grams t 1, t2, and t3?",
        "Entity": "Reference"
    },
    {
        "Text": "The renormalization needed in equation (3) requires a sum over manypossible sentences, for which we do not know of an efficient algorithm",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 6: EM Algorithm For Estimating TTS Templates and Semantic Features framework (May and Knight, 2007).",
        "Entity": "Reference"
    },
    {
        "Text": "Another interesting example is shown in Figure 1(b), where the base-NP of the second entity town is a possessive NP and there is no relationship between the entities one and town defined in the ACE corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "In general, as shown in this figure, there may be additional transformations to make the translation task simpler for the algorithm.",
        "Entity": "Reference"
    },
    {
        "Text": "The joint probability of the two sequences is defined as the product of context probabilities and lexical probabilities over all POS tags:",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Accuracy of part of speech estimation each part of speech and word type (POS + WT + Poisson + bigram).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "The graph displayed in Figure 1 is the graph constructed for the mentions Leaders, Paris, recent developments and They from the example sentence at the beginning of this Section, where R = {P AnaPron, P Subject, N Number}.",
        "Entity": "Reference"
    },
    {
        "Text": "able 1 shows the bilingual phrases containing between two and seven words that result from the application of this algorithm to the alignment of Figure 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 7: Test set results. Maamouri et al. (2009b) evaluated the Bikel parser using the same ATB split, but only reported dev set results with gold POS tags for sentences of length \u2264 40. The Bikel GoldPOS configuration only supplies the gold POS tags; it does not force the parser to use them. We are unaware of prior results for the Stanford parser.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 shows examples of the feature SRR.",
        "Entity": "Reference"
    },
    {
        "Text": "Consider the example in Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Tables 4 and 5 show the effect of the pruning parameter tp with the histogram pruning parameter Np = 50,000",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: A sentence from the article Islamic GoldenAge, with the supersense tagging from one of two anno tators.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2 shows that the tagging accuracy tends to increase with the context size.",
        "Entity": "Reference"
    },
    {
        "Text": "The distinctions in the ATB are linguistically justified, but complicate parsing.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 reports experimental results using lexical features only; we observe that the stemming n-gram features boost the performance by one point (64.7 vs. 65.8).",
        "Entity": "Reference"
    },
    {
        "Text": "Examples are given in Table 4.",
        "Entity": "Reference"
    },
    {
        "Text": "whose unvocalized surface forms 0 an are indistinguishable.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5 Effect of pruning parameter tp and heuristic function on error rate for direct-translation model (Np = 50,000).",
        "Entity": "Caption"
    },
    {
        "Text": "The breakdown of the different types of words found by ST in the test corpus is given in Table 3.",
        "Entity": "Reference"
    },
    {
        "Text": "The Stanford parser (Klein and Manning, 2002) is unable to recover the verbal reading of the unvocalized surface form 0 an (Table 1).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Snapshot of the supersense-annotated data.",
        "Entity": "Reference"
    },
    {
        "Text": "and 3",
        "Entity": "Reference"
    },
    {
        "Text": "Examples are given in Table 4.",
        "Entity": "Reference"
    },
    {
        "Text": "equation 4",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 presents the distributions of some examples of morphs and their targets in English Twitter and Chinese Sina Weibo.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6: Sample targets for meta alternations with high AP and mid-coherence values.",
        "Entity": "Reference"
    },
    {
        "Text": "In Table 5 we present results from small test cor pora for the productive affixes handled by the current version of the system; as with names, the segmentation of morphologically derived words is generally either right or wrong.",
        "Entity": "Reference"
    },
    {
        "Text": "hLEX(eI , f J , K , zK ) = #CO-OCCURRENCES (LEX, eI , f J ) (20) 1 1 1 1 1 1",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5 gives an example of the word alignment and phrase alignment of a German English sentence pair.We describe our model using a log-linear modeling approach",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Character type configuration of infrequent words in the EDR corpus",
        "Entity": "Reference"
    },
    {
        "Text": "It should be emphasized that this constraint to consecutive phrases limits the expressive power.",
        "Entity": "Reference"
    },
    {
        "Text": "hAL(eI , f J , K , zK ) = |j 1 j | (16) 1 1 1 1 k k=1 k 1",
        "Entity": "Caption"
    },
    {
        "Text": "The results for French to English and for English to French are shown in Table 10",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Illustration of dictionary based segmenta tion finite state transducer 3.1 Bootstrapping.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "composite kernel 83.0 72.0 77.1 Zhou et al., (2007): composite kernel 82.2 70.2 75.8 Zhang et al., (2006): composite kernel 76.1 68.4 72.1 Zhao and Grishman, (2005):4 composite kernel 69.2 70.5 70.4 Ours: CTK with UPST 80.1 70.7 75.1Zhou et al., (2007): context sensitive CTK with CS-SPT 81.1 66.7 73.2 Zhang et al., (2006): CTK with SPT 74.1 62.4 67.7 Table 4.",
        "Entity": "Reference"
    },
    {
        "Text": "W can be encoded by a undi rected graph G (Figure 1(a)), where the verbs are mapped to vertices and the Wij is the edge weight between vertices i and j.",
        "Entity": "Reference"
    },
    {
        "Text": "Shortest-enclosed Path Tree from P/R/F (81.1/6.7/73.2) of Dynamic Context-Sensitive Shortest- enclosed Path Tree according to Table 2 (Zhou et al., 2007)",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3 shows the distribution of SSTs in the corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "tables 2",
        "Entity": "Reference"
    },
    {
        "Text": "In general, as shown in this figure, there may be additional transformations to make the translation task simpler for the algorithm.",
        "Entity": "Reference"
    },
    {
        "Text": "hLEX(eI , f J , K , zK ) = #CO-OCCURRENCES (LEX, eI , f J ) (20) 1 1 1 1 1 1",
        "Entity": "Caption"
    },
    {
        "Text": "figure.",
        "Entity": "Reference"
    },
    {
        "Text": "For example, looking at Figure 1(b), V on G can be grouped into three clusters u1, u2 and u3.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "where ECD|S,T (fi), the expected count of a feature over all derivations given a pair of tree and string, can be computed using the modified inside- outside algorithm described in Section 3.2, and ECS |T (fi), the expected count of a feature over all possible target strings given the source tree, can be computed in a similar way to the partition function described in Figure 7.",
        "Entity": "Reference"
    },
    {
        "Text": "his optimization can be performed using the expectation maximization (EM) algorithm (Dempster, Laird, and Rubin 1977).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1shows the word length distribution of in frequent words in the EDR corpus, and the estimate of word length distribution by Equation (6) whose parameter (.A = 4.8) is the average word length of infrequent words.",
        "Entity": "Reference"
    },
    {
        "Text": "The statistics of 96 these splits are shown in Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "As we will see from Table 3, not much improvement is derived from this feature.",
        "Entity": "Reference"
    },
    {
        "Text": "The renormalization needed in equation (3) requires a sum over manypossible sentences, for which we do not know of an efficient algorithm",
        "Entity": "Reference"
    },
    {
        "Text": "Comparison of different systems on the ACE RDC 2004 corpus In Table 3 we summarize the improvements of different tree setups over SPT.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Examples of word bigrams including un known word tags example",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: BLEU4 scores of different systems Source Launching1 New2 Diplomatic3 Offensive4 SRF On 1 2 3 4 SRF Off 2 3 4 It1 is2 therefore3 necessary4 to5 speed6 up7 the8 equal better worse With SRF vs. W/O SRF 72% 20.2% 7.8% Source transformation9 of10 traditional11 industries12 with13 high14 technologies15",
        "Entity": "Reference"
    },
    {
        "Text": "The distinctions in the ATB are linguistically justified, but complicate parsing.",
        "Entity": "Caption"
    },
    {
        "Text": "The algorithm takes into account possibly unaligned words at the boundaries of the source or target language phrases.",
        "Entity": "Reference"
    },
    {
        "Text": "eI = argmax {Pr(eI | f J )} (1)",
        "Entity": "Caption"
    },
    {
        "Text": "Tables 6 and 7 show the effect of the pruning pa rameter Np with the pruning parameter tp = 10 12",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 shows examples of common char acter bigrams for each part of speech in the infre quent words of the EDR corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "This class-based model gives reasonable results: for six radical classes, Table 1 gives the estimated cost for an unseen hanzi in the class occurring as the second hanzi in a double GIVEN name.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: BLEU4 scores of different systems Source Launching1 New2 Diplomatic3 Offensive4 SRF On 1 2 3 4 SRF Off 2 3 4 It1 is2 therefore3 necessary4 to5 speed6 up7 the8 equal better worse With SRF vs. W/O SRF 72% 20.2% 7.8% Source transformation9 of10 traditional11 industries12 with13 high14 technologies15",
        "Entity": "Reference"
    },
    {
        "Text": "Again, Table 2 shows that using stem n-grams features gave a small boost to the whole main-type classification system4.",
        "Entity": "Reference"
    },
    {
        "Text": "IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM",
        "Entity": "Reference"
    },
    {
        "Text": "Word Sense clusters Class-label arms knees trousers feet biceps hips elbows backs wings body part breasts shoulders thighs bones buttocks ankles legs inches wrists shoes necks horses muskets charges weapons methods firearms weapon knives explosives bombs bases mines projectiles drugs missiles uniforms jersey israel colomho guernsey luxeinhourg denmark maim European greece belgium swede, turkey gibraltar portugal ire- country land mauritius britain cyprus netherlands norway aus tralia italy japan canada kingdom spain austria zealand england france germany switzerland finland poland a merica usa iceland holland scotland uk crucifix bow apron sweater tie anorak hose bracelet garment helmet waistcoat jacket pullover equipment cap collar suit fleece tunic shirt scarf belt head voice torso back chest face abdomen side belly groin body part spine breast bill rump midhair hat collar waist tail stomach skin throat neck speculum ceo treasurer justice chancellor principal founder pres- person ident commander deputy administrator constable li brarian secretary governor captain premier executive chief curator assistant committee patron ruler oil heat coal power water gas food wood fuel steam tax object heating kerosene fire petroleum dust sand light steel telephone timber supply drainage diesel electricity acid air insurance petrol tempera gouache watercolour poster pastel collage paint acrylic lemon bread cheese [flint butter jam cream pudding yogurt foodstuff sprinkling honey jelly toast ham chocolate pie syrup milk meat beef cake yoghurt grain hazel elder holly family virgin hawthorn shrub cherry cedar larch mahogany water sycamore lime teak ash wood hornbeam oak walnut hazel pine beech alder thorn poplar birch chestnut blackthorn spruce holly yew lau rel maple elm fir hawthorn willow bacon cream honey pie grape blackcurrant cake ha- foodstuff mama",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 contains results for two different translation models.",
        "Entity": "Reference"
    },
    {
        "Text": "we directly model the posterior probability Pr(eI| f J )",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Results for different user simulations.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 Architecture of the statistical translation approach based on Bayes decision rule.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "The algorithm takes into account possibly unaligned words at the boundaries of the source or target language phrases.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4 shows examples of alignment templates",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 shows the results of an evaluation based on the plain STTS tagset.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8: Part of speech tagging accuracy of unknown words (the last column represents the percentage of correctly tagged unknown words in the correctly segmented unknown words",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5 Example of segmentation of German sentence and its English translation into alignment templates",
        "Entity": "Caption"
    },
    {
        "Text": "Typically, the translation probability Pr(eI | f J ) is decomposed via additional hid 1 1 den variables",
        "Entity": "Reference"
    },
    {
        "Text": "Pr(f J | eI ) = Pr(f J , aJ | eI ) (5) 1 1 1 1 1 J 1",
        "Entity": "Caption"
    },
    {
        "Text": "(2004) makes use of a coding manual designed for a project studying genitive modification (Garretson et al., 2004) and presents an explicit annotation scheme for an _ Samma _ PO _ KP erfarenhet NN _ gjorde VV PT engelsmannen NN DD|HH imacy, illustrated by figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Bootstrapping new heuristics.",
        "Entity": "Reference"
    },
    {
        "Text": "For example, the pairwise words orange and peel form a collocation.",
        "Entity": "Reference"
    },
    {
        "Text": "In these experiments, the input lacks segmentation markers, hence the slightly different dev set baseline than in Table 6.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6: Incremental dev set results for the manually annotated grammar (sentences of length \u2264 70).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5 Example of segmentation of German sentence and its English translation into alignment templates",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8a shows that the best model recovers SBAR at only 71.0% F1.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5",
        "Entity": "Reference"
    },
    {
        "Text": "Cik Figure 1: Local graph of the word mouse",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8a shows that the best model recovers SBAR at only 71.0% F1.",
        "Entity": "Caption"
    },
    {
        "Text": "Thus,we have crafted more specific explanations, sum marized for nouns in figure 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 6: Examples of word, morpheme, and compatible-bracket errors.",
        "Entity": "Reference"
    },
    {
        "Text": "his optimization can be performed using the expectation maximization (EM) algorithm (Dempster, Laird, and Rubin 1977).",
        "Entity": "Reference"
    },
    {
        "Text": "E.g., the city Fez, Mo rocco (figure 1) was tagged as a single LOCATION by one annotator and as two by the other.",
        "Entity": "Reference"
    },
    {
        "Text": "of the 43 words are translated to English multi-word phrases (denoted as \u201cphrase\u201d in Table 3).",
        "Entity": "Reference"
    },
    {
        "Text": "First, the source sentence words f J are grouped into phrases f K . For each phrase f an 1 1 alignment template z is chosen and the sequence of chosen alignment templates is reordered (according to K ).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Performance of Algorithms switch under different input data.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 lists new conceptsthat CAM introduces to manipulate vector represen tations.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4 shows a constituent headed by a process nominal with an embedded adjective phrase.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2",
        "Entity": "Reference"
    },
    {
        "Text": "In our segmentation system, a hybrid strategy is applied (Figure 1): First, forward maximum matching (Chen and Liu, 1992), which is a dictionary-based method, is used to generate a segmentation result.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Length Distribution In word segmentation, one of the major problems of the word length model of Equation (6) is the decom position of unknown words.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "In this section, we try to compare our results with those obtained by IdentiFinder ' 97 (Bikel et al., 1997), IdentiFinder ' 99 (Bikel et al., 1999), and MENE (Borthwick, 1999).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 7: Computing the partition function of the conditional probability P r(S|T ).",
        "Entity": "Reference"
    },
    {
        "Text": "= argmax S I n s=1 a l) p (fs , a | es ) (7)",
        "Entity": "Caption"
    },
    {
        "Text": "We choose t = 1, 5, and 30 for the fertility HMM",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1, Figure 1, and Figure 2 shows the AER results for different models.",
        "Entity": "Reference"
    },
    {
        "Text": "Examples of the deletion features can be found in Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "For example, look ing at Figure 2(b), V on G can be grouped into three clusters u1, u2 and u3.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5 shows the decoding algorithm incorporating the SRR features.",
        "Entity": "Reference"
    },
    {
        "Text": "This approach has been suggested by Papineni, Roukos, and Ward (1997, 1998) for a natural language understanding task.",
        "Entity": "Reference"
    },
    {
        "Text": "In MUC6, the best result is achieved by SRA (Krupka, 1995).",
        "Entity": "Reference"
    },
    {
        "Text": "Table (1) and Eq.",
        "Entity": "Reference"
    },
    {
        "Text": "We establish a corresponding feature function by multiplying the probability of all used alignment templates and taking the logarithm",
        "Entity": "Reference"
    },
    {
        "Text": "Tables 6 and 7 show the effect of the pruning pa rameter Np with the pruning parameter tp = 10 12",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6 Effect of pruning parameter Np and heuristic function on search efficiency for direct-translation model (tp = 10 12 )",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "If M = 10, 15 Chinese words (i.e., the first 19 Chinese words in Table 3 except \u53f6\u739b\u65af,\u5df4\u4f50\u4e9a,\u5769\u57da,\u666e\u5229\u6cd5) have their correct English translations at rank one position.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Sources of Dictionaries",
        "Entity": "Caption"
    },
    {
        "Text": "PRO (c) Entity-Paired Tree(EPT)Figure 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6 shows the word segmentation accuracy of four unknown word models over test set-2.",
        "Entity": "Reference"
    },
    {
        "Text": "Results are shown in Table 2; we see that better word alignment results do not lead to better translations.",
        "Entity": "Reference"
    },
    {
        "Text": "(e | f , i, j): p(ei | fj , i 1 i =1 [(i , j) A], j 1 j =1 [(i, j ) A]) (15)",
        "Entity": "Caption"
    },
    {
        "Text": "If we compare the error rates in Table 7, which correspond to about 55 search errors in Table 6, we obtain an mWER of 36.7% (53 search errors) using no heuristic function and an mWER of 32.6% (57 search errors) using the combined heuristic function",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 shows four words",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Accuracy for MBL and SVM classifiers on Talbanken05 nouns in accumulated frequency bins by Parole frequency.",
        "Entity": "Reference"
    },
    {
        "Text": "Typically, the translation probability Pr(eI | f J ) is decomposed via additional hid 1 1 den variables",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 lists both the success rate maximally achievable (broken down according to different types of pronouns) and the average number of antecedents remaining after applying each factor.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Precision statistics for pronouns.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8b shows that verbal nouns are the hardest pre-terminal categories to identify.",
        "Entity": "Reference"
    },
    {
        "Text": "Figures 4",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 gives the mean values for the comparison of troughs placed by the segmentation algorithm to the segmentation points identified by the test subjects for all the texts",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: AER results",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Average Precision and Coherence ( ) for each meta alternation.",
        "Entity": "Reference"
    },
    {
        "Text": "Our experimental data was drawn from 150 megabytes of 1993 Nikkei newswire (see Figure I).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 6",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Organisation of the hierarchical graph of concepts Following previous semantic noun classification experiments (Pantel and Lin, 2002; Bergsma et al., 2008), we use the grammatical relations (GRs) as features for clustering.",
        "Entity": "Reference"
    },
    {
        "Text": "In Table 1, period 1 is Jul 01 \u2013 Jul 15, period 2 is Jul 16 \u2013 Jul 31, \u2026, period 12 is Dec 16 \u2013 Dec 31.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3.",
        "Entity": "Reference"
    },
    {
        "Text": "eI K 1 = e1 , ek = eik 1 +1 , .",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 shows the effect of the length of the language model history on translation quality.",
        "Entity": "Reference"
    },
    {
        "Text": "The overall architecture of the log-linear modeling approach is summarized in Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 shows the correctness evaluation results.",
        "Entity": "Reference"
    },
    {
        "Text": "The performance of these systems is shown in Table 1.",
        "Entity": "Reference"
    },
    {
        "Text": "of the 43 words are translated to English multi-word phrases (denoted as \u201cphrase\u201d in Table 3).",
        "Entity": "Reference"
    },
    {
        "Text": "If the token starts with a capital letter (initCaps), then an additional feature (init- Caps, zone) is set to 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "able 1 shows the bilingual phrases containing between two and seven words that result from the application of this algorithm to the alignment of Figure 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 9 shows that in fact both contribute to producing good segmentations.",
        "Entity": "Reference"
    },
    {
        "Text": "= argmax S I n s=1 a l) p (fs , a | es ) (7)",
        "Entity": "Caption"
    },
    {
        "Text": "aJ = argmax p (f J , aJ | eI ) (8) 1 1 1 1 J 1",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Time to read and accept or reject proposals versus their length tion, because the empirical probability of acceptance is very low when it is less than zero and rises rapidly as it increases.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1), i.e., the degree to which a sense pair (s1, s2) matches a meta alternation a.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 compares the results of the unconstrained version of HGFC against those of AGG on our largest test set T2.",
        "Entity": "Reference"
    },
    {
        "Text": "Collocations were automatically located in a text by looking up pairwise words in this lexicon",
        "Entity": "Reference"
    },
    {
        "Text": "and 3",
        "Entity": "Reference"
    },
    {
        "Text": "The overall architecture of the statistical translation approach is summarized in Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "p(z = (FJ , EI , A ) J f ) = 1 1 | N(C( f )) (10",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 Architecture of the translation approach based on a log-linear modeling approach",
        "Entity": "Caption"
    },
    {
        "Text": "Tree setups P(%) R(%) F CS-SPT over SPT3 1.5 1.1 1.3 DSPT over SPT 1.1 5.6 3.8 UPST (FPT) over SPT 3.8 10.9 8.0 Table 3.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "An abridged version of the grammatical representation produced by the implemented grammar for this sentence is presented in Figure 1, where the feature structures below the tree correspond to partial grammatical representations of the constituents 16 See Kamp and Reyle (1993) for a comprehensive rendering of DRT, and Branco (2000, Chapter 5) for an.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 is, in fact, a weighted sum of these two distributions.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2 shows the F1 scores of the proposed model (SegTagDep) on CTB5c1 with respect to the training epoch and different parsing feature weights, where Seg , Tag , and Dep respectively denote the F1 scores of word segmentation, POS tagging, and dependency parsing.",
        "Entity": "Reference"
    },
    {
        "Text": "Nc Nl HGFC uncons trained A G G N MI F N M I F 13 0 13 3 57 .3 1 36 .6 5 54 .2 2 32 .6 2 11 4 11 7 54 .6 7 37 .9 6 51 .3 5 32 .4 4 50 51 37 .7 5 40 .0 0 32 .6 1 32 .7 8 Table 2: Performance on T2 using a predefined tree structure.",
        "Entity": "Reference"
    },
    {
        "Text": "The sources of our dictionaries are listed in Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "The TnT tagger achieves 86.3% accuracy on the default tagset.",
        "Entity": "Reference"
    },
    {
        "Text": "(prec2 in Table 8)",
        "Entity": "Reference"
    },
    {
        "Text": "of the 43 words are translated to English multi-word phrases (denoted as \u201cphrase\u201d in Table 3).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 shows examples of the feature SRR.",
        "Entity": "Reference"
    },
    {
        "Text": "In Figure 1(c) we show a sentence one of about 500 people nominated for , where there exists a DISC relationship between the entities one and people",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: (a) An undirected graph G representing the similarity matrix; (b) The bipartite graph showing three clusters on G; (c) The induced clusters U ; (d) The new graph G1 over clusters U ; (e) The new bipartite graph over G1 p |vi ) = Vl 1 ...",
        "Entity": "Reference"
    },
    {
        "Text": "Table 11 gives an overview on the training and test data.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 7",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: Comparison of results for MUC6",
        "Entity": "Caption"
    },
    {
        "Text": "eI = argmax {Pr(eI | f J )} (1)",
        "Entity": "Caption"
    },
    {
        "Text": "#Cor is the number of correct English translations output.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 8 Algorithm min-jumps to compute the minimum number of needed jumps D(cJ , j) to complete the translation",
        "Entity": "Caption"
    },
    {
        "Text": "Equation (12), which is a set of word models trained for each part of speech (POS + Poisson + bigram).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2",
        "Entity": "Reference"
    },
    {
        "Text": "Evaluation results are listed in Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "The second factor of Equation (13) is estimated from the Poisson distribution whose parameter",
        "Entity": "Reference"
    },
    {
        "Text": "PRO (c) Entity-Paired Tree(EPT)Figure 2.",
        "Entity": "Reference"
    },
    {
        "Text": "The last is exhibited for the first mention in figure 1, where one annotator chose ARTIFACT (referring tothe physical book) while the other chose COMMUNICATION (the content)",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: Accuracy on development data depend ing on context size",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 illustrates the effects of different components of the user model by showing results for simulated users who read infinitely fast and accept only predictions having positive benefit (superman); who read normally but accept like superman (rational); and who match the standard user model (real).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 shows examples of common char acter bigrams for each part of speech in the infre quent words of the EDR corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2",
        "Entity": "Reference"
    },
    {
        "Text": "Thus,we have crafted more specific explanations, sum marized for nouns in figure 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 suggests that the best setup is dependent on the specific meta sense or meta alternation being MACRO MICRO repM gram gramlex lex space type MACRO MICRO repA",
        "Entity": "Reference"
    },
    {
        "Text": "(e | f , i, j): p(ei | fj , i 1 i =1 [(i , j) A], j 1 j =1 [(i, j ) A]) (15)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7 Algorithm for breadth-first search with pruning",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4 shows an example of calculating the target side SRS based on a complicated TTS template.",
        "Entity": "Reference"
    },
    {
        "Text": "This corresponds to maximizing the equivocation or maximizing the likelihood of the direct-translation model",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3 gives the algorithm phrase-extract that computes the phrases",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "As shown in Figure 8, relative word performance was not degraded and sometimes even slightly better.",
        "Entity": "Reference"
    },
    {
        "Text": "For example, the expressions in Figure 2 are identified as paraphrases by this method; so these three patterns will be placed in the same pattern set.",
        "Entity": "Reference"
    },
    {
        "Text": "MUC7 test accuracy.",
        "Entity": "Reference"
    },
    {
        "Text": "The test 1:ART.Nom checks if the preceding word is a nominative article.",
        "Entity": "Caption"
    },
    {
        "Text": "If it is made up of all capital letters, then (allCaps, zone) is set to 1",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "vecI : IL Rk instance vector computation C : Rk m Rk centroid computation vecL : L Rk lemma (type) vector computation repM : M Rk meta sense representation Table 3: Additional notation and signatures for CAM explicit sense disambiguation, CAM represents lemmas by their type vectors, i.e., the centroid of their instances, and compares their vectors (attributes) to those of the meta alternation hence the name.",
        "Entity": "Reference"
    },
    {
        "Text": "A simple lexicalized PCFG with second order Markovization gives relatively poor performance: 75.95% F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline (Table 7).",
        "Entity": "Reference"
    },
    {
        "Text": "The annotation manual (Teleman, 1974) states that a markable should be tagged as human (H H) if it may be replaced by the interrogative pronoun vem who and be referred to by the personal pronouns han he or hon she .There are clear similarities between the anno tation for human reference found in Talbanken05 and the annotation scheme for animacy discussed HUM Other animate Inanimate ORG ANIM CONC NCONC TIME PLACE Figure 1: Animacy classification scheme (Zaenen et al., 2004)",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Statistics of datasets.",
        "Entity": "Reference"
    },
    {
        "Text": "A comparison of the lines for grammatical roles and for surface order in Table 1 shows that the same is true in German.",
        "Entity": "Reference"
    },
    {
        "Text": "The first example in Table 3 shows that words ending in ' -' are likely to be nouns.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2",
        "Entity": "Reference"
    },
    {
        "Text": "Tables 4 and 5 show the effect of the pruning parameter tp with the histogram pruning parameter Np = 50,000",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 shows a comparison with some existing toolkits that build deterministic, minimized automata/transducers.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 contains results for two different translation models.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 9 Corpus statistics for Hansards task (Words*: words without punctuation marks",
        "Entity": "Caption"
    },
    {
        "Text": "The dataset (table 1) consists of the main text of 28 articles selected from the topical domains of history, sports, science, and technology",
        "Entity": "Reference"
    },
    {
        "Text": "where ECD|S,T (fi), the expected count of a feature over all derivations given a pair of tree and string, can be computed using the modified inside- outside algorithm described in Section 3.2, and ECS |T (fi), the expected count of a feature over all possible target strings given the source tree, can be computed in a similar way to the partition function described in Figure 7.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "(2004) makes use of a coding manual designed for a project studying genitive modification (Garretson et al., 2004) and presents an explicit annotation scheme for an _ Samma _ PO _ KP erfarenhet NN _ gjorde VV PT engelsmannen NN DD|HH imacy, illustrated by figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Average Precision and Coherence ( ) for each meta alternation.",
        "Entity": "Reference"
    },
    {
        "Text": "We compare in Table 2 the performance of Unified Parse and Semantic Trees with different kinds of Entity Semantic Tree setups using standard convolution tree kernel, while the SPT and DSPT with only entity-type information are listed for reference.",
        "Entity": "Reference"
    },
    {
        "Text": "Examples of the deletion features can be found in Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2 plots AP by for all meta alternations.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Approximate times in seconds to generate predictions of maximum word sequence length M , on a 1.2GHz processor, for the MEMD model.",
        "Entity": "Reference"
    },
    {
        "Text": "equation 4",
        "Entity": "Reference"
    },
    {
        "Text": "This sentence should be tagged as shown in table 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 7: Word segmentation accuracy of unknown words r e c pr ec F Po iss on + bi gr a m W T + P oi ss o n + b i g r a m P O S + P o is s o n + b i g r a m P O S + W T + P o is s o n + bi g ra m 31 .8 45 .5 39 .7 42.",
        "Entity": "Reference"
    },
    {
        "Text": "This leaves us with 60 meta alternations, shown in Table 5.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Comparison against Stevenson and Joanis (2003) s result on T1 (using similar features).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1(d) shows a sentence maintain rental property he owns in the state , where the ART.User-or-Owner relation holds between the entities property and he",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Scores for MSRA corpus",
        "Entity": "Reference"
    },
    {
        "Text": "By far the most frequent tagging error was the confusion of nominative and accusative case.",
        "Entity": "Reference"
    },
    {
        "Text": "We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4, 5, and 6.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8 Effect of the length of the language model history (Unigram/Bigram/Trigram: word-based; CLM: class-based 5-gram).",
        "Entity": "Caption"
    },
    {
        "Text": "However, the spelling model, especially the character bigrams in Equation (17) are hard to es timate because of the data sparseness.",
        "Entity": "Reference"
    },
    {
        "Text": "These semantic features Figure 8: Examples of the MT outputs with and without SRFs",
        "Entity": "Reference"
    },
    {
        "Text": "This class-based model gives reasonable results: for six radical classes, Table 1 gives the estimated cost for an unseen hanzi in the class occurring as the second hanzi in a double GIVEN name.",
        "Entity": "Reference"
    },
    {
        "Text": "(e | f , i, j): p(ei | fj , i 1 i =1 [(i , j) A], j 1 j =1 [(i, j ) A]) (15)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 7: Word segmentation accuracy of unknown words r e c pr ec F Po iss on + bi gr a m W T + P oi ss o n + b i g r a m P O S + P o is s o n + b i g r a m P O S + W T + P o is s o n + bi g ra m 31 .8 45 .5 39 .7 42.",
        "Entity": "Reference"
    },
    {
        "Text": "Cik Figure 1: Local graph of the word mouse",
        "Entity": "Reference"
    },
    {
        "Text": "Improvements of different tree setups over SPT on the ACE RDC 2004 corpus Finally, Table 4 compares our system with other state-of-the-art kernel-based systems on the 7 relation types of the ACE RDC 2004 corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 presents the wide range of cases that are used to create the morphs.",
        "Entity": "Reference"
    },
    {
        "Text": "First, the source sentence words f J are grouped into phrases f K . For each phrase f an 1 1 alignment template z is chosen and the sequence of chosen alignment templates is reordered (according to K ).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 illustrates the effects of different components of the user model by showing results for simulated users who read infinitely fast and accept only predictions having positive benefit (superman); who read normally but accept like superman (rational); and who match the standard user model (real).",
        "Entity": "Reference"
    },
    {
        "Text": "A natural unit for B(x, k, a) is the number of keystrokes saved, so all elements of the above equation are converted to this measure.",
        "Entity": "Reference"
    },
    {
        "Text": "An extract of the results is listed in table 1",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8b shows that verbal nouns are the hardest pre-terminal categories to identify.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3",
        "Entity": "Reference"
    },
    {
        "Text": "Results for 2 and for 10 preceding POS tags as context are reported for our tagger.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "vecI : IL Rk instance vector computation C : Rk m Rk centroid computation vecL : L Rk lemma (type) vector computation repM : M Rk meta sense representation Table 3: Additional notation and signatures for CAM explicit sense disambiguation, CAM represents lemmas by their type vectors, i.e., the centroid of their instances, and compares their vectors (attributes) to those of the meta alternation hence the name.",
        "Entity": "Reference"
    },
    {
        "Text": "The probability of using an alignment template to translate a specific source language phrase f is estimated by means of relative frequency",
        "Entity": "Reference"
    },
    {
        "Text": "A relative comparison of running a selection of regular expressions and scripts against other finite-state toolkits.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 shows the distribution of character type sequences that constitute the infrequent words in the EDR corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "If we compare the error rates in Table 7, which correspond to about 55 search errors in Table 6, we obtain an mWER of 36.7% (53 search errors) using no heuristic function and an mWER of 32.6% (57 search errors) using the combined heuristic function",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2",
        "Entity": "Reference"
    },
    {
        "Text": "Ta SegTag 97.66 93.61 SegTagDep 97.73 94.46 SegTag(d) 98.18 94.08 SegTagDep(d) 98.26 94.64 Table 5: Final results on CTB5j 76 75 74 ble 4 shows the segmentation, POS tagging, and dependency parsing F1 scores of these models on CTB5c.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 9 Corpus statistics for Hansards task (Words*: words without punctuation marks",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5 Example of segmentation of German sentence and its English translation into alignment templates",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Results for different user simulations.",
        "Entity": "Reference"
    },
    {
        "Text": "We set Table 6: Word segmentation accuracy of all words r e c pr ec F Po iss on +b igr a m W T +P oi ss on +b igr a m P O S + Po iss on +b igr a m P O S + W T + Po iss on + bi gr a m 94 .5 94 .4 94 .4 94 .6 93 .1 93 .8 93 .6 93 .7 93 .8 94 .1 94 .0 94 .1",
        "Entity": "Reference"
    },
    {
        "Text": "Considering that the way the semantic where all(T ) denotes all the possible target strings which can be generated from the source tree T . Given a set of TTS templates, the new partition function can be efficiently computed using the dynamic programming algorithm shown in Figure 7.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 9 shows the training and test corpus statistics.",
        "Entity": "Reference"
    },
    {
        "Text": "As Table 1 shows, word bigrams whose infrequent word bigram",
        "Entity": "Reference"
    },
    {
        "Text": "In Figure 1(c) we show a sentence one of about 500 people nominated for , where there exists a DISC relationship between the entities one and people",
        "Entity": "Reference"
    },
    {
        "Text": "Prec. is the precision.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 portrays how the states are aligned using the proposed scheme, where a subtree is denoted as a rectangle with its partial index shown inside it.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Scores for CityU corpus",
        "Entity": "Reference"
    },
    {
        "Text": "The annotation manual (Teleman, 1974) states that a markable should be tagged as human (H H) if it may be replaced by the interrogative pronoun vem who and be referred to by the personal pronouns han he or hon she .There are clear similarities between the anno tation for human reference found in Talbanken05 and the annotation scheme for animacy discussed HUM Other animate Inanimate ORG ANIM CONC NCONC TIME PLACE Figure 1: Animacy classification scheme (Zaenen et al., 2004)",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 summarizes the results obtained with different taggers and tagsets on the development data.",
        "Entity": "Reference"
    },
    {
        "Text": "= argmax M 1 S s=1 ) log p M (es | fs ) (4)",
        "Entity": "Caption"
    },
    {
        "Text": "alignment models Pr(f J , aJ | eI ),",
        "Entity": "Reference"
    },
    {
        "Text": "We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c).",
        "Entity": "Reference"
    },
    {
        "Text": "(2)",
        "Entity": "Caption"
    },
    {
        "Text": "The sources of our dictionaries are listed in Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "As shown in figure 1, a similarity matrix W models one-hop transitions that follow the links from vertices to neighbors.",
        "Entity": "Reference"
    },
    {
        "Text": "Results: Table I gives the results for the comparison of the troughs placed by the segmentation algorithm to the known subject change points.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 portrays how the states are aligned using the proposed scheme, where a subtree is denoted as a rectangle with its partial index shown inside it.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6 shows the word segmentation accuracy of four unknown word models over test set-2.",
        "Entity": "Reference"
    },
    {
        "Text": "The distribution of errors is displayed in Table 4.",
        "Entity": "Reference"
    },
    {
        "Text": "Then, every phrase f produces its translation e (using the corresponding alignment template z).",
        "Entity": "Reference"
    },
    {
        "Text": "= argmax M 1 S s=1 ) log p M (es | fs ) (4)",
        "Entity": "Caption"
    },
    {
        "Text": "exp[ M m hm (eI , f J )] m=1 1 1 M I J (3)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Differences in performance between our system and Wang, Li, and Chang (1992).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Results of different systems on the CoNLL 12 English data sets.",
        "Entity": "Reference"
    },
    {
        "Text": "the target Chen Guangcheng only appears once in Weibo",
        "Entity": "Reference"
    },
    {
        "Text": "The first example in Table 3 shows that words ending in ' -' are likely to be nouns.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 evaluates the contributions of different kinds of constituent dependencies to extraction performance on the 7 relation types of the ACE RDC 2004 corpus",
        "Entity": "Reference"
    },
    {
        "Text": "composite kernel 83.0 72.0 77.1 Zhou et al., (2007): composite kernel 82.2 70.2 75.8 Zhang et al., (2006): composite kernel 76.1 68.4 72.1 Zhao and Grishman, (2005):4 composite kernel 69.2 70.5 70.4 Ours: CTK with UPST 80.1 70.7 75.1Zhou et al., (2007): context sensitive CTK with CS-SPT 81.1 66.7 73.2 Zhang et al., (2006): CTK with SPT 74.1 62.4 67.7 Table 4.",
        "Entity": "Reference"
    },
    {
        "Text": "In all four tables, we provide theresults for using no heuristic functions and three variants of an increasingly infor mative heuristic function",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1, Figure 1, and Figure 2 shows the AER results for different models.",
        "Entity": "Reference"
    },
    {
        "Text": "The annotation manual (Teleman, 1974) states that a markable should be tagged as human (H H) if it may be replaced by the interrogative pronoun vem who and be referred to by the personal pronouns han he or hon she .There are clear similarities between the anno tation for human reference found in Talbanken05 and the annotation scheme for animacy discussed HUM Other animate Inanimate ORG ANIM CONC NCONC TIME PLACE Figure 1: Animacy classification scheme (Zaenen et al., 2004)",
        "Entity": "Reference"
    },
    {
        "Text": "The regular expressions available in Foma from highest to lower precedence.",
        "Entity": "Caption"
    },
    {
        "Text": "A relative comparison of running a selection of regular expressions and scripts against other finite-state toolkits.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 9 shows that in fact both contribute to producing good segmentations.",
        "Entity": "Reference"
    },
    {
        "Text": "However, the spelling model, especially the character bigrams in Equation (17) are hard to es timate because of the data sparseness.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 shows the manual evaluation results based on the entire test set, and the improvement from SRF is significant at p < 0.005 based on a t-test.",
        "Entity": "Reference"
    },
    {
        "Text": "Tables 6 and 7 show the effect of the pruning pa rameter Np with the pruning parameter tp = 10 12",
        "Entity": "Reference"
    },
    {
        "Text": "equation (2)",
        "Entity": "Reference"
    },
    {
        "Text": "We establish a corresponding feature function by multiplying the probability of all used alignment templates and taking the logarithm",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5 Performance on morphological analysis.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 shows empirical search timings for various values of M , for the MEMD model described in the next section.",
        "Entity": "Reference"
    },
    {
        "Text": "and Table 6)",
        "Entity": "Reference"
    },
    {
        "Text": "The joint probability of the two sequences is defined as the product of context probabilities and lexical probabilities over all POS tags:",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5:",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: Meta alternations and their average precision values for the task.",
        "Entity": "Reference"
    },
    {
        "Text": "If M = 10, 15 Chinese words (i.e., the first 19 Chinese words in Table 3 except \u53f6\u739b\u65af,\u5df4\u4f50\u4e9a,\u5769\u57da,\u666e\u5229\u6cd5) have their correct English translations at rank one position.",
        "Entity": "Reference"
    },
    {
        "Text": "The graphical structure depicted in Figure 1 models these relations between the four mentions Leaders, Paris, recent developments and They.",
        "Entity": "Reference"
    },
    {
        "Text": "In all four tables, we provide theresults for using no heuristic functions and three variants of an increasingly infor mative heuristic function",
        "Entity": "Reference"
    },
    {
        "Text": "This approach has been suggested by Papineni, Roukos, and Ward (1997, 1998) for a natural language understanding task.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1.",
        "Entity": "Reference"
    },
    {
        "Text": "Examples of the deletion features can be found in Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5",
        "Entity": "Reference"
    },
    {
        "Text": "Fi gure 7 depic ts the comp atible brack ets and all comp atible brack ets rates.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Tagging accuracies on test data.",
        "Entity": "Caption"
    },
    {
        "Text": "MUC7 test accuracy.",
        "Entity": "Reference"
    },
    {
        "Text": "We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4, 5, and 6.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5 shows that by changing the word spelling model from zerogram to bigram, character perplex ity is greatly reduced.",
        "Entity": "Reference"
    },
    {
        "Text": "K): J k = fj k 1 +1 , ..",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 shows the bilingual phrases containing between two and seven words that result from the application of this algorithm to the alignment of Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "Based on this experiment, we set the beam size of SegTagDep to 64 throughout the exper 64 96.28 92.37 74.96 0.48 Table 3: F1 scores and speed (in sentences per sec.)",
        "Entity": "Reference"
    },
    {
        "Text": "Letting u1 be the prefix of the word that ends in v1 (eg, r in figure 1), w1 = u1v1, and h = htu1:is less accurate because it ignores the alignment rela tion between s and h, which is captured by even the simplest noisy-channel models.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8 shows that by using word type and part of speech information, recall is improved from 28.1% to 40.6% and precision is improved from 57.3% to 64.1%.",
        "Entity": "Reference"
    },
    {
        "Text": "Collocations were automatically located in a text by looking up pairwise words in this lexicon",
        "Entity": "Reference"
    },
    {
        "Text": "E.g., the city Fez, Mo rocco (figure 1) was tagged as a single LOCATION by one annotator and as two by the other.",
        "Entity": "Reference"
    },
    {
        "Text": "The results are displayed in Table 3.",
        "Entity": "Reference"
    },
    {
        "Text": "exp[ M m hm (eI , f J )] m=1 1 1 M I J (3)",
        "Entity": "Caption"
    },
    {
        "Text": "Tree setups P(%) R(%) F SPT 76.3 59.8 67.1 DSPT 77.4 65.4 70.9 UPST (BOF) 80.4 69.7 74.7 UPST (FPT) 80.1 70.7 75.1 UPST (EPT) 79.9 70.2 74.8 Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "However, the learning curves in Figure 3 show that the Berkeley parser does not exceed our manual grammar by as wide a margin as has been shown for other languages (Petrov, 2009).",
        "Entity": "Reference"
    },
    {
        "Text": "As shown in figure 3, read times are much higher for predictions that get accepted, re B(x, k, a) = R1(x) + T (x, k) E(x, k), a = 1 R0(x), a = 0 flecting both a more careful perusal by the translator and the fact the rejected predictions are often simplywhere Ra(x) is the cost of reading x when it ulignored.2 In both cases there is a weak linear rela timately gets accepted (a = 1) or rejected (a = 0), T (x, k) is the cost of manually typing xk , and E(x, k) is the edit cost of accepting x and erasing to the end of its first k characters.",
        "Entity": "Reference"
    },
    {
        "Text": "The distribution of errors is displayed in Table 4.",
        "Entity": "Reference"
    },
    {
        "Text": "Cik Figure 1: Local graph of the word mouse",
        "Entity": "Reference"
    },
    {
        "Text": "Thetheoretical upper bound of the decoding complex Figure 5: Decoding algorithm using semantic role features.",
        "Entity": "Reference"
    },
    {
        "Text": "We illustrate its use with an example (see Then, we average the contributions of each n-gram order: Figure 2).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: CoreLex s basic types with their corresponding WordNet anchors.",
        "Entity": "Reference"
    },
    {
        "Text": "he largest effect seems to come from taking into account the bigram dependence, which achieves an mWER of 32.9%",
        "Entity": "Reference"
    },
    {
        "Text": "The first model is Equation (5), which is the combina . tion of Poisson distribution and character zerogram",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5 shows that this reimplementation almost reproduces the accuracy of their implementation.",
        "Entity": "Reference"
    },
    {
        "Text": "As shown in figure 1, a similarity matrix W models one-hop transitions that follow the links from vertices to neighbors.",
        "Entity": "Reference"
    },
    {
        "Text": "So we estimate that English translations are present in the English part of the corpus for Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 suggests that the best setup is dependent on the specific meta sense or meta alternation being MACRO MICRO repM gram gramlex lex space type MACRO MICRO repA",
        "Entity": "Reference"
    },
    {
        "Text": "For MUC6, the reduction in error due to global features is 27%, and for MUC7,14%.",
        "Entity": "Reference"
    },
    {
        "Text": "Although kanji sequences are difficult to seg ment, they can comprise a significant portion of Japanese text, as shown in Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 shows the usefulness evaluation result.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 7 Algorithm for breadth-first search with pruning",
        "Entity": "Caption"
    },
    {
        "Text": "alignment models Pr(f J , aJ | eI ),",
        "Entity": "Reference"
    },
    {
        "Text": "As for the unknown word model, word-based char acter bigrams are computed from the words with Table 5: Cross entropy (CE) per word and character perplexity (PP) of each unknown word model Part of Speech Estimation Accuracy 0.95 0.9 frequency one (49,653 words).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 displays the performance of our model and of the systems that obtained the best (Fernandes et al., 2012) and the median performance in the MUC B3 CEAFe average R P F1 R P F1 R P F1 CoNLL 12 English development data be st 64.",
        "Entity": "Reference"
    },
    {
        "Text": "Probabilities We find that Equation (7) assigns too little proba bilities to long words (5 or more characters).",
        "Entity": "Reference"
    },
    {
        "Text": "MUC7 test accuracy.",
        "Entity": "Reference"
    },
    {
        "Text": "The results for French to English and for English to French are shown in Table 10",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: Accuracy of part of speech estimation each part of speech and word type (POS + WT + Poisson + bigram).",
        "Entity": "Reference"
    },
    {
        "Text": "To simplify the description, we assume in Figure 2 that a bigram language model is used and all the TTS templates are binarized.",
        "Entity": "Reference"
    },
    {
        "Text": "As for the unknown word model, word-based char acter bigrams are computed from the words with Table 5: Cross entropy (CE) per word and character perplexity (PP) of each unknown word model Part of Speech Estimation Accuracy 0.95 0.9 frequency one (49,653 words).",
        "Entity": "Reference"
    },
    {
        "Text": "(Abbreviations are listed in Table 2.)",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 suggests that the best setup is dependent on the specific meta sense or meta alternation being MACRO MICRO repM gram gramlex lex space type MACRO MICRO repA",
        "Entity": "Reference"
    },
    {
        "Text": "see table 3) as the impact of soft constraints is the weakest for the constrained method at this level.",
        "Entity": "Reference"
    },
    {
        "Text": "The corresponding figures for the test data are. 89.53% for our tagger and 88.88% for the TnT tag- ger.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3",
        "Entity": "Reference"
    },
    {
        "Text": "More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; Paa and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "Table 7 shows the results.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 7 Algorithm for breadth-first search with pruning",
        "Entity": "Caption"
    },
    {
        "Text": "#e is the total number of English translation candidates in the period.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5 Performance on morphological analysis.",
        "Entity": "Reference"
    },
    {
        "Text": "We obtain the following decision rule: eI = argmax Pr(eI | f J ) 1 1 1 I 1 M ) = argmax m hm (eI , f J ) 1 1 I m=1",
        "Entity": "Reference"
    },
    {
        "Text": "For the graph depicted in Figure 1 this algorithm computes the clusters {They, Leaders}, {Paris} and {recent developments}.",
        "Entity": "Reference"
    },
    {
        "Text": "using the convolution parse tree kernel as depicted in Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "As a result, Arabic sentences are usually long relative to English, especially after",
        "Entity": "Reference"
    },
    {
        "Text": "Fi gure 7 depic ts the comp atible brack ets and all comp atible brack ets rates.",
        "Entity": "Reference"
    },
    {
        "Text": "It should be emphasized that this constraint to consecutive phrases limits the expressive power.",
        "Entity": "Reference"
    },
    {
        "Text": "We include a list of per-category results for selected phrasal labels, POS tags, and dependencies in Table 8.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4 shows the number of sentences, words, and characters of the training and test sets.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 Architecture of the statistical translation approach based on Bayes decision rule.",
        "Entity": "Reference"
    },
    {
        "Text": "The model 1 The total 74,597 sentence pairs used in experiments are those in the FBIS corpus whose English part can be parsed using Charniak (2000)",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Sources of Dictionaries",
        "Entity": "Caption"
    },
    {
        "Text": "With an absolute frequency threshold of 10, we obtain an accuracy of 95.4%, which constitutes a 50% reduction of error rate.Table 3 presents the experimental results rela tive to class.",
        "Entity": "Reference"
    },
    {
        "Text": "The result is shown in Table 4: the baseline numbers without stem features are listed under Base, and the results of the coreference system with stem features are listed under Base+Stem.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2 shows the labeled dependency graph of example (2), taken from Talbanken05.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6: Comparison of results for MUC7",
        "Entity": "Caption"
    },
    {
        "Text": "In Table 5 we present results from small test cor pora for the productive affixes handled by the current version of the system; as with names, the segmentation of morphologically derived words is generally either right or wrong.",
        "Entity": "Reference"
    },
    {
        "Text": "even after removal of the wing-node, the two areas of meaning are still linked via tail",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5.",
        "Entity": "Reference"
    },
    {
        "Text": "Results are shown in Table 2; we see that better word alignment results do not lead to better translations.",
        "Entity": "Reference"
    },
    {
        "Text": "This is especially true in the case of quotations\u2014which are common in the ATB\u2014where (1) will follow a verb like (2) (Figure 1).",
        "Entity": "Reference"
    },
    {
        "Text": "In the following, we describe the criterion that defines the set of phrases that is consistent with the word alignment matrix",
        "Entity": "Reference"
    },
    {
        "Text": "The second factor of Equation (13) is estimated from the Poisson distribution whose parameter",
        "Entity": "Reference"
    },
    {
        "Text": "Consider the example in Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Scores for MSRA corpus",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3 shows the performance and processing time comparison of various models and their combinations.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": ", C(e )) (18) 1 1 1 1 i=1 i 1",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3. Rank of correct translation for period Dec 01 \u2013 Dec 15 and Dec 16 \u2013 Dec 31. \u2018Cont. rank\u2019 is the context rank, \u2018Trans. Rank\u2019 is the transliteration rank. \u2018NA\u2019 means the word cannot be transliterated. \u2018insuff\u2019 means the correct translation appears less than 10 times in the English part of the comparable corpus. \u2018comm\u2019 means the correct translation is a word appearing in the dictionary we used or is a stop word. \u2018phrase\u2019 means the correct translation contains multiple English words.",
        "Entity": "Caption"
    },
    {
        "Text": "In addition to the basic regular expression operators shown in table 1, the formalism is extended in various ways.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt.",
        "Entity": "Caption"
    },
    {
        "Text": "If it is made up of all capital letters, then (allCaps, zone) is set to 1",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 shows a probability estimation tree for the prediction of the probability of the nominative attribute of nouns.",
        "Entity": "Reference"
    },
    {
        "Text": "Pr(eI | f J ) = p M (eI | f J )",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Thetheoretical upper bound of the decoding complex Figure 5: Decoding algorithm using semantic role features.",
        "Entity": "Reference"
    },
    {
        "Text": "The accuracy of a baseline tagger which chooses the most probable tag9 ignoring the context is 67.3% without and 69.4% with the supple 92.3 92.2 92.1 92 91.9 91.8 91.7 91.6 91.5 91.4 2 3 4 5 6 7 8 9 10 mentary lexicon.",
        "Entity": "Reference"
    },
    {
        "Text": "The corresponding translation quality improves from an mWER of 45.9% to an mWER of 31.8%.",
        "Entity": "Reference"
    },
    {
        "Text": "equation (2)",
        "Entity": "Reference"
    },
    {
        "Text": "able 2 shows the corpus statistics for this task.",
        "Entity": "Reference"
    },
    {
        "Text": "Evaluation results are listed in Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: F-measure after successive addition of each global feature group",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2 shows an example of a symmetrized alignment",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Local graph of the word wing of a graph",
        "Entity": "Reference"
    },
    {
        "Text": "Training time comparison.",
        "Entity": "Caption"
    },
    {
        "Text": "Recall is somewhat difficult to estimate because we do not know whether the English translation of a Chinese word appears in the English part of the corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Results for different user simulations.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 Classes of words found by ST for the test corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Organisation of the hierarchical graph of concepts Following previous semantic noun classification experiments (Pantel and Lin, 2002; Bergsma et al., 2008), we use the grammatical relations (GRs) as features for clustering.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3. Rank of correct translation for period Dec 01 \u2013 Dec 15 and Dec 16 \u2013 Dec 31. \u2018Cont. rank\u2019 is the context rank, \u2018Trans. Rank\u2019 is the transliteration rank. \u2018NA\u2019 means the word cannot be transliterated. \u2018insuff\u2019 means the correct translation appears less than 10 times in the English part of the comparable corpus. \u2018comm\u2019 means the correct translation is a word appearing in the dictionary we used or is a stop word. \u2018phrase\u2019 means the correct translation contains multiple English words.",
        "Entity": "Caption"
    },
    {
        "Text": "As shown in figure 3, read times are much higher for predictions that get accepted, re B(x, k, a) = R1(x) + T (x, k) E(x, k), a = 1 R0(x), a = 0 flecting both a more careful perusal by the translator and the fact the rejected predictions are often simplywhere Ra(x) is the cost of reading x when it ulignored.2 In both cases there is a weak linear rela timately gets accepted (a = 1) or rejected (a = 0), T (x, k) is the cost of manually typing xk , and E(x, k) is the edit cost of accepting x and erasing to the end of its first k characters.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Effect of Arabic stemming features on coreference resolution.",
        "Entity": "Reference"
    },
    {
        "Text": "igure 1 gives an example.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 shows the performance and speed of the full joint model (with no dictionaries) on CTB5c1 with respect to the beam size.",
        "Entity": "Reference"
    },
    {
        "Text": "In addition to the basic regular expression operators shown in table 1, the formalism is extended in various ways.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1; capital letters designate sets and small letters elements of sets).2 For a lemma l like lamb, we want to knowhow well a meta alternation (such as ANIMAL FOOD) explains a pair of its senses (such as the animal and food senses of lamb).3 This is formalized through the function score, which maps a meta alternation and two senses onto a score.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 lists new conceptsthat CAM introduces to manipulate vector represen tations.",
        "Entity": "Reference"
    },
    {
        "Text": "The statistics of 96 these splits are shown in Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 presents the performance in terms of precision, recall, and F- measure of the whole system.",
        "Entity": "Reference"
    },
    {
        "Text": "Based on this experiment, we set the beam size of SegTagDep to 64 throughout the exper 64 96.28 92.37 74.96 0.48 Table 3: F1 scores and speed (in sentences per sec.)",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: The Stanford parser (Klein and Manning, 2002) is unable to recover the verbal reading of the unvocalized surface form 0 an (Table 1)",
        "Entity": "Caption"
    },
    {
        "Text": "Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006) and the CRF model using minimum subword-based tagging, both of which are statistical methods, are used individually to solve the Figure 1: Outline of the segmentation process 2.1 Forward Maximum Matching.",
        "Entity": "Reference"
    },
    {
        "Text": "The second model is Equa tion (13), which is a set of word models trained for",
        "Entity": "Reference"
    },
    {
        "Text": "The translations of 6 of the 43 words are words in the dictionary (denoted as \u201ccomm.\u201d in Table 3) and 4 of the 43 words appear less than 10 times in the English part of the corpus (denoted as \u201cinsuff\u201d).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Diacritized particles and pseudo-verbs that, after orthographic normalization, have the equivalent surface form 0 an.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 lists new conceptsthat CAM introduces to manipulate vector represen tations.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 contains results for two different translation models.",
        "Entity": "Reference"
    },
    {
        "Text": "A natural unit for B(x, k, a) is the number of keystrokes saved, so all elements of the above equation are converted to this measure.",
        "Entity": "Reference"
    },
    {
        "Text": "See Table 1 for details.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 evaluates the contributions of different kinds of constituent dependencies to extraction performance on the 7 relation types of the ACE RDC 2004 corpus",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 The cost as a novel given name (second position) for hanzi from various radical classes.",
        "Entity": "Reference"
    },
    {
        "Text": "As we will see from Table 3, not much improvement is derived from this feature.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Dependency representation of example (2) from Talbanken05.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3:",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Illustration of dictionary based segmenta tion finite state transducer 3.1 Bootstrapping.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": ", C(e )) (18) 1 1 1 1 i=1 i 1",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7",
        "Entity": "Reference"
    },
    {
        "Text": "and ap plied to an Arabic sentence in figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 7",
        "Entity": "Reference"
    },
    {
        "Text": "(Equation (7)) (Poisson + hi gram).",
        "Entity": "Reference"
    },
    {
        "Text": "At the morp heme level, stems are divid ed from their affixe s. For exam ple, altho ugh both naga no (Naga no) and shi (city) can appea r as indivi dual words , nagano shi (Nag ano city) is brack eted as [[naga no][s hi]], since here shi Figure 3: Determining word boundaries.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "Table 7: Test set results. Maamouri et al. (2009b) evaluated the Bikel parser using the same ATB split, but only reported dev set results with gold POS tags for sentences of length \u2264 40. The Bikel GoldPOS configuration only supplies the gold POS tags; it does not force the parser to use them. We are unaware of prior results for the Stanford parser.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6 gives an overview of the decisions made in the alignment template model.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8 shows the tagging accuracy of unknown words.",
        "Entity": "Reference"
    },
    {
        "Text": "The results are shown in the corr row of table 2, for exact character-probability estimates.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "This sum can be computed efficiently using the algorithm shown in Figure 8",
        "Entity": "Reference"
    },
    {
        "Text": "the target Chen Guangcheng only appears once in Weibo",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Example of a prediction for English to French translation.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 shows the results of an evaluation based on the plain STTS tagset.",
        "Entity": "Reference"
    },
    {
        "Text": "However, this information is hard to extract reliably from the available data; and even if were obtainable, many of the 0.3 0.2 0.1 0 60 50 40 30 20 10 0 10 20 30 40 50 60 gain (length of correct prefix length of incorrect suffix) Figure 2: Probability that a prediction will be accepted versus its gain.",
        "Entity": "Reference"
    },
    {
        "Text": "We choose t = 1, 5, and 30 for the fertility HMM",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 Similarity matrix for segmentation judgments.",
        "Entity": "Reference"
    },
    {
        "Text": "The sources of our dictionaries are listed in Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 7",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2 shows an example of a symmetrized alignment",
        "Entity": "Reference"
    },
    {
        "Text": "equation (2)",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4 Differences in performance between our system and Wang, Li, and Chang (1992).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3 shows the part of speech prediction accu racy of two unknown word model without context.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4",
        "Entity": "Reference"
    },
    {
        "Text": "We establish a corresponding feature function by multiplying the probability of all used alignment templates and taking the logarithm",
        "Entity": "Reference"
    },
    {
        "Text": "figure)",
        "Entity": "Reference"
    },
    {
        "Text": "To illustrate the complete user model, in the figure 1 example the benefit of accepting would be7 2 4.2 = .8 keystrokes and the benefit of reject ing would be .2 keystrokes.",
        "Entity": "Reference"
    },
    {
        "Text": "We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4, 5, and 6.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "As can be seen in figure 2, wing \"part of a bird\" is closely related to tail, as is wing \"part of a plane\"",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 shows the manual evaluation results based on the entire test set, and the improvement from SRF is significant at p < 0.005 based on a t-test.",
        "Entity": "Reference"
    },
    {
        "Text": "In the table, Nc indicates the number of clusters in the inferred tree, while Nl indicates the closest match to the number of classes in the gold standard.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: An ATB sample from the human evaluation. The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a). But the city name Sharm Al- Sheikh is also iDafa, hence the possibility for the incorrect annotation in (b).",
        "Entity": "Caption"
    },
    {
        "Text": "Table (1) and Eq.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Word length distribution of unknown words and its estimate by Poisson distribution",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 Similarity matrix for segmentation judgments.",
        "Entity": "Reference"
    },
    {
        "Text": "I+1 hLM(eI , f J , K , zK ) = log n p(ei | ei 2 , e ) (17) 1 1 1 1 i=1 i 1",
        "Entity": "Caption"
    },
    {
        "Text": "For MUC7, there are also no published results on systems trained on only the official training data of 200 aviation disaster articles.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Scores for CityU corpus",
        "Entity": "Reference"
    },
    {
        "Text": "Throughout in this paper, we used Equation (9) to compute the word spelling probabilities.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure",
        "Entity": "Reference"
    },
    {
        "Text": "bothmentions are in a parallel construction in adja Figure 1: An example graph modeling relations between mentions.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Examples of common character bigrams for each part of speech in the infrequent words pa rt of sp ee ch ch ar ac ter bi gr a m fre qu en cy no un nu m be r a dj e ct iv al v er b v er b ad je cti ve ad ve rb < e o w > <b o w > 1 S \" J < e o w > I t < e o w > L < e o w > < e o w > 13 43 4 8 4 3 2 7 2 1 3 69 63 resented all unknown words by one length model.",
        "Entity": "Reference"
    },
    {
        "Text": "and 3",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5 gives an example of the word alignment and phrase alignment of a German English sentence pair.We describe our model using a log-linear modeling approach",
        "Entity": "Reference"
    },
    {
        "Text": "A simple lexicalized PCFG with second order Markovization gives relatively poor performance: 75.95% F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline (Table 7).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: Distribution of supersense mentions by domain (left), and counts for tags occurring over 800 times (below).",
        "Entity": "Reference"
    },
    {
        "Text": "The Stanford parser (Klein and Manning, 2002) is unable to recover the verbal reading of the unvocalized surface form 0 an (Table 1).",
        "Entity": "Reference"
    },
    {
        "Text": "Length Distribution In word segmentation, one of the major problems of the word length model of Equation (6) is the decom position of unknown words.",
        "Entity": "Reference"
    },
    {
        "Text": "tion (13) is estimated from the relative frequency of the corresponding events in the training corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "To differentiate between the coordinating and discourse separator functions of conjunctions (Table 3), we mark each CC with the label of its right sister (splitCC).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2 shows an example of a symmetrized alignment",
        "Entity": "Reference"
    },
    {
        "Text": "The bottom-up decoding algorithm for the TTS transducer is sketched in Figure 2.",
        "Entity": "Reference"
    },
    {
        "Text": "The tagset refinement increases the accuracy by about 0.6%, and the external lexicon by another 3.5%.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Illustration of dictionary based segmenta tion finite state transducer 3.1 Bootstrapping.",
        "Entity": "Reference"
    },
    {
        "Text": "U = {up}m represent the hidden m struct a new graph G1 (Figure 1(d)) with the clusters U as vertices.",
        "Entity": "Reference"
    },
    {
        "Text": "MENE has only been tested on MUC7.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2",
        "Entity": "Reference"
    },
    {
        "Text": "Table 7: Test set results. Maamouri et al. (2009b) evaluated the Bikel parser using the same ATB split, but only reported dev set results with gold POS tags for sentences of length \u2264 40. The Bikel GoldPOS configuration only supplies the gold POS tags; it does not force the parser to use them. We are unaware of prior results for the Stanford parser.",
        "Entity": "Caption"
    },
    {
        "Text": "A comparison of the lines for grammatical roles and for surface order in Table 1 shows that the same is true in German.",
        "Entity": "Reference"
    },
    {
        "Text": "This approach can be seen as a generalization of the originally suggested source channel modeling framework for statistical machine translation",
        "Entity": "Reference"
    },
    {
        "Text": "Nc Nl HGFC uncons trained A G G N MI F N M I F 13 0 13 3 57 .3 1 36 .6 5 54 .2 2 32 .6 2 11 4 11 7 54 .6 7 37 .9 6 51 .3 5 32 .4 4 50 51 37 .7 5 40 .0 0 32 .6 1 32 .7 8 Table 2: Performance on T2 using a predefined tree structure.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five trials on CTB5c.",
        "Entity": "Reference"
    },
    {
        "Text": "The third model is Equation (11), which is a set of word models trained for each word type (WT +Poisson+ bigram).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 8 Algorithm min-jumps to compute the minimum number of needed jumps D(cJ , j) to complete the translation",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 describes the components and how this system works.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 shows the number of sentences, words, and characters of the training and test sets.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: Meta alternations and their average precision values for the task.",
        "Entity": "Reference"
    },
    {
        "Text": "Consider the example in Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "We compared the ATB5 to tree- banks for Chinese (CTB6), German (Negra), and English (WSJ) (Table 4)",
        "Entity": "Reference"
    },
    {
        "Text": "eik (12",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Word length distribution of unknown words and its estimate by Poisson distribution",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1, Figure 1, and Figure 2 shows the AER results for different models.",
        "Entity": "Reference"
    },
    {
        "Text": "(2)",
        "Entity": "Caption"
    },
    {
        "Text": "Reduction of conjuncts for NP coordination Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "For instance, the gain for the prediction in figure 1 would be 2 7 8 = 6.",
        "Entity": "Reference"
    },
    {
        "Text": ", fjk (11)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Snapshot of the supersense-annotated data.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 shows the record for the headword orange followed by its collocates",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 Classes of words found by ST for the test corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "The use of the language model feature in equation (18) helps take long-range dependencies better into account",
        "Entity": "Reference"
    },
    {
        "Text": "aJ = argmax p (f J , aJ | eI ) (8) 1 1 1 1 J 1",
        "Entity": "Caption"
    },
    {
        "Text": "his optimization can be performed using the expectation maximization (EM) algorithm (Dempster, Laird, and Rubin 1977).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: AER comparison (en\u2192cn)",
        "Entity": "Caption"
    },
    {
        "Text": "we directly model the posterior probability Pr(eI| f J )",
        "Entity": "Reference"
    },
    {
        "Text": "The second condition is necessary to allow for single-character words (see Figure 3).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3 show the training time for different models.",
        "Entity": "Reference"
    },
    {
        "Text": "segmentation (Table 2).",
        "Entity": "Reference"
    },
    {
        "Text": "Letting u1 be the prefix of the word that ends in v1 (eg, r in figure 1), w1 = u1v1, and h = htu1:is less accurate because it ignores the alignment rela tion between s and h, which is captured by even the simplest noisy-channel models.",
        "Entity": "Reference"
    },
    {
        "Text": "e suggest the use of a log-linear model to incorporate the various knowledge sources into an overall translation system and to perform discriminative training of the free model parameters",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 shows the effect of the role-based preference on our data.",
        "Entity": "Reference"
    },
    {
        "Text": "This table also shows that: (1) Both modification within base-NPs and modification to NPs contribute much to performance improvement, acquiring the increase of F- measure by 4.4/2.4 units in mode M1 and 4.4/2.3 units in mode M2 respectively.",
        "Entity": "Reference"
    },
    {
        "Text": "In these experiments, the input lacks segmentation markers, hence the slightly different dev set baseline than in Table 6.",
        "Entity": "Reference"
    },
    {
        "Text": "If we compare the error rates in Table 7, which correspond to about 55 search errors in Table 6, we obtain an mWER of 36.7% (53 search errors) using no heuristic function and an mWER of 32.6% (57 search errors) using the combined heuristic function.",
        "Entity": "Reference"
    },
    {
        "Text": "For example, in the sentence bought one of town s two meat- packing plants as illustrated in Figure 1(a), the constituents before the headword plants can be removed from the parse tree.",
        "Entity": "Reference"
    },
    {
        "Text": "and 2",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2 shows examples of the discovered patterns for the merger and acquisition topic.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: The Stanford parser (Klein and Manning, 2002) is unable to recover the verbal reading of the unvocalized surface form 0 an (Table 1)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 shows empirical search timings for various values of M , for the MEMD model described in the next section.",
        "Entity": "Reference"
    },
    {
        "Text": "For example, if x in figure 1 were evenir aux choses, then x14 would map to v1 = evenir, w2 = aux, and u3 = cho.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4",
        "Entity": "Reference"
    },
    {
        "Text": "The results for French to English and for English to French are shown in Table 10",
        "Entity": "Reference"
    },
    {
        "Text": "#o is the total number of output English translations.",
        "Entity": "Reference"
    },
    {
        "Text": "The statistics of 96 these splits are shown in Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Scores for MSRA corpus",
        "Entity": "Reference"
    },
    {
        "Text": "To illustrate how SRF impacts the translation results, Figure 8 gives 3 examples of the MT outputs with and without the SRFs",
        "Entity": "Reference"
    },
    {
        "Text": "Column four (MBL) in table 2 shows the accuracy obtained with all features in the general feature space.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Examples of word bigrams including un known word tags example",
        "Entity": "Reference"
    },
    {
        "Text": "We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4, 5, and 6.",
        "Entity": "Reference"
    },
    {
        "Text": "This sentence should be tagged as shown in table 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: The amount of training and test sets The first factor in the righthand side of Equa tion (13) is estimated from the relative frequency of the corresponding events in the training corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "The second model is Equa tion (13), which is a set of word models trained for",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 shows a confusion matrix for the classification of the nouns.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4",
        "Entity": "Reference"
    },
    {
        "Text": "The breakdown of the different types of words found by ST in the test corpus is given in Table 3.",
        "Entity": "Reference"
    },
    {
        "Text": "To differentiate between the coordinating and discourse separator functions of conjunctions (Table 3), we mark each CC with the label of its right sister (splitCC).",
        "Entity": "Reference"
    },
    {
        "Text": "2",
        "Entity": "Reference"
    },
    {
        "Text": "To reduce the memory requirement of the alignment templates, we compute these probabilities only for phrases up to a certain maximal length in the source language.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 shows the influence of the four parameters.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Tagging accuracies on development data in percent.",
        "Entity": "Caption"
    },
    {
        "Text": "The graphical structure depicted in Figure 1 models these relations between the four mentions Leaders, Paris, recent developments and They.",
        "Entity": "Reference"
    },
    {
        "Text": "Throughout in this paper, we used Equation (9) to compute the word spelling probabilities.",
        "Entity": "Reference"
    },
    {
        "Text": "This group consists of 10 features based on the string , as listed in Table 1.",
        "Entity": "Reference"
    },
    {
        "Text": "BP(f J , eI , A) = f j+m , ei+n 1 1 j i : (i , j ) A : j j j + m i i i + n (9) (i , j ) A : j j j + m i i i + n",
        "Entity": "Caption"
    },
    {
        "Text": "On the other hand, using our method of combining both sources of information and setting M = \u221e, 19 Chinese words (i.e., the first 22 Chinese words in Table 3 except \u5df4\u4f50\u4e9a,\u5769\u57da,\u666e\u5229\u6cd5) have their correct English translations at rank one position.",
        "Entity": "Reference"
    },
    {
        "Text": "To differentiate between the coordinating and discourse separator functions of conjunctions (Table 3), we mark each CC with the label of its right sister (splitCC).",
        "Entity": "Reference"
    },
    {
        "Text": "Ta SegTag 97.66 93.61 SegTagDep 97.73 94.46 SegTag(d) 98.18 94.08 SegTagDep(d) 98.26 94.64 Table 5: Final results on CTB5j 76 75 74 ble 4 shows the segmentation, POS tagging, and dependency parsing F1 scores of these models on CTB5c.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5",
        "Entity": "Reference"
    },
    {
        "Text": "equation (2)",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Collecting evidence for a word boundary - are the non-straddling n-grams 8 1 and 82 more frequent than the straddling n-grams t 1, t2, and t3?",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "If we compare the error rates in Table 7, which correspond to about 55 search errors in Table 6, we obtain an mWER of 36.7% (53 search errors) using no heuristic function and an mWER of 32.6% (57 search errors) using the combined heuristic function",
        "Entity": "Reference"
    },
    {
        "Text": "As can be seen in Table 4, our training data is a lot less than those used by MENE and IdentiFinder3.",
        "Entity": "Reference"
    },
    {
        "Text": "Then, every phrase f produces its translation e (using the corresponding alignment template z).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: Accuracy on development data depend ing on context size",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4 Examples of alignment templates obtained in training",
        "Entity": "Caption"
    },
    {
        "Text": "Among all possible target sentences, we will choose the sentence with the highest probability",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: Performance of baseline and joint models w.r.t. the average processing time (in sec.)",
        "Entity": "Reference"
    },
    {
        "Text": "E.g. rat and printer are very different in meaning, but they are both closely related to different meanings of mouse",
        "Entity": "Reference"
    },
    {
        "Text": "Pr(f J | eI ) = Pr(f J , aJ | eI ) (5) 1 1 1 1 1 J 1",
        "Entity": "Caption"
    },
    {
        "Text": "and 3",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4",
        "Entity": "Reference"
    },
    {
        "Text": "and 3",
        "Entity": "Reference"
    },
    {
        "Text": "Pr(f J , aJ | eI ) = p (f J , aJ | eI ) (6) 1 1 1 1 1 1",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 Architecture of the translation approach based on a log-linear modeling approach",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "Probabilities We find that Equation (7) assigns too little proba bilities to long words (5 or more characters).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 11 gives an overview on the training and test data.",
        "Entity": "Reference"
    },
    {
        "Text": "In all four tables, we provide theresults for using no heuristic functions and three variants of an increasingly infor mative heuristic function",
        "Entity": "Reference"
    },
    {
        "Text": "equation (1)",
        "Entity": "Reference"
    },
    {
        "Text": "e suggest the use of a log-linear model to incorporate the various knowledge sources into an overall translation system and to perform discriminative training of the free model parameters",
        "Entity": "Reference"
    },
    {
        "Text": "hAL(eI , f J , K , zK ) = |j 1 j | (16) 1 1 1 1 k k=1 k 1",
        "Entity": "Caption"
    },
    {
        "Text": "Strube (1998) s centeri ng appro ach (whos e senten ce orderi ng is designate d as SR2 in Table 2) also deals with and even prefer s intrase ntenti al anaph ora, which raises the upper limit to a more accept able 80.2% .",
        "Entity": "Reference"
    },
    {
        "Text": "As can be seen in figure 2, wing \"part of a bird\" is closely related to tail, as is wing \"part of a plane\"",
        "Entity": "Reference"
    },
    {
        "Text": "As each global feature group is added to the list of features, we see improvements to both MUC6 and",
        "Entity": "Reference"
    },
    {
        "Text": "In general, as shown in this figure, there may be additional transformations to make the translation task simpler for the algorithm.",
        "Entity": "Reference"
    },
    {
        "Text": "K hAT(eI , f J , K , zK ) = log n p(zk | f j k ) (13) 1 1 1 1 k=1 j k 1 +1",
        "Entity": "Caption"
    },
    {
        "Text": "In MUC6, the best result is achieved by SRA (Krupka, 1995).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1. Accuracy of our system in each period (M = 10)",
        "Entity": "Caption"
    },
    {
        "Text": "(equation 2",
        "Entity": "Reference"
    },
    {
        "Text": "Pr(eI | f J ) = p M (eI | f J )",
        "Entity": "Caption"
    },
    {
        "Text": "This table also shows that: (1) Both modification within base-NPs and modification to NPs contribute much to performance improvement, acquiring the increase of F- measure by 4.4/2.4 units in mode M1 and 4.4/2.3 units in mode M2 respectively.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: BLEU results",
        "Entity": "Caption"
    },
    {
        "Text": "eI K 1 = e1 , ek = eik 1 +1 , .",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: The amount of training and test sets The first factor in the righthand side of Equa tion (13) is estimated from the relative frequency of the corresponding events in the training corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 compares the results of the unconstrained version of HGFC against those of AGG on our largest test set T2.",
        "Entity": "Reference"
    },
    {
        "Text": "equation",
        "Entity": "Reference"
    },
    {
        "Text": "equation (3)",
        "Entity": "Reference"
    },
    {
        "Text": "The last is exhibited for the first mention in figure 1, where one annotator chose ARTIFACT (referring tothe physical book) while the other chose COMMUNICATION (the content)",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "We see that the language model perplexity improves from 4,781 for a unigram model to 29.9 for a trigram model.",
        "Entity": "Reference"
    },
    {
        "Text": "and Table 6)",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Features based on the token string that are based on the probability of each name class during training.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 shows the effect of the role-based preference on our data.",
        "Entity": "Reference"
    },
    {
        "Text": "I exp[ m=1 m hm (e 1 , f1 )]",
        "Entity": "Caption"
    },
    {
        "Text": "For fair comparison, we have tabulated all results with the size of training data used (Table 5",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Comparison against Stevenson and Joanis (2003) s result on T1 (using similar features).",
        "Entity": "Reference"
    },
    {
        "Text": "We see that the language model perplexity improves from 4,781 for a unigram model to 29.9 for a trigram model.",
        "Entity": "Reference"
    },
    {
        "Text": "the target Chen Guangcheng only appears once in Weibo",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3:",
        "Entity": "Reference"
    },
    {
        "Text": "The overall architecture of the log-linear modeling approach is summarized in Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Tables 4 and 5 show the effect of the pruning parameter tp with the histogram pruning parameter Np = 50,000.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 8: Relative word accuracy as a function of training set size.",
        "Entity": "Reference"
    },
    {
        "Text": "Pr(f J , aJ | eI ) = p (f J , aJ | eI ) (6) 1 1 1 1 1 1",
        "Entity": "Caption"
    },
    {
        "Text": "On the contrary, in the above training stage, although the samples are not accurate enough to represent the distribution defined by Equation 7 for each alignment aj , it is accurate enough for computing the expected counts, which are defined at the corpus level.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 6: Examples of word, morpheme, and compatible-bracket errors.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 shows the performance and speed of the full joint model (with no dictionaries) on CTB5c1 with respect to the beam size.",
        "Entity": "Reference"
    },
    {
        "Text": "Comparison of segmentation algorithm using different linguistic features.",
        "Entity": "Reference"
    },
    {
        "Text": "This is because the lefthand side of Equation (7) represents the probability of the string c1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Results of different systems on the CoNLL 12 English data sets.",
        "Entity": "Reference"
    },
    {
        "Text": "In these experiments, the input lacks segmentation markers, hence the slightly different dev set baseline than in Table 6.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "A token that is allCaps will also be initCaps.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4 Differences in performance between our system and Wang, Li, and Chang (1992).",
        "Entity": "Reference"
    },
    {
        "Text": "hLEX(eI , f J , K , zK ) = #CO-OCCURRENCES (LEX, eI , f J ) (20) 1 1 1 1 1 1",
        "Entity": "Caption"
    },
    {
        "Text": "This is usually straightforward, with the exception of the case where the words that are aligned to a particular role s span in the source side are not continuous in the target side, as shown in Figure 4.",
        "Entity": "Reference"
    },
    {
        "Text": "As Figure 3 shows, word type information improves the prediction accuracy significantly.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Tagging accuracy on development data depending on context size",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2 shows examples of the discovered patterns for the merger and acquisition topic.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6: Comparison of results for MUC7",
        "Entity": "Caption"
    },
    {
        "Text": "The first model is Equation (5), which is the combina . tion of Poisson distribution and character zerogram",
        "Entity": "Reference"
    },
    {
        "Text": "This is because the lefthand side of Equation (7) represents the probability of the string c1",
        "Entity": "Reference"
    },
    {
        "Text": "In Table 5 we present results from small test cor pora for the productive affixes handled by the current version of the system; as with names, the segmentation of morphologically derived words is generally either right or wrong.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 10 Translation results on the Hansards task",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5:",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Snapshot of the supersense-annotated data.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4 shows an example of calculating the target side SRS based on a complicated TTS template.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Sample of experimental items for the meta alternation anmfod.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8: Per category performance of the Berkeley parser on sentence lengths \u2264 70 (dev set, gold segmentation). (a) Of the high frequency phrasal categories, ADJP and SBAR are the hardest to parse. We showed in \u00a72 that lexical ambiguity explains the underperformance of these categories. (b) POS tagging accuracy is lowest for maSdar verbal nouns (VBG,VN) and adjectives (e.g., JJ). Richer tag sets have been suggested for modeling morphologically complex distinctions (Diab, 2007), but we find that linguistically rich tag sets do not help parsing. (c) Coordination ambiguity is shown in dependency scores by e.g., \u2217SSS R) and \u2217NP NP NP R). \u2217NP NP PP R) and \u2217NP NP ADJP R) are both iDafa attachment.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 shows the training and test corpus statistics.",
        "Entity": "Reference"
    },
    {
        "Text": "equation 4",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Distributions of Morph Examples",
        "Entity": "Reference"
    },
    {
        "Text": "The results are displayed in Table 3.",
        "Entity": "Reference"
    },
    {
        "Text": "equation",
        "Entity": "Reference"
    },
    {
        "Text": "The corresponding translation quality improves from an mWER of 45.9% to an mWER of 31.8%.",
        "Entity": "Reference"
    },
    {
        "Text": "#o is the total number of output English translations.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Bootstrapping new heuristics.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Collecting evidence for a word boundary - are the non-straddling n-grams 8 1 and 82 more frequent than the straddling n-grams t 1, t2, and t3?",
        "Entity": "Reference"
    },
    {
        "Text": "The performance of these systems is shown in Table 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4 shows the performance on the test data.",
        "Entity": "Reference"
    },
    {
        "Text": "= argmax M 1 S s=1 ) log p M (es | fs ) (4)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 reports experimental results using lexical features only; we observe that the stemming n-gram features boost the performance by one point (64.7 vs. 65.8).",
        "Entity": "Reference"
    },
    {
        "Text": "The baseline system in Table 3 refers to the maximum entropy system that uses only local features.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2",
        "Entity": "Reference"
    },
    {
        "Text": "Column four (MBL) in table 2 shows the accuracy obtained with all features in the general feature space.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 shows a comparison with some existing toolkits that build deterministic, minimized automata/transducers.",
        "Entity": "Reference"
    },
    {
        "Text": "The joint probability of the two sequences is defined as the product of context probabilities and lexical probabilities over all POS tags:",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 underscores the virtues of Sentence Recency: In the most recent sentence with antecedents satisfying the filters, there are on aver ble.",
        "Entity": "Reference"
    },
    {
        "Text": "We see from Table 5, that the improvement in overall parse results is mainly in terms of dependency labeling, reflected in the LAS score.",
        "Entity": "Reference"
    },
    {
        "Text": "composite kernel 83.0 72.0 77.1 Zhou et al., (2007): composite kernel 82.2 70.2 75.8 Zhang et al., (2006): composite kernel 76.1 68.4 72.1 Zhao and Grishman, (2005):4 composite kernel 69.2 70.5 70.4 Ours: CTK with UPST 80.1 70.7 75.1Zhou et al., (2007): context sensitive CTK with CS-SPT 81.1 66.7 73.2 Zhang et al., (2006): CTK with SPT 74.1 62.4 67.7 Table 4.",
        "Entity": "Reference"
    },
    {
        "Text": "The translations of 6 of the 43 words are words in the dictionary (denoted as \u201ccomm.\u201d in Table 3) and 4 of the 43 words appear less than 10 times in the English part of the corpus (denoted as \u201cinsuff\u201d).",
        "Entity": "Reference"
    },
    {
        "Text": "The tagset refinement increases the accuracy by about 0.6%, and the external lexicon by another 3.5%.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 6",
        "Entity": "Reference"
    },
    {
        "Text": "equation (1)",
        "Entity": "Reference"
    },
    {
        "Text": "To compute the third factor of Equation (13), we have to estimate the character bigram probabilities that are classified by word type and part of speech.",
        "Entity": "Reference"
    },
    {
        "Text": "The third model is Equation (11), which is a set of word models trained for each word type (WT +Poisson+ bigram).",
        "Entity": "Reference"
    },
    {
        "Text": "The coupling between B and is removed by setting H = B 1: n min (W, H H T ), s.t. hip = 1 (1) H, i=1 BT Dl Bl according to equation 4 l end for return BL , BL 1 ...B1 Additional steps need to be performed in order to extract a tree from the hierarchical graph.",
        "Entity": "Reference"
    },
    {
        "Text": "The first machine, illustrated in Figure 1 encodes the prefix and suffix expansion rules, producing a lattice of possible segmentations.",
        "Entity": "Reference"
    },
    {
        "Text": "The first model is Equation (5), which is the combina . tion of Poisson distribution and character zerogram",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 6: Metaphors tagged by the system (in bold) whereby the main source of disagreement was the presence of lexicalized metaphors, e.g. verbs such as impose, decline etc.",
        "Entity": "Reference"
    },
    {
        "Text": "E.g., the city Fez, Mo rocco (figure 1) was tagged as a single LOCATION by one annotator and as two by the other.",
        "Entity": "Reference"
    },
    {
        "Text": "Column four (MBL) in table 2 shows the accuracy obtained with all features in the general feature space.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "and 2",
        "Entity": "Reference"
    },
    {
        "Text": "For example, in the sentence bought one of town s two meat- packing plants as illustrated in Figure 1(a), the constituents before the headword plants can be removed from the parse tree.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 presents an overview of the animacy data Clas s Ani mat e Type s Tok 6 4 4 ens cover ed 6 0 1 0 Inan imat e 691 0 3 4 8 2 2 Tota l 755 4 4 0 8 3 2 Table 1: The animacy data set from Talbanken05; number of noun lemmas (Types) and tokens in each class.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: NMI of unconstrained HGFC when trees for T2 and T3 are inferred automatically.",
        "Entity": "Reference"
    },
    {
        "Text": "For fair comparison, we have tabulated all results with the size of training data used (Table 5",
        "Entity": "Reference"
    },
    {
        "Text": "bothmentions are in a parallel construction in adja Figure 1: An example graph modeling relations between mentions.",
        "Entity": "Reference"
    },
    {
        "Text": "The unknown parameters are determined by maximizing the likelihood on the parallel training corpus:",
        "Entity": "Reference"
    },
    {
        "Text": "I exp[ m=1 m hm (e 1 , f1 )]",
        "Entity": "Caption"
    },
    {
        "Text": ">10 nouns (a) (b) classified as 222 125 (a) class animate 49 3390 (b) class inanimate Table 4: Confusion matrix for the MBLclassifier with a general feature space on the >10 data set on Talbanken05 nouns.",
        "Entity": "Reference"
    },
    {
        "Text": "The list of the features used in our joint model is presented in Table 1, where S01 S05, W01 W21, and T01 05 are taken from Zhang and Clark (2010), and P01 P28 are taken from Huang and Sagae (2010).",
        "Entity": "Reference"
    },
    {
        "Text": "This list of 43 words is shown in Table 3.",
        "Entity": "Reference"
    },
    {
        "Text": "Again, Table 2 shows that using stem n-grams features gave a small boost to the whole main-type classification system4.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3 shows the part of speech prediction accu racy of two unknown word model without context.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 7 shows a structogram of the algorithm",
        "Entity": "Reference"
    },
    {
        "Text": "3",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: An ATB sample from the human evaluation. The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a). But the city name Sharm Al- Sheikh is also iDafa, hence the possibility for the incorrect annotation in (b).",
        "Entity": "Caption"
    },
    {
        "Text": "The alignment aJ that has the highest probability (under a certain model) is also called the Viterbi alignment (of that model)",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4 shows a constituent headed by a process nominal with an embedded adjective phrase.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Statistics from 1993 Japanese newswire (NIKKEI), 79,326,406 characters total.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Distribution of the sentences where the semantic role features give no/positive/negative impact to the sentence fluency in terms of the completeness and ordering of the semantic roles.classes in VerbNet (Dang et al., 1998).",
        "Entity": "Reference"
    },
    {
        "Text": "Preliminary observations show that the different neighbours in Table 1 can be used to indicate with great accuracy which of the senses is being used",
        "Entity": "Reference"
    },
    {
        "Text": "hWRD(eI , f J , K , zK ) = log n p(ei | {fj | (i, j) A}, Ei ) (14) 1 1 1 1 i=1",
        "Entity": "Caption"
    },
    {
        "Text": "and 8 show word accuracy for Chasen, Juman, and our algorithm for parameter settings optimizing word precision, recall, and F-measure rates.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "The entity features can be attached under the top node, the entity nodes, or directly combined with the entity nodes as in Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1, Figure 1, and Figure 2 shows the AER results for different models.",
        "Entity": "Reference"
    },
    {
        "Text": "We compare in Table 2 the performance of Unified Parse and Semantic Trees with different kinds of Entity Semantic Tree setups using standard convolution tree kernel, while the SPT and DSPT with only entity-type information are listed for reference.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 7",
        "Entity": "Reference"
    },
    {
        "Text": "Reduction of conjuncts for NP coordination Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2",
        "Entity": "Reference"
    },
    {
        "Text": "Except our own and MENE + reference resolution, the results in Table 6 are all official MUC7 results.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1(d) shows a sentence maintain rental property he owns in the state , where the ART.User-or-Owner relation holds between the entities property and he",
        "Entity": "Reference"
    },
    {
        "Text": "For MUC7, there are also no published results on systems trained on only the official training data of 200 aviation disaster articles.",
        "Entity": "Reference"
    },
    {
        "Text": "As shown in Figure 8, relative word performance was not degraded and sometimes even slightly better.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Bootstrapping new heuristics.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1.<",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "Another interesting example is shown in Figure 1(b), where the base-NP of the second entity town is a possessive NP and there is no relationship between the entities one and town defined in the ACE corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "To illustrate how SRF impacts the translation results, Figure 8 gives 3 examples of the MT outputs with and without the SRFs",
        "Entity": "Reference"
    },
    {
        "Text": "This is usually straightforward, with the exception of the case where the words that are aligned to a particular role s span in the source side are not continuous in the target side, as shown in Figure 4.",
        "Entity": "Reference"
    },
    {
        "Text": "Only tokens with initCaps not found in commonWords are tested against each list in Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "(2004) in figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3 shows the distribution of SSTs in the corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "The entity features can be attached under the top node, the entity nodes, or directly combined with the entity nodes as in Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5 shows the cross entropy per word and char acter perplexity of three unknown word model.",
        "Entity": "Reference"
    },
    {
        "Text": "This table also shows that: (1) Both modification within base-NPs and modification to NPs contribute much to performance improvement, acquiring the increase of F- measure by 4.4/2.4 units in mode M1 and 4.4/2.3 units in mode M2 respectively.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4",
        "Entity": "Reference"
    },
    {
        "Text": "equation (3)",
        "Entity": "Reference"
    },
    {
        "Text": "Strube (1998) s centeri ng appro ach (whos e senten ce orderi ng is designate d as SR2 in Table 2) also deals with and even prefer s intrase ntenti al anaph ora, which raises the upper limit to a more accept able 80.2% .",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 6 gives an overview of the decisions made in the alignment template model.",
        "Entity": "Reference"
    },
    {
        "Text": "Recall is somewhat difficult to estimate because we do not know whether the English translation of a Chinese word appears in the English part of the corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4",
        "Entity": "Reference"
    },
    {
        "Text": "Comparison of different systems on the ACE RDC 2004 corpus In Table 3 we summarize the improvements of different tree setups over SPT.",
        "Entity": "Reference"
    },
    {
        "Text": "(Equation (7)) (Poisson + hi gram).",
        "Entity": "Reference"
    },
    {
        "Text": "As a result, Arabic sentences are usually long relative to English, especially after",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 shows the distribution of character type sequences that constitute the infrequent words in the EDR corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Local graph of the word wing of a graph",
        "Entity": "Reference"
    },
    {
        "Text": "Recall is somewhat difficult to estimate because we do not know whether the English translation of a Chinese word appears in the English part of the corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "The coupling between B and is removed by setting H = B 1: n min (W, H H T ), s.t. hip = 1 (1) H, i=1 BT Dl Bl according to equation 4 l end for return BL , BL 1 ...B1 Additional steps need to be performed in order to extract a tree from the hierarchical graph.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 shows the results of both unconstrained and constrained versions of HGFC and those of AGG on the test set T3 (where singular classes are removed to enable proper evaluation of the constrained method).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1.",
        "Entity": "Caption"
    },
    {
        "Text": "and Table 6 show a comparison of the segmentation and POS tagging accuracies with other state-of-the-art models.",
        "Entity": "Reference"
    },
    {
        "Text": "using the convolution parse tree kernel as depicted in Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "For example, in the sentence bought one of town s two meat- packing plants as illustrated in Figure 1(a), the constituents before the headword plants can be removed from the parse tree.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 7: Word segmentation accuracy of unknown words r e c pr ec F Po iss on + bi gr a m W T + P oi ss o n + b i g r a m P O S + P o is s o n + b i g r a m P O S + W T + P o is s o n + bi g ra m 31 .8 45 .5 39 .7 42.",
        "Entity": "Reference"
    },
    {
        "Text": "However, the learning curves in Figure 3 show that the Berkeley parser does not exceed our manual grammar by as wide a margin as has been shown for other languages (Petrov, 2009).",
        "Entity": "Reference"
    },
    {
        "Text": "The direct translation probability is given by",
        "Entity": "Reference"
    },
    {
        "Text": "where N is role features when combining two children states, and ex amples can be found in Figure 3.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2",
        "Entity": "Reference"
    },
    {
        "Text": "This leaves us with 60 meta alternations, shown in Table 5.",
        "Entity": "Reference"
    },
    {
        "Text": "Results for 2 and for 10 preceding POS tags as context are reported for our tagger.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Metaphors tagged by the system (in bold) whereby the main source of disagreement was the presence of lexicalized metaphors, e.g. verbs such as impose, decline etc.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 shows the results of both unconstrained and constrained versions of HGFC and those of AGG on the test set T3 (where singular classes are removed to enable proper evaluation of the constrained method).",
        "Entity": "Reference"
    },
    {
        "Text": "K hAT(eI , f J , K , zK ) = log n p(zk | f j k ) (13) 1 1 1 1 k=1 j k 1 +1",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Effect of model parameters on performance.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4: An example showing how to compute the target side position of a semantic role by using the median of its aligning points.",
        "Entity": "Reference"
    },
    {
        "Text": "and ap plied to an Arabic sentence in figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "(prec2 in Table 8)",
        "Entity": "Reference"
    },
    {
        "Text": "This sum can be computed efficiently using the algorithm shown in Figure 8",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5",
        "Entity": "Reference"
    },
    {
        "Text": "This approach can be seen as a generalization of the originally suggested source channel modeling framework for statistical machine translation",
        "Entity": "Reference"
    },
    {
        "Text": "The alignment aJ that has the highest probability (under a certain model) is also called the Viterbi alignment (of that model)",
        "Entity": "Reference"
    },
    {
        "Text": "Except our own and MENE + reference resolution, the results in Table 6 are all official MUC7 results.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1shows the word length distribution of in frequent words in the EDR corpus, and the estimate of word length distribution by Equation (6) whose parameter (.A = 4.8) is the average word length of infrequent words.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 shows the effect of constraining the maximum length of the alignment templates in the source language.",
        "Entity": "Reference"
    },
    {
        "Text": "n our experimentations, SVMlight (Joachims, 1998) with the tree kernel function (75.0) (53.7) (62.6) Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 displays the performance of our model and of the systems that obtained the best (Fernandes et al., 2012) and the median performance in the MUC B3 CEAFe average R P F1 R P F1 R P F1 CoNLL 12 English development data be st 64.",
        "Entity": "Reference"
    },
    {
        "Text": "More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; Paa and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3:",
        "Entity": "Reference"
    },
    {
        "Text": "table",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5 shows our morpheme accuracy results.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6: Final results on CTB6 and CTB7 accuracies of POS tagging and dependency parsing were remarkably improved by 0.6% and 2.4%, respectively corresponding to 8.3% and 10.2% error reduction.",
        "Entity": "Reference"
    },
    {
        "Text": "(Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1, Figure 1, and Figure 2 shows the AER results for different models.",
        "Entity": "Reference"
    },
    {
        "Text": "eik (12",
        "Entity": "Caption"
    },
    {
        "Text": "Preliminary observations show that the different neighbours in Table 1 can be used to indicate with great accuracy which of the senses is being used",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: AER comparison (cn \u2192en)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 breaks down the performance of the best CAM model by meta alternation.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: Time to read and accept or reject proposals versus their length tion, because the empirical probability of acceptance is very low when it is less than zero and rises rapidly as it increases.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "igure 1 gives an example.",
        "Entity": "Reference"
    },
    {
        "Text": "Although kanji sequences are difficult to seg ment, they can comprise a significant portion of Japanese text, as shown in Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "The result is shown in Table 4: the baseline numbers without stem features are listed under Base, and the results of the coreference system with stem features are listed under Base+Stem.",
        "Entity": "Reference"
    },
    {
        "Text": "igure 1 gives an example.",
        "Entity": "Reference"
    },
    {
        "Text": "aJ = argmax p (f J , aJ | eI ) (8) 1 1 1 1 J 1",
        "Entity": "Caption"
    },
    {
        "Text": "The reordering of the semantic roles from source to target is computed for each TTS template as part of the template extraction process, using the word-level alignments between the LHS/RHS of the TTS template (e.g., Figure 3).",
        "Entity": "Reference"
    },
    {
        "Text": "he largest effect seems to come from taking into account the bigram dependence, which achieves an mWER of 32.9%",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 6 gives an overview of the decisions made in the alignment template model.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8a shows that the best model recovers SBAR at only 71.0% F1.",
        "Entity": "Reference"
    },
    {
        "Text": "In Figure 4 we show an example of variation between the parsing models.",
        "Entity": "Reference"
    },
    {
        "Text": "We set Table 6: Word segmentation accuracy of all words r e c pr ec F Po iss on +b igr a m W T +P oi ss on +b igr a m P O S + Po iss on +b igr a m P O S + W T + Po iss on + bi gr a m 94 .5 94 .4 94 .4 94 .6 93 .1 93 .8 93 .6 93 .7 93 .8 94 .1 94 .0 94 .1",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 7",
        "Entity": "Reference"
    },
    {
        "Text": "In the following, we describe the criterion that defines the set of phrases that is consistent with the word alignment matrix",
        "Entity": "Reference"
    },
    {
        "Text": "In Table 1, period 1 is Jul 01 \u2013 Jul 15, period 2 is Jul 16 \u2013 Jul 31, \u2026, period 12 is Dec 16 \u2013 Dec 31.",
        "Entity": "Reference"
    },
    {
        "Text": "Equation (12), which is a set of word models trained for each part of speech (POS + Poisson + bigram).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4 shows an example of calculating the target side SRS based on a complicated TTS template.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4",
        "Entity": "Reference"
    },
    {
        "Text": "An extract of the results is listed in table 1",
        "Entity": "Reference"
    },
    {
        "Text": "The first example in Table 3 shows that words ending in ' -' are likely to be nouns.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2 shows that the tagging accuracy tends to increase with the context size.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 shows the performance and speed of the full joint model (with no dictionaries) on CTB5c1 with respect to the beam size.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: An example showing the combination of the semantic role sequences of the states.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5: Evaluation of 100 randomly sampled variation nuclei types.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Per category performance of the Berkeley parser on sentence lengths \u2264 70 (dev set, gold segmentation). (a) Of the high frequency phrasal categories, ADJP and SBAR are the hardest to parse. We showed in \u00a72 that lexical ambiguity explains the underperformance of these categories. (b) POS tagging accuracy is lowest for maSdar verbal nouns (VBG,VN) and adjectives (e.g., JJ). Richer tag sets have been suggested for modeling morphologically complex distinctions (Diab, 2007), but we find that linguistically rich tag sets do not help parsing. (c) Coordination ambiguity is shown in dependency scores by e.g., \u2217SSS R) and \u2217NP NP NP R). \u2217NP NP PP R) and \u2217NP NP ADJP R) are both iDafa attachment.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Performance on T3 using a predefined tree structure.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 7 Effect of pruning parameter Np and heuristic function on error rate for direct-translation model (tp = 10 12 ).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 presents the distributions of some examples of morphs and their targets in English Twitter and Chinese Sina Weibo.",
        "Entity": "Reference"
    },
    {
        "Text": "The model is described using a log-linear modeling approach, which is a generalization of the often used source channel approach",
        "Entity": "Reference"
    },
    {
        "Text": "As the search space increases expo nentially, it is not possible to explicitly represent it.",
        "Entity": "Reference"
    },
    {
        "Text": "The probability of using an alignment template to translate a specific source language phrase f is estimated by means of relative frequency",
        "Entity": "Reference"
    },
    {
        "Text": "As Figure 3 shows, word type information improves the prediction accuracy significantly.",
        "Entity": "Reference"
    },
    {
        "Text": "In addition to the basic regular expression operators shown in table 1, the formalism is extended in various ways.",
        "Entity": "Reference"
    },
    {
        "Text": "I+1 hCLM(eI , f J , K , zK ) = log n p(C(ei ) | C(ei 4 ), ..",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Comparison of results for MUC6",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Scores for UPUC corpusFrom those tables, we can see that a simple ma jority voting algorithm produces accuracy that is higher than each individual system and reasonably high F-scores overall.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1).",
        "Entity": "Reference"
    },
    {
        "Text": "Word Sense clusters Class-label arms knees trousers feet biceps hips elbows backs wings body part breasts shoulders thighs bones buttocks ankles legs inches wrists shoes necks horses muskets charges weapons methods firearms weapon knives explosives bombs bases mines projectiles drugs missiles uniforms jersey israel colomho guernsey luxeinhourg denmark maim European greece belgium swede, turkey gibraltar portugal ire- country land mauritius britain cyprus netherlands norway aus tralia italy japan canada kingdom spain austria zealand england france germany switzerland finland poland a merica usa iceland holland scotland uk crucifix bow apron sweater tie anorak hose bracelet garment helmet waistcoat jacket pullover equipment cap collar suit fleece tunic shirt scarf belt head voice torso back chest face abdomen side belly groin body part spine breast bill rump midhair hat collar waist tail stomach skin throat neck speculum ceo treasurer justice chancellor principal founder pres- person ident commander deputy administrator constable li brarian secretary governor captain premier executive chief curator assistant committee patron ruler oil heat coal power water gas food wood fuel steam tax object heating kerosene fire petroleum dust sand light steel telephone timber supply drainage diesel electricity acid air insurance petrol tempera gouache watercolour poster pastel collage paint acrylic lemon bread cheese [flint butter jam cream pudding yogurt foodstuff sprinkling honey jelly toast ham chocolate pie syrup milk meat beef cake yoghurt grain hazel elder holly family virgin hawthorn shrub cherry cedar larch mahogany water sycamore lime teak ash wood hornbeam oak walnut hazel pine beech alder thorn poplar birch chestnut blackthorn spruce holly yew lau rel maple elm fir hawthorn willow bacon cream honey pie grape blackcurrant cake ha- foodstuff mama",
        "Entity": "Caption"
    },
    {
        "Text": "#c is the total number of new Chinese source words in the period",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Word length distribution of kanji words and katakana words length model does not reflect the variation of the word length distribution resulting from the Japanese orthography.",
        "Entity": "Reference"
    },
    {
        "Text": "Finally, Table 4 shows the results for the unconstrained HGFC on T2 and and T3 when the tree structure is not predefined but inferred automatically as described in section 3.2.3.",
        "Entity": "Reference"
    },
    {
        "Text": "We see from Table 5, that the improvement in overall parse results is mainly in terms of dependency labeling, reflected in the LAS score.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1; capital letters designate sets and small letters elements of sets).2 For a lemma l like lamb, we want to knowhow well a meta alternation (such as ANIMAL FOOD) explains a pair of its senses (such as the animal and food senses of lamb).3 This is formalized through the function score, which maps a meta alternation and two senses onto a score.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 shows the absolute frequencies of sen tence recency values when only the most recent antecedent (in the order just stated) is considered.",
        "Entity": "Reference"
    },
    {
        "Text": "The results are displayed in Table 3.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4 shows a confusion matrix for the classification of the nouns.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Tagging accuracies on test data.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3 gives the algorithm phrase-extract that computes the phrases",
        "Entity": "Reference"
    },
    {
        "Text": "this error is not counted, the tagging accuracy on the development data rises from 92.17% to 94.27%.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Sample of experimental items for the meta alternation anmfod.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5",
        "Entity": "Reference"
    },
    {
        "Text": "The reordering of the semantic roles from source to target is computed for each TTS template as part of the template extraction process, using the word-level alignments between the LHS/RHS of the TTS template (e.g., Figure 3).",
        "Entity": "Reference"
    },
    {
        "Text": "If it starts with a lower case letter, and contains both upper and lower case letters, then (mixedCaps, zone) is set to 1.",
        "Entity": "Reference"
    },
    {
        "Text": "An abridged version of the grammatical representation produced by the implemented grammar for this sentence is presented in Figure 1, where the feature structures below the tree correspond to partial grammatical representations of the constituents 16 See Kamp and Reyle (1993) for a comprehensive rendering of DRT, and Branco (2000, Chapter 5) for an.",
        "Entity": "Reference"
    },
    {
        "Text": "The corresponding figures for the test data are. 89.53% for our tagger and 88.88% for the TnT tag- ger.",
        "Entity": "Reference"
    },
    {
        "Text": "Comparison of different systems on the ACE RDC 2004 corpus In Table 3 we summarize the improvements of different tree setups over SPT.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: BLEU4 scores of different systems Source Launching1 New2 Diplomatic3 Offensive4 SRF On 1 2 3 4 SRF Off 2 3 4 It1 is2 therefore3 necessary4 to5 speed6 up7 the8 equal better worse With SRF vs. W/O SRF 72% 20.2% 7.8% Source transformation9 of10 traditional11 industries12 with13 high14 technologies15",
        "Entity": "Reference"
    },
    {
        "Text": "To illustrate the complete user model, in the figure 1 example the benefit of accepting would be7 2 4.2 = .8 keystrokes and the benefit of reject ing would be .2 keystrokes.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 9 shows the training and test corpus statistics.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Statistics of datasets.",
        "Entity": "Reference"
    },
    {
        "Text": "For example, the expressions in Figure 2 are identified as paraphrases by this method; so these three patterns will be placed in the same pattern set.",
        "Entity": "Reference"
    },
    {
        "Text": "equation (3)",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6: Incremental dev set results for the manually annotated grammar (sentences of length \u2264 70).",
        "Entity": "Caption"
    },
    {
        "Text": "This corresponds to maximizing the equivocation or maximizing the likelihood of the direct-translation model",
        "Entity": "Reference"
    },
    {
        "Text": "The last is exhibited for the first mention in figure 1, where one annotator chose ARTIFACT (referring tothe physical book) while the other chose COMMUNICATION (the content)",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 shows the usefulness evaluation result.",
        "Entity": "Reference"
    },
    {
        "Text": "See Table 1 for details.",
        "Entity": "Reference"
    },
    {
        "Text": "Results: Table I gives the results for the comparison of the troughs placed by the segmentation algorithm to the known subject change points.",
        "Entity": "Reference"
    },
    {
        "Text": "The results are shown in the corr row of table 2, for exact character-probability estimates.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2",
        "Entity": "Reference"
    },
    {
        "Text": "For example, if a token starts with a capital letter and ends with a period (such as Mr.), then the feature InitCapPeriod is set to 1, etc",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Effect of Arabic stemming features on coreference resolution.",
        "Entity": "Reference"
    },
    {
        "Text": "We choose t = 1, 5, and 30 for the fertility HMM",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 describes the components and how this system works.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 7 shows a structogram of the algorithm",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4 shows examples of alignment templates",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "Tables 4 and 5 show the effect of the pruning parameter tp with the histogram pruning parameter Np = 50,000.",
        "Entity": "Reference"
    },
    {
        "Text": "For fair comparison, we have tabulated all results with the size of training data used (Table 5",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6 Effect of pruning parameter Np and heuristic function on search efficiency for direct-translation model (tp = 10 12 )",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: F-measure after successive addition of each global feature group",
        "Entity": "Caption"
    },
    {
        "Text": "2",
        "Entity": "Reference"
    },
    {
        "Text": "(Abbreviations are listed in Table 2.)",
        "Entity": "Reference"
    },
    {
        "Text": "The direct translation probability is given by",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2.",
        "Entity": "Reference"
    },
    {
        "Text": "(Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Scores for UPUC corpusFrom those tables, we can see that a simple ma jority voting algorithm produces accuracy that is higher than each individual system and reasonably high F-scores overall.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5:",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3 shows the performance and processing time comparison of various models and their combinations.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 presents an overview of the animacy data Clas s Ani mat e Type s Tok 6 4 4 ens cover ed 6 0 1 0 Inan imat e 691 0 3 4 8 2 2 Tota l 755 4 4 0 8 3 2 Table 1: The animacy data set from Talbanken05; number of noun lemmas (Types) and tokens in each class.",
        "Entity": "Reference"
    },
    {
        "Text": "The results when we set M = 10 are shown in Table 1.",
        "Entity": "Reference"
    },
    {
        "Text": "See Fig ure 6 for some examples.",
        "Entity": "Reference"
    },
    {
        "Text": "As for the unknown word model, word-based char acter bigrams are computed from the words with Table 5: Cross entropy (CE) per word and character perplexity (PP) of each unknown word model Part of Speech Estimation Accuracy 0.95 0.9 frequency one (49,653 words).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: AER comparison (cn \u2192en)",
        "Entity": "Caption"
    },
    {
        "Text": "Although kanji sequences are difficult to seg ment, they can comprise a significant portion of Japanese text, as shown in Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "eI K 1 = e1 , ek = eik 1 +1 , .",
        "Entity": "Caption"
    },
    {
        "Text": "As each global feature group is added to the list of features, we see improvements to both MUC6 and",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3:",
        "Entity": "Reference"
    },
    {
        "Text": "We also propose to use the features U01 U03, which we found are effective to adjust the character Figure 1: Illustration of the alignment of steps.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: AER results",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Evaluation of 100 randomly sampled variation nuclei types.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1; capital letters designate sets and small letters elements of sets).2 For a lemma l like lamb, we want to knowhow well a meta alternation (such as ANIMAL FOOD) explains a pair of its senses (such as the animal and food senses of lamb).3 This is formalized through the function score, which maps a meta alternation and two senses onto a score.",
        "Entity": "Reference"
    },
    {
        "Text": "This list of 43 words is shown in Table 3.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: F-measure after successive addition of each global feature group",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 shows type- and token-level error rates for each corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Comparison of results for MUC7",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Accuracy of part of speech estimation each part of speech and word type (POS + WT + Poisson + bigram).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Distribution of the sentences where the semantic role features give no/positive/negative impact to the sentence fluency in terms of the completeness and ordering of the semantic roles.classes in VerbNet (Dang et al., 1998).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Tagging accuracy on development data depending on context size",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Rank of correct translation for period Dec 01 \u2013 Dec 15 and Dec 16 \u2013 Dec 31. \u2018Cont. rank\u2019 is the context rank, \u2018Trans. Rank\u2019 is the transliteration rank. \u2018NA\u2019 means the word cannot be transliterated. \u2018insuff\u2019 means the correct translation appears less than 10 times in the English part of the comparable corpus. \u2018comm\u2019 means the correct translation is a word appearing in the dictionary we used or is a stop word. \u2018phrase\u2019 means the correct translation contains multiple English words.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2",
        "Entity": "Reference"
    },
    {
        "Text": "tion (13) is estimated from the relative frequency of the corresponding events in the training corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8 shows that tagging precision is im proved from 88.2% to 96.6%.",
        "Entity": "Reference"
    },
    {
        "Text": "figure.",
        "Entity": "Reference"
    },
    {
        "Text": "In fact, from the last column of Figure 8 we see that even if our algorithm has access to only five anno tated sequences when Juman has access to ten times as many, we still achieve better precision and better F measure.",
        "Entity": "Reference"
    },
    {
        "Text": "MENE has only been tested on MUC7.",
        "Entity": "Reference"
    },
    {
        "Text": "For example, the pairwise words orange and peel form a collocation.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5",
        "Entity": "Reference"
    },
    {
        "Text": "CoreLex defines a layer of abstraction above WordNet consisting of 39 basic types, coarse- grained ontological classes (Table 2).",
        "Entity": "Reference"
    },
    {
        "Text": "igure 2 Example of a (symmetrized) word alignment (Verbmobil task).",
        "Entity": "Caption"
    },
    {
        "Text": "In all four tables, we provide theresults for using no heuristic functions and three variants of an increasingly infor mative heuristic function.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Statistics from 1993 Japanese newswire (NIKKEI), 79,326,406 characters total.",
        "Entity": "Reference"
    },
    {
        "Text": "The dataset (table 1) consists of the main text of 28 articles selected from the topical domains of history, sports, science, and technology",
        "Entity": "Reference"
    },
    {
        "Text": "14http://www.cis.upenn.edu/ dbikel/software.html Gold standard Automatic UAS LAS UAS LAS Baseline 89.87 84.92 89.87 84.92 Anim 89.81 84.94 89.87 84.99 Table 5: Overall results in experiments with automatic features compared to gold standard features, expressed as unlabeled and labeled attachment scores.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2 plots AP by for all meta alternations.",
        "Entity": "Reference"
    },
    {
        "Text": "Evaluation results are listed in Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "The third model is Equation (11), which is a set of word models trained for each word type (WT +Poisson+ bigram).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 shows the influence of the four parameters.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 shows examples of common char acter bigrams for each part of speech in the infre quent words of the EDR corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "(2011), we confirmed that omission of the look-ahead features results in a 0.26% decrease in the parsing accuracy on CTB5d (dev).Figure 2: F1 scores (in %) of SegTagDep on CTB 5c1 w.r.t. the training epoch (x-axis) and parsing feature weights (in legend).",
        "Entity": "Reference"
    },
    {
        "Text": "(Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Statistics of datasets.",
        "Entity": "Reference"
    },
    {
        "Text": "In our segmentation system, a hybrid strategy is applied (Figure 1): First, forward maximum matching (Chen and Liu, 1992), which is a dictionary-based method, is used to generate a segmentation result.",
        "Entity": "Reference"
    },
    {
        "Text": "We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4, 5, and 6.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5 shows our morpheme accuracy results.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8a shows that the best model recovers SBAR at only 71.0% F1.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1(d) shows a sentence maintain rental property he owns in the state , where the ART.User-or-Owner relation holds between the entities property and he",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: AER comparison (en\u2192cn)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Word length distribution of unknown words and its estimate by Poisson distribution",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 6: Examples of word, morpheme, and compatible-bracket errors.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1.",
        "Entity": "Caption"
    },
    {
        "Text": "As a result, Arabic sentences are usually long relative to English, especially after",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Above: The complete supersense tagset for nouns; each tag is briefly described by its symbol, NAME, short description, and examples.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2 shows the F1 scores of the proposed model (SegTagDep) on CTB5c1 with respect to the training epoch and different parsing feature weights, where Seg , Tag , and Dep respectively denote the F1 scores of word segmentation, POS tagging, and dependency parsing.",
        "Entity": "Reference"
    },
    {
        "Text": "using the convolution parse tree kernel as depicted in Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 shows the effect of constraining the maximum length of the alignment templates in the source language.",
        "Entity": "Reference"
    },
    {
        "Text": "Letting u1 be the prefix of the word that ends in v1 (eg, r in figure 1), w1 = u1v1, and h = htu1:is less accurate because it ignores the alignment rela tion between s and h, which is captured by even the simplest noisy-channel models.",
        "Entity": "Reference"
    },
    {
        "Text": "With an absolute frequency threshold of 10, we obtain an accuracy of 95.4%, which constitutes a 50% reduction of error rate.Table 3 presents the experimental results rela tive to class.",
        "Entity": "Reference"
    },
    {
        "Text": "As illus trated in Figure 1(e), the NP coordination in the Qian et al.",
        "Entity": "Reference"
    },
    {
        "Text": "We see that the language model perplexity improves from 4,781 for a unigram model to 29.9 for a trigram model.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Word length distribution of kanji words and katakana words length model does not reflect the variation of the word length distribution resulting from the Japanese orthography.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 shows the experimental results with and without the stem n-grams features.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2 shows empirical estimates of p(a = 1|2k l) from the TransType data.",
        "Entity": "Reference"
    },
    {
        "Text": "2",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2 shows the word length distribution of words consists of only kanji characters and words consists of only katakana characters.",
        "Entity": "Reference"
    },
    {
        "Text": "A token that is allCaps will also be initCaps.",
        "Entity": "Reference"
    },
    {
        "Text": "3",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Probability estimation tree for the nomi native case of nouns.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: AER comparison (en\u2192cn)",
        "Entity": "Caption"
    },
    {
        "Text": "For example, if x in figure 1 were evenir aux choses, then x14 would map to v1 = evenir, w2 = aux, and u3 = cho.",
        "Entity": "Reference"
    },
    {
        "Text": "Excerpt from the collocation lexicon",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "U = {up}m represent the hidden m struct a new graph G1 (Figure 1(d)) with the clusters U as vertices.",
        "Entity": "Reference"
    },
    {
        "Text": "#Cor is the number of correct English translations output.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: AER results",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 presents the performance in terms of precision, recall, and F- measure of the whole system.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8 Effect of the length of the language model history (Unigram/Bigram/Trigram: word-based; CLM: class-based 5-gram).",
        "Entity": "Caption"
    },
    {
        "Text": "(prec2 in Table 8)",
        "Entity": "Reference"
    },
    {
        "Text": "Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006) and the CRF model using minimum subword-based tagging, both of which are statistical methods, are used individually to solve the Figure 1: Outline of the segmentation process 2.1 Forward Maximum Matching.",
        "Entity": "Reference"
    },
    {
        "Text": "In Figure 4 we show an example of variation between the parsing models.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Performance on T3 using a predefined tree structure.",
        "Entity": "Reference"
    },
    {
        "Text": "We see from Table 5, that the improvement in overall parse results is mainly in terms of dependency labeling, reflected in the LAS score.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Example of a prediction for English to French translation.",
        "Entity": "Reference"
    },
    {
        "Text": "Tables 4 and 5 show the effect of the pruning parameter tp with the histogram pruning parameter Np = 50,000",
        "Entity": "Reference"
    },
    {
        "Text": "In our segmentation system, a hybrid strategy is applied (Figure 1): First, forward maximum matching (Chen and Liu, 1992), which is a dictionary-based method, is used to generate a segmentation result.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1shows the word length distribution of in frequent words in the EDR corpus, and the estimate of word length distribution by Equation (6) whose parameter (.A = 4.8) is the average word length of infrequent words.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: A sentence from the article Islamic GoldenAge, with the supersense tagging from one of two anno tators.",
        "Entity": "Reference"
    },
    {
        "Text": "whose unvocalized surface forms 0 an are indistinguishable.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Features based on the token string that are based on the probability of each name class during training.",
        "Entity": "Caption"
    },
    {
        "Text": "Pr(f J , aJ | eI ) = p (f J , aJ | eI ) (6) 1 1 1 1 1 1",
        "Entity": "Caption"
    },
    {
        "Text": "Finally, Table 4 shows the results for the unconstrained HGFC on T2 and and T3 when the tree structure is not predefined but inferred automatically as described in section 3.2.3.",
        "Entity": "Reference"
    },
    {
        "Text": "the time-consuming renormalization in equation (3) is not needed in search",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 shows the results of an evaluation based on the plain STTS tagset.",
        "Entity": "Reference"
    },
    {
        "Text": "The accuracy of a baseline tagger which chooses the most probable tag9 ignoring the context is 67.3% without and 69.4% with the supple 92.3 92.2 92.1 92 91.9 91.8 91.7 91.6 91.5 91.4 2 3 4 5 6 7 8 9 10 mentary lexicon.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4",
        "Entity": "Reference"
    },
    {
        "Text": "see table 3) as the impact of soft constraints is the weakest for the constrained method at this level.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure",
        "Entity": "Reference"
    },
    {
        "Text": "The results when we set M = 10 are shown in Table 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2 shows the F1 scores of the proposed model (SegTagDep) on CTB5c1 with respect to the training epoch and different parsing feature weights, where Seg , Tag , and Dep respectively denote the F1 scores of word segmentation, POS tagging, and dependency parsing.",
        "Entity": "Reference"
    },
    {
        "Text": "see table 3) as the impact of soft constraints is the weakest for the constrained method at this level.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Effect of model parameters on performance.",
        "Entity": "Reference"
    },
    {
        "Text": "Nc Nl HGFC uncons trained A G G N MI F N M I F 13 0 13 3 57 .3 1 36 .6 5 54 .2 2 32 .6 2 11 4 11 7 54 .6 7 37 .9 6 51 .3 5 32 .4 4 50 51 37 .7 5 40 .0 0 32 .6 1 32 .7 8 Table 2: Performance on T2 using a predefined tree structure.",
        "Entity": "Reference"
    },
    {
        "Text": "In Figure 1(c) we show a sentence one of about 500 people nominated for , where there exists a DISC relationship between the entities one and people",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Output of word sense clustering.",
        "Entity": "Reference"
    },
    {
        "Text": "For example, if a token starts with a capital letter and ends with a period (such as Mr.), then the feature InitCapPeriod is set to 1, etc",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "However, the spelling model, especially the character bigrams in Equation (17) are hard to es timate because of the data sparseness.",
        "Entity": "Reference"
    },
    {
        "Text": "If the token starts with a capital letter (initCaps), then an additional feature (init- Caps, zone) is set to 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6 lists a sample of targets for the five meta alternations involved.",
        "Entity": "Reference"
    },
    {
        "Text": "equation 2) says that the prob ability of starting a new entity, given the current mention m and the previous entities e1, e2, , et, is simply 1 minus the maximum link probability between the current mention and one of the previous entities.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "BP(f J , eI , A) = f j+m , ei+n 1 1 j i : (i , j ) A : j j j + m i i i + n (9) (i , j ) A : j j j + m i i i + n",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 shows the results.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5 shows our morpheme accuracy results.",
        "Entity": "Reference"
    },
    {
        "Text": "This approach has been suggested by Papineni, Roukos, and Ward (1997, 1998) for a natural language understanding task.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: Distribution of supersense mentions by domain (left), and counts for tags occurring over 800 times (below).",
        "Entity": "Reference"
    },
    {
        "Text": "For the Verbmobil task, we train the model parameters M according to the maximum class posterior probability criterion (equation (4)).",
        "Entity": "Reference"
    },
    {
        "Text": "Table (1) and Eq.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.",
        "Entity": "Reference"
    },
    {
        "Text": "this error is not counted, the tagging accuracy on the development data rises from 92.17% to 94.27%.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 Examples of two- to seven-word bilingual phrases obtained by applying the algorithm phrase-extract to the alignment of Figure 2",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Examples of common character bigrams for each part of speech in the infrequent words pa rt of sp ee ch ch ar ac ter bi gr a m fre qu en cy no un nu m be r a dj e ct iv al v er b v er b ad je cti ve ad ve rb < e o w > <b o w > 1 S \" J < e o w > I t < e o w > L < e o w > < e o w > 13 43 4 8 4 3 2 7 2 1 3 69 63 resented all unknown words by one length model.",
        "Entity": "Reference"
    },
    {
        "Text": "However, if we remove the mouse-node from its local graph illustrated in figure 1, the graph decomposes into two parts, one representing the electronic device meaning of mouse and the other one representing its animal sense.",
        "Entity": "Reference"
    },
    {
        "Text": "If it starts with a lower case letter, and contains both upper and lower case letters, then (mixedCaps, zone) is set to 1.",
        "Entity": "Reference"
    },
    {
        "Text": "#Cor is the number of correct English translations output.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 6 Dependencies in the alignment template mode",
        "Entity": "Caption"
    },
    {
        "Text": "Collocation: Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio (Church and Hanks, 1990) and outputted to a lexicon.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4",
        "Entity": "Reference"
    },
    {
        "Text": "(equation 2",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 shows a probability estimation tree for the prediction of the probability of the nominative attribute of nouns.",
        "Entity": "Reference"
    },
    {
        "Text": "#e is the total number of English translation candidates in the period.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Tree setups P(%) R(%) F CS-SPT over SPT3 1.5 1.1 1.3 DSPT over SPT 1.1 5.6 3.8 UPST (FPT) over SPT 3.8 10.9 8.0 Table 3.",
        "Entity": "Reference"
    },
    {
        "Text": "For example, if x in figure 1 were evenir aux choses, then x14 would map to v1 = evenir, w2 = aux, and u3 = cho.",
        "Entity": "Reference"
    },
    {
        "Text": "(Abbreviations are listed in Table 2.)",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 6: EM Algorithm For Estimating TTS Templates and Semantic Features framework (May and Knight, 2007).",
        "Entity": "Reference"
    },
    {
        "Text": "The figure schematically shows a small portion of the graph describing the concepts of mechanism (concrete), political system and relationship (abstract) at two levels of generality.",
        "Entity": "Reference"
    },
    {
        "Text": "For example, no synset covers any combinations of the main words in Figure 2, namely buy , acquire and merger",
        "Entity": "Reference"
    },
    {
        "Text": "In the last two lines of Equation 3, \u03c6\u01eb and each P (f |e) = \"\u00a3s c (f |e; f (s), e(s)) (4) \u03c6i are not free variables, but are determined by f s c(f |e; f (s), e(s))the alignments.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8 shows that tagging precision is im proved from 88.2% to 96.6%.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Examples of the semantic role features assuming that the semantic roles have been tagged for the source sentences.",
        "Entity": "Reference"
    },
    {
        "Text": "this error is not counted, the tagging accuracy on the development data rises from 92.17% to 94.27%.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8a shows that the best model recovers SBAR at only 71.0% F1.",
        "Entity": "Caption"
    },
    {
        "Text": "See Figure 3 for examples.",
        "Entity": "Reference"
    },
    {
        "Text": "However, the learning curves in Figure 3 show that the Berkeley parser does not exceed our manual grammar by as wide a margin as has been shown for other languages (Petrov, 2009).",
        "Entity": "Reference"
    },
    {
        "Text": "Contribution of constituent dependencies in respective mode (inside parentheses) and accumulative mode (outside parentheses) The table shows that the final DSPT achieves the best performance of 77.4%/65.4%/70.9 in precision/recall/F-measure respectively after applying all the dependencies, with the increase of F-measure by 8.2 units compared to the baseline MCT.",
        "Entity": "Reference"
    },
    {
        "Text": "If it is made up of all capital letters, then (allCaps, zone) is set to 1",
        "Entity": "Reference"
    },
    {
        "Text": "To compute the third factor of Equation (13), we have to estimate the character bigram probabilities that are classified by word type and part of speech.",
        "Entity": "Reference"
    },
    {
        "Text": "The baseline system in Table 3 refers to the maximum entropy system that uses only local features.",
        "Entity": "Reference"
    },
    {
        "Text": "I+1 hLM(eI , f J , K , zK ) = log n p(ei | ei 2 , e ) (17) 1 1 1 1 i=1 i 1",
        "Entity": "Caption"
    },
    {
        "Text": "The bottom-up decoding algorithm for the TTS transducer is sketched in Figure 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1), i.e., the degree to which a sense pair (s1, s2) matches a meta alternation a.",
        "Entity": "Reference"
    },
    {
        "Text": "e suggest the use of a log-linear model to incorporate the various knowledge sources into an overall translation system and to perform discriminative training of the free model parameters",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5 shows type- and token-level error rates for each corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6 Effect of pruning parameter Np and heuristic function on search efficiency for direct-translation model (tp = 10 12 )",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9: Dev set results for sentences of length \u2264 70.",
        "Entity": "Caption"
    },
    {
        "Text": "In order to effectively capture entity-related semantic features, and their combined features as well, especially bi-gram or tri-gram features, we build an Entity-related Semantic Tree (EST) in three ways as illustrated in Figure 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Dependency representation of example (2) from Talbanken05.",
        "Entity": "Reference"
    },
    {
        "Text": "even after removal of the wing-node, the two areas of meaning are still linked via tail",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5 shows type- and token-level error rates for each corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 shows the NA M N O M PR O NA M 34 13 (21 %) 67 (6 6 %) 11 (4 6 %) N O M 43 (67 %) 21 48 (4 9 %) 9 (8 9 %) PR O 86 8 (32 %) 17 71 (5 5 %) 53 08 (2 4 %) Table 2: Number of clustering decisions made according to mention type (rows anaphor, columns antecedent) and percentage of wrong decisions.",
        "Entity": "Reference"
    },
    {
        "Text": "However, this information is hard to extract reliably from the available data; and even if were obtainable, many of the 0.3 0.2 0.1 0 60 50 40 30 20 10 0 10 20 30 40 50 60 gain (length of correct prefix length of incorrect suffix) Figure 2: Probability that a prediction will be accepted versus its gain.",
        "Entity": "Reference"
    },
    {
        "Text": "The points labelled smoothed in figure 2 were obtained using a sliding-average smoother, and the model curve was obtained using two-component Gaussian mixtures to fit the smoothed empirical likelihoods p(gain|a = 0) and p(gain|a = 1).",
        "Entity": "Reference"
    },
    {
        "Text": "This sentence should be tagged as shown in table 1.",
        "Entity": "Reference"
    },
    {
        "Text": "As illus trated in Figure 1(e), the NP coordination in the Qian et al.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4 shows a confusion matrix for the classification of the nouns.",
        "Entity": "Reference"
    },
    {
        "Text": "hAL(eI , f J , K , zK ) = |j 1 j | (16) 1 1 1 1 k k=1 k 1",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6",
        "Entity": "Reference"
    },
    {
        "Text": "An abridged version of the grammatical representation produced by the implemented grammar for this sentence is presented in Figure 1, where the feature structures below the tree correspond to partial grammatical representations of the constituents 16 See Kamp and Reyle (1993) for a comprehensive rendering of DRT, and Branco (2000, Chapter 5) for an.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2",
        "Entity": "Reference"
    },
    {
        "Text": "For example, if a token starts with a capital letter and ends with a period (such as Mr.), then the feature InitCapPeriod is set to 1, etc",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: CoreLex s basic types with their corresponding WordNet anchors.",
        "Entity": "Reference"
    },
    {
        "Text": "The graph displayed in Figure 1 is the graph constructed for the mentions Leaders, Paris, recent developments and They from the example sentence at the beginning of this Section, where R = {P AnaPron, P Subject, N Number}.",
        "Entity": "Reference"
    },
    {
        "Text": "For example, no synset covers any combinations of the main words in Figure 2, namely buy , acquire and merger",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "A standard criterion on a parallel training corpus consisting of S sentence pairs {(fs , es ): s = 1, .",
        "Entity": "Reference"
    },
    {
        "Text": "CoreLex defines a layer of abstraction above WordNet consisting of 39 basic types, coarse- grained ontological classes (Table 2).",
        "Entity": "Reference"
    },
    {
        "Text": "The bipartite graph K also induces a similarityagain (Figure 1(e)).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 shows the correctness evaluation results.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 shows the experimental results with and without the stem n-grams features.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: Time to read and accept or reject proposals versus their length tion, because the empirical probability of acceptance is very low when it is less than zero and rises rapidly as it increases.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: (a) An undirected graph G representing the similarity matrix; (b) The bipartite graph showing three clusters on G; (c) The induced clusters U ; (d) The new graph G1 over clusters U ; (e) The new bipartite graph over G1 p |vi ) = Vl 1 ...",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 Examples of two- to seven-word bilingual phrases obtained by applying the algorithm phrase-extract to the alignment of Figure 2",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Examples of word bigrams including un known word tags example",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 shows the results of both unconstrained and constrained versions of HGFC and those of AGG on the test set T3 (where singular classes are removed to enable proper evaluation of the constrained method).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "Thetheoretical upper bound of the decoding complex Figure 5: Decoding algorithm using semantic role features.",
        "Entity": "Reference"
    },
    {
        "Text": "the time-consuming renormalization in equation (3) is not needed in search",
        "Entity": "Reference"
    },
    {
        "Text": "The overall architecture of the log-linear modeling approach is summarized in Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 shows the manual evaluation results based on the entire test set, and the improvement from SRF is significant at p < 0.005 based on a t-test.",
        "Entity": "Reference"
    },
    {
        "Text": "For the Verbmobil task, we train the model parameters M according to the maximum class posterior probability criterion (equation (4)).",
        "Entity": "Reference"
    },
    {
        "Text": "We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4, 5, and 6.",
        "Entity": "Reference"
    },
    {
        "Text": "range free green lemon peel red state yellow",
        "Entity": "Reference"
    },
    {
        "Text": "To compute the third factor of Equation (13), we have to estimate the character bigram probabilities that are classified by word type and part of speech.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 9: Entries indicate whether best performance is achieved using the local maximum condition (M), the threshold condition (T), or both.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "The model 1 The total 74,597 sentence pairs used in experiments are those in the FBIS corpus whose English part can be parsed using Charniak (2000)",
        "Entity": "Reference"
    },
    {
        "Text": "the time-consuming renormalization in equation (3) is not needed in search",
        "Entity": "Reference"
    },
    {
        "Text": "This is because the lefthand side of Equation (7) represents the probability of the string c1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 shows the experimental results with and without the stem n-grams features.",
        "Entity": "Reference"
    },
    {
        "Text": "p(z = (FJ , EI , A ) J f ) = 1 1 | N(C( f )) (10",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 presents the wide range of cases that are used to create the morphs.",
        "Entity": "Reference"
    },
    {
        "Text": "In the last two lines of Equation 3, \u03c6\u01eb and each P (f |e) = \"\u00a3s c (f |e; f (s), e(s)) (4) \u03c6i are not free variables, but are determined by f s c(f |e; f (s), e(s))the alignments.",
        "Entity": "Reference"
    },
    {
        "Text": "L set of lemmas IL set of (lemma-wise) instances SL set of (lemma-wise) senses inst : L (IL ) mapping lemma instances sns : L (SL ) mapping lemma senses M set of meta senses meta : SL M mapping senses meta senses A M M set of meta alternations (MAs) A set of MA representations score : A S2 R scoring function for MAs repA : A A MA representation function comp : A S2 R compatibility function Table 1: Notation and signatures for our framework.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 7: Compatible brackets and all-compatible bracket rates when word accuracy is optimized.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: BLEU results",
        "Entity": "Caption"
    },
    {
        "Text": "Pr(f J | eI ) = Pr(f J , aJ | eI ) (5) 1 1 1 1 1 J 1",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 gives the mean values for the comparison of troughs placed by the segmentation algorithm to the segmentation points identified by the test subjects for all the texts",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Distributions of Morph Examples",
        "Entity": "Reference"
    },
    {
        "Text": "Among all possible target sentences, we will choose the sentence with the highest probability",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Feature templates for the full joint model.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 portrays how the states are aligned using the proposed scheme, where a subtree is denoted as a rectangle with its partial index shown inside it.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "On the other hand, using our method of combining both sources of information and setting M = \u221e, 19 Chinese words (i.e., the first 22 Chinese words in Table 3 except \u5df4\u4f50\u4e9a,\u5769\u57da,\u666e\u5229\u6cd5) have their correct English translations at rank one position.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Considering that the way the semantic where all(T ) denotes all the possible target strings which can be generated from the source tree T . Given a set of TTS templates, the new partition function can be efficiently computed using the dynamic programming algorithm shown in Figure 7.",
        "Entity": "Reference"
    },
    {
        "Text": "We illustrate its use with an example (see Then, we average the contributions of each n-gram order: Figure 2).",
        "Entity": "Reference"
    },
    {
        "Text": "(2004) in figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "14http://www.cis.upenn.edu/ dbikel/software.html Gold standard Automatic UAS LAS UAS LAS Baseline 89.87 84.92 89.87 84.92 Anim 89.81 84.94 89.87 84.99 Table 5: Overall results in experiments with automatic features compared to gold standard features, expressed as unlabeled and labeled attachment scores.",
        "Entity": "Reference"
    },
    {
        "Text": "Training time comparison.",
        "Entity": "Caption"
    },
    {
        "Text": "This approach can be seen as a generalization of the originally suggested source channel modeling framework for statistical machine translation",
        "Entity": "Reference"
    },
    {
        "Text": "This group consists of 10 features based on the string , as listed in Table 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Reduction of conjuncts for NP coordination Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "n all four tables, we provide theresults for using no heuristic functions and three variants of an increasingly infor mative heuristic function.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 shows these similarity measures.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4",
        "Entity": "Reference"
    },
    {
        "Text": "We also propose to use the features U01 U03, which we found are effective to adjust the character Figure 1: Illustration of the alignment of steps.",
        "Entity": "Reference"
    },
    {
        "Text": "The direct translation probability is given by",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Output of word sense clustering.",
        "Entity": "Reference"
    },
    {
        "Text": "bothmentions are in a parallel construction in adja Figure 1: An example graph modeling relations between mentions.",
        "Entity": "Reference"
    },
    {
        "Text": "The Stanford parser (Klein and Manning, 2002) is unable to recover the verbal reading of the unvocalized surface form 0 an (Table 1).",
        "Entity": "Reference"
    },
    {
        "Text": "To illustrate how SRF impacts the translation results, Figure 8 gives 3 examples of the MT outputs with and without the SRFs",
        "Entity": "Reference"
    },
    {
        "Text": "The graphical structure depicted in Figure 1 models these relations between the four mentions Leaders, Paris, recent developments and They.",
        "Entity": "Reference"
    },
    {
        "Text": "The bipartite graph K also induces a similarityagain (Figure 1(e)).",
        "Entity": "Reference"
    },
    {
        "Text": "The unknown parameters are determined by maximizing the likelihood on the parallel training corpus:",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 9: Entries indicate whether best performance is achieved using the local maximum condition (M), the threshold condition (T), or both.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Probability estimation tree for the nomi native case of nouns.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Effect of Factors antecedent is found in the previous context, subsequent sentences are inspected (cataphora), also ordered by proximity to the pronoun.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 lists both the success rate maximally achievable (broken down according to different types of pronouns) and the average number of antecedents remaining after applying each factor.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5",
        "Entity": "Reference"
    },
    {
        "Text": "(2011), we confirmed that omission of the look-ahead features results in a 0.26% decrease in the parsing accuracy on CTB5d (dev).Figure 2: F1 scores (in %) of SegTagDep on CTB 5c1 w.r.t. the training epoch (x-axis) and parsing feature weights (in legend).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 shows the effect of the role-based preference on our data.",
        "Entity": "Reference"
    },
    {
        "Text": "3",
        "Entity": "Reference"
    },
    {
        "Text": "The test 1:ART.Nom checks if the preceding word is a nominative article.",
        "Entity": "Caption"
    },
    {
        "Text": "Tree setups P(%) R(%) F SPT 76.3 59.8 67.1 DSPT 77.4 65.4 70.9 UPST (BOF) 80.4 69.7 74.7 UPST (FPT) 80.1 70.7 75.1 UPST (EPT) 79.9 70.2 74.8 Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 presents an overview of the animacy data Clas s Ani mat e Type s Tok 6 4 4 ens cover ed 6 0 1 0 Inan imat e 691 0 3 4 8 2 2 Tota l 755 4 4 0 8 3 2 Table 1: The animacy data set from Talbanken05; number of noun lemmas (Types) and tokens in each class.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1, Figure 1, and Figure 2 shows the AER results for different models.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2 shows the correctness evaluation results.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5 shows that by changing the word spelling model from zerogram to bigram, character perplex ity is greatly reduced.",
        "Entity": "Reference"
    },
    {
        "Text": "As Table 1 shows, word bigrams whose infrequent word bigram",
        "Entity": "Reference"
    },
    {
        "Text": "But it conflates the coordinating and discourse separator functions of wa (<..4.b \ufffd \ufffd) into one analysis: conjunction(Table 3).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Precision statistics for pronouns.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 shows the bilingual phrases containing between two and seven words that result from the application of this algorithm to the alignment of Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "5http://cactus.aistnara.ac.jp/lab/nltlchasen.html 6http://pine.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html Word accuracy 90 CHASEN JUMAN opllnizt oplnuo recall opiJTozt F Figure 4: Word accuracy.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Word length distribution of kanji words and katakana words length model does not reflect the variation of the word length distribution resulting from the Japanese orthography.",
        "Entity": "Reference"
    },
    {
        "Text": "Fi gure 7 depic ts the comp atible brack ets and all comp atible brack ets rates.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 shows the record for the headword orange followed by its collocates",
        "Entity": "Reference"
    },
    {
        "Text": "The regular expressions available in Foma from highest to lower precedence.",
        "Entity": "Caption"
    },
    {
        "Text": "But it conflates the coordinating and discourse separator functions of wa (<..4.b \ufffd \ufffd) into one analysis: conjunction(Table 3).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 10 Translation results on the Hansards task",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 shows the results.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5 shows that this reimplementation almost reproduces the accuracy of their implementation.",
        "Entity": "Reference"
    },
    {
        "Text": "The reordering of the semantic roles from source to target is computed for each TTS template as part of the template extraction process, using the word-level alignments between the LHS/RHS of the TTS template (e.g., Figure 3).",
        "Entity": "Reference"
    },
    {
        "Text": "igure 2 Example of a (symmetrized) word alignment (Verbmobil task).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Performance of the mention detection system including all ACE 04 subtasks",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Character type configuration of infrequent words in the EDR corpus",
        "Entity": "Reference"
    },
    {
        "Text": "The second factor of Equation (13) is estimated from the Poisson distribution whose parameter",
        "Entity": "Reference"
    },
    {
        "Text": "To illustrate the complete user model, in the figure 1 example the benefit of accepting would be7 2 4.2 = .8 keystrokes and the benefit of reject ing would be .2 keystrokes.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6: Final results on CTB6 and CTB7 accuracies of POS tagging and dependency parsing were remarkably improved by 0.6% and 2.4%, respectively corresponding to 8.3% and 10.2% error reduction.",
        "Entity": "Reference"
    },
    {
        "Text": "W can be encoded by an undirected graph G (Figure 2(a)), where the nouns are mapped to vertices and Wij is the edge weight between vertices i and j",
        "Entity": "Reference"
    },
    {
        "Text": "We also investigated the effect of varying M . The results are shown in Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "In Table 1, period 1 is Jul 01 \u2013 Jul 15, period 2 is Jul 16 \u2013 Jul 31, \u2026, period 12 is Dec 16 \u2013 Dec 31.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Sample of experimental items for the meta alternation anmfod.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5",
        "Entity": "Reference"
    },
    {
        "Text": "This leaves us with 60 meta alternations, shown in Table 5.",
        "Entity": "Reference"
    },
    {
        "Text": "As an example, the probability of accepting the prediction in figure 1 is about .25.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 7 shows a structogram of the algorithm",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 Architecture of the statistical translation approach based on Bayes decision rule.",
        "Entity": "Reference"
    },
    {
        "Text": "For the Verbmobil task, we train the model parameters M according to the maximum class posterior probability criterion (equation (4)).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5 shows the decoding algorithm incorporating the SRR features.",
        "Entity": "Reference"
    },
    {
        "Text": "By far the most frequent tagging error was the confusion of nominative and accusative case.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Training Data",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 shows these similarity measures.",
        "Entity": "Reference"
    },
    {
        "Text": "The breakdown of the different types of words found by ST in the test corpus is given in Table 3.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3:",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "For example, looking at Figure 1(b), V on G can be grouped into three clusters u1, u2 and u3.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Precision statistics for pronouns.",
        "Entity": "Reference"
    },
    {
        "Text": "The corresponding translation quality improves from an mWER of 45.9% to an mWER of 31.8%.",
        "Entity": "Reference"
    },
    {
        "Text": "n our experimentations, SVMlight (Joachims, 1998) with the tree kernel function (75.0) (53.7) (62.6) Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "See Table 1 for details.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Above: The complete supersense tagset for nouns; each tag is briefly described by its symbol, NAME, short description, and examples.",
        "Entity": "Reference"
    },
    {
        "Text": "able 2 shows the corpus statistics for this task.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8: Per category performance of the Berkeley parser on sentence lengths \u2264 70 (dev set, gold segmentation). (a) Of the high frequency phrasal categories, ADJP and SBAR are the hardest to parse. We showed in \u00a72 that lexical ambiguity explains the underperformance of these categories. (b) POS tagging accuracy is lowest for maSdar verbal nouns (VBG,VN) and adjectives (e.g., JJ). Richer tag sets have been suggested for modeling morphologically complex distinctions (Diab, 2007), but we find that linguistically rich tag sets do not help parsing. (c) Coordination ambiguity is shown in dependency scores by e.g., \u2217SSS R) and \u2217NP NP NP R). \u2217NP NP PP R) and \u2217NP NP ADJP R) are both iDafa attachment.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Examples of the semantic role features assuming that the semantic roles have been tagged for the source sentences.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 6: Final results on CTB6 and CTB7 accuracies of POS tagging and dependency parsing were remarkably improved by 0.6% and 2.4%, respectively corresponding to 8.3% and 10.2% error reduction.",
        "Entity": "Reference"
    },
    {
        "Text": "The bipartite graph K also induces a similarityagain (Figure 1(e)).",
        "Entity": "Reference"
    },
    {
        "Text": "I+1 hLM(eI , f J , K , zK ) = log n p(ei | ei 2 , e ) (17) 1 1 1 1 i=1 i 1",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "The graph displayed in Figure 1 is the graph constructed for the mentions Leaders, Paris, recent developments and They from the example sentence at the beginning of this Section, where R = {P AnaPron, P Subject, N Number}.",
        "Entity": "Reference"
    },
    {
        "Text": "The list of the features used in our joint model is presented in Table 1, where S01 S05, W01 W21, and T01 05 are taken from Zhang and Clark (2010), and P01 P28 are taken from Huang and Sagae (2010).",
        "Entity": "Reference"
    },
    {
        "Text": "For example, the expressions in Figure 2 are identified as paraphrases by this method; so these three patterns will be placed in the same pattern set.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: Performance of baseline and joint models w.r.t. the average processing time (in sec.)",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Performance of the mention detection system including all ACE 04 subtasks",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 4",
        "Entity": "Reference"
    },
    {
        "Text": "In fact, from the last column of Figure 8 we see that even if our algorithm has access to only five anno tated sequences when Juman has access to ten times as many, we still achieve better precision and better F measure.",
        "Entity": "Reference"
    },
    {
        "Text": "As can be seen in figure 2, wing \"part of a bird\" is closely related to tail, as is wing \"part of a plane\"",
        "Entity": "Reference"
    },
    {
        "Text": "and 3",
        "Entity": "Reference"
    },
    {
        "Text": "Improvements of different tree setups over SPT on the ACE RDC 2004 corpus Finally, Table 4 compares our system with other state-of-the-art kernel-based systems on the 7 relation types of the ACE RDC 2004 corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "The second condition is necessary to allow for single-character words (see Figure 3).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 6 shows example sentences annotated by HGFC.",
        "Entity": "Reference"
    },
    {
        "Text": "In all four tables, we provide theresults for using no heuristic functions and three variants of an increasingly infor mative heuristic function.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3 show the training time for different models.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 underscores the virtues of Sentence Recency: In the most recent sentence with antecedents satisfying the filters, there are on aver ble.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Example of a prediction for English to French translation.",
        "Entity": "Reference"
    },
    {
        "Text": "figure)",
        "Entity": "Reference"
    },
    {
        "Text": "We include a list of per-category results for selected phrasal labels, POS tags, and dependencies in Table 8.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Output of word sense clustering.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 illustrates the effects of different components of the user model by showing results for simulated users who read infinitely fast and accept only predictions having positive benefit (superman); who read normally but accept like superman (rational); and who match the standard user model (real).",
        "Entity": "Reference"
    },
    {
        "Text": "The use of the language model feature in equation (18) helps take long-range dependencies better into account",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Frequency distribution for sentence lengths in the WSJ (sections 2\u201323) and the ATB (p1\u20133).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7: Compatible brackets and all-compatible bracket rates when word accuracy is optimized.",
        "Entity": "Reference"
    },
    {
        "Text": "The argmax operation denotes the search problem, that is, the generation of the output sentence in the target language",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8a shows that the best model recovers SBAR at only 71.0% F1.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 6 Dependencies in the alignment template mode",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 summarizes the results obtained with different taggers and tagsets on the development data.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1.",
        "Entity": "Reference"
    },
    {
        "Text": "A natural unit for B(x, k, a) is the number of keystrokes saved, so all elements of the above equation are converted to this measure.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 7: Compatible brackets and all-compatible bracket rates when word accuracy is optimized.",
        "Entity": "Reference"
    },
    {
        "Text": "The coupling between B and is removed by setting H = B 1: n min (W, H H T ), s.t. hip = 1 (1) H, i=1 BT Dl Bl according to equation 4 l end for return BL , BL 1 ...B1 Additional steps need to be performed in order to extract a tree from the hierarchical graph.",
        "Entity": "Reference"
    },
    {
        "Text": "he largest effect seems to come from taking into account the bigram dependence, which achieves an mWER of 32.9%",
        "Entity": "Reference"
    },
    {
        "Text": "14http://www.cis.upenn.edu/ dbikel/software.html Gold standard Automatic UAS LAS UAS LAS Baseline 89.87 84.92 89.87 84.92 Anim 89.81 84.94 89.87 84.99 Table 5: Overall results in experiments with automatic features compared to gold standard features, expressed as unlabeled and labeled attachment scores.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1.<",
        "Entity": "Reference"
    },
    {
        "Text": "To reduce the memory requirement of the alignment templates, we compute these probabilities only for phrases up to a certain maximal length in the source language.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2 shows the labeled dependency graph of example (2), taken from Talbanken05.",
        "Entity": "Reference"
    },
    {
        "Text": "Prec. is the precision.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "A comparison of the lines for grammatical roles and for surface order in Table 1 shows that the same is true in German.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "The renormalization needed in equation (3) requires a sum over manypossible sentences, for which we do not know of an efficient algorithm",
        "Entity": "Reference"
    },
    {
        "Text": "In order to effectively capture entity-related semantic features, and their combined features as well, especially bi-gram or tri-gram features, we build an Entity-related Semantic Tree (EST) in three ways as illustrated in Figure 2.",
        "Entity": "Reference"
    },
    {
        "Text": "We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4, 5, and 6.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Diacritized particles and pseudo-verbs that, after orthographic normalization, have the equivalent surface form 0 an.",
        "Entity": "Caption"
    },
    {
        "Text": "The test 1:ART.Nom checks if the preceding word is a nominative article.",
        "Entity": "Caption"
    },
    {
        "Text": "By introducing the distinction of word type to the model of Equation(12),we can derive a more sophis ticated unknown word model that reflects both word 3 When a Chinese character is used to represent a seman tically equivalent Japanese verb, its root is written in the Chinese character and its inflectional suffix is written in hi ragana.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: CoreLex s basic types with their corresponding WordNet anchors.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 9 shows that in fact both contribute to producing good segmentations.",
        "Entity": "Reference"
    },
    {
        "Text": "The model 1 The total 74,597 sentence pairs used in experiments are those in the FBIS corpus whose English part can be parsed using Charniak (2000)",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "n all four tables, we provide theresults for using no heuristic functions and three variants of an increasingly infor mative heuristic function.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Distribution of the sentences where the semantic role features give no/positive/negative impact to the sentence fluency in terms of the completeness and ordering of the semantic roles.classes in VerbNet (Dang et al., 1998).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3 shows the effect of constraining the maximum length of the alignment templates in the source language.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 7: Computing the partition function of the conditional probability P r(S|T ).",
        "Entity": "Reference"
    },
    {
        "Text": "We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c).",
        "Entity": "Reference"
    },
    {
        "Text": "The bottom-up decoding algorithm for the TTS transducer is sketched in Figure 2.",
        "Entity": "Reference"
    },
    {
        "Text": "tables 2",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 shows the absolute frequencies of sen tence recency values when only the most recent antecedent (in the order just stated) is considered.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4",
        "Entity": "Reference"
    },
    {
        "Text": "The points labelled smoothed in figure 2 were obtained using a sliding-average smoother, and the model curve was obtained using two-component Gaussian mixtures to fit the smoothed empirical likelihoods p(gain|a = 0) and p(gain|a = 1).",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Decoding algorithm for the standard Tree-to-String transducer.",
        "Entity": "Reference"
    },
    {
        "Text": "Tree setups P(%) R(%) F SPT 76.3 59.8 67.1 DSPT 77.4 65.4 70.9 UPST (BOF) 80.4 69.7 74.7 UPST (FPT) 80.1 70.7 75.1 UPST (EPT) 79.9 70.2 74.8 Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Local graph of the word wing of a graph",
        "Entity": "Reference"
    },
    {
        "Text": "Comparison of segmentation algorithm using different linguistic features.",
        "Entity": "Reference"
    },
    {
        "Text": "#e is the total number of English translation candidates in the period.",
        "Entity": "Reference"
    },
    {
        "Text": "5http://cactus.aistnara.ac.jp/lab/nltlchasen.html 6http://pine.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html Word accuracy 90 CHASEN JUMAN opllnizt oplnuo recall opiJTozt F Figure 4: Word accuracy.",
        "Entity": "Reference"
    },
    {
        "Text": "For the graph depicted in Figure 1 this algorithm computes the clusters {They, Leaders}, {Paris} and {recent developments}.",
        "Entity": "Reference"
    },
    {
        "Text": "A simple lexicalized PCFG with second order Markovization gives relatively poor performance: 75.95% F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline (Table 7).",
        "Entity": "Reference"
    },
    {
        "Text": "The overall architecture of the statistical translation approach is summarized in Figure 1.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: Distribution of supersense mentions by domain (left), and counts for tags occurring over 800 times (below).",
        "Entity": "Reference"
    },
    {
        "Text": "In MUC6, the best result is achieved by SRA (Krupka, 1995).",
        "Entity": "Reference"
    },
    {
        "Text": "igure 2 Example of a (symmetrized) word alignment (Verbmobil task).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 The cost as a novel given name (second position) for hanzi from various radical classes.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2 shows that the tagging accuracy tends to increase with the context size.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "We set Table 6: Word segmentation accuracy of all words r e c pr ec F Po iss on +b igr a m W T +P oi ss on +b igr a m P O S + Po iss on +b igr a m P O S + W T + Po iss on + bi gr a m 94 .5 94 .4 94 .4 94 .6 93 .1 93 .8 93 .6 93 .7 93 .8 94 .1 94 .0 94 .1",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1",
        "Entity": "Reference"
    },
    {
        "Text": "We also investigated the effect of varying M . The results are shown in Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5 gives an example of the word alignment and phrase alignment of a German English sentence pair.We describe our model using a log-linear modeling approach",
        "Entity": "Reference"
    },
    {
        "Text": "As shown in figure 1, a similarity matrix W models one-hop transitions that follow the links from vertices to neighbors.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5 shows the cross entropy per word and char acter perplexity of three unknown word model.",
        "Entity": "Reference"
    },
    {
        "Text": "Improvements of different tree setups over SPT on the ACE RDC 2004 corpus Finally, Table 4 compares our system with other state-of-the-art kernel-based systems on the 7 relation types of the ACE RDC 2004 corpus.",
        "Entity": "Reference"
    },
    {
        "Text": "Results for 2 and for 10 preceding POS tags as context are reported for our tagger.",
        "Entity": "Caption"
    },
    {
        "Text": "eI = argmax {Pr(eI | f J )} (1)",
        "Entity": "Caption"
    },
    {
        "Text": "K hAT(eI , f J , K , zK ) = log n p(zk | f j k ) (13) 1 1 1 1 k=1 j k 1 +1",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2 shows the word length distribution of words consists of only kanji characters and words consists of only katakana characters.",
        "Entity": "Reference"
    },
    {
        "Text": "The translations of 6 of the 43 words are words in the dictionary (denoted as \u201ccomm.\u201d in Table 3) and 4 of the 43 words appear less than 10 times in the English part of the corpus (denoted as \u201cinsuff\u201d).",
        "Entity": "Reference"
    },
    {
        "Text": "The result is shown in Table 4: the baseline numbers without stem features are listed under Base, and the results of the coreference system with stem features are listed under Base+Stem.",
        "Entity": "Reference"
    },
    {
        "Text": "First, the source sentence words f J are grouped into phrases f K . For each phrase f an 1 1 alignment template z is chosen and the sequence of chosen alignment templates is reordered (according to K ).",
        "Entity": "Reference"
    },
    {
        "Text": "As Figure 3 shows, word type information improves the prediction accuracy significantly.",
        "Entity": "Reference"
    },
    {
        "Text": "More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; Paa and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4 shows the performance on the test data.",
        "Entity": "Reference"
    },
    {
        "Text": "As shown in Table 3, using just context information alone, 10 Chinese words (the first 10) have their correct English translations at rank one position.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: Examples of the semantic role features assuming that the semantic roles have been tagged for the source sentences.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1).",
        "Entity": "Reference"
    },
    {
        "Text": "n all four tables, we provide theresults for using no heuristic functions and three variants of an increasingly infor mative heuristic function.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3 gives the algorithm phrase-extract that computes the phrases",
        "Entity": "Reference"
    },
    {
        "Text": "Table 2: Frequency distribution for sentence lengths in the WSJ (sections 2\u201323) and the ATB (p1\u20133).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Performance of the mention detection system using lexical features only.",
        "Entity": "Reference"
    },
    {
        "Text": "We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c).",
        "Entity": "Reference"
    },
    {
        "Text": "For example, looking at Figure 1(b), V on G can be grouped into three clusters u1, u2 and u3.",
        "Entity": "Reference"
    },
    {
        "Text": "eik (12",
        "Entity": "Caption"
    },
    {
        "Text": "The figure schematically shows a small portion of the graph describing the concepts of mechanism (concrete), political system and relationship (abstract) at two levels of generality.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1 Examples of two- to seven-word bilingual phrases obtained by applying the algorithm phrase-extract to the alignment of Figure 2",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "These semantic features Figure 8: Examples of the MT outputs with and without SRFs",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5 Effect of pruning parameter tp and heuristic function on error rate for direct-translation model (Np = 50,000).",
        "Entity": "Caption"
    },
    {
        "Text": "Tree setups P(%) R(%) F CS-SPT over SPT3 1.5 1.1 1.3 DSPT over SPT 1.1 5.6 3.8 UPST (FPT) over SPT 3.8 10.9 8.0 Table 3.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1, Figure 1, and Figure 2 shows the AER results for different models.",
        "Entity": "Reference"
    },
    {
        "Text": "As illus trated in Figure 1(e), the NP coordination in the Qian et al.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1: (a) An undirected graph G representing the similarity matrix; (b) The bipartite graph showing three clusters on G; (c) The induced clusters U ; (d) The new graph G1 over clusters U ; (e) The new bipartite graph over G1 p |vi ) = Vl 1 ...",
        "Entity": "Reference"
    },
    {
        "Text": "This is especially true in the case of quotations\u2014which are common in the ATB\u2014where (1) will follow a verb like (2) (Figure 1).",
        "Entity": "Reference"
    },
    {
        "Text": "Table 5",
        "Entity": "Reference"
    },
    {
        "Text": "The figure schematically shows a small portion of the graph describing the concepts of mechanism (concrete), political system and relationship (abstract) at two levels of generality.",
        "Entity": "Reference"
    },
    {
        "Text": "As an example, the probability of accepting the prediction in figure 1 is about .25.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Effect of Factors antecedent is found in the previous context, subsequent sentences are inspected (cataphora), also ordered by proximity to the pronoun.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2",
        "Entity": "Reference"
    },
    {
        "Text": "Table 8: Part of speech tagging accuracy of unknown words (the last column represents the percentage of correctly tagged unknown words in the correctly segmented unknown words",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1: Effect of Factors antecedent is found in the previous context, subsequent sentences are inspected (cataphora), also ordered by proximity to the pronoun.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4 shows the performance on the test data.",
        "Entity": "Reference"
    },
    {
        "Text": "We also investigated the effect of varying M . The results are shown in Table 2.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: An ATB sample from the human evaluation. The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a). But the city name Sharm Al- Sheikh is also iDafa, hence the possibility for the incorrect annotation in (b).",
        "Entity": "Caption"
    },
    {
        "Text": "K): J k = fj k 1 +1 , ..",
        "Entity": "Caption"
    },
    {
        "Text": "The accuracy of a baseline tagger which chooses the most probable tag9 ignoring the context is 67.3% without and 69.4% with the supple 92.3 92.2 92.1 92 91.9 91.8 91.7 91.6 91.5 91.4 2 3 4 5 6 7 8 9 10 mentary lexicon.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Gross statistics for several different treebanks.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Examples of common character bigrams for each part of speech in the infrequent words pa rt of sp ee ch ch ar ac ter bi gr a m fre qu en cy no un nu m be r a dj e ct iv al v er b v er b ad je cti ve ad ve rb < e o w > <b o w > 1 S \" J < e o w > I t < e o w > L < e o w > < e o w > 13 43 4 8 4 3 2 7 2 1 3 69 63 resented all unknown words by one length model.",
        "Entity": "Reference"
    },
    {
        "Text": "By introducing the distinction of word type to the model of Equation(12),we can derive a more sophis ticated unknown word model that reflects both word 3 When a Chinese character is used to represent a seman tically equivalent Japanese verb, its root is written in the Chinese character and its inflectional suffix is written in hi ragana.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3: Scores for UPUC corpusFrom those tables, we can see that a simple ma jority voting algorithm produces accuracy that is higher than each individual system and reasonably high F-scores overall.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "W can be encoded by a undi rected graph G (Figure 1(a)), where the verbs are mapped to vertices and the Wij is the edge weight between vertices i and j.",
        "Entity": "Reference"
    },
    {
        "Text": "For example, look ing at Figure 2(b), V on G can be grouped into three clusters u1, u2 and u3.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 1 Architecture of the translation approach based on a log-linear modeling approach",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4: Training Data",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 shows that by changing the word spelling model from zerogram to bigram, character perplex ity is greatly reduced.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 6 shows example sentences annotated by HGFC.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 3: An example showing the combination of the semantic role sequences of the states.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 5 shows the decoding algorithm incorporating the SRR features.",
        "Entity": "Reference"
    },
    {
        "Text": "and Table 6 show a comparison of the segmentation and POS tagging accuracies with other state-of-the-art models.",
        "Entity": "Reference"
    },
    {
        "Text": "Our experimental data was drawn from 150 megabytes of 1993 Nikkei newswire (see Figure I).",
        "Entity": "Reference"
    },
    {
        "Text": "We include a list of per-category results for selected phrasal labels, POS tags, and dependencies in Table 8.",
        "Entity": "Reference"
    },
    {
        "Text": "Length Distribution In word segmentation, one of the major problems of the word length model of Equation (6) is the decom position of unknown words.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Decoding algorithm for the standard Tree-to-String transducer.",
        "Entity": "Reference"
    },
    {
        "Text": "where ECD|S,T (fi), the expected count of a feature over all derivations given a pair of tree and string, can be computed using the modified inside- outside algorithm described in Section 3.2, and ECS |T (fi), the expected count of a feature over all possible target strings given the source tree, can be computed in a similar way to the partition function described in Figure 7.",
        "Entity": "Reference"
    },
    {
        "Text": "Figure 2: Above: The complete supersense tagset for nouns; each tag is briefly described by its symbol, NAME, short description, and examples.",
        "Entity": "Reference"
    },
    {
        "Text": "By far the most frequent tagging error was the confusion of nominative and accusative case.",
        "Entity": "Reference"
    },
    {
        "Text": "The argmax operation denotes the search problem, that is, the generation of the output sentence in the target language",
        "Entity": "Reference"
    },
    {
        "Text": "and 8 show word accuracy for Chasen, Juman, and our algorithm for parameter settings optimizing word precision, recall, and F-measure rates.",
        "Entity": "Reference"
    },
    {
        "Text": "If M = 10, 15 Chinese words (i.e., the first 19 Chinese words in Table 3 except \u53f6\u739b\u65af,\u5df4\u4f50\u4e9a,\u5769\u57da,\u666e\u5229\u6cd5) have their correct English translations at rank one position.",
        "Entity": "Reference"
    },
    {
        "Text": "Shortest-enclosed Path Tree from P/R/F (81.1/6.7/73.2) of Dynamic Context-Sensitive Shortest- enclosed Path Tree according to Table 2 (Zhou et al., 2007)",
        "Entity": "Reference"
    },
    {
        "Text": "Collocation: Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio (Church and Hanks, 1990) and outputted to a lexicon.",
        "Entity": "Reference"
    },
    {
        "Text": "MENE has only been tested on MUC7.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 4",
        "Entity": "Reference"
    },
    {
        "Text": "I+1 hCLM(eI , f J , K , zK ) = log n p(C(ei ) | C(ei 4 ), ..",
        "Entity": "Caption"
    },
    {
        "Text": "See Figure 3 for examples.",
        "Entity": "Reference"
    },
    {
        "Text": "Table 1",
        "Entity": "Reference"
    },
    {
        "Text": "The second model is the combination of Poisson distribution (Equation (6)) and character bigram",
        "Entity": "Reference"
    }
]