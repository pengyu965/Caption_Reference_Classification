[
    {
        "Text": "Table 3. Committee-Based Unsupervised Learning",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Two fragments of a hierarchy over word class distributions ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Test set results. Maamouri et al. (2009b) evaluated the Bikel parser using the same ATB split, but only reported dev set results with gold POS tags for sentences of length \u2264 40. The Bikel GoldPOS configuration only supplies the gold POS tags; it does not force the parser to use them. We are unaware of prior results for the Stanford parser.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Accuracy in Lexical Sample Tasks",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7. Features with Set Values",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Average Precision, Recall and F1 at dif- ferent top K rule cutoff points. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Evaluation of topic segmentation for the French corpus (Pk and WD as percentages) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Result of synthetic data learning experiment for MERT and PRO, with and without added noise. As the dimensionality increases MERT is unable to learn the original weights but PRO still performs adequately. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 14: Intermediate Result.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 Top 10 features of country by the Bootstrapped feature weighting. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Example of the Hybrid Method",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Example training run of a pruned 1st -order model on German showing the fraction of pruned gold se- quences (= sentences) during training for training (train) and development sets (dev). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 16 FSRA* for Arabic nominative definite nouns. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Best results: For English, name lists are used. For German, part-of-speech tags are used ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Derivation by Means of Adding a Suffix",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5 Word sense disambiguation accuracy for \u201cNP1 V NP2 for NP3\u201d frame. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Disambiguation scores on nine confusable set, attained by the all-words prediction classifier trained on 30 million examples of TRAIN - REUTERS, and by confusable experts on the same training set. The second column displays the number of exam- ples of each confusable set in the 30-million word training set; the list is ordered on this column. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: An example of MOD feature extraction. An oval in the dependency tree denotes a bunsetsu. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Properties of the manually aligned corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Table 13 Macro-average recall, precision, and F1 on the development set and test set using the parameters that maximize F1 of the learned edges over the development set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: An example CCG parse obtained from [60]",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: F1 scores and speed (in sentences per sec.) of SegTagDep on CTB-5c-1 w.r.t. the beam size. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Example of a word alignment and of ex- tracted alignment templates. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Accuracy of all slots on the TST3 and TST4 test set ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Using over 10,000 word-context features leads to overfitting, but its detrimental effects are modest. Scores on the tuning set were obtained from the 1-best output of the online learning algorithm, whereas scores on the test set were obtained using averaged weights. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The first sense heuristic compared with the SENSEVAL -2 English all-words task results ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7 Word sense disambiguation accuracy for \u201cNP1 V NP2 NP3\u201d frame. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A graphical representation of the HMM ap- proach for speaker role labeling. This is a simple first order HMM. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 Unknown word model features for Arabic and French. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Source, transformed and extracted trees given headline British soldier killed in Afghanistan",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Four synchronous rules with topic distributions. Each sub-graph shows a rule with its topic distribution, where the X-axis means topic index and the Y-axis means the topic probability. Notably, the rule (b) and rule (c) shares the same source Chinese string, but they have different topic distributions due to the different English translations. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5. Sample poster scores.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Effect of language and error models to quality (recall, proportion of suggestion sets containing a cor- rectly suggested word) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results on MUC-4 entity extraction. C&J 2011 +granularity refers to their experiment in which they mapped one of their templates to five learned clusters rather than one. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4. Examples of the top-3 candidates in the       transliteration of English \u2013 Chinese ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Performance using FBIS training corpus (top) and NIST corpus (bottom). Improvements are significant at the p <0.05 level, except where indicated (ns ). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Development Sets Results.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: F-score on development data",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Most frequent phrase dependencies in DE\u2192EN data, shown with their counts and attachment directions. Child phrases point to their parents. To focus on interesting phrase dependencies, we only show those in which one phrase has at least two tokens and neither phrase is entirely punctuation. The words forming the longest lexical dependency in each extracted phrase dependency are shown in bold; these are used for back-off features. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Comparison of f-scores when changing the size of labeled data. (1/10, 1/4, 1/2 and all labeled data. The size of unlabeled data is \ufb01xed as 5 million characters.) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Weights learned for generating syntactic nodes of various types anywhere in the English translation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Descriptive statistics for Web scores and BNC scores for other-anaphora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Parsing performance with each POS tag set, on gold and predicted input. L AS = labeled attachment accuracy (dependency + relation). U AS = unlabeled attachment accuracy (dependency only). L S = relation label prediction accuracy. L AS diff = difference between labeled attachment accuracy on gold and predicted input. POS acc = POS tag prediction accuracy. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 \u2013 Pk for C99 corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Categories of spurious relation mentions in fp1 (on a sample of 10% of relation mentions), ranked by the percentage of the examples in each category. In the sample text, red text (also marked with dotted underlines) shows head words of the first arguments and the underlined text shows head words of the second arguments. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: A partial frame learned by P RO F INDER from the MUC-4 data set, with the most probable emissions for each event and slot. Labels are assigned by the authors for readability. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A translation forest which is the running example throughout this paper. The reference translation is \u201cthe gunman was killed by the police\u201d. (1) Solid hyperedges denote a \u201creference\u201d derivation tree t1 which exactly yields the reference translation. (2) Replacing e3 in t1 with e4 results a competing non-reference derivation t2 , which fails to swap the order of X3,4 . (3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally, this is done by deleting a node X0,1 . ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Search errors [%].",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 Initial NE recognition type-insensitive (type-sensitive) performance across various domains. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. MAP of different IR systems with differ- ent segmenters. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6 Distribution of probabilities given by the classi\ufb01er over all node pairs of the test-set graphs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 Performance of our method for paraphrase acquisition.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results of the second run (with postprocessing)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: A protein domain-referring phrase example",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results of our alignment quality experiments. All timing and accuracy figures use means from five independently initial- ized runs. Note that lower is better for AER, higher is better for F0.5 . All experiments are run on a system with two Intel Xeon E5645 CPUs running at 2.4 GHz, in total 12 physical (24 virtual) cores. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: R-iv and R-oov varing as the confidence threshold, t.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Chinese character usage in 3 corpora. The   numbers in brackets indicate the percentage of  characters that are shared by at least 2 corpora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Pr(eI | f J ) = p M (eI | f J )",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Official BakeOff2005 results. Keys: F - Regular Tagging only, all training data are used P1 - Regular Tagging only, 90% of training data are used P2 - Regular Tagging only, 70% of training data are used S - Regular and Correctional Tagging, Separated Mode I - Regular and Correctional Tagging, Integrated Mode ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 Numbers of parse-tree nodes in the 1-best parses of the development set that triggered gender or number agreement checks, and the results of these checks. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: An example graph modeling relations between mentions. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Most frequent sense analysis for all polysemous lemmas in the Senseval-2 and -3 test data, broken down by their frequencies of occurrence in SemCor (adverb data is only from Senseval-2). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4. Gold standard length distribution.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Space comparison between FSAs and FSRAs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5 Example of segmentation of German sentence and its English translation into alignment templates. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Post-hoc analysis on the models built by the DAC system: some of the top features with corresponding feature weights in parentheses, for each individual tagger. (POS tags are capitalized; BOS stands for Beginning Of Sentence) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Accuracy Trends on MicroWnOp Corpus.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 10: MUC-5: Level Distribution of the Five Facts Combined",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Comparison of difference systems on the      performs the state-of-the-art Collins and Duffy\u2019s con- ACE RDC 2003 corpus over both 5 types (outside the    volution tree kernel. It also shows that feature-based parentheses) and 24 subtypes (inside the parentheses) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10: Performance of Altavista counts and BNC counts for compound interpretation (data from Lauer 1995) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Performance of WSD system over individual ab- breviations in three reduced corpora ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2. Incremental alignment with TERp resulting in a confusion network.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Individual Performance of KSs for Terrorism",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A typed narrative chain. The four top arguments are given. The ordering O is not shown. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 10: Example of parser error. Tree (a) is correct, and (b) is the wrong result by our parser. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 12 Common values (in percentages) for parse tree path in PropBank data, using gold-standard parses. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Dependency between amount of training data for syntactic parser and quality of morphological prediction.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Frequency distribution for sentence lengths in the WSJ (sections 2\u201323) and the ATB (p1\u20133).",
        "Entity": "Caption"
    },
    {
        "Text": "The distinctions in the ATB are linguistically justified, but complicate parsing.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Effectiveness of Latent Topic Extraction from Multi-Language Corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10: English to German Final System Re- sults. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Examples of similar syntactic structures across different relation types. The head words of the first and the second arguments are shown in italic and bold, respectively. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Models with lexical morpho-semantic features. Top: Adding all lexical features together on top of the CORE 12 baseline. Center: Adding each feature separately. Bottom: Greedily adding best features from previous part, on predicted input. Statistical significance tested only on predicted (non-gold) input, against the CORE 12 baseline. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Precision, Recall, and F1-score of Engkoo, Google, and Ours with head and tail datasets",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Phrase pairs extracted from a document pair               with an economic topic ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4 Examples of alignment templates obtained in training",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Results using 5-fold cross validation on S ENSEVAL- 1 training data (English) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Caseframe Network Examples",
        "Entity": "Caption"
    },
    {
        "Text": "Table 13: An annotation example for the necessity of species information ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Mutual information between feature subset and class label with \u03c72 based feature ranking. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: EM input for our example sentence. j-values follow each lexical candidate. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: POS Tagging of known words using con- textual features (accuracy in percent). one-vs-all denotes training where example ` serves as positive example to the true tag and as negative example to all the other tags. SM| \u00a8R\u00a9 denotes training where                            2  example ` serves as positive example to the true tag ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: English-French translation results in terms of BLEU score and TER estimated on newstest2010 with the NIST script. All means that the translation model is trained on news-commentary, Europarl, and the whole GigaWord. The rows upper quartile and median corre- spond to the use of a filtered version of the GigaWord. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. The Lattice of the 8 Patterns.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Hypegraph size measured by the average number of hyperedges (h = 1 for CF). \u201clattice\u201d is the average number of edges in the original CN. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 7. Example of fuzzy divisive clustering.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Additional RE features.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: ROUGE-W in empirical approach",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Number of translations generated by each method in the final translation output of system COMB: decoder (Orig.), re-decoding (RD), n-gram expansion (NE) and confusion network (CN). \u201cTot.\u201d is the size of the dev/test set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Results for the systems and original headline: \u2020 and \u2021 stand for significantly better than Unsupervised and Our system at 95% confidence, respectively ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: MAP(%), under the \u201850 rules, All\u2019 setup, when adding component match scores to Precision (P) or prior- only MAP baselines, and when ranking with allCP or allCP+pr methods but ignoring that component scores. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Statistics about the training/tuning/test datasets used in our experiments. The token counts are calculated before MADA segmentation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Texts used for the evaluation",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Semantic drift in CELL (n=20, m=20)",
        "Entity": "Caption"
    },
    {
        "Text": "                   \u00fd Figure 7 (a) A Chinese OAS      . (b) Two sentences in the training set, which contain t whose OASs have been replaced with the single tokens <OAS>. (Li et al. 2003). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: N/P classifier with and without SWSD",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Rule types in SSTb and HeiST",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6 Effect of pruning parameter Np and heuristic function on search efficiency for direct-translation model (tp = 10 12 )",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Comparing F-measure, precision, and recall of different voting schemes for Chinese relation extraction. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7 String-to-tree configurations; each is associated with a feature that counts its occurrences in a derivation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7 The accuracy/speed tradeoff graph for the joint segmentor and POS-taggers and the two-stage baseline. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: a) An example of a document from Tu\u0308Ba-D/Z, b) an abbreviated entity grid representation of it, and c) the feature vector representation of the abbreviated entity grid for transitions of length two. Mentions of the entity Frauen are underlined. nom: nominative, acc: accusative, oth: dative, oblique, and other arguments ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Statistics of the Verbmobil test corpus for German-to-English translation. Unknowns are word forms not contained in the training corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: An excerpt from SemLink",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Comparison of accuracy scores across linguistic levels.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: The middle node gets the grey or the black class. Small numbers denote edge weights. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: The highest ranked phrasal verb candidates from our full system that do not appear in either Wiktionary set. Candidates are presented in decreasing rank; \u201cpat on\u201d is the second highest ranked candidate. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Illustrative tableau for a simple constraint sys- tem not capturable as a regular relation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Evaluation of the manual annotation improvement - summarization ratio: 30%.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Precision at rank for the different sys- tems on the Athletes class. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Translated fragments, according to the lexicon.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Feature sets used for learning relationships. The size of a set is the number of features in that set.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Illustration on temporality",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Sources of con\ufb02ict in cross-lingual subjectivity transfer. Definitions and synonyms of the fourth sense of the noun argument, the fourth sense of verb decide, and the first sense of adjective free as provided by the English and Romanian WordNets; for Romanian we also provide the manual translation into English. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Tagging accuracies on test data.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Comparison with other approaches",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Devoicing transducer compiled through a rule.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The Ensemble Semantics framework for information extraction.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Relation weights (Method 2)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Final results on CTB-6 and CTB-7",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: oscillating states in matrix CW for an unweighted graph ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Final PARSEVAL F1 scores for constituents on the test set for the predicted setting. ST Baseline denotes the best baseline (out of 2) provided by the Shared Task organizers. Our submission is underlined. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 Best F1-measure values for all possible combination. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Our boosted ranker combining monolingual and bilingual features (bottom) compared to three base- lines (top) gives comparable performance to the human- curated upper bound. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: False negative matches on the Riedel (Riedel et al., 2010) and KBP dataset (Surdeanu et al., 2012). All numbers are on bag (pairs of entities) level. BD* are the numbers before downsampling the negative set to 10% and 5% in Riedel and KBP dataset, respectively. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 6. German English BLEU scores of various al EM(Co), GS(Co), EM(Co)+GS(Co), and VB(Co). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 Comparing selectional preference slot definitions. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Network after incremental TER alignment.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: The parts of taxonomic names",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Example outputs of matching implementation of Finnish OT. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Character tagging with deterministic constraints.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: F-measure after successive addition of each global feature group ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Conditioning features for the probabilistic CFG used in the reported empirical trials ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: WCN x Baseline",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Result of our topic similarity model in terms of BLEU and speed (words per second), comparing with the traditional hierarchical system (\u201cBaseline\u201d) and the topic-specific lexicon translation method (\u201cTopicLex\u201d). \u201cSimSrc\u201d and \u201cSimTgt\u201d denote similarity by source-side and target-side rule-distribution respectively, while \u201cSim+Sen\u201d acti- vates the two similarity and two sensitivity features. \u201cAvg\u201d is the average B LEU score on the two test sets. Scores marked in bold mean significantly (Koehn, 2004) better than Baseline (p < 0.01). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Most frequent BLC\u201320 semantic classes on WordNet 3.0",
        "Entity": "Caption"
    },
    {
        "Text": "I+1 hLM(eI , f J , K , zK ) = log n p(ei | ei 2 , e ) (17) 1 1 1 1 i=1 i 1",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Human Assessment of Errors",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8a shows that the best model recovers SBAR at only 71.0% F1.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Parameters used in our system.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Learning curves on the development dataset of the HK City Univ. corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Comparison of the performance of the bootstrapped SVM method from (Zhang, 2004) and LP method with 100 seed labeled examples for relation type classification task. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Graphical model of HM-BiTAM",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: The average number of source text sen- tences needed to cover a summary sentence. The model average is statistically significantly differ- ent from all the other conditions p < 10\u22127 (Study 1). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: A partially scaled and inverted identity matrix J\u00b5 . Such a matrix can be used to trans- form a vector storing a domain and value repre- sentation into one containing the same domain but a partially inverted value, such as W and \u00acW de- scribed in Figure 1. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Definition of the operations used to transform the structure of the underspecified logical form l0 to match the ontology O. The function type(c) calculates a constant c\u2019s type. The function freev(lf ) returns the set of variables that are free in lf (not bound by a lambda term or quantifier). The function subexps(lf ) generates the set of all subexpressions of the lambda calculus expression lf . ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Filters applied to candidate pair (H, S)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results of semantic classification.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Performance comparison of two SLU systems through weakly supervised and super- vised training on the three test sets (TER: Topic Error Rate; SER: Slot Error Rate) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Contingency table for the children of  canine  in the subject position of run. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: RM gain over other optimizers averaged over all test sets. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Effectiveness of score propagation.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11 French MWE identification per category and overall results (test set, sentences \u2264 40 words). MWI and MWCL do not occur in the test set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Parsing results of different models using manual (gold) segmentation. Performances significantly superior to HILDA (with p<7.1e-05) are denoted by *. Significant differences between TSP 1-1 and TSP SW (with p<0.01) are denoted by \u2020. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: How the IBM models model the translation process. This is a hypothetical example and not taken from any actual training or decoding logs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Training phase: effect of question bias (d) on Ave. MRR and TRDR. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9: Results of the combined model for classify- ing unknown words into major and medium catego- ries: best guess ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Experimental results (F-measure).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Baseline and oracle-best 4-gram BLEU scores with 4 references for NIST Chinese-English MT datasets. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: A sample CCG parse.",
        "Entity": "Caption"
    },
    {
        "Text": "K): J k = fj k 1 +1 , ..",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 4. Examples of rules used during decoding.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: The regular expressions available in Foma from highest to lower precedence. Horizontal lines separate precedence classes. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Confidence scores for diese in ex. (1)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8 An example Chinese dependency tree. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: BLEU scores for the French-to-English translation task measured on nt10 with systems tuned on development sets selected according to their original language (adapted tuning). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: POS tagging accuracy on texts without punctuation and capitalization, for tagging on the original data, the gold-standard normalization, and automatic normalizations using the first n tokens as training data ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Modified query.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 The top 10 ranked features for country produced by MI, the weighting function employed in the LIN method. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Probability of # of boundaries f10 (m\u2032 ; 3).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: A WSD example that shows the influence of syntactic, collocational and long-distance context features, the                       \u00ae probability estimates used by Na\u00efve Bayes and MM and their associated weights ( ), and the posterior probabilities of the true sense as computed by the two models. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Results of task#17",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Effect of discriminatively learned penalties for OOV words. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Plate diagram representation of the model. ti - s, wi -s and si -s denote the tags, words and segmentations respectively. G-s are various DP-s in the model, Ej -s and \u03b2j -s are the tag-specific emission distributions and their respective Dirichlet prior parameters. H is Gamma base distribution. S is the base distribution over segments. Coupled DP concetrations parameters have been omitted for clarity. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Values obtained for Precision, Recall and F- score with method 1 by changing the threshold frequency of the correspondences and applying a post-filter. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Average sentence cover size: the average number of sentences needed to generate the case- frames in a summary sentence (Study 1). Model summaries are shown in darker bars. Peer system numbers that we focus on are in bold. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9: Example patterns of nominal interaction keywords ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The Stanford parser (Klein and Manning, 2002) is unable to recover the verbal reading of the unvocalized surface form    an (Table 1). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: the accuracies over the first SIGHAN bake- off data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Opinion HITS model",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 5. Two real translation examples,",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Two STs composing a STN",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Distribution of reasons for false negatives (missed argument mentions) by BInc at K=20. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Sample Minipar parse and extracted gram- matical function features ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: The 10 most important features and their respective category and values for the English word \u201cwhich\u201d.          f ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Parameters of WSMs (Section 2) which, combined with particular Measures, achieved the highest average correlation in TrValD. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Corpus characteristics for perplexity quality experiments. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 French standard parsing experiments (test set, sentences \u2264 40 words). FactLex uses basic POS tags predicted by the parser and morphological analyses from Morfette. FactLex* uses gold morphological analyses. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Clustering an 11-nodes graph with CW in two iterations ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: The parse tree for This car is not blue, highlighting the limited scope of the negation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Distribution of the lexical items",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Number of extracted paraphrases.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 Corpus statistics for Hansards task (Words*: words without punctuation marks). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The Lattice for the Hebrew Phrase bclm hneim",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example queries for abbreviation \u201cBSA\u201d",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Feature set for the baseline pronoun res- olution system ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: System Pairwise Agreement",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Size of the vocabularies for the \u201cNo LP\u201d and \u201cWith LP\u201d models for which we can impose constraints. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 The accuracies of various word segmentors over the first SIGHAN bakeoff data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: The effect of syntactic features when predicting morphology using lexicons. * mark statistically signifi- cantly better models compared to our baseline (sentence- based t-test with \u03b1 = 0.05). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Average precisions over the 10 corpora of different window size (3 seeds)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Possibility combination of neighboring        tokens within the corpus for PER ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Mechanical evaluation of translation",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Sample of extracted entailment rules.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6 Order in which the German source positions are covered for the German-to-English reordering example given in Figure 5. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 11 Participle-forming combinations in German. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Illustration of the alignment of steps.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 DP-based algorithm for solving traveling-salesman problems due to Held and Karp. The outermost loop is over the cardinality of subsets of already visited cities. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7 A scenario where ILP-Global makes a mistake, but ILP-Local is correct. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Summary table on the various methods investigated for POS tagging ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Accuracy on seen and unseen tokens.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 12: The Malay fst After the Application of Compile-Replace to the Lower-Side Language",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Classifier combination accuracy over 5 base classifiers: NB, BR, TBL, DL, MMVC. Best perform- ing methods are shown in bold. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 23: A snippet from Russian and Czech tag comparison",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Frequency of Major Relation SubTypes in the ACE training and devtest corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Results for the various experiments (Exp) for both the development and test portions of the data, including per- token clitic separation (tokenization) accuracy, part-of-speech tagging F1, affix boundary detection F1, affix labeling F1, and both unlabeled and labeled attachment scores. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Unary rule normalization. Nonterminal-yield unary chains are collapsed to single unary rules. Identity unary rules are added to spans that have no unary rule. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10 Accuracy of semantic-role prediction (in percentages) for known boundaries (the system is given the constituents to classify). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Comparison of various groups of parsers. All percentages refer to the share of the total words in test data, attached correctly. The \u201csingle parser\u201d part shows shares of the data where a single parser is the only one to know how to parse them. The sizes of the shares should correlate with the uniqueness of the individual parsers\u2019 strategies and with their contributions to the overall success. The \u201cat least\u201d rows give clues about what can be got by majority voting (if the number represents over 50 % of parsers compared) or by hypothetical oracle selection (if the number represents 50 % of the parsers or less, an oracle would generally be needed to point to the parsers that know the correct attachment). ",
        "Entity": "Caption"
    },
    {
        "Text": "eI K 1 = e1 , ek = eik 1 +1 , .",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: The whole process of re-training the upper case NER. Q signifies that the text is converted to upper case before processing. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8: Examples of the MT outputs with and without SRFs. The first and second example shows that SRFs improve the completeness and the ordering of the MT outputs respectively, the third example shows that SRFs improve both properties. The subscripts of each Chinese phrase show their aligned words in English. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Examples of features and associated costs. Pseudofeatures are shown in boldface. Exceptional denotes a situation such as the semivowel [j] substituting for the affricate [dZ]. Substitutions between these two sounds actually occur frequently in second-language error data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Average feature values across best translations of sentences in the MT03 tuning set, both before MERT (column 2) and after (column 3). \u201cSame\u201d versions of tree- to-tree configuration features are shown; the rarer \u201cswap\u201d features showed a similar trend. ",
        "Entity": "Caption"
    },
    {
        "Text": "                                                           Table 4: Impact of feature categories. Numbers after are the standard deviations. * indicates that the result                #\"    is significantly (pair-wise t-test) different from the line above at             . ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 12 SemCor results for Nouns using jcn. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: \u201cAcceptance radius\u201d of an outlier within the training set (left) and a more \u201cnormal\u201d training set object (right) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 14: An example result of BioAR",
        "Entity": "Caption"
    },
    {
        "Text": "I+1 hCLM(eI , f J , K , zK ) = log n p(C(ei ) | C(ei 4 ), ..",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2. Position of news story boundaries in a CNN news summary in relation to troughs found by the algorithm. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Deterministic constraints for POS tagging.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 13: Domain Numbers of MUC-4, MUC-5, MUC-6, and MUC-7",
        "Entity": "Caption"
    },
    {
        "Text": "Table 16 Methods of resolving OAs in word segmentation, on the MSR test set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Smoothed histograms of the probability of the",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: F1 scores of the local CRF and non-local models on the CoNLL 2003 named entity recognition dataset. We also provide the results from Bunescu and Mooney (2004) for comparison. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example of semantic trees",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Reachability of 1000 training sentences: can they be translated with the model? ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Experiments with the ILP method using a thresh- old of 1\u20134 (times a word-pair is seen) to trigger rule learn- ing. The figures in parentheses are the same results with the added postprocessing unigram filter that, given sev- eral output candidates of the standard dialect, chooses the most frequent one. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: WMT10 system combination tuning/testing data ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: LL results tested against gs-swaco",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Glue Semantics proof for (80), Swedish Directed Motion Construction",
        "Entity": "Caption"
    },
    {
        "Text": "Pr(f J | eI ) = Pr(f J , aJ | eI ) (5) 1 1 1 1 1 J 1",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Resolution accuracy (%)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Bagging with 50 gold seed sets",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Overall accuracy of maximum entropy sys- tem using different subsets of features for Penn Chi- nese Treebank words (manually segmented, part-of- speech-tagged, parsed). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Results on the NIST MATR 2008 test set for several variations of paraphrase usage.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Topic-specific translation lexicons are learned by a 3-topic BiTAM-1. The third lexicon (Topic-3) prefers to translate the word Korean into ChaoXian (\u008am:North Korean). The co-occurrence (Cooc), IBM-1&4 and HMM only prefer to translate into HanGuo (\u00b8I:South Korean). The two candidate translations may both fade out in the learned translation lexicons. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Effects of one-to-one and one-to-many topic pro- jection. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Distribution of dialogue acts in our dataset.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Show type detection: Using the neural net- work described in Sec. 2 the show type was detected. If there is a number in the word column the word feature is being used. The number indicates how many word/part of speech pairs are in the vocabu- lary additionally to the parts of speech. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results on Chinese Semantic Similarity",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Biography Text Evaluations.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 Training, development, and test data for word segmentation on CTB5. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Dependency parsing: LAS scores for full and 5k training sets and for gold and predicted input. Results in bold show the best results per language and setting. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10: Comparison of our approach with the state-of-art systems",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8. Subject and Object Agreement Features",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: English MWEs and their components with their translation in Persian. Direct matches between the trans- lation of a MWE and its components are shown in bold; partial matches are underlined. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: TreeTagger and RFTagger outputs. Starred word forms are modified during preprocessing.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Tagging accuracy on development data depending on context size",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7. Statistics of the ACE corpus.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: string insertion operation for right-to-left decoding method. A string e0 was prepended before the partial output string, e, and the first word in e 0 was aligned from f j . ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. The scored results of our CWS in the MSR_C track (OOV is 0.034) for 3rd bakeoff. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Results on the standard 14 CSSC data sets",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 Corpus statistics for Hansards task (Words*: words without punctuation marks",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Some training events for the English word \u201cwhich\u201d. The symbol \u201c \u201d is the placeholder of the English word \u201cwhich\u201d in the English context. In the German part the placeholder (\u201c \u201d) corresponds to the word aligned to \u201cwhich\u201d, in the first example the German word \u201cdie\u201d, the word \u201cdas\u201d in the second and the word \u201cwas\u201d in the third. The considered English and German contexts are separated by the double bar \u201c  p  \u201d.The last number in the rightmost position is the number of occurrences of the event in the whole corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. A corpus of two trees",
        "Entity": "Caption"
    },
    {
        "Text": "p(z = (FJ , EI , A ) J f ) = 1 1 | N(C( f )) (10",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: English Eve corpus results. Standard deviations are in parentheses; \u2217 denotes a significant difference from the M ORTAG model. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Basic features used in the maximum entropy model.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Features based on the token string",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: The number of vocabularies in the 10k, 50k and 100k data sets. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Experimental results using different phrase ta- bles. OutBp: the out-of-domain phrase table. AdapBp: the adapted phrase table. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Lookup of \u201cis one of\u201d in a reverse trie. Children of each node are sorted by vocabulary identifier so order is consistent but not alphabetical: \u201cis\u201d always appears be- fore \u201care\u201d. Nodes are stored in column-major order. For example, nodes corresponding to these n-grams appear in this order: \u201care one\u201d, \u201c<s> Australia\u201d, \u201cis one of\u201d, \u201care one of\u201d, \u201c<s> Australia is\u201d, and \u201cAustralia is one\u201d. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Pipeline architecture for dialogue act recognition and re-ranking component. Here, the input is a list of dialogue acts with confidence scores, and the output is the same list of dialogue acts but with recomputed confidence scores. A dialogue act is represented as DialogueActType(attribute-value pairs). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: good#a#15 gloss and examples.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Accuracy with different sizes of labeled data",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Parallel Corpus.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Results for the three ACE data sets obtained via the B-CUBED scoring program.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: WS: word-segmentation.           Baseline: language-independent features. LexFeat: plus lex- ical features. Numbers are averaged over the 10 ex- periments in Figure 2. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Learning curve of BLC20 on SE2",
        "Entity": "Caption"
    },
    {
        "Text": "Table 14 Graph-based feature templates for the dependency parser. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Size of translation unit n-grams (%) seen in test for different n-gram models. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Performance on Internet data",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Our alignment model, represented as a graphi- cal model. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 4. Arabic  English BLEU and TER scores of various a methods: EM(Co), GS(Co), EM(Co)+GS(Co), and VB(Co). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Single systems (Basque) in cross- validation, sorted by recall. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Test corpora statistics.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Comparison of a confusion network and a lat- tice. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: PTB vs. Wiktionary type coverage across sec- tions of the Brown corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 9. Performance evaluation of proposed and existing systems.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Accuracy on development data depend ing on context size",
        "Entity": "Caption"
    },
    {
        "Text": "Table 14: Arabic Equational Sentences",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Results for OOV-processing and MBR, English\u2192German. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Particles and prepositions allowed in phrasal verbs gathered from Wiktionary. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Sample dataset (constructed by hand): Finnish verbs, with inflection for person and number.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Average precision (AP) and coverage (Cov) results for our proposed system ES-all and the baselines. \u2021 indicates AP statistical signifi- cance at the 0.95 level wrt all baselines. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 2. Schematic \ufb02owchart of the work\ufb02ow we followed, regarding the datasets, the training techniques and the operations.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: The average improvements of BLEU scores on the test08 and news08 (out-of-domain) when we trained the paramenters using only 400 development sentences with MERT and SVM-based algorithms four times. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 A focused entailment graph. For clarity, edges that can be inferred by transitivity are omitted. The single strongly connected component is surrounded by a dashed line. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Contribution of employing the dynamic         cache on different test documents ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2. F1-measure with \uf062 in [0,1]",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Diffs in the course of iteration. All models were with back-off mixing (+BM). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Metaphors tagged by the system (in bold)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: BLEU scores on the Europarl development test data ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Precision for each phrase type (Ev.Ling).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: F1s of some individual FN role classifiers and the overall multiclassifier accuracy (454 roles).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Average scores by cluster: baseline versus LR[0.20,0.95]. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: English word perplexity (PPL) on the RT04 test set using a unigram LM. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Number and ratio of statistically signifi- cant distinction between system performance. Au- tomatic scores are computed on a larger tested than manual scores (3064 sentences vs. 300\u2013400 sen- tences). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Bootstrapping for Name Tagging",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Comparison of difference systems on the ACE RDC 2004 corpus over both 7 types (outside the parentheses) and 23 subtypes (inside the parentheses) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Segmentation, tagging and parsing results on the Standard dev/train Split, for all Sentences",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Structure of a term in the original documents",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Decoder performance on the June 2002 TIDES MT evluation test set with multiple searches from randomized starting points (MSD=2, MSSS=5). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Matches between MUC and Soderland data at field level",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Combined systems (Basque) in cross- validation, best recall in bold. Only vector(f) was used for combination. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: The results for three systems associ- ated with the project for the NP bracketing task, the shared task at CoNLL-99. The baseline re- sults have been obtained by finding NP chunks in the text with an algorithm which selects the most frequent chunk tag associated with each part-of- speech tag. The best results at CoNLL-99 was obtained with a bottom-up memory-based learner. An improved version of that system (MBL) deliv- ered the best project result. The MDL results have been obtained on a different data set and therefore combination of the three systems was not feasible. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 14: Correlation between manual and automatic scores for English-French",
        "Entity": "Caption"
    },
    {
        "Text": "A relative comparison of running a selection of regular expressions and scripts against other finite-state toolkits.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Examples of context- free and context-sensitive sub- trees related with Figure 1(b). Note: the bold node is the root for a sub-tree. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: All the parameters of Measures for de- termining semantic compositionality described in Section 3 used in our experiments. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Example confounders for \u201cfestival\u201d and \u201claws\u201d and their similarities ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Tagging part of log-likelihood plotted against V-measure ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Results on WMT-2013 (blindtest)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 The comparison between DLG, AV, BE, and ESA. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 13 Accuracy of semantic-role prediction for unknown boundaries (the system must identify the correct constituents as arguments and give them the correct roles). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Learning curves in terms of word predic- tion accuracy on deciding between the confusable pair there, their, and they\u2019re, by IGT REE trained on TRAIN - REUTERS, and tested on REUTERS, AL - ICE, and BROWN . The top graphs are accuracies at- tained by the confusable expert; the bottom graphs are attained by the all-words predictor trained on TRAIN - REUTERS until 130 million examples, and on TRAIN - NYT beyond (marked by the vertical bar). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 13: Zero-Copula in Russian",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Growth of the Wiktionary over the last three years, showing total number of entries for all languages and for the 9 languages we consider (left axis). We also show the corresponding increase in average accuracy (right axis) achieved by our model across the 9 languages (see details below). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A correct tree (tree1) and an incorrect tree (tree2) for \u201cBCLM HNEIM\u201d, indexed by terminal boundaries. Erroneous nodes in the parse hypothesis are marked in italics. Missing nodes from the hypothesis are marked in bold. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: MUC, CEAF, and B3 coreference results using true mentions.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag. Across all languages, high performance can be attained by selecting a single tag per word type. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Translations output by system RESC2 and COMB on IWSLT task (case-insensitive).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Results of different systems for pronoun resolution on MUC-6 and MUC-7 (*Here we only list backward feature assigner for pronominal candidates. In RealResolve-1 to RealResolve-4, the backward features for non-pronominal candidates are all found by DTnon\u2212pron .) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Summary of results in terms of the MTO and VM scores. Standard errors are given in parentheses when available. Starred entries have been reported in the review paper (Christodoulopoulos et al., 2010). Distributional models use only the identity of the target word and its context. The models on the right incorporate orthographic and morphological features. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 Results for Mutiple Document System         with additional retrieved texts ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: The animacy data set from Talbanken05; number of noun lemmas (Types) and tokens in each class. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5. Performance Comparison of Different                Pruning Methods ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6. Perplexity Comparison of Different               Pruning Methods ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Senses found by our algorithm from first order cooccurrences (LM-1 and LAT-1)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Ungrammatical Arabic output of Google Trans- late for the English input The car goes quickly. The subject should agree with the verb in both gender and number, but the verb has masculine inflection. For clarity, the Arabic tokens are arranged left-to-right. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 Na\u0131\u0308ve FSA with duplicated paths. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: MRR of baseline and reinforced matrices",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Baseline performance and n-best oracle scores (UAS/LAS) on the development sets. mate\u2019 uses the prepro- cessing provided by the organizers, the other parsers use the preprocessing described in Section 2. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Categories of Message Speech Act.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Graphical model for the Bayesian Query-Focused Summarization Model. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: New Verb Classes",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Bayesian network: \u03b1 and \u03b2 are vectors of hy- perparameters, and \u03b8 i (for i \u2208 {1, . . . , nc }) and \u03c6 are distributions. u is a vector of underlying forms, generated from \u03c6, and si (for i \u2208 nu ) is a set of observed surface forms generated from the hidden variable ui according to \u03b8i ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: A real translation example",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: BLEU scores of English to Russian ma- chine translation system evaluated on tst2012 and tst2013 using baseline GIZA++ alignment and transliteration augmented-GIZA++ alignment and post-processed the output by transliterating OOVs. Human evaluation in WMT13 is performed on TA-GIZA++ tested on tst2013 (marked with *) ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 4. Integration of confidence measures and interpolation \u2013 recall/precision curves.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Results on unseen test set for models which performed best on dev set \u2013 predicted input. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Example of Learned Name Pairs with Gloss Translations in Parentheses",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Plate diagram of the basic model with a single feature per token (the observed variable f ). M , Z, and nj are the number of word types, syntactic classes z, and features (= tokens) per word type, respectively. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3. The framework of our approach",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: the influence of features. (F: F-measure. Feature numbers are from Table 1) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: An ATB sample from the human evaluation. The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a). But the city name Sharm Al- Sheikh is also iDafa, hence the possibility for the incorrect annotation in (b). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Example Demonstrating Advantages of Full Parsing ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 NER type-insensitive (type-sensitive) performance of different English NE recognizers. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 Japanese-to-English Display of NICT- ATR Speech-to-Speech Translation System ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Comparison of results for MUC7",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Translation results for English-French",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 4. F-measure for the objective and subjective classes for cross-lingual bootstrapping.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 Effect of the length of the language model history (Unigram/Bigram/Trigram: word-based; CLM: class-based 5-gram).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Turning distributional similarity into a weighted inference rule ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5. Another example of some discovered paraphrases.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Accuracy and error reduction (ER) results (in percents) for our model and the MF baseline. Error reduction is computed as M ODEL\u2212M                                     100\u2212M F                                              F                                                . Results are given for the WSJ and GENIA corpora test sets. The top table is for a model receiving gold standard parses of the test data. The bottom is for a model using (Charniak and Johnson, 2005) state-of-the-art parses of the test data. In the main scenario (left), instances were always mapped to VN classes, while in the OIP one (right) it was possible (during both training and test) to map instances as not belonging to any existing class. For the latter, no results are displayed for polysemous verbs, since each verb can be mapped both to \u2018other\u2019 and to at least one class. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Dendrogram of the participants cluster based on their feedback profile ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Experimental results for Japanese\u2013English",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Coverage of summary text caseframes in source text (Study 3).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Accuracy and frequency of the top 5% for each iteration ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. F1-measures with \uf061 in [0 3]",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8 FSRA-2 for Arabic nominative definite and indefinite nouns. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4 4-tape representation for the Hebrew word htpqdut. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Frequencies and scores for each resolution class.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Subgraph of Local\u22171 output for\u201cheadache\u201d",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Contribution of feature sets (causality).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Sizes of the extracted datasets.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: CCG and LTAG supertag sequences.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 12 Results of human evaluation performed via Amazon Mechanical Turk. The percentages represent the portion of sentences for which one system had more preference judgments than the other system. If a sentence had an equal number of judgments for the two systems, it was counted in the final row (\u201cneither preferred\u201d). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: CTB 10-fold CV POS tagging accuracy using an all-at-once approach ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8: Extracting sub-trees for S2 .",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Effect of pruning parameter tp and heuristic function on error rate for direct-translation model (Np = 50,000). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10 Numeric-type compounds extracted. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10 Experiments with words and parts-of-speech as contextual features",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: MTO is fairly stable as long as the Z\u0303 constant 5.4 Morphological and orthographic features is within an order of magnitude of the real Z value. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Decoding algorithm using semantic role features. Sema(c1 .role, c2 .role, t) denotes the triggered semantic role features when combining two children states, and ex- amples can be found in Figure 3. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Feature set for our pronoun resolution system(*ed feature is only for the single-candidate model while **ed feature is only for the twin-candidate mode) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Overall results by Vieira and Poesio",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Feature space growing curve. The horizontal scope X[i:j] denotes the introduction of different tem- plates. X[0:5]: Cn (n = \u22122..2); X[5:9]: Cn Cn+1 (n = \u22122..1); X[9:10]: C\u22121 C1 ; X[10:15]: C0 Cn (n = \u22122..2); X[15:19]: C0 Cn Cn+1 (n = \u22122..1); X[19:20]: C0 C\u22121 C1 ; X[20:21]: W0 ; X[21:22]: W\u22121 W0 . W0 de- notes the current considering word, while W\u22121 denotes the word in front of W0 . All the data are collected from the training procedure on MSR corpus of SIGHAN bake- off 2. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Example of feature forest representation of linear chain CRFs. Feature functions are as- signed to \u201cand\u201d nodes. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7 Summaries Recall and Precision",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Feature templates used for CRF in our system",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: A narrative chain and its reverse order.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Overlaid bilingual embeddings: English words are plotted in yellow boxes, and Chinese words in green; reference translations to English are provided in boxes with green borders directly below the original word. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Comparison of word segmentation F- measure for SIGHAN bakeoff3 tasks ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Running example of graph creation",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Feature growth rate: For N-best list i in the table, we have (#NewFt = number of new fea- tures introduced since N-best i \u2212 1) ; (#SoFar = Total number of features defined so far); and (#Ac- tive = number of active features for N-best i). E.g., we extracted 7535 new features from N-best 2; combined with the 3900 from N-best 1, the total features so far is 11435. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: LLDA Fmeausres for 3 feature conditions",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Precision and recall for articles.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: normalized Mutual Information values for three graphs and different iterations in %. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Lexical variations creating new rules based on DIRT rule X face threat of Y \u2192 X at risk of Y ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 2. Collecting paraphrases using a Paraphrase Recognizer.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 18 Precision of person name recognition on the MSR test set, using Viterbi iterative training, initialized by four seed sets with different sizes. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Results tested against gs-swaco-subjective",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Translation results for German-English",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11 Error analysis for false positives and false negatives. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. GETARUNS pronouns collapsed at structural level",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Clustering evaluation for the experiment without Named Entities ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example entries for the Transfer of a Message - levels 1 and 2 classes",
        "Entity": "Caption"
    },
    {
        "Text": "Table 20: Phonetic Stress in Russian: Fake Homograph",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A Motivating Example",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Macro-accuracy for multilingual bootstrapping (versus cross-lingual framework) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Overview of the ACE 2005 data.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. An example discussion thread",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4 Word sense disambiguation accuracy for \u201cNP1 V NP2 to NP3\u201d frame. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Comparison of performance across the five PPI corpora",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 Example levels of generalization for different values of \u03b1. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Part of a sample headline cluster, with sub-clusters ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Performance relationship between WMEB and BASILISK on Sgold UNION ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Number of extracted instances and the sample sizes (P and N indicate positive and neg- ative annotations). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The relationship extraction system.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: the influence of agenda size.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Optimization & Test Set Pearson Correlation Results",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: An example packed forest representing hy- potheses in Figure 1(a). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Automatic evaluation with 50% of Freebase relation data held out and 50% used in training on the 102 largest relations we use. Precision for three different feature sets (lexical features, syntactic features, and both) is reported at recall levels from 10 to 100,000. At the 100,000 recall level, we classify most of the instances into three relations: 60% as location-contains, 13% as person-place-of-birth, and 10% as person-nationality. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Comparison of Min-, Simple-, Full-and Dynamic-Expansions: More Examples",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Accuracy of 5-fold cross-validation with sta-            tistics-based semantic features ",
        "Entity": "Caption"
    },
    {
        "Text": "The test 1:ART.Nom checks if the preceding word is a nominative article.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 19 English NE recognition on test data after semi-supervised learning. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Computation of probabilities using the language model.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 15 Size of training data set and the adaptation results on AS open. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3 FSA for the pattern hit a e . ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Effect of language and error models to speed (time in seconds per 10,000 word forms) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: MUC-7: Level Distribution of Each of the Facts",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Results for different user simulations. Numbers give % reductions in keystrokes. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Three types of transitivity constraint violations.",
        "Entity": "Caption"
    },
    {
        "Text": "                            C2 Figure 3: An underspecified d ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Experiment results (as F1 scores) where IM is identification of mentions and S - Setting. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7 An example showing the generalization of the word lattice (a) into a slotted lattice (b). The word lattice is produced by aligning seven sentences. Nodes having in-degrees > 1 occur in more than one sentence. Nodes with thick incoming edges occur in all sentences. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Effect of two-level lexicon combination. For the baseline we used the conventional one-level full form lexicon. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3:  Recall for morphological hasXY() descriptions ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: SC classification accuracies of different methods for the ACE training set and test set.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Comparison of raw input and constrained input.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Tune and test curves of five repetitions of the same Urdu-English PBMT baseline feature experiment. PRO is more stable than MERT. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: ROUGE-2 measures in EM learning",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Translation results in lower-case BLEU. CN for confusion network and CF for confusion forest with different vertical (v) and horizontal (h) Markovization order. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Two methods for constructing multilingual distributions over words. On the left, paths to the German word \u201cwunsch\u201d in GermaNet are shown. On the right, paths to the English word \u201croom\u201d are shown. Both English and German words are shown; some internal nodes in GermaNet have been omitted for space (represented by dashed lines). Note that different senses are denoted by different internal paths, and that internal paths are distinct from the per-language expression. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Results for the three ACE data sets obtained via the MUC scoring program.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Priority Order for First Person ADs",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results of the syntactic structured fea- tures ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6 Context model, word classes, class models, and feature functions. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 15 Overview of the results for all Web algorithms for coreference. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4. Results for initial ranking manner.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Rhetorical relations in RST Spanish Treebank",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Features used in the naive Bayes Classi- fier for the entity candidate: ws , ws+1 , ..., we . spi is the result of shallow parsing at wi . ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 9 BLEU difference curves of four context-informed models using TRIBL",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Usefulness of syntactic information: (black) dash-dotted line \u2013 word boundaries only, (red) dashed line \u2013 POS info, and (blue) solid line \u2013 full parse trees. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 17 Comparison of performance of MSRSeg: The versions that are trained using (semi-)supervised iterative training with different initial training sets (Rows 1 to 8) versus the version that is trained on annotated corpus of 20 million words (Row 9). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Feature templates for the word segmentor. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: A wRTG modelling Fig. 1",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 \u2013 Pk for Le Monde corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results on the Arabic GALE Phase 2 system combination tuning set with four reference translations. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9: Impact of the improved morphology on the qual- ity of the dependency parser for Czech and German. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. F-scores of U-DOP, UML-DOP and a  supervised treebank PCFG (ML-PCFG) for a   random 90/10 split of WSJ10 and WSJ40. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: LO cosine sentence configuration scores",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Relation types for ACE 05 corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Notation used in this paper. The convention eIi indicates a subsequence of a length I sequence. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Results on a truly independent test set, consisting of data harvested from Egyptian Facebook pages that are entirely distinct from the our dialectal training set. The improvements over the MSA baseline are still considerable: +2.9 BLEU points when no Facebook data is available for tuning and +2.7 with a Facebook tuning set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 3. Algorithm for Fuzzy Agglomerative Clustering based on verbs.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 9: Confusion matrix for relation labels on the RST-DT test set. Y-axis represents true and X-axis repre- sents predicted relations. The relations are Topic-Change (T-C), Topic-Comment (T-CM), Textual Organization (T- O), Manner-Means (M-M), Comparison (CMP), Evaluation (EV), Summary (SU), Condition (CND), Enablement (EN), Cause (CA), Temporal (TE), Explanation (EX), Background (BA), Contrast (CO), Joint (JO), Same-Unit (S-U), Attribu- tion (AT) and Elaboration (EL). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 16 Accuracy comparisons between various dependency parsers on English data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Examples given in the description of Task 2.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. PoCoS: Core Scheme, Extended Scheme and language-specific instantiations",
        "Entity": "Caption"
    },
    {
        "Text": "The regular expressions available in Foma from highest to lower precedence.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Influence of the n-gram model on the perfor- mance of the statistical approach. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Corpus statistics of the MATR MT06 corpus that was used for experimental evaluation of the proposed measures. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Results using different parsers",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: F-measure on SIGHAN bakeoff 2. SIGHAN best: best scores SIGHAN reported on the four corpus, cited from Zhang and Clark (2007). ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 6 Distances found between phrase boundaries with linked modifier words and with parent words",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Results for cascading minority-preference DAC system \u2014 DACCMP (consult classifiers in reverse order of frequency of class); \u201cER\u201d refers to error reduction in percent over standard multiclass SVM (Table 2) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example of a prediction for English to French translation. s is the source sentence, h is the part of its translation that has already been typed, x\u2217 is what the translator wants to type, and x is the prediction. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Translation results in terms of BLEU score and translation edit rate (TER) estimated on newstest2010 with the NIST scoring script. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Learning curves showing the effects of increas- ing the size of dialectal training data, when combined with the 150M-word MSA parallel corpus, and when used alone. Adding the MSA training data is only use- ful when the dialectal data is scarce (200k words). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results for GigaPairs (all numbers in %); re- sults that significantly differ from Full are marked with asterisks (* p<0.05; ** p<0.01). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: space distribution by part-of-speech of",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: The total costs for the three MTurk subtasks in- volved with the creation of our Dialectal Arabic-English parallel corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: CTB 10-fold CV word segmentation F- measure for our word segmenter ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 14 Experiments applying combinations of features in English-to-Hindi translation",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: List of results in Sighan Bakeoff 2005",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Gender detection accuracies (%) using a 4-gram language model for the letter sequence of          the source name in Latin script. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: MRR, Precision, Recall, and F1-score",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4 The path of Selection. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Our probabilistic model: a question x is mapped to a latent logical form z, which is then evaluated with respect to a world w (database of facts), producing an answer y. We represent logical forms z as labeled trees, induced automatically from (x, y) pairs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Rule type distribution of a sample of 200 rules that extracted incorrect mentions. The corre- sponding numbers of incorrect mentions extracted by the sampled rules is shown in parentheses. ",
        "Entity": "Caption"
    },
    {
        "Text": "Pr(f J , aJ | eI ) = p (f J , aJ | eI ) (6) 1 1 1 1 1 1",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5 Example output of our model for Chinese\u2192English translation. The word-segmented Chinese sentence and dependency tree are inputs. Our model\u2019s outputs include the English translation, phrase segmentations for each sentence (a box surrounds each phrase), a one-to-one alignment between the English and Chinese phrases, and a projective dependency tree on the English phrases. Note that the Chinese dependency tree is on words whereas the English dependency tree is on phrases. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Comparison of the effect of morphological segmentation when translating MSA web text and Dialectal Arabic web text. The morphological segmentation uniformly improves translation quality, but the improvements are more dramatic for MSA than for Dialectal Arabic when comparing similarly-sized training corpora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Examples for the effect of the combined lexica.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Basic Travel Expression Corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 2. Multilingual bootstrapping.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Results with Coref Rules Alone",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Summary of features used in experiments in this paper.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Sample pairs of similar caseframes by relation type, and the similarity score assigned to them by our distributional model. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: The sizes of error models as automata",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11: ILP problem size and segmentation speed.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 17 WSD using predominant senses, training, and testing on all domain combinations (automatically classified corpora). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: From DRS to DRG: labelling.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Example DCS trees for utterances in which syntactic and semantic scope diverge. These trees reflect the syntactic structure, which facilitates parsing, but importantly, these trees also precisely encode the correct semantic scope. The main mechanism is using a mark relation (E, Q, or C) low in the tree paired with an execute relation (Xi ) higher up at the desired semantic point. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Plurality language families across 20 clusters. The columns indicate portion of lan- guages in the plurality family, number of lan- guages, and entropy over families. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Intrinsic evaluation accuracy [%] (development set) for Arabic segmentation and tagging. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Correlation between Perplexity",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example of semantic trees",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7. System performance on the reaction relation on the CHEM dataset. ",
        "Entity": "Caption"
    },
    {
        "Text": "hAL(eI , f J , K , zK ) = |j 1 j | (16) 1 1 1 1 k k=1 k 1",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Performance of Unsupervised Name Mining",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Three different vocabulary sizes used in subword- based tagging. s1 contains all the characters. s2 and s3 contains some common words. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8: Contribution of feature sets (material).",
        "Entity": "Caption"
    },
    {
        "Text": "K hAT(eI , f J , K , zK ) = log n p(zk | f j k ) (13) 1 1 1 1 k=1 j k 1 +1",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The greedy binding problem. (a) The correct binding, (b) the greedy binding, (c) the result.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Graphical model for PLTM.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: learning curves of the averaged and non- averaged perceptron algorithms ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Classification results with decision tree on vectors of frequency of rarest n-grams (Method 4) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: BS on IWSLT 2007 task",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Coreference Resolution Performance",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5 A comparison between ILP-Global and Greedy-Global. Parts A1\u2013A3 depict the incremental progress of Greedy Global for a fragment of the headache graph. Part B depicts the corresponding fragment in ILP-Global. Nodes surrounded by a bold oval shape are strongly connected components. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: F-measure(%) Breakdown by Mention Type: NAM(e), NOM(inal), PRE(modifier) and PRO(noun). Chinese data does not have the PRE type. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: BLEU difference of 1000 bootstrap sam- ples. 95% confidence interval is [.15, .90] The proposed approach therefore seems to be a stable method. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: POS tagging of unknown words using contextual and lexical Features (accuracy in per- cent).   \u008d is based only on contextual features,  T\u008d \u008e is based on contextual and lexical features. SM(    _ #\u00a7 )                                                  2 denotes that   \u00a7 follows   in the sequential model.                           2 ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The screenshot of our web-based system shows a simple quantitative analysis of the frequency of two terms in news articles over time. While in the 90s the term Friedensmission (peace operation) was predominant a reverse tendency can be observed since 2001 with Auslandseinsatz (foreign intervention) being now frequently used. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Candidates for equivalence classes.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results of the first run (without postprocessing)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: MUC-7: Level Distribution of Each of the Facts",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 10 Dutch-to-English Learning curves (left-hand side graphs) and difference curves (right-hand side graphs) comparing the Moses baseline against four context-informed models (PR, OE, POS\u00b12 and Word\u00b12). These curves are plotted with scores obtained using three evaluation metrics: BLEU (top), METEOR (centre) and TER (bottom) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Extract of a French-English sentence pair segmented into bilingual units. The original (org) French sentence appears at the top of the figure, just above the reordered source s and target t. The pair (s, t) decomposes into a sequence of L bilingual units (tuples) u1 , ..., uL . Each tuple ui contains a source and a target phrase: si and ti . ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Inferred Dirichlet transition hyperparameters for bigram CLUST on three-way classification task with four latent clusters. Row gives starting state, column gives target state. Size of red blobs are proportional to magnitude of corresponding hyperparameters. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Causes of Error for FPs",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Macro-accuracy for cross-lingual bootstrapping",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Finite-state cascades for five natural language problems.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Example context for the spelling confusion set {piece,peace} and extracted features ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Parameters of Measures (Section 3) which, combined with particular WSMs, achieved the highest average correlation in TrValD. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 Architecture of the statistical translation approach based on Bayes\u2019 decision rule. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 The similarity score features used to represent pairs of templates. The columns specify the corpus over which the similarity score was computed, the template representation, the similarity measure employed, and the feature representation (as described in Section 4.1). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: The average results among all 7 speakers when train with different combinations of speaker specific data and other speakers\u2019 data are displayed. In both Constant adaptation and Reweighted adaptation models the num- ber of speaker specific data are varied from 200, 500, 1000, 1500 to 2000. In Generic model, only all other speakers\u2019 data are used for training data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Results for 2 and for 10 preceding POS tags as context are reported for our tagger.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3. Results obtained in the detection of zero-pronouns.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6 Dependencies in the alignment template mode",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Dependency tree for the sentence \u201cPROT1 contains a sequence motif binds to PROT2.\u201d ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 9: Example error cases, with associated frequencies, illustrating system output and gold standard references. 5% of the cases were miscellaneous or otherwise difficult to categorize. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: An example where syntactic features help to link the PRO mention \u00d1\u00eb (hm) with its antecedent, the NAM                   ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5. Templates for feedback.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Comparison to Related Approaches",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 Bootstrapped weighting: top 10 common features for country\u2013state and country\u2013party along with their corresponding ranks in the two (sorted) feature vectors. ",
        "Entity": "Caption"
    },
    {
        "Text": "(e | f , i, j): p(ei | fj , i 1 i =1 [(i , j) A], j 1 j =1 [(i, j ) A]) (15)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 25 Comparison of dependency accuracies between phrase-structure parsing and dependency parsing using CTB5 data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4. Impact of Data Size (Chinese)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Distribution of domain labels of predom- inant senses for 38 polysemous words ranked using the SPORTS and FINANCE corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Evaluation of Correction and Inference Mechanisms",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11: POS tagging error patterns. # means the error number of the corresponding pattern made by the pipeline tagging model. \u2193 and \u2191 mean the error number reduced or increased by the joint model. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Summary for graphs and test datasets obtained from each seed pair ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Glue Semantics proof for (83), English Way Construction (means interpretation)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5. System performance on the part-of relation on the CHEM dataset. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Performance of TL-comb and TL-auto as H changes. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Results on the MUC6 formal test set.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results for sentence-based predicte alignment in the three benchmark settings MTC, Leagues and MSR (all numbers in %); results that significantly differ from Full are marked with asterisks (* p<0.05; ** p<0.01). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Segmentation results of dictionary-based segmentation in closed test of Bakeoff 2005. A \u201c/\u201d separates the results of unigram, bigram and trigram. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Syntactic features. h and ld mark features from the head and the left-most daughter, dir is a binary fea- ture marking the direction of the head with respect to the current token. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Cross-domain B3 (Bagga and Baldwin, 1998) results for Reconcile with its general feature set. The Paired Permutation test (Pesarin, 2001) was used for statistical significance testing and gray cells represent results that are not significantly different from the best result. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Pearson\u2019s r and Kendall\u2019s \u03c4 (absolute) between adequacy and automatic evaluation measures on different levels of the MATR MT06 data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9: Comparison of our approach with using only the Gigaword corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Coverage of caseframes in summaries with respect to the source text. The model aver- age is statistically significantly different from all the other conditions p < 10\u22128 (Study 3). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Context-sensitive similarity scores (in bold) for the Y slots of four rule applications. The components of the score calculation are shown for the topics of Table 1. For each rule application, the table shows a couple of the topic-biased scores Lint of the rule (as in Table 1), along with the topic relevance for the given context p(t|dv , w), which weighs the topic-biased scores in the LinW T cal- culation. The context-insensitive Lin score is shown for comparison. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 12 Overview of the results for all baselines for coreference. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Tuninig test for hyperparameter Q of structural SVM (fixed \u03bb=1.0) by increasing it. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9: Performance of each individual relation type based on 5-fold cross-validation.",
        "Entity": "Caption"
    },
    {
        "Text": "BP(f J , eI , A) = f j+m , ei+n 1 1 j i : (i , j ) A : j j j + m i i i + n (9) (i , j ) A : j j j + m i i i + n",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Relative recall evaluation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Russian to English machine translation system evaluated on WMT-2012 and WMT-2013. Human evaluation in WMT13 is performed on the system trained using the original corpus with TA- GIZA++ for alignment (marked with *). ",
        "Entity": "Caption"
    },
    {
        "Text": "exp[ M m hm (eI , f J )] m=1 1 1 M I J (3)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7. Weight of co-occurring words",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: The conditioning structure of the hierarchical PYP with an embedded character language models. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Translation extraction from comparable corpora using cross-lingual WSI and WSD.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Sentiment lexicon description",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: accuracy using non-averaged and averaged perceptron.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Number of evaluated English NEs.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Results of the character-category association model: best 5 guesses ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2 \u2013 Automaton for topic shift detection",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Case frame of \u201chaken (dispatch).\u201d",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Comparison of frames. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: BLEU scores for GBM features. Model parameters were optimized on the Tune set. For PRO and regularized MERT, we optimized with different hyperparameters (regularization weight, etc.), and retained for each experimental condition the model that worked best on Dev. The table shows the performance of these retained models. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Coreference Definition Differences for MUC and ACE. (GPE refers to geo-political entities.) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Approximate times in seconds to generate predictions of maximum word sequence length M , on a 1.2GHz processor, for the MEMD model. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. 5-fold cross-validation results. All are trained on fp1 (except the last row showing the unchanged algorithm trained on adj for comparison), and tested on adj. McNemar's test show that the improvement from +purify to +tSVM, and from +tSVM to ADJ are statistically significant (with p<0.05). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Three representations of NP modifications: (a) the original treebank representation; (b) Selective left-corner representation; and (c) a flat structure that is unambiguously equivalent to (b) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Incremental results for the four sieves using our dictionary on the development set. Baseline is the Stanford system without the WordNet sieves. Scores are on gold mentions. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Precision and recall of direct dependency projection via one-to-one links alone. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: MRR performance of phonetic translit- eration for 3 corpora using unigram and bigram                 language models. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example of a German noun phrase. First and last word agree in number, gender, and case value.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Histogram of token movement size ver- sus its occurrences performed by the model Neu- big on the source english data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Gross statistics for several different treebanks. Test set OOV rate is computed using the following splits: ATB (Chiang et al., 2006); CTB6 (Huang and Harper, 2009); Ne- gra (Dubey and Keller, 2003); English, sections 2-21 (train) and section 23 (test). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Individual Classifier Properties (cross-validation on SENSEVAL training data)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Number of entries in 3 corpora",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: NIL expression forms based on POS attribute.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Estimation of F(v, c) for the verb feed. ",
        "Entity": "Caption"
    },
    {
        "Text": ", C(e )) (18) 1 1 1 1 i=1 i 1",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Sample analysis of an English sentence. Input: Do we have to reserve rooms?. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Language families in our data set. The Other category includes 9 language isolates and 21 language family singletons. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Sources of conflict in cross-lingual subjectivity transfer. Definitions and synonyms of the fourth sense of the noun argument, the fourth sense of verb decide, and the first sense of adjective free as provided by the English and Romanian WordNets; for Romanian we also provide the manual translation into English. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Results for verbs",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Illustration of entity-relationship graphs",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Results for OOV-processing and MBR, German\u2192English. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Basic Features for CRF-based Segmenter",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Feature set used in the Stage 2 classifier, and their number for the causal relation experiments.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Different Context Window Size Setting",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Experiments 2 and 3: Results by the num- ber of senses of a lemma, condition All, \u03b8 = 1.0 ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: The decision tree (Nwire) for the system using the single semantic relatedness feature ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Statistics about the results of our word sense discovery algorithm",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Sample of subjective features appearing in the top 100 discriminant attributes selected with Information Gain on the 3rd fold training data at iteration 10. The words in italics in the multilingual features represent equivalent translations in English and Romanian. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Top 10 POS mistakes made more often by either the CTF-TM with parsing or the CTF-TM without on the ATB part 1, 2, and 3 development set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Total annotation time, portion spent se- lecting annotation type, and absolute improve- ment for rapid mode. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Statistics of training, development and test data for NIST task. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3 Word sense disambiguation accuracy for \u201cNP1 V NP2 NP3\u201d frame. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: POS Tagging of Unknown Word using Contextual and Lexical features in a Sequential Model. The input for capitalized classifier has 2 values and therefore 2 ways to create confusion                                      k\u0097 sets. There are at most \u0089\u0095\u0094 &F\u0096 \u0081 \u0081 +!\u0098 different in- puts for the suffix classifier (26 character + 10 digits + 5 other symbols), therefore suffix may                      k\u0097 emit up to \u0089 \u0094 &R\u0096 \u0081 \u0081 +R\u0098 confusion sets. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Results - Evaluation B",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Average Precision and Coherence (\u03ba) for each meta alternation. Correlation: r = 0.743 (p < 0.001) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Impact of role sequence information on the HMM and Maxent classifiers. The combination results of the HMM and Maxent are also provided. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Language-dependent lexical features. A word list can be collected to encode different ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: DP algorithm for statistical machine translation.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Overall performance of the 3 systems",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6. Weight-measure of co-occurring words",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Example of Morph-Related Heteroge- neous Information Network ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Probability estimation tree for the nomi native case of nouns.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Micro-averaged (across the 5 folds) RE results using gold mentions.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Frequency distribution for sentence lengths in the WSJ (sections 2\u201323) and the ATB (p1\u20133). English parsing evaluations usually report results on sentences up to length 40. Arabic sentences of up to length 63 would need to be evaluated to account for the same fraction of the data. We propose a limit of 70 words for Arabic parsing evaluations. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results on wide-coverage Question Answer- ing task. CCG-Distributional ranks question/answer pairs by confidence\u2014@250 means we evaluate the top 250 of these. It is not possible to give a recall figure, as the total number of correct answers in the corpus is unknown. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6 Results of different approach used in our experiments (White background lines are the results we repeat Zhang\u201fs methods and they have some trivial difference with Table 1.) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 Results for Mutiple Document System",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Density of signature caseframes (Study 2).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Accuracy of various instantiations of the system",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example of the non-relation Same-Unit",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 The speeds of joint word segmentation and POS-tagging by 10-fold cross validation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Evaluation of coarse-grained POS tagging on test data ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 IV and OOV recall in   (Zhang et al., 2006a) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 13 Comparison scores for PK open and CTB open. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Results for ODP system using various sources of DA tags ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: The deductive system for Earley\u2019s genera- tion algorithm ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Examples of the three top candidates in the transliteration of English/Arabic, English/Hindi and English/Chinese. The second column is the rank. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: BLEU scores for pre-ordering experi- ments with a n-code system and the approach pro- posed by (Neubig et al., 2012) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 13 Evaluating predominant sense information for polysemous nouns on the Senseval-2 all-words task data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Total corpus sizes (in sentences) and number of (S)ure and (P)ossible alignment links in their respective evaluation sets. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: A decoding trace using improvement caching and tiling (ICT). The search in the second and later iterations is limited to areas where a change has been applied (marked in bold print) \u2014 note that the number of alignment checked goes down over time. The higher number of alignments checked in the second iteration is due to the insertion of an additional word, which increases the number of possible swap and insertion operations. Decoding without ICT results in the same translation but requires 11 iterations and checks a total of 17701 alignments as opposed to 5 iterations with a total of 4464 alignments with caching. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Results for choosing the correct ordered chain. (\u2265 10) means there were at least 10 pairs of ordered events in the chain. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: CPU time, memory usage, and uncased BLEU (Papineni et al., 2002) score for single-threaded Moses translating the same test set. We ran each lossy model twice: once with specially-tuned weights and once with weights tuned using an exact model. The difference in BLEU was minor and we report the better result. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Precision and recall for different values of",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: BasicRE gives the performance of our basic RE system on predicting fine-grained relations, obtained by performing 5-fold cross validation on only the news wire corpus of ACE-2004. Each sub- sequent row +Hier, +Hier+relEntC, +Coref, +Wiki, and +Cluster gives the individual contribution from using each knowledge. The bottom row +ALL gives the performance improvements from adding +Hier+relEntC+Coref+Wiki+Cluster. \u223c indicates no change in score. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Results for Positive and Negative Classes.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Segmentation performance on words that have the same final suffix as their preceding words. The F1 scores are computed based on all boundaries within the words, but the accuracies are obtained using only the final suffixes. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Selected morphosyntactic categories in the OLiA Reference Model ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4. All binary trees for NNS VBD JJ NNS         (Investors suffered heavy losses) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Factoring of global feature collections g into f . xji denotes hxi , . . . xj i in sequence x = hx1 , . . .i. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Different Context Window Size Setting",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Run-time in seconds for various \u2212\u03bb values.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: WSI and WSD Pipeline",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Performance of Algorithms",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Illustration of search in statistical trans- lation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Examples of disagreement in segmentation guidelines",
        "Entity": "Caption"
    },
    {
        "Text": "  Table 5. Timings from the word alignments for our SMT evaluation. The values are averaged over both alignment directions. For these experiments we used systems with                  8-core Intel E5-2670 processors running at 2.6 GHz. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Percentage of non-projective arcs recovered correctly (number of labels in parentheses)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6 The initial frequencies of character sequences. ",
        "Entity": "Caption"
    },
    {
        "Text": "eI = argmax {Pr(eI | f J )} (1)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2. Stratefied Sampling for initial seeds",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Hinton diagram comparing most frequent tags and clusters.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Mixed-case TER and BLEU, and lower- case METEOR scores on Arabic NIST MT05. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The clausal and topological field structure of a German sentence. Notice that the subordinate clause receives its own topology. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Precision, recall and F-scores for the two classes in MBL-experiments with a general feature space. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Results of combining the character-category association and rule-based models: best guess ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10: Results for the joint segmentation, tagging, and parsing task using pipeline and joint models. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Variation in performance, by number of sentence boundaries (n), and by training corpus size.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Sample of Gold Standard entries",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Processed Data Statistics",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Backward features used to capture the coreferential information of a candidate",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Tagging accuracies on test data.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Dependency representation of example (2) from Talbanken05. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Segmentation and tagging of the Arabic token A\u00eeE\u00f1J. J\u00baJ\u0083\u00f0 \u2018and they will write it\u2019. This token has four seg- ments with conflicting grammatical features. For example, the number feature is singular for the pronominal object and plural for the verb. Our model segments the raw to- ken, tags each segment with a morpho-syntactic class (e.g., \u201cPron+Fem+Sg\u201d), and then scores the class sequences. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Results for basic DAC system (per-class feature optimization followed by maximum confidence based choice); \u201cER\u201d refers to error reduction in percent over standard multiclass SVM (Table 2) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Structure of the out-of-vocabulary word \u623d\u4282 \u483d\u543c \u2018English People\u2019. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Derivation with soft syntax model",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Automatic role labeling results (%) using the HMM and Maxent classifiers. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Snapshot of the supersense-annotated data. The 7 article titles (translated) in each domain, with total counts of sentences, tokens, and supersense mentions. Overall, there are 2,219 sentences with 65,452 tokens and 23,239 mentions (1.3 tokens/mention on average). Counts exclude sentences marked as problematic and mentions marked ?. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Automatic Evaluations.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Performance of Altavista counts and BNC counts for adjective ordering (data from Malouf 2000) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10 Simplified prevalence score, evaluation on SemCor, polysemous words only. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4. Performance for different p values",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Example questions correctly answered by CCG-Distributional.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: F-measure for the objective and subjective classes for multilingual bootstrapping (versus cross-lingual framework) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Results for the unsupervised baseline and the supervised system trained on three kinds of feature sets ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 Architecture of the translation approach based on a log-linear modeling approach",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: An excerpt from the graph for Italian. Three of the Italian vertices are connected to an automatically la- beled English vertex. Label propagation is used to propa- gate these tags inwards and results in tag distributions for the middle word of each Italian trigram. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Improvement in f-score through restoring case.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Examples of two- to seven-word bilingual phrases obtained by applying the algorithm phrase-extract to the alignment of Figure 2",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Data examined by the two systems for the ATB",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Averaged perceptron learning curves with Non- lexical-target and Lexical-target feature templates. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2. Extracted NE pair instances and context",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Comparison with other PPI extraction systems in the AIMed corpus ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Grammatical relations from S EXTANT",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Learning curve of SuperSense on SE3",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Decoding as lattice parsing, with the highest-scoring translation denoted by black lattice arcs (others are grayed out) and thicker blue arcs forming a dependency tree over them. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 16: Arabic Order-Free Structure",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: CTB 10-fold CV word segmentation F- measure using an all-at-once approach ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Gender classification performance (%)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Syntactical Variations of \u201cactivate\u201d",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11 Training, development, and test data from CTB5 for joint word segmentation and POS-tagging. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Non-projective sentences and arcs in PDT and DDT (NonP = non-projective)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4. Summary of results for random and first sense baselines and supersense tagger, \u03c3 is the standard error computed on the five trials results. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Global features in the entity kernel for reranking. These features are anchored for each entity instance and adapted to entity categories. For example, the entity string (first feature) of the entity \u201cUnited Nations\u201d with entity type \u201cORG\u201d is \u201cORG United Nations\u201d. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A verse written in the BAD web application.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. F1 and accuracy of the argument classifiers and the overall multiclassifier for                               FrameNet semantic roles ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11 Large-scale clustering on D1. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Segmentation precision/recall relative to gold word length in training data.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Baseline vs. Submitted Results",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Relationship types and their argument type con- straints. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results for 4-fold site-wise cross-validation us- ing the DP corpus ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Per category performance of the Berkeley parser on sentence lengths \u2264 70 (dev set, gold segmentation). (a) Of the high frequency phrasal categories, ADJP and SBAR are the hardest to parse. We showed in \u00a72 that lexical ambiguity explains the underperformance of these categories. (b) POS tagging accuracy is lowest for maSdar verbal nouns (VBG,VN) and adjectives (e.g., JJ). Richer tag sets have been suggested for modeling morphologically complex distinctions (Diab, 2007), but we find that linguistically rich tag sets do not help parsing. (c) Coordination ambiguity is shown in dependency scores by e.g., \u2217SSS R) and \u2217NP NP NP R). \u2217NP NP PP R) and \u2217NP NP ADJP R) are both iDafa attachment.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6. System performance on the succession relation on the TREC-9 dataset. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Learning curves of systems with different features",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Features used in predicting the next parser action",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Overview of experiments applying WSMs to determine semantic compositionality of word expressions. BNC - British National Corpus, GR - grammatical relations, GNC - German newspaper corpus, TREC - TREC corpus; SY - substitutability-based methods, CT - component-based methods, CTn - component-based methods comparing WSM neighbors of expressions and their components, CY - compositionality-based methods; NVAP c. - noun, verb, adjective, adverb combinations, NN - noun-noun, VP - verb-particles, AN - adjective-noun, VO - verb-object, SV - subject-verb, PV - phrasal-verb, PNV - preposition-noun-verb; dicts. - dictionaries of idioms, WN - Wordnet, MA - use of manually annotated data, S - Spearman correlation, PC - Pearson correlation, CR - Spearman and Kendall correlations, APD - average point difference, CL - classification, P/R - Precision/Recall, P/Rc - Precision/Recall curves, Fm - F measure, R2 - goodness. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Average accuracy of three procedures with various settings over 4 datasets. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: AER comparison (cn \u2192en)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Graphical depiction of our model and summary of latent variables and parameters. The type-level tag assignments T generate features associated with word types W . The tag assignments constrain the HMM emission parameters \u03b8. The tokens w are generated by token-level tags t from an HMM parameterized by the lexicon structure. The hyperparameters \u03b1 and \u03b2 represent the concentration parameters of the token- and type-level components of the model respectively. They are set to fixed constants. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Accuracies for models with and without oracle pruning. * indicates models significantly worse than the oracle model. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: An underspecified discourse structure and its five configurations",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 DP-based algorithm for statistical MT that consecutively processes subsets C of source sentence positions of increasing cardinality. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Evolution of \u03c4A means relative to the length of the n-best sequence ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: AER comparison (en\u2192cn)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: A multiword expression in HeiST",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Examples for the effect of equivalence classes resulting from dropping morpho-syntactic tags not relevant for translation. First the translation using the original representation, then the new representation, its reduced form and the resulting translation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Filtered 5-gram dataset statistics.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Results with varying sizes of training data. Year 2003 is not explicitly shown because it has an unusually small number of documents compared to other years. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Fact vs. Statistical Cross-Doc Features",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Upper triangle of the sentence-similarity matrix.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Statistics of paraphrase pairs retrieved from MSRPC. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Features based on the token string that are based on the probability of each name class during training.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Accuracy scores for the CoNLL 2009 shared task test sets. Rows 1\u20132: Top performing systems in the shared CoNLL Shared Task 2009; Gesmundo et al. (2009) was placed first in the shared task; for Bohnet (2010), we include the updated scores later reported due to some improvements of the parser. Rows 3\u20134: Baseline (k = 1) and best settings for k and \u03b1 on development set. Rows 5\u20136: Wider beam (b1 = 80) and added graph features (G) and cluster features (C). Second beam parameter b2 fixed at 4 in all cases. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 Relation between number of classes and alternations. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Adapting a parser to a new annotation style. We learn to parse in a \u201ctarget\u201d style (wide column label) given some number (narrow column label) of supervised target-style training sentences. As a font of additional features, all training and test sentences have already been augmented with parses in some \u201csource\u201d style (row label): either gold-standard parses (an oracle experiment) or else the output of a parser trained on 18k source trees (more realistic). If we have 0 training sentences, we simply output the source-style parse. But with 10 or 100 target-style training sentences, each off-diagonal block learns to adapt, mostly closing the gap with the diagonal block in the same column. In the diagonal blocks, source and target styles match, and the QG parser degrades performance when acting as a \u201cstacked\u201d parser. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Examples of the semantic role features",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Sentence error rates of end-to-end evalua- tion (speech recognizer with WER=25%; corpus of 5069 and 4136 dialogue turns for translation Ger- man to English and English to German, respec- tively). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: MUC-7: Level Distribution of Each of the Facts",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Non-anaphoric DNP examples",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Rank of correct translation for period Dec 01 \u2013 Dec 15 and Dec 16 \u2013 Dec 31. \u2018Cont. rank\u2019 is the context rank, \u2018Trans. Rank\u2019 is the transliteration rank. \u2018NA\u2019 means the word cannot be transliterated. \u2018insuff\u2019 means the correct translation appears less than 10 times in the English part of the comparable corpus. \u2018comm\u2019 means the correct translation is a word appearing in the dictionary we used or is a stop word. \u2018phrase\u2019 means the correct translation contains multiple English words.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The value of the penalized loss based on the number of iterations: DPLVMs vs. CRFs on the MSR data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example candidate space of dimensionality 2. Note: I = {1, 2}, J(1) = J(2) = {1, 2, 3}. We also show a local scoring function hw (i, j) (where w = [\u22122, 1]) and a local gold scoring function g(i, j). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Known topic changes found in 90 generated texts using a block size of six. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Plate diagram of our model.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Performance of different relation types and major subtypes in the test data",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Spanish Ornat corpus results. Standard devia- tions are in parentheses; \u2217 denotes a significant difference from the M ORTAG model. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Corpus Excerpt with Dialogue Act Annotation",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: BS on IWSLT 2006 task",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Regular expression notation in foma.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5. Extracted UW and noun set",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: The search graph on development set of IWSLT task ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Statistics for various corpora utilized in exper- iments. See Section 5. The English data comes from the WSJ portion of the Penn Treebank and the other lan- guages from the training set of the CoNLL-X multilin- gual dependency parsing shared task. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 1. The polarity classification (positive and negative) based on product aspect framework",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Opinion PageRank Performance with varying parameter \u00b5 (\u03bb = 0.2) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Morph features of frequent words and rare words as computed from the WSJ Corpus of Penn Treebank. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Meta-evaluation results at document and system level for submitted metrics",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Number of affected words by OOV- preprocessing ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Eq. 9: Log-likelihood. Eq. 10: Pseudolikelihood. In both cases we maximize w.r.t. \u03b8. Eqs. 11\u201313: Recursive DP equations for summing over t and a. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 Effect of pruning parameter Np and heuristic function on error rate for direct-translation model (tp = 10\u221212 ). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example of a long jump alignment grid. All possible deletion, insertion, identity and substitution op- erations are depicted. Only long jump edges from the best path are drawn. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Lexical Caseframe Expectations",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Accuracy for the 14-class task",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Performance analysis of HRGs, CWU, CWW & HAC for different parameter combinations (Table 2). (A) All combinations of p1 , p2 and p3 = 0.05. (B) All combinations of p1 , p2 and p3 = 0.09. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Number of unique entries in training and    test sets, categorized by semantic attributes ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Empirical results for the baseline models as well as BAYE S UM, when all query fields are used. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Effect on %BLEU of varying number of non-terminals ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 Split constituents: In this case, a single semantic role label points to multiple nodes in the original treebank tree. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Data similarity measures. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Model accuracy across the Brown cor- pus sections. ST: Stanford tagger, Wik: Wiktionary- tag-set-trained SHMM-ME, PTBD: PTB-tag-set-trained SHMM-ME, PTB: Supervised SHMM-ME. Wik outper- forms PTB and PTBD overall. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Feature templates used in R-phase. Ex- ample used is \u201c32 ddd\u201d. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Complexity Analysis of Algorithm 1.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5. Bootstrapping time for different p values ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10. MRRs for the phonetic transliteration 2",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Scores for MSRA corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5 Example of segmentation of German sentence and its English translation into alignment templates",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 11 The four types of changes. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Morphological Analysis/Generation as a Relation between Analyses and Words ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6 Evaluation of Feature and Their Combinations",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Under-sampled system for the task of rela- tion detection. The proportion of positive examples in the training and test corpus is 50.0% and 20.6% respectively. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 9: Chunk - Length and count of glue rules used decoding test set ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: extracts from the Akkadian project",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 The best two performing systems of each type (according to fine-grained recall) in Senseval-2 and -3. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9. MRRs of the phonetic transliteration",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10. Speci\ufb01c Subject and Object Agreement Rules",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2. The procedure of TBL entity track- ing/coreference model ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Parser performance on WSJ;23, supervised adaptation. All models use Brown;T,H as the out-of-domain treebank. Baseline models are built from the fractions of WSJ;2-21, with no out-of-domain treebank. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Experimental Results. C5.0 is supervised accuracy; Base         is      on random clusters.  set; Ling is manually selected subset; Seed is seed-verb-selected set. See text for further description. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Results of the mapping algorithm.",
        "Entity": "Caption"
    },
    {
        "Text": "hWRD(eI , f J , K , zK ) = log n p(ei | {fj | (i, j) A}, Ei ) (14) 1 1 1 1 i=1",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10 Translation results on the Hansards task",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Relation phrase compliance with semantic/lexical constraints [32]",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Results of MET2 under different configurations",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: The response of the rhyme search engine.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Simple parser vs full parser \u2013 syntactic quality. Trained on first 5,000 sentences of the training set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Three narrative events and the six most likely events to include in the same chain. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Rule evaluation examples and their judgment.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: The 10 best languages for the verb component of BANNARD using LCS. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Accuracy(%) of \u2018obscure\u2019 name recognition",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 Comparing feature descriptions. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Accuracies (%) for Coarse and Fine-Grained PSD, Using MALT and Heuristics. Sorted by preposition. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 10: Evaluation of translation from English on out-of-domain test data",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: The set of types and subtypes of relations used in the 2004 ACE evaluation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results on Penn (English) Treebank, Wall Street Journal, sentences with 100 words or fewer.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Derivation with Hierarchical model",
        "Entity": "Caption"
    },
    {
        "Text": "                     #name tokens/#all tokens(%) Figure 3: Word alignment gains according to the percentage of name words in each sentence. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results obtained by adding different types of features incrementally to the Baseline system.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Resolution accuracies for the ACE test set.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: BLEU score for those 25% utterances which resulted in different translations after bLSA adaptation (manual transcriptions) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 12 The set of new features. The last two columns denote the number and percentage of examples for which the value of the feature is non-zero in examples generated from the 23 gold-standard graphs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Examples of viterbi chunking and chunk alignment for English-to-Japanese translation model. Chunks are bracketed and the words with \u2217 to the left are head words. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: New generated hypotheses through n- gram expansion and one reference. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Impact of the topic cache size",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Comparison of dynamic context-sensitive tree span with SPT using our context-sensitive convolution tree kernel on the major relation types of the ACE RDC 2003 (inside the parentheses) and 2004 (outside the parentheses) corpora. 18% of positive instances in the ACE RDC 2003 test data belong to the predicate-linked category. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Shallow parsing: chunking (Extracted from: http://kontext.fraunhofer.de)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Comparison of different context-sensitive",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2. Modified Viterbi search \u2013 stop-word treatment",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Results when tuning for performance over the development set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Synonyms for chain",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Weights learned for word-context features, which fire when English word e is generated aligned to Chinese word f , with Chinese word f\u22121 to the left or f+1 to the right. Glosses for Chinese words are not part of features. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: General Knowledge Sources",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Semantic expansion example. Note that the expanded queries that were generated in the first two retrieved texts (listed under \u2018matched query\u2019) do not contain the original query. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Dev set frequencies for the two most significant discourse markers in Arabic are skewed toward analysis as a conjunction.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Parser performance on WSJ;23, baselines. Note that the Gildea results are for sentences \u2264 40 words in length. All others include all sentences. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: O\ufb03cial results for the English and Basque lexical tasks (recall). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Screenshot of ConAno",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: F1 scores of the local CRF and non-local models on the CMU Seminar Announcements dataset. We also provide the results from Sutton and McCallum (2004) for comparison. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Results of the fill-in-the-blank exercise",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: DP corpus comparison for OPUS features based on frequent vs. domain-relevant verbs ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Examples of positive and negative words. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Features used by paraphrase classifier.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results for the pronoun resolution",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Topics with MWEs",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The combined sequence and parse tree representation of the relation instance \u201cleader of a minority government.\u201d The non-essential nodes for \u201ca\u201d and for \u201cminority\u201d are removed based on the algorithm from Qian et al. (2008). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: BLEU scores as a function of development data size. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10 Alternatives to training on gold-only feature values. Top: Select MaltParser CORE 12+. . . models re-trained on predicted or gold + predicted feature values. Bottom: Similar models to the top half, with the Easy-First Parser. Statistical significance tested only for CORE 12+. . . models on predicted input: significance of the MaltParser models from the MaltParser CORE 12 baseline model, and significance of the Easy-First Parser models from the Easy-First Parser CORE 12 baseline. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Datasets for the two experimental conditions.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: How to get the ordered set B t (i, j, \u03b8)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Active sparse feature templates",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Training and test set sources, genres, sizes in terms of numbers of tokens, and unigram and bi- gram coverage (%) of the training set on the test sets. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Word alignment based translation model P(J, A|E) (IBM Model 4) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Evaluation of topic segmentation for the English corpus (Pk and WD as percentages) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Summary of supersense tagging accuracies",
        "Entity": "Caption"
    },
    {
        "Text": "Table 12 Accuracy comparisons between various joint segmentors and POS-taggers on CTB5. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Results on the FQ dataset.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Processing steps for the input sentence dire warnings from pentagon over potential defence cuts.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Bell tree representation for three mentions: numbers in [] denote a partial entity. In-focus entities are marked on the solid arrows, and active mentions are marked by *. Solid arrows signify that a mention is linked with an in-focus partial entity while dashed arrows indicate starting of a new entity. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Dependency Parsing: MWE results",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results for different predictor configura- tions. Numbers give % reductions in keystrokes. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: The contribution of MMVC in a rank-based classi- fier combination on S ENSEVAL -1 and S ENSEVAL -2 English as computed by 5-fold cross validation over training data ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8. Comparisons among different strategies on Medstract",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: (a) An undirected graph G representing the similarity matrix; (b) The bipartite graph showing three clusters on G; (c) The induced clusters U ; (d) The new graph G1 over clusters U ; (e) The new bipartite graph over G1 ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Test verbs and their monosemous/polysemic gold standard senses",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Meta-evaluation results at document level",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: BLEU scores for SparseHRM features. Notes in Table 2 also apply here.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Distribution of SCs in the ACE corpus.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 Varying the number of clusters (evaluation: Randadj ). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Three kinds of tree kernels.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Comparison of the existing efforts on ACE RDC task.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Rules for simplifying the morphological complexity for RU. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Rhetorical pattern of C-Colon",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 Overview of the results for the best algorithms for other-anaphora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Supersense evaluation results. Values are the percentage of correctly assigned supersenses. k indicates the number of nearest neighbours considered. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Size (in words) of reorderings (%) ob- served in training bi-texts. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Performance of the mention detection sys- tem using lexical, syntactic, gazetteer features as well as features obtained by running other named-entity classifiers ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Few examples of the untranslatable tokens in forum posts ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results of different systems on the CoNLL\u201912 English data sets.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Word prediction speed, in terms of the number of classified test examples per second, mea- sured on the three test sets, with increasing training examples. Both axes have a logarithmic scale. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: The Effects of Temporal Constraint",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Statistical Information of Corpora",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Sense-tagged corpus for the example in Figure 3",
        "Entity": "Caption"
    },
    {
        "Text": "Table 21 MSRSeg system results for the MSR test set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Corpus of complex news stories.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4. Performance with SVM trained on a fraction of adj. It shows 5 fold cross validation results.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Type-level Results: Each cell report the type- level accuracy computed against the most frequent tag of each word type. The state-to-tag mapping is obtained from the best hyperparameter setting for 1-1 mapping shown in Table 3. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11: German-English Official Test Submis- sion. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Framework for MWE acquisition from corpora",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 2. Procedure to mine key lexicons for each semantic type",
        "Entity": "Caption"
    },
    {
        "Text": "igure 2 Example of a (symmetrized) word alignment (Verbmobil task).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Topics are meaningful within languages but di-",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Performance of our proposed method (Spectral- based clustering) compared with other unsupervised methods: ((Hasegawa et al., 2004))\u2019s clustering method and K-means clustering. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: The increase in performance for successive variants of Bayes and Mixture Model as evaluated by 5-fold cross vali- dation on S ENSEVAL -2 English data ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Translation results for English-German",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. System performance on the is-a relation on the CHEM dataset. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A portion of the local co-occurrence graph for \u201cmouse\u201d from the SemEval-2010 Task 14 corpus ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Segmentation results on different languages. Results are calculated based on word types. For each language we report precision, recall and F1 measure, number of word types in the corpus and number of word types with gold standard segmentation available. For each language we report the segmentation result without and with emission likelihood scaling (without LLS and with LLS respectively). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: The values of AP, Spearman (\u03c1) and Kendall (\u03c4 ) correlations between the LSA-based and PMI-based model respectively and the Gold data with regards to the expression type. Every zero value in the table corresponds to the theoretically achieved mean value of correlation calculated from the infinite number of correlation values between the ranking of scores assigned by the annotators and the rankings of scores being obtained by a random number genarator. Reddy-WSM stands for the best performing WSM in the DISCO task (Reddy et al., 2011b). StatMix stands for the best performing system based upon association measures (Chakraborty et al., 2011). Only \u03c1-All and \u03c4 -All are available for the models explored by Reddy et al. (2011b) and Chakraborty et al. (2011). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Contribution of individual features to overall performance.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8 Algorithm min-jumps to compute the minimum number of needed jumps D(cJ , j) to complete the translation",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: GC examples.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Segmentation recall relative to gold word frequency. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 Estimation of F(c, f , v) and F(v, c). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: User Interface with Arabic Script Dis- play in Java. Mouse clicks on the virtual keyboard or key presses on the physical keyboard are inter- cepted, converted to Arabic Unicode characters, and stored in a buffer, which has a start and an end but no inherent ordering. The Arabic Canvas Object observes the buffer and contains an Ara- bic Scribe object that renders the string of Uni- code characters right-to-left as connected Arabic glyphs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Sources of Dictionaries",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: The solid line shows recall-at-1220 when com- bining the k best-performing bilingual statistics and three monolingual statistics. The dotted line shows the indi- vidual performance of the kth best-performing bilingual statistic, when applied in isolation to rank candidates. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Synthetic Data Set from Xinhua News",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Density of signature caseframes after merging to various threshold for the initial (Init.) and update (Up.) summarization tasks (Study 2). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Performance of our system versus a baseline",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Translation results for English\u2192French",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt. ",
        "Entity": "Caption"
    },
    {
        "Text": "= argmax M 1 S s=1 ) log p M (es | fs ) (4)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Final results on CTB-5j",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4. Comparison of semi-supervised relation classification systems on the ACE RDC 2003 corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Kendall\u2019s (\u03c4 ) correlation over WMT 2013 (all- en), for the full dataset and also the subset of the data containing a noun compound in both the reference and the MT output ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7 Algorithm for breadth-first search with pruning. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The density of the F1 -scores with the three approaches. The prior used is a symmetric Dirichlet with \u03b1 = 0.1. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8 Quasi-synchronous tree-to-tree configurations from Smith and Eisner (2006). There are additional configurations involving NULL alignments and an \u201cother\u201d category for those that do not fit into any of the named categories. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Performance comparison with the literature for context sensitive spelling correction ",
        "Entity": "Caption"
    },
    {
        "Text": "  Figure 5. Results for webpage snippet number. 7.3 Experiment on Multiple Feature Fusion To verify the effectiveness for multiple feature fusion, the test on the feature combination for OOV term translation is implemented. As shown in Table 1, the highest accuracy (the percentage of the correct translations in all the extracted translations) of 83.1367% can be ac- ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: F1s of some individual ILC classifiers and the overall multiclassifier accuracy (180 classes on PB and 133 on FN). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Time to read and accept or reject proposals versus their length",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Misspellings of receive",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 12 Interdigitation FSRA \u2013 general. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Word type coverage by normalized frequency: words are grouped by word count / highest word count ratio: low [0, 0.01), medium [0.01, 0.1), high [0.1, 1]. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8 Results of 70 high-frequency two-character CASs. \u2018Voting\u2019 indicates the accuracy of the baseline method that always chooses the more frequent case of a given CAS. \u2018ME\u2019 indicates the accuracy of the maximum-entropy classifier. \u2018VSM\u2019 indicates the accuracy of the method of using VSM for disambiguation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Translation quality.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Usefulness evaluation result",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Lexical features. Top part: Adding each feature separately; difference from CORE 12 (predicted). Bottom part: Greedily adding best features from previous part. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Per category performance of the Berkeley parser on sentence lengths \u2264 70 (dev set, gold segmentation). (a) Of the high frequency phrasal categories, ADJP and SBAR are the hardest to parse. We showed in \u00a72 that lexical ambiguity explains the underperformance of these categories. (b) POS tagging accuracy is lowest for maSdar verbal nouns (VBG,VN) and adjectives (e.g., JJ). Richer tag sets have been suggested for modeling morphologically complex distinctions (Diab, 2007), but we find that linguistically rich tag sets do not help parsing. (c) Coordination ambiguity is shown in dependency scores by e.g.,  S S S R  and  NP NP NP R .  NP NP PP R  and  NP NP ADJP R  are both iDafa attachment. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Precision, Recall and F1 Results (%) for Coarse-Grained Classification. Comparison to O\u2019Hara and Wiebe (2009). Classes ordered by frequency ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Stemmed results on 3,138-utterance test set. Asterisked results are significantly better than the baseline (p \u2264 0.05) using 1,000 iterations of paired bootstrap re-sampling (Koehn, 2004). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5. Error distribution of major types on both the 2003 and 2004 data for the compos- ite kernel by polynomial expansion ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6. Examples of transformation rules of",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Weights learned for inserting target English words with rules that lack Chinese words. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Accuracy by sentence length for Method 5 measured on separate grammatical and ungrammat- ical data: Gr = Grammatical, AG = Agreement, RW = Real-Word, EW = Extra Word, MW = Missing Word ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Comparison of the news and reports corpora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Weather Text Evaluations.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 Models with functional features: GENDER, NUMBER, rationality (RAT). F N * = functional feature(s) based on Alkuhlani and Habash (2011); GN = GENDER + NUMBER ; GNR = GENDER + NUMBER + RAT . Statistical significance tested only for CORE 12+. . . models on predicted input, against the CORE 12 baseline. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Performance of the mention detection sys- tem using lexical features only. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: TV show types: The distribution of show types in a large database of TV shows (1067 shows) that has been recorded over the period of a couple of months until April 2000 in Pittsburgh, PA ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Tradeoffs of precision and recall values in the experiments with method 1 using various different pa- rameters. When the unigram filter is applied the precision is much better, but the recall drops. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Combined systems (English) in cross- validation, best recall in bold. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Re-ordering for the German verbgroup.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 24 Accuracies of our phrase-structure parser on CTB5 using gold-standard and automatically assigned POS-tags. ",
        "Entity": "Caption"
    },
    {
        "Text": "  Table 1. Data sets used for our alignment quality experiments. The total number of sentences in the respective corpora are given along with the number of sentences and    gold-standard (S)ure and (P)ossible alignment links in the corresponding test set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11 Corpus statistics for Chinese\u2013English corpora\u2014large data track (Words*: words without punctuation marks). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 15 Training the MaltParser on gold tags, accuracy by gold attachment type (selected): subject, object, modification (of a verb or a noun) by a noun, modification (of a verb or a noun) by a preposition, idafa, and overall results (repeated). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: The effect of syntactic features when predicting morphological information. * mark statistically signifi- cantly better models compared to our baseline (sentence- based t-test with \u03b1 = 0.05). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Recall (R), Precision (P) and Mean Average Pre- cision (MAP) when only matching template hypotheses directly. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Weather Sentence Evaluations.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Part-of-speech tags of the Penn Chinese   Treebank that are referenced in this paper.      Please see (Xia, 2000) for the full list. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Counts of the number of files, sentences (Sent), original space-delimited tokens (Tok), ATB tree tokens (Tree Toks), and affixes in the experimental data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: cumulative distribution of frequency (CDF) of the relative ranking of model-predicted probability of being positive for false negatives in a pool mixed of false negatives and true negatives; and the CDF of the relative ranking of model-predicted probability of being negative for false positives in a pool mixed of false positives and true positives. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 Percentage of correct entailments within the top 40 candidate pairs of each of the methods, LIN and Bootstrapped LIN (denoted as LINB in the \ufb01gure), when using varying numbers of top-ranked features in the feature vector. The value of \u201cAll\u201d corresponds to the full size of vectors and is typically in the range of 300\u2013400 features. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: An ATB sample from the human evaluation. The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a). But the city name Sharm Al- Sheikh is also iDafa, hence the possibility for the incorrect annotation in (b).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Learning curve with different sizes of labeled data ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Results for the non-pronoun resolution",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Organisation of the hierarchical graph of concepts",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Graph of the one-to-one accuracy of our full model (+FEATS) under the best hyperparameter setting by iteration (see Section 5). Performance typically stabi- lizes across languages after only a few number of itera- tions. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. The decision tree for ec+mc+z\u017e, learned by C5. Besides pairwise agreement be- tween the parsers, only morphological case and negativeness matter. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Top patterns chosen under different scoring schemes",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Example of a word lattice",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Dynamic-Expansion Tree Span Scheme",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Discourse tree for two sentences in RST-DT. Each of the sentences contains three EDUs. The second sentence has a well-formed discourse tree, but the first sentence does not have one. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: An example of NE and non-NE",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Missing argument examples of biological interactions ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Distribution over number of hits",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Comparison of Moses and KIT phrase extraction systems ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Performance on Ar-En with basic (left) and sparse (right) feature sets on MT05 and MT08.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: An example of alignment units",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Syntactic dependency scheme used in this work. Labels that aren\u2019t self-explanatory or similar to the labels used by Tratz and Hovy (2011) for English or CATiB for Arabic (Habash and Roth, 2009) are in bold (for completely new relations) or italics (for similarly named but semantically different relations) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Statistics and Name Distribution of Test Data Sets.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Identified metaphorical expressions for the mappings FEELING IS FIRE and CRIME IS A DISEASE ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Counts for the POS tags mentioned in Table 5.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Examples of aggregated instances.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Proposed method: data flow.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Combination results (using SVMacc)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Smoothed F1-Measure curves over the five corpora.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example of a term construction rule as a branch in a decision tree.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Comparison of our system with the best-reported systems on MUC-6 and MUC-7",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Three most distinctive topics are displayed. The English words for each topic are ranked according to p(e|z) estimated from the topic-specific English sentences weighted by {\u03c6dnk }. 33 functional words were removed to highlight the main content of each topic. Topic A is about Us-China economic relationships; Topic B relates to Chinese companies\u2019 merging; Topic C shows the sports of handicapped people. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Features Used for Initial Distribution",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Feature interpolation of translation models: A=ICTCLAS, B=dict-hybrid, C=dict-PKU-LDC, D=dict-CITYU, E=CRF-AS",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 ESA and input/output data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Parsing accuracy (AS = attachment score, EM = exact match; U = unlabeled, L = labeled)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Accuracy for induced verb classes.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 Results of all distributional similarity measures when tuning K over the development set. We encode the description of the measures presented in Table 2 in the following manner\u2014 h = health-care corpus; R = RCV1 corpus; b = binary templates; u = unary templates; L = Lin similarity measure; B = BInc similarity measure; pCt = pair of CUI tuples representation; pC = pair of CUIs representation; Ct = CUI tuple representation; C = CUI representation; Lin & Pantel = similarity lists learned by Lin and Pantel. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results on TAC 2010 entity extraction with N - best mapping for N = 1 and N = 5. Intermediate values of N produce intermediate results, and are not shown for brevity. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: ROUGE F-scores for different systems",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Semantic Caseframe Expectations",
        "Entity": "Caption"
    },
    {
        "Text": " Table 4. Performance of English system with system mentions and system relations ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Nouns and verbs supersense labels, and short description (from the Wordnet documentation).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Participants in the shared task. Not all groups participated in all translation directions.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: The Performance of SVM and LP algorithm with different sizes of labeled data for relation detection on relation subtypes. The LP algorithm is run with two similarity measures: cosine similarity and JS divergence. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Distribution of isolated vs. initial posi- tion for the most frequent lexical items ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Summary of the previous work on coreference resolution that employs the learning algorithms, the clustering algorithms, the feature sets, and the training instance creation methods discussed in Section 3.1. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Effect of pruning parameter tp and heuristic function on search efficiency for direct-translation model (Np = 50,000). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Arabic Verbal Inflection",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Results obtained by a combination of the best statistical and knowledge-based configuration. \u2018Best- Single\u2019 is the best precision or recall obtained by a sin- gle measure. \u2018Union\u2019 merges the detections of both approaches. \u2018Intersection\u2019 only detects an error if both methods agree on a detection. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A NE detection window",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 6. Algorithm for fuzzy divisive clustering based on nouns.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Propagation: All items",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11 Model accuracy using unequal distribution of verb frequencies for the estimation of P(c). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Opinion Question Answering System",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: MM and MMVC performance by performing 5- fold cross validation on S ENSEVAL -2 data for 4 languages ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Closed test, in percentages (%)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Acceptance rates for a noun phrase in the course of iteration. All models were with back-off mix- ing (+BM). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Comparative performance on MSRPC. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 20 Experimental results for large-scale English-to-Japanese translation",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: One possible breakdown of spoken Arabic into dialect groups: Maghrebi, Egyptian, Levantine, Gulf and Iraqi. Habash (2010) gives a breakdown along mostly the same lines. We used this map as an illustration for annotators in our dialect classification task (Section 3.1), with Arabic names for the dialects instead of English. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Source span lengths",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: The incompleteness of Freebase (* are must- have attributes for a person). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Comparison of our method (FEATS) to state-of-the-art methods. Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): The KM model uses a variety of orthographic features and employs the EM or LBFGS optimization algorithm; Posterior regulariation model (Grac\u0327a et al., 2009): The G10 model uses the posterior regular- ization approach to ensure tag sparsity constraint. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Lexicon-based phrase labeling",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Accuracy on Section 1 of the FraCaS suite. Problems are divided into those with one premise sen- tence (44) and those with multiple premises (30). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7 Accuracy of different methods in predicting OOV words polarity. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Related research integrating context into alternative SMT models",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: A comparison of translation quality of Egyptian, Levantine, and MSA web text, using various training corpora. The highest BLEU scores are achieved using the full set of dialectal data (which combines Levantine and Egyptian), since the Egyptian alone is sparse. For Levantine, adding Egyptian has no effect. In both cases, adding MSA to the dialectal data results in marginally worse translations. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Confusion matrix among subtypes of ArgM, defined in Table 1. Entries are fraction of all ArgM labels. Entries are a fraction of all ArgM labels; true zeros are omitted, while other entries are rounded to zero. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10 Comparing selectional preference frame definitions. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Number of candidates for each target                 language. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 22 Cross-system comparison results. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results on G EO with 250 training and 250 test examples. Our results are averaged over 10 random 250+250 splits taken from our 600 training examples. Of the three systems that do not use logical forms, our two systems yield significant improvements. Our better sys- tem even outperforms the system that uses logical forms. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: 14 classes used in Joanis et al. (2008) and their corresponding Levin class numbers ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Contribution of combining the dynamic",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6. The performance on the set of unknown",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9: The effect of gender detection schemes",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Sizes of rule application test set for each learned rule-set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Part of a sample headline cluster, with aligned paraphrases ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results for morphological processing, English\u2192German ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: A summary of the parsing and evaluation sce- narios. X depicts gold information, \u2013 depicts unknown information, to be predicted by the system. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Are the single most probable words for a given",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5. The number of OAS (types), CAS (types), LUW (types) and EIW (types) for our CWS. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 7 BLEU learning curves (left) and difference curves (right) comparing the Moses baseline against two IGTree (LTAG\u00b11 and PR) and TRIBL (Super-Pair\u00b11 and PR) classifiers ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Results of system combination on Dev7 (development) corpus and Test09,             the o\ufb03cial test corpus of IWSLT\u201909 evaluation campaign. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Numbers of expressions of all the differ- ent types from the DISCO and Reddy datasets. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Dependency structure of a sentence.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 23 Accuracies of various phrase-structure parsers on CTB2 with automatically assigned tags. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4. Comparison of Unsupervised Learning                   Methods ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Experiments on the threshold\u2013partial recall relationship of the small corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Semantic features.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Performance comparison on the ACE 2004 data over the 7 relation types. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Association frequencies for target verb.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Sorted frequency of tags for WSJ. The gold standard distribution follows a steep exponential curve while the induced model distributions are more uniform. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Performance of knowledge-based approach using different relatedness measures. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 6. F-measure for the objective and subjective classes for multilingual bootstrapping (versus cross-lingual framework).",
        "Entity": "Caption"
    },
    {
        "Text": "                                   3 .  (7     $    %  19: 6       7( Figure 2 Error analysis example. . . . )  82                                 mrt \u00c2yAm \u03c2 l\u00fd A\u030cxtfA\u2019 Alzmyl Almhnd . . . (\u2018Several days have passed since the disappearance of the colleague the engineer . . . \u2019), as parsed by the baseline system using only CORE 12 (left) and as using the best performing model (right). Bad predictions are marked with <<< . . . >>>. The words in the tree are presented in the Arabic reading direction (from right to left). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Entries from the English-Slovene sense cluster inventory.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Screenshot of the main BRAT user-interface, showing a connection being made between the annotations for \u201cmoving\u201d and \u201cCitibank\u201d. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Training corpus statistics (* without punctuation marks). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: The best results (per F1 -score of the two meth- ods). The parameters of method 1 included using only those string transformations that occur at least 2 times in the training data, and limiting rule application to a maxi- mum of 2 times within a word, and including a unigram post-filter. Rules were contextually conditioned. For method 2, all the examples (threshold 1) in the training data were used as positive and negative evidence, with- out a unigram filter. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: A text segment from MUC-6 data set",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Performance on the test set. Scores are on gold mentions. Stars indicate a statistically significant difference with respect to the baseline. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Properties of the training and test sets used in the shared task. The training data is the Europarl cor- pus, from which also the in-domain test set is taken. There is twice as much language modelling data, since training data for the machine translation system is filtered against sentences of length larger than 40 words. Out-of-domain test data is from the Project Syndicate web site, a compendium of political commentary. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Rule expansion with minimal context (Example 3)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Oracle lower-case BLEU",
        "Entity": "Caption"
    },
    {
        "Text": "The joint probability of the two sequences is defined as the product of context probabilities and lexical probabilities over all POS tags:",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Summary of LINGUA performance",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Feature blending of translation models",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Rhetorical pattern of C-Semicolon",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Using the type model for disambiguation in the derivation of file a suit. Type distributions are shown after the variable declarations. Both suit and the object of file are lexically ambiguous between different types, but after the \u03b2 -reduction only one interpretation is likely. If the verb were wear, a different interpretation would be preferred. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Evaluation results for all combinations of mixture adapted language and translation models: Baseline(bl) scores are italicized, best scores are in bold ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Accuracy of our system in each period (M = 10)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11 The comparative error rates of the pseudo-disambiguation task for the three examined similarity measures, with and without applying the bootstrapped weighting for each of them. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Some words extracted from the large corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7. Performance Comparison of Combined              Model and KLD Model ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 shows the result of varying the number of samplers and iterations for all",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Most frequent phrase dependencies with at least 2 words in one of the phrases (dependencies in which one phrase is entirely punctuation are not shown). $ indicates the root of the tree. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Russian to English machine translation system evaluated on tst2012 and tst2013. Human evaluation in WMT13 is performed on the system trained using the original corpus with TA-GIZA++ for alignment (marked with *) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example of the effects of OOV processing for German\u2192English",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Final UAS/LAS scores for dependencies on the test sets for the predicted setting. Other denotes the highest scoring other participant in the Shared Task. ST Baseline denotes the MaltParser baseline provided by the Shared Task organizers. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 3: NEs after agents-based modification",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2. Growing Algorithm for Language              Model Pruning ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Most frequent monosemic words in BG",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 Results per concept for the ILP-Global. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. The highest performance of applying various sampling strategies in selecting the initial seed set on the ACE RDC 2004 corpus ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Example Translations for the Verbmobil task.",
        "Entity": "Caption"
    },
    {
        "Text": "= argmax S I n s=1 a l) p (fs , a | es ) (7)",
        "Entity": "Caption"
    },
    {
        "Text": "  Figure 2. Automatically detected posture points (H = headDepth, M = midTorsoDepth, L = lowerTorsoDepth) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Domain specific results",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 11: MUC-6: Level Distribution of Each of the Six Facts",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: a) A related work section extracted from (Wu and Oard, 2008); b) An associated topic hierar- chy tree of a); c) An associated topic tree, annotated with key words/phrases. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: One of the 69 test documents, containing 10 narrative events. The protagonist is President Bush. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Two characteristic topics for the Y slot of \u2018acquire\u2019, along with their topic-biased Lin sim- ilarities scores Lint , compared with the original Lin similarity, for two rules. The relevance of each topic to different arguments of \u2018acquire\u2019 is illus- trated by showing the top 5 words in the argument           y vector vacquire for which the illustrated topic is the most likely one. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Mention Detection Results",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4. TSVM optimization function for non-separable case (Joachims, 1999) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Results of 1000 sentences",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Example of original hypotheses and 3- grams collected from them. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Example patterns for parallelism",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A tree showing head information",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10 Translation results on the Hansards task. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Translation of PCC sample commentary",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 Experiments on the word length\u2013precision relationship of the large corpus with threshold three. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Example segmentations (\u201e|\u201f indicates the       separator between adjacent snippets) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Contribution of each feture. ALL: all features, PER: perceptron model, WLM: word language model, PLM: POS language model, GPR: generating model, LPR: labelling model, LEN: word count penalty. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: EuroParl topics (T=400)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Improvement in (gold mention) RE.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Evaluation of the GUITAR improvement - summarization ratio: 30%.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Multiple Analyses for suis",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Inuktitut: Parimunngaujumaniralauqsimanngittunga = \u201cI never said I wanted to go to Paris\u201d ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: (a) An undirected graph G representing the similarity matrix; (b) The bipartite graph showing three clusters on G; (c) The induced clusters U ; (d) The new graph G1 over clusters U ; (e) The new bipartite graph over G1 ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Syntactic features for featurama (Czech). * mark statistically significantly better models compared to feat- urama (sentence-based t-test with \u03b1 = 0.05). ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 1. System architecture overview",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4 A comparison between ILP-Global and ILP-Local for two fragments of the test-set concept seizure. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: BiTAM models for Bilingual document- and sentence-pairs. A node in the graph represents a random variable, and a hexagon denotes a parameter. Un-shaded nodes are hidden variables. All the plates represent replicates. The outmost plate (M -plate) represents M bilingual document-pairs, while the inner N -plate represents the N repeated choice of topics for each sentence-pairs in the document; the inner J-plate represents J word-pairs within each sentence-pair. (a) BiTAM-1 samples one topic (denoted by z) per sentence-pair; (b) BiTAM-2 utilizes the sentence-level topics for both the translation model (i.e., p(f |e, z)) and the monolingual word distribution (i.e., p(e|z)); (c) BiTAM-3 samples one topic per word-pair. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 13 Descriptive statistics for WordNet hyp/syn relations on the coreference data set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Parser performance on Brown;E, supervised adaptation",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Standards and corpora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: OT grammar for devoicing compiled into an FST. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results of segmentation of entry titles (F-score (precision/recall)).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3 Back-off lattice with more specific distributions towards the top. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7: A Path in a Transducer for English",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Comparison results with TAC 2008 Three Top Ranked Systems (system 1-3 demonstrate top 3 systems in TAC) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Key notation. Feature factorings are elaborated in Tab. 2.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Correlation between cohesion-driven functions.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Effect of pruning parameter tp and heuristic function on error rate for direct-translation model (Np = 50,000).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Trivial and single-feature baselines (using SVM- acc unless noted otherwise) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Possible relations appearing on the edges of a DCS tree. Here, j, j 0 \u2208 {1, 2, . . . } and i \u2208 {1, 2, . . . }\u2217 . ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Training phase: systems outperforming the baseline in terms of TRDR score. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 1. Fuzzy hierarchical clustering for Paraphrase Extraction.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Comparison of Number of Bigrams            at F-Measure 96.33% ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3. Calculation of \"Importance\"              of Bigrams ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Example of first and second order features using a predefined n-gram size of 2.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Language Model Pruning Algorithm",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 9 Illustration of the IBM-style reordering constraint. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9: Performance comparison with the literature for compound bracketing ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Experiment 2: Results by training set size, \u03b8 = 1.0 ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Results of negated event/property detection on gold standard cue and scope annotation ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11 Distribution of antecedent NP types for definite NP anaphora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Evaluation closed results on all data sets",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3 Positional sentence weight for varying",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Average scores for different language pairs. Manual scoring is done by different judges, resulting in a not very meaningful comparison. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6 Comparison between three different decoders for word segmentation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11: Cross Framework Evaluation: Unlabeled TedEval on generalized gold trees in gold scenario, trained on 5k sentences and tested on 5k terminals. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Tagging accuracy on development data depending on context size ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 8 Average number of target phrase distribution sizes for source phrases for TRIBL and IGTree com- pared to the Moses baseline ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results for the submitted runs",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Plate diagram depicting the morphology model (adapted from Goldwater et al. (2006)). Hyperparameters have been omitted for clarity. The left-hand plate depicts the base distribution P0 ; note that the morphological anal- yses lk are generated deterministically as (tk , sk , fk ). The observed words wi are also deterministic given zi = k and lk , since wi = sk \u2295 fk . ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Cross-source Comparable Data Example (each morph and target pair is shown in the same color) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 A framework for jointly identifying and aligning bilingual NEs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Comparison of results for MUC6",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Duration (in seconds) of each lexical type ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Connected components nearest neighbour (NN) clustering. D is the Kullback-Leibler distance. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 1. Integration of confidence measures \u2013 recall/precision curves (figures in the legend correspond to resp. \u03b41 and \u03b42 ).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 The official vocabularies in Verbmobil. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Dev set learning curves for sentence lengths \u2264 70. All three curves remain steep at the maximum training set size of 18818 trees.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 14 Overview of the results for all WordNet algorithms for coreference. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Comparing disagreements between the best local and global algorithms against the gold standard ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Syntactic Seeding Heuristics",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 %BLEU on tune and test sets for ZH\u2192EN translation, showing the contribution of feature sets in our QPD model. Both QPD models are significantly better than the best Moses numbers on test sets 1 and 2, but not on test set 3. The full QPD model is significantly better than the version with only T GT T REE features on test set 1 but statistically indistinguishable on the other two test sets. Hiero is significantly better than the full QPD model on test set 2 but not on the other two. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Unbalanced vs. balanced combining. All runs ignored the context. Evaluated on the Test data set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Similarity graph after its sparsification",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Improvements of different tree setups",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Performance of various clustering-based seed sampling strategies on the held-out test data with the optimal cluster number for each clustering algorithm ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4. Accuracy of 5-fold cross-validation with self-              extracted semantic features ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Example translations from the different methods. Boldface indicates correct translations. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: NIST BLEU scores on the German-English (de- en) and French-English (fr-en) Europarl test2008 set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Using an oracle",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Gross statistics for several different treebanks.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results of paraphrases evaluation for 100 sentences in French using English as the pivot lan- guage. Comparison between the baseline system MOSES and our algorithm MCPG. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2. Percentage of examples of major syntactic classes.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 10: Analysis of NP coordination, in a distributive (left) and a collective interpretation (right).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Improvement in (predicted mention) RE.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Translation performance of baseline and bLSA-Adapted Chinese-English SMT systems on manual transcriptions and 1-best ASR hypotheses ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6 Models with inflectional and lexical morphological features together (predicted value-guided heuristic). Statistical significance tested only on predicted input, against the CORE 12 baseline. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Extending training sets: an example",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The NLM-WSD test set and some of its sub- sets. Note that the test set used by (Joshi et al., 2005) comprises the set union of the terms used by (Liu et al., 2004) and (Leroy and Rindflesch, 2005) while the \u201ccom- mon subset\u201d is formed from their intersection. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 4. Performance of the training algorithms, supervised against semi-supervised techniques. The semi-supervised precision is evaluated indirectly by using the predicted dataset as train-set against the human-annotated manual small dataset. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Example derivation for the query \u2018how many people visit the public library of new york annu- ally.\u2019 Underspecified constants are labelled with the words from the query that they are associated with for readability. Constants from O, written in typeset, are introduced in step (c). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Results on three query categories.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results on the STS video dataset.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Related research integrating context into word-based SMT (WB-SMT) models",
        "Entity": "Caption"
    },
    {
        "Text": "Table 13 Example translations for the translation direction English to German using three different reordering constraints: MON, EG, and S3. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Consistently formatted term translation                       pairs ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 29: Different Arabic Transliterations of \"Los Angeles\"",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Learning curve of the hierarchical strategy and its comparison with the flat strategy for some      major relation subtypes (Note: FS for the flat strategy and HS for the hierarchical strategy) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Performance of different FS machines in terms of the percentage of unclassified entries. All the classified entries were correctly classified, yielding, as a result, a precision of 100%. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Dependency graph for Czech sentence from the Prague Dependency Treebank1",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 Scenarios in which we added hard constraints to the ILP. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: S/O classifier with and without SWSD.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Symmetry of window size",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: A sentence decomposed into its depen- dency edges, and the caseframes derived from those edges that we consider (in black). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Performance of WSD system using various combinations of learning algorithms and features.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Optimized Edit Costs",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Words (excluding multiwords) in WordNet 1.7.1 and the BNC without any data in SemCor. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Feature templates used for the chunk s := ws ws+1 ... we where ws and we represent the words at the beginning and ending of the target chunk respectively. pi is the part of speech tag of wi and sci is the shallow parse result of wi . ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Graphical representation of our model. Hyper- parameters, the stickiness factor, and the frame and event initial and transition distributions are not shown for clar- ity. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 17 Example translations for the translation direction French to English using the S3 reordering constraint. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 Top 60 most frequent root phrases in DE\u2192EN data with at least two words, shown with their counts. Shown in bold are the actual root words in the lexical dependency trees from which these phrases were extracted; these are extracted along with the phrases and used for back-off features. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results for the acquisition of subcategori- sation frames. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 13 Example translations for Chinese\u2013English MT. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6 Baseline + Word Clustering by Relation +           Re-ranking by Coreference +              Re-ranking by Relation ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 Polysemous word types in the Senseval-2 and -3 English all-words tasks test documents with no data in SemCor (0 columns), or with very little data (\u2264 1 and \u2264 5 occurrences). Note that there are no annotations for adverbs in the Senseval-3 documents. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Example of trellis of the modified Viterbi search",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Word-to-word alignment.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Evaluation of the GUITAR system without DN detection over a hand-annotated treebank ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Examples of templates suggested by DIRT and TEASE as having an entailment relation, in some direction, with the input template \u2018X change Y \u2019. The entailment direction arrows were judged manually and added for readability. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 The results of setting 1 (Punctuation and other encoding information are not used; the maximum length is 30). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 Effect of pruning parameter Np and heuristic function on error rate for direct-translation model (tp = 10 12 ).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: The Wiktionary vs. tree bank tag sets. Around 90% of the Wiktionary tag sets are identical or subsume tree bank tag sets. See text for details. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Pearson\u2019s (r) correlation results over the WMT all-en dataset, and the subset of the dataset that contains noun compounds ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Performance of Paraphrase Recognition system on MSRPC [25]. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Sequence of POS-tagged units used to estimate the bilingual n-gram LM. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A Dictionary based Word Graph",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Best LO and LL configurations scores",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: A noun-phrase with sub-structure",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Accuracy for Different Part-Of-Speech",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The first set of features in our model. All of them are binary. The final feature set includes two sets: the set here, and a set obtained by its conjunction with the verb\u2019s lemma. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Examples of SMT errors due to MWEs.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Key notation.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Disambiguation results.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: A toy instance of lattice construction",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 FT detection results on the MSR gold test set. The \u2018All\u2019 column shows the results of detecting all 10 types of factoids, as described in Table 1, which amount to 6630 factoids, as shown in Table 3. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 \u2013 Precision/recall for Le Monde corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Sample targets for meta alternations with high AP and mid-coherence values. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Examples of signature caseframes found in Study 2. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Notation and signatures for our framework.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Examples of unigram and bigram features extracted from Figure 1. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1                                CATiB Annotation example. &  ( -  )  23 1+                                                        ,    4   !       $%                                                                          ./0   t\u03c2 ml HfydAt AlkA                                                                        Al\u00f0kyAt fy AlmdArs AlHkwmy  (\u2018The writer\u2019s smart      granddaughters work for public schools\u2019). The words in the tree are presented in the Arabic reading direction (from right to left). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Speed in lookups per microsecond by data structure and number of 64-bit entries. Performance dips as each data structure outgrows the processor\u2019s 12 MB L2 cache. Among hash tables, indicated by shapes, probing is initially slower but converges to 43% faster than un- ordered or hash set. Interpolation search has a more ex- pensive pivot function but does less reads and iterations, so it is initially slower than binary search and set, but be- comes faster above 4096 entries. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Time comparison between FSAs and FSRAs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Results of the rule-based model: best guess",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: File formats. Trees (a) and (b) are aligned constituency and dependency trees for a mockup English example. Boxed labels are shared across the treebanks. Figure (c) shows an ambiguous lattice. The red part represents the yield of the gold tree. For brevity, we use empty feature columns, but of course lattice arcs may carry any morphological features, in the FEATS CoNLL format. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 11: Two Paths in the Initial Malay Transducer Defined via Concatenation",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: English parse tree with empty elements marked. (a) As annotated in the Penn Treebank. (b) With empty elements recon\ufb01gured and slash categories added. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Baseline: Out-of-the-box BerkeleyParser performance on the dev-set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Listing of all seeds used for KEdis and KEpat , as well as the top-10 entities discovered by ES-all on one of our test folds. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: CoreLex\u2019s basic types with their corresponding WordNet anchors. CAM adopts these as meta senses.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Taxonomy of Chinese words used in developing MSRSeg. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep (\u2021 : p < 0.01). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 13 The accuracy/speed tradeoff graph for the phrase-structure parser. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 The accuracies of various word segmentors over the second SIGHAN bakeoff data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10 NW 21 identification results on PK test set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Framework overview.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Latent Dependency coupling for the RE task. The D-C ONNECT factor expresses ternary connection re- lations because the shared head word of the proposed re- lation is unknown. As is convention, variables are repre- sented by circles, factors by rectangles. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Impact of scaling techinques (ILP\u2212 /ILPscale ).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2 A character sequence and its subsequence pairs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9: Dev set results for sentences of length \u2264 70.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Accuracy of the proposed system on a set which contains misspelled queries that the users had reformulated ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 13 Sample of human-interpretable Arabic TSG rules. Recursive rules like MWA\u2192A MWA result from memoryless binarization of n-ary rules. This pre-processing step not only increases parsing accuracy, but also allows the generation of previously unseen MWEs of a given type. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 Examples of word alignment patterns in German\u2013English that require the increased expressive power of synchronous tree adjoining grammar. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Statistics for Verbmobil task: training corpus (Train), conventional dictionary (Lex), development corpus (Dev), test corpus (Test) (Words*: words without punctuation marks). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Linking FrameNet frames and VerbNet classes ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: System Architecture.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9: German-to-English Final System Results.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Comparison our closed results with the top three in all test sets",
        "Entity": "Caption"
    },
    {
        "Text": "Table 15 Experimental results on the WMT 2009 test set",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results for all experiments",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Scores for UPUC corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: The 10 best languages for the particle compo- nent of BANNARD using LCS. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11: Effects of Popularity of Morphs",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Experiment 1: Results for label unknown sense, WSD confidence level approach. \u03b8: confi- dence threshold. \u03c3: std. dev. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: In-domain (first column) and out-of-domain performance (columns two to four) on ACE 2005. PET and BOW are abbreviated by P and B, respectively. If not specified BOW is marked. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 Model accuracy using equal distribution of verb frequencies for the estimation of P(c). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Unsupervised and Supervised scores on the SemEval-2010 WSI Task for each feature and clustering models, with reference scores for the top performing systems for each evaluation shown below. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Recall on different types of empty categories. YX = (Yang and Xue, 2010), Ours = split 6\u00d7. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6 Performance of proposed system on MSRVDC Dataset 1. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Distribution of activity types: Both databases contain a lot of discussing, informing and story-telling activities however the meeting data contains a lot more planning and advising. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Precision-recall curve for rescoring",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Dependency accuracy on 13 languages. Unlabeled (UA) and Labeled Accuracy (LA). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Devtest Set Statistics by Language",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Precision for 200 candidates (Ev.Rec).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: F-measure for the objective and subjective classes for cross-lingual bootstrapping ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Processing time for POS tagging of known words using contextual features (In CPU seconds). Train: training time over     + sentences. Brill\u2019s learner was interrupted after 12 days of train- ing (default threshold was used). Test: average number of seconds to evaluate a single sentence. All runs were done on the same machine. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8: The actual output of our parser trained with a fully annotated treebank. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: BLEU-4 scores of different systems",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: A complex Turkish-English word alignment (alignment points in gray: EM/PY-U(V ); black: PY- U(S)). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Results of IE Experiment",
        "Entity": "Caption"
    },
    {
        "Text": "  Figure 3. High TF/ITF words in \u201cCom-Com\u201d (Numbers are TF/ITF score, frequency in the collec-  tion (TF), frequency in the corpus (TF) and word) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Details of the corpora. W.T. represents word types; C.T. represents character types; S.C. represents simpli\ufb01ed Chinese; T.C. represents traditional Chinese. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Comparison of three statistical translation approaches (test on text input: 251 sentences = 2197 words + 430 punctuation marks). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Word segmentation on IWSLT data sets",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: An example for the \u201cBMES\u201d representa- tion. The sentence is \u201c\u6211\u7231\u5317\u4eac\u5929\u5b89\u95e8\u201d (I love Bei- jing Tian-an-men square), which consists of 4 Chi- nese words: \u201c\u6211\u201d (I), \u201c\u7231\u201d (love), \u201c\u5317\u4eac\u201d (Beijing), and \u201c\u5929\u5b89\u95e8\u201d (Tian-an-men square). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: ROUGE-W measures in EM learning",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Tagging accuracies on development data in percent.",
        "Entity": "Caption"
    },
    {
        "Text": "hLEX(eI , f J , K , zK ) = #CO-OCCURRENCES (LEX, eI , f J ) (20) 1 1 1 1 1 1",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Using different amounts of annotated training data for the article meta-classifier. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. NPs in a sample from the Catalan training data (left) and the English translation (right). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Precision-recall curve for the algorithms.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Example of the Character Tagging Method: Word boundaries are indicated by vertical lines (\u2018|\u2019).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Average polysemy on SE2 and SE3",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Dataset characteristics including the number of documents, annotated CEs, coreference chains, annotated CEs per chain (average), and number of documents in the train/test split. We use st to indicate a standard train/test split. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Example patterns of proteins and their do- mains ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 14 Comparison scores for HK open and AS open. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Ironic tweets that received every party and their election results. The \ufb02uctuation describes the difference between the May 2012 election results and the previous. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Performance of our system on the evalu- ation set ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Incremental evaluations, by incrementally adding new features (word features and high dimensional edge features), new word detection, and ADF training (replacing SGD training with ADF training). Number of passes is decided by empirical convergence of the training methods. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Results for two kinds of headlines",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 A general architecture for paraphrasing approaches leveraging the distributional similarity hypothesis. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Examples of constructing Universal POS tag sets from the Wiktionary.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Statistics on the Italian EVALITA 2009       and English CoNLL 2003 corpora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Baseline results for human word lists. Data: 700 positive and 700 negative reviews.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: V-Measure and paired FScore results for different partitionings of the dendrogram. The dashed vertical line indicates SP D ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Breadth-first beam search algorithm of Och and Ney (2004). Typically, a hypothesis stack H is maintained for each unique source coverage set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Evaluating Word Alignment Accuracies and Machine Translation Qualities for BiTAM Models, IBM Models, HMMs, and boosted BiTAMs using all the training data listed in Table. 1. Other experimental conditions are similar to Table. 4. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Equation 1 settings",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: F-score of two segmenters, with (\u2212) and without (+) word token/type features. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Structure of a typical two-pass ma- chine translation system. N-best translations are generated by the decoder and the 1-best transla- tion is returned after rescored with additional feature functions. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 DP-TSG notation. For consistency, we largely follow the notation of Liang, Jordan, and Klein (2010). ",
        "Entity": "Caption"
    },
    {
        "Text": "I exp[ m=1 m hm (e 1 , f1 )]",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Propagation: Core items",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Entity type constraints.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Manual analysis of suggested corrections on CLC data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: The performance of different resolution systems",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Op. Target - Op. Word Pair Extraction",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The Stanford parser (Klein and Manning, 2002) is unable to recover the verbal reading of the unvocalized surface form 0 an (Table 1)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: The BLEU score of self-trained cascaded trans- lation model under five initial training sets. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6. Feature impact experiments",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: General Knowledge Sources",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Perplexity measured on nt08 with the baseline LM (std), with the LM estimated on the sampled texts (generated texts), and with the inter- polation of both. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2. Self-Training for Name Tagging",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: The System Performance of Integrating Cross Source and Cross Genre Information. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Training phase: effect of similarity thresh- old (a) on Ave. MRR and TRDR. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: good#a#15 SentiWN scores.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Changing a decision in the derivation lattice. All paths generate the observed data. The bold path rep- resents the current sample, and the dotted path represents a sidetrack in which one decision is changed. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 10 FSRA for a given CNF formula. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Lexical features for relation extraction.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7 The adjusted frequencies of character sequences. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Lexical entailment precision values for top-n similar words by the Bootstrapped LIN and the original LIN method. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Performance of RDC trained on fp1/fp2/adj, and tested on adj.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Mixed-case TER and BLEU, and lower-case METEOR scores on Arabic NIST MT03+MT04. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11 Precision and partial recall of word lengths two to four of the first experiment on IT and AV. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 The results of setting 3 (Punctuation is used; the maximum length is 30). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Performance comparison, the numbers in parentheses report the performance over the 24 ACE subtypes while the numbers outside paren- theses is for the 5 ACE major types ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: M-1 accuracy vs. number of samples.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Composition Gold Standards",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: The top-ranking feature for each group of features and the classifier of a slot",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: New training and testing procedures",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: The semantic roles of cases beside C-3 verb cluster ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6 Most frequent semantic roles for each syntactic position. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Dev set learning curves for sentence lengths \u2264 70. All three curves remain steep at the maximum training set size of 18818 trees. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: F-measure after successive addition of each global feature group",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Average accuracy for EM baseline and model variants across 503 languages. First panel: results on all languages. Second panel: results for 30 isolate and singleton languages. Third panel: results for 27 non-Latin alphabet languages (Cyril- lic and Greek). Standard Deviations across lan- guages are about 2%. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example consensus network with votes on word arcs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: A filter RTG corresponding to Ex. 2",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Classification results with 5-gram and fre- quency threshold 4 (Method 2) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 20 Feature templates for the phrase-structure parser. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 15 Reduplication \u2013 general case. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Syncretism Example 2",
        "Entity": "Caption"
    },
    {
        "Text": "Table 17 Results on large-scale Dutch-to-English translation",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10 Model accuracy using equal distribution of verb frequencies for the estimation of P(c). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: IBM Model 3",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11 The comparison between IWSLRR(I), SS(S), TONGO(O), TH(T), and ESA. ",
        "Entity": "Caption"
    },
    {
        "Text": "                                                                   ings of the 13th International Conference, pages 182\u2013190. Table 6: Accuracy on S ENSEVAL-1 and S ENSEVAL-2 En-                                                                 A. R. Golding and D. Roth. 1999. A winnow-based appro glish test data (only the supervised systems with a coverage of    to context-sensitive spelling correction. Machine Learni at least 97% were used to compute the mean and variance)           34(1-3):107\u2013130. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6. Size of the test data",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 11 An example Chinese lexicalized phrase-structure parse tree. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10: Scalability of BS on NIST task",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Features for SVM Learning of Prediction Model ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Glue Semantics proof for (86), English Way Construction (means interpretation)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 Disambiguation results for G2 and X2 . ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6 NER type-insensitive (type-sensitive) performance of different Chinese NE recognizers. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 Web results for other-anaphora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Example of a MUC-4 template",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Ordered List of Increased/Decreased Number of Correctly Tagged Words ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Thread length distribution.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 13: A Template Network and Two Filler Networks ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4. Results from our SMT evaluation. The BLEU scores are the maximum over the        Moses parameters explored for the given word alignment con\ufb01guration. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 13 Large-scale clustering on D3 with n/na/nd/nad/ns-dass. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10 Occurrences of error types for the best other-anaphora algorithm algoWebv4 . ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: F-Score of the RF and SVM, GIZA++ and Levenshtein distance-based classifier on the second order dataset ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Examples of non-phonetic translations.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Comparison of the three decoders by the ratio each decoder produced search errors. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7 Algorithm for breadth-first search with pruning",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Table showing the number of pairs of different occurrences of the same token sequence, where one occurrence is given a certain label and the other occurrence is given a certain label. We show these counts both within documents, as well as over the whole corpus. As we would expect, most pairs of the same entity sequence are labeled the same(i.e. the diagonal has most of the density) at both the document and corpus levels. These statistics are from the CoNLL 2003 English training set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: The fraction of verb pairs clustered together, as a function of the number of different senses between pair mem- bers (results of the NN algorithm) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 Some of the possible Spanish translations of the English phrase make with their memory-based con- text-dependent translation probabilities (rightmost column) compared against context-independent transla- tion probabilities of the baseline system ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 14 Results of feature analysis. The second column denotes the proportion of manually annotated examples for which the feature value is non-zero. A detailed explanation of the other columns is provided in the body of the article. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6 %BLEU on tune and test sets for DE\u2192EN translation, comparing the baselines to our QPD model with target syntactic features (T GT T REE) and then also with source syntax (+ T REE T O T REE). Here, merely using the additional round of tuning with the SSVM reranker improves the BLEU score to 19.9, which is statistically indistinguishable from the two QPD feature sets. Differences between Hiero and the three 19.9 numbers are at the border of statistical significance; the first two are statistically indistinguishable from Hiero but the third is different at p = 0.04. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Graph of words for the target word paper. Numbers inside vertices correspond to their degree. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Precision and recall for prepositions.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 10 Lattice dependency parsing using an arc-factored dependency model. Lone indices like p and i denote nodes in the lattice, and an ordered pair like (i, j) denotes the lattice edge from node i to node j. S TART is the single start node in the lattice and F INAL is a set of final nodes. We use edgeScore(i, j) to denote the model score of crossing lattice edge (i, j), which only includes the phrase-based features h 0 . We use arcScore((i, j), (l, m)) to denote the score of building the dependency arc from lattice edge (i, j) to its parent (l, m); arcScore only includes the QPD features h 00 . ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: The F1-Measure value is shown for every kernel on each ACE-2005 main relation type. For every relation type the best result is shown in bold font. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Architecture of the Structured Output Layer Neural Network language model. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Rank trajectories of 4 LDA inferred topics, with incremental topic inference. The x-axis indicates the utterance number. The y-axis indicates a topic\u2019s rank at each utterance. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: General architecture of LINGUA",
        "Entity": "Caption"
    },
    {
        "Text": "Table 21 Experimental results for large-scale English-to-Chinese translation ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Comparison of the structured feature and the flat features extracted from parse trees ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 Model accuracy using unequal distribution of verb frequencies for the estimation of P(c). ",
        "Entity": "Caption"
    },
    {
        "Text": " Table 3. Performance of Chinese system with perfect mentions and perfect relations ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Proposed discourse structures for Ex. 4: (a) In terms of informational relations; (b) in terms of inten- tional relations ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Gibbs sampling for Bayesian inference outperforms both EM and Variational Bayesian EM. \u2217 The output of EM alignment was used as the gold standard. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Data used for training SMT models (all counts in millions). Parallel data sets refer to the bitexts aligned to English and their token counts include both languages. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Voting under hand-invented schemes.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Mixed Membership MEDLDA",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Number of features used according to different cut-off threshold. In the second column of the table are shown the number of features used when only the English context is considered. The third column correspond to English, German and Word-Classes contexts. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: MRRs of the frequency correlation meth- ods. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example of word alignment",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Comparison of word alignment accuracy. The best results are indicated in bold type. The additional data set sizes are (a) 10k, (b) 50k, (c) 100k. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Distribution of Pronoun Mentions and Fre- quency of c-command Features ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Quantitative Evaluation of Common Topic Finding (\u201ccross-collection\u201d log-likelihood) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: MUC-7: Level Distribution of Each of the Facts",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Tags used in LMR Tagging scheme.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8 Extracting consistent bilingual phrasal correspondences from the shown sentence pairs. (i1 , j1 ) \u00d7 (i2 , j2 ) denotes the correspondence   fi1 . . . fj1 , ei2 . . . ej2  . Not all extracted correspondences are shown. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 The scales of corpora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Kleene Regular-Expression Assignment Examples.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Instructions for judging of unsharpened factoids.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: BLEU scores on the test08 and news08 test data obtained by models trained by MERT and SVM. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 14 Summary of results for unknown-boundary condition. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: MT06 Dev. Optimization & Test Set Spearman Correlation Results",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Initial type-sensitive Chinese/English NER performance. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Features for \u2018Astronomer Edwin Hubble was born in Marshfield, Missouri\u2019.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: LO sentence configuration scores",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: A wRTG modelling the interdependency constraint for Fig. 1",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Results for three procedures over 4 datases. The horizontal axis corresponds to the context window size. Solid line represents the result of F SGM M + binary, dashed line denotes the result of CGDSV D + idf , and dotted line is the result of CGDterm + idf . Square marker denotes \u03c72 based feature ranking, while cross marker denotes f req based feature ranking. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 NEA type-insensitive (type-sensitive) performance with the same English NE recognizer (Mallet system) and different Chinese NE recognizers. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 5 The semantic graph of an English sentence and the semantic features extracted from it for an SMT phrase ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Values obtained for Precision, Recall and F- scores with method 1 by changing the minimum fre- quency of the correspondences to construct rules for foma. The rest of the options are the same in all three experiments: only one rule is applied within a word. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Results for nouns",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Ten relation instances extracted by our system that did not appear in Freebase.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: (a) RM and large margin solution comparison and (b) the spread of the projections given by each. RM and large margin solutions are shown with a darker dotted line and a darker solid line, respectively. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Example input and best output found",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4. An example of some discovered paraphrases.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: First ten words with weigths and number of senses in WN of the Topic Signature for airport#n#1 obtained from BNC using InfoMap ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Dirichlet-Tree prior of depth two.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: An example sequence representation. The subgraph on the left represents a bigram feature. The subgraph on the right represents a unigram feature that states the entity type of arg 2 . ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: The NP chunking results for six sys- tems associated with the project. The baseline results have been obtained by selecting the most frequent chunk tag associated with each part-of- speech tag. The best results for this task have been obtained with a combination of seven learn- ers, five of which were operated by project mem- bers. The combination of these five performances is not far off these best results. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8: Surface composition of embedded structures.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Translation performances (BLEU% and NIST scores) of NIST task: decoder (1-best), rescoring on original 2,400 N-best (RESC1) and 4,000 N-best hypotheses (RESC2), re-decoding (RD), n-gram expansion (NE), confusion network (CN) and combination of all hypotheses (COMB). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Synonyms for home",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Feature counts for Ling and Seed feature sets.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Examples of zero anaphora",
        "Entity": "Caption"
    },
    {
        "Text": "                 sentence length Figure 6: Time consumption of the various change types in ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: The pool of features for all languages.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Learning curves using different sam- pling strategies. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Jumping POS in WordNet.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Effect of different sets of reference translations used during tuning. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Experiment 2: Results for label unknown sense, NN-based outlier detection, \u03b8 = 1.0. \u03c3: stan- dard deviation ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: F1 scores (in %) of SegTagDep on CTB- 5c-1 w.r.t. the training epoch (x-axis) and parsing feature weights (in legend). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Filtering results using the naive Bayes classifier. The number of entity candidates for the training set was 4179662, and that of the develop- ment set was 418628. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The semantic representations of a word W , its inverse W inv and its negation \u00acW . The domain part of the representation remains un- changed, while the value part will partially be in- verted (inverse), or inverted and scaled (negation) with 0 < \u00b5 < 1. The (separate) functional repre- sentation also remains unchanged. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Example of the denotation for a DCS tree with a compare relation C. This denotation has two columns, one for each active node\u2014the root node state and the marked node size. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Opinion PageRank Performance with varying parameter \u03bb (\u00b5 = 0.5) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: A Portion of the Syntactic Tree.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Segmentation results by a pure subword-based IOB tagging. The upper numbers are of the character- based and the lower ones, the subword-based. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10 The superiority of our joint model on three different domains indicated by type-insensitive (type-sensitive) performance (those signi\ufb01cant entries are marked in comparison with baseline). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Training set characteristics",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9: Scale-up to 160K on IWSLT data sets",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Correlations of resolution class scores with respect to the average. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Type-level English POS Tag Ranking: We list the top 5 and bottom 5 POS tags in the lexicon and the predictions of our models under the best hyperparameter setting. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Comparison of average per-document ter- comTER with invWER on the EVAL07 GALE Newswire (\u201cNW\u201d) and Weblogs (\u201cWB\u201d) data sets. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: POS tagging of unknown words using contextual features (accuracy in percent).   \u008d is a classifier that uses only contextual features,   \u008d + baseline is the same classifier with the addition of the baseline feature (\u201cNNP\u201d or \u201cNN\u201d). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Average number of Pareto points",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Size and percentage of overlapping relations between KnowNet versions and WN+XWN ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: MRRs on the augmented candidate list.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Accuracies (%) for Word-Extraction Us- Litkowski and Hargraves (2007) selected exam- ing MALT Parser or Heuristics.                  ples based on a search for governors8 , most anno- ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Error analysis of confidence measure with and without EIV tag",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: LO cosine sentence configuration scores",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11: Performance comparison with the literature for compound interpretation ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Analysis of the number of relevant documents out of the top 10 and the total number of retrieved documents (up to 100) for a sample of queries. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Learning curve of SuperSense on SE2",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: The Division of LDC annotated data into training and development test sets. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Comparison of different methods on ACE 2004 data set. P, R and F stand for precision, recall and F1, respectively. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 9 The difference between the results of four settings. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Logical form graphs aligned with sur- face forms in two languages. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Example distributions of German verbs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. The F-measure improvement between the BMM-based CWS and it with WSM in the MSR_C track (OOV is 0.034) using a, b, and c system dictionary. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Performance on T3 using a pre-defined tree structure. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Overall steps of proposed method",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Contribution of combining the three caches",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Part-of-speech annotations of the three-    character strings \u7d30\u67f3\u71df xi liu ying \u2018Little Willow military camp\u2019 and \u65b0\u8c50\u5e02 xin feng shi  \u2018Xinfeng city\u2019. Both are \u2018strings with internal structures\u2019, with nested structures that perfectly   match at all three levels. They are the noun phrases that end both verses in the couplet \u5ffd\u904e               \u65b0\u8c50\u5e02, \u9084\u6b78\u7d30\u67f3\u71df. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Creation of a Lexical Transducer. The .o. operator represents the composition operation.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 15 The training, development, and test data for English dependency parsing. ",
        "Entity": "Caption"
    },
    {
        "Text": "aJ = argmax p (f J , aJ | eI ) (8) 1 1 1 1 J 1",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5 The merging algorithm. (a) How the merging algorithm works for two simple parse trees to produce a shared forest. Note that for clarity, not all constituents are expanded fully. Leaf nodes with two entries represent paraphrases. (b) The word lattice generated by linearizing the forest in (a). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Mixed-case TER and BLEU, and lower- case METEOR scores on Chinese NIST MT05. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4. Active Learning with Large Corpora",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Incremental dev set results for the manually annotated grammar (sentences of length \u2264 70).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Simple parser vs full parser \u2013 morphological quality. The parsing models were trained on the first 5,000 sentences of the training data, the morphological tagger was trained on the full training set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Words with the highest association scores, in decreasing order, for the word \u201ccigarette\u201d, as extracted automatically. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Linking FrameNet frames and VerbNet classes. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2 A latent layered POS tag representation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: 2 billion word corpus statistics",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Final system results (as F1 scores) where IM is identification of mentions and S - Setting. For more details cf. (Recasens et al., 2010). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Gibbs sampling algorithm for IBM Model 1 (im- plemented in the accompanying software). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7: Percentage of obtaining two clusters when applying CW on n-bipartite cliques ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8 The effect of varying the number of extracted related words on accuracy. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7: Computing the partition function of the conditional probability P r(S|T ). Sema(s1 , s2 , t) denotes all the seman- tic role features generated by combining s1 and s2 using t. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: All the parameters of WSMs described in Section 2 used in all our experiments. Semicolon denotes OR. All the examined combinations of parameters are implied from reading the diagram from left to right. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 11: Correlation between manual and automatic scores for French-English",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 17: After Applying Compile-Replace to the Lower Side",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Merging left-to-right and right-to-left hypotheses (ef and eb ) in bidirectional decoding method. Figure 5(a) merge two open hypotheses, while Figure 5(b) merge them with inserted zero fer- tility words. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A fragment of an entailment graph (a), its SCC graph (b) and its reduced graph (c). Nodes are predicates with typed variables (see Section 5), which are omitted in (b) and (c) for compactness. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: MTO is not sensitive to the number of random substitutes sampled per word token. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Mutual information between feature subset and class label with f req based feature ranking. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Statistics of datasets.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Time consumption of transduction.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Evaluation of context-sensitive convolution tree kernels using SPT on the major relation types of the ACE RDC 2003 (inside the parentheses) and 2004 (outside the parentheses) corpora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Our approach for detecting parallel fragments. The lower part of the figure shows the source and target sentence together with their alignment. Above are displayed the initial signal and the filtered signal. The circles indicate which fragments of the target sentence are selected by the procedure. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Signature caseframe densities for differ- ent sets of summarizers, for the initial and update guided summarization tasks (Study 2). \u2217 : p < 0.005. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: With the English tree and alignment provided by a parser and aligner at test time, the Chinese parser finds the correct dependencies (see \u00a76). A monolingual parser\u2019s incor- rect edges are shown with dashed lines. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Graphical model of synonym pair gen- erative process ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3 The lattice for the Hebrew sequence !\u202b( \u05d1\u05e6\u200c\u05dc\u05dd \u05d4\u05e0\u200c\u05e2\u05d9\u05dd\u202csee footnote 19). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Dataset statistics: development (dev) and test.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Architecture of Name-aware Machine Translation System.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Boxer output for Shared Task Text 2",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters. Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner using increasing numbers of training sentences. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Comparison of MERT, PRO, and MIRA on tuning Urdu-English SBMT systems, and test results at every iteration. PRO performs comparably to MERT and MIRA. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Evaluation of the manual annotation improvement - summarization ratio: 15%.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 9: Bi-partite neighboring co-occurrence graph (a) and second-order graph on neighboring co-occurrences (b) clustered with CW. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Training, tuning, and test conditions",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Fraction of the sentences that were transduced.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Diacritized particles and pseudo-verbs that, after orthographic normalization, have the equivalent surface form 0 an.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Size of Seed Lexicons",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Evaluation of the Turkish n-gram model.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Evaluation of 100 randomly sampled variation nuclei types.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results from the empirical evaluation, including the Bayesian model without PoS tags (Base- line), the alternating alignment-annotation algorithm (AAA), the corresponding method but with super- vised PoS taggers for both languages (Supervised), and comparable previous results on the same data. The number of alignment links |A|, of which |A \u2229 S| are considered (S)ure, and |A \u2229 P | (P)ossible, are reported. For convenience, precision (P ), recall (R), F1 score (F ) and Alignment Error Rate (AER) are also given. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: The 10 best languages for R EDDY using LCS.",
        "Entity": "Caption"
    }
]