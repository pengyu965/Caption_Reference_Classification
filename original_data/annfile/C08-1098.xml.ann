T1	Caption 2404 2542	The joint probability of the two sequences is defined as the product of context probabilities and lexical probabilities over all POS tags:
A1	Type T1 Equation
A2	Num T1 1
T2	Reference 2604 2655	This sentence should be tagged as shown in table 1.
A3	RefType T2 Direct
A4	Type T2 Table
A5	Num T2 1
T3	Caption 3341 3427	Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt.
A6	Type T3 Table
A7	Num T3 1
T4	Reference 7995 8115	Figure 1 shows a probability estimation tree for the prediction of the probability of the nominative attribute of nouns.
A8	RefType T4 Direct
A9	Type T4 Figure
A10	Num T4 1
T5	Caption 9683 9755	Figure 1: Probability estimation tree for the nomi native case of nouns.
A11	Type T5 Figure
A12	Num T5 1
T6	Caption 9788 9860	The test 1:ART.Nom checks if the preceding word is a nominative article.
A13	Type T6 Figure
A14	Num T6 1
T7	Caption 25585 25644	Table 2: Tagging accuracies on development data in percent.
A15	Type T7 Table
A16	Num T7 2
T8	Caption 25678 25761	Results for 2 and for 10 preceding POS tags as context are reported for our tagger.
A17	Type T8 Table
A18	Num T8 2
T9	Reference 25842 25916	Table 3 shows the results of an evaluation based on the plain STTS tagset.
A19	RefType T9 Direct
A20	Type T9 Table
A21	Num T9 3
T10	Reference 26345 26444	Table 2 summarizes the results obtained with different taggers and tagsets on the development data.
A22	RefType T10 Direct
A23	Type T10 Table
A24	Num T10 2
T11	Reference 26478 26696	The accuracy of a baseline tagger which chooses the most probable tag9 ignoring the context is 67.3% without and 69.4% with the supple 92.3 92.2 92.1 92 91.9 91.8 91.7 91.6 91.5 91.4 2 3 4 5 6 7 8 9 10 mentary lexicon.
A25	RefType T11 Indirect
A26	Type T11 Table
A27	Num T11 2
T12	Reference 26730 26791	The TnT tagger achieves 86.3% accuracy on the default tagset.
A28	RefType T12 Indirect
A29	Type T12 Table
A30	Num T12 2
T13	Reference 26917 27018	The tagset refinement increases the accuracy by about 0.6%, and the external lexicon by another 3.5%.
A31	RefType T13 Indirect
A32	Type T13 Table
A33	Num T13 2
T14	Caption 27991 28149	Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset.
A34	Type T14 Table
A35	Num T14 3
T15	Caption 28537 28609	Figure 2: Tagging accuracy on development data depending on context size
A36	Type T15 Figure
A37	Num T15 2
T16	Reference 28610 28691	Figure 2 shows that the tagging accuracy tends to increase with the context size.
A38	RefType T16 Direct
A39	Type T16 Figure
A40	Num T16 2
T17	Reference 29267 29314	Table 4 shows the performance on the test data.
A41	RefType T17 Direct
A42	Type T17 Table
A43	Num T17 4
T18	Caption 30204 30245	Table 4: Tagging accuracies on test data.
A44	Type T18 Table
A45	Num T18 4
T19	Reference 30279 30370	By far the most frequent tagging error was the confusion of nominative and accusative case.
A46	RefType T19 Indirect
A47	Type T19 Table
A48	Num T19 4
T20	Reference 30644 30744	this error is not counted, the tagging accuracy on the development data rises from 92.17% to 94.27%.
A49	RefType T20 Indirect
A50	Type T20 Table
A51	Num T20 4
T21	Caption 32871 32936	Figure 3: Accuracy on development data depend ing on context size
A52	Type T21 Figure
A53	Num T21 3
T22	Reference 33198 33250;33254 33334	The corresponding figures for the test data are.</S> <S sid ="206" ssid = "86">89.53% for our tagger and 88.88% for the TnT tag- ger.
A54	RefType T22 Indirect
A55	Type T22 Figure
A56	Num T22 3
