[
    {
        "Text": "Figure 1: AER comparison (en\u2192cn)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: AER comparison (cn \u2192en)",
        "Entity": "Caption"
    },
    {
        "Text": "The regular expressions available in Foma from highest to lower precedence.",
        "Entity": "Caption"
    },
    {
        "Text": "A relative comparison of running a selection of regular expressions and scripts against other finite-state toolkits.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Features based on the token string that are based on the probability of each name class during training.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Sources of Dictionaries",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: F-measure after successive addition of each global feature group",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Comparison of results for MUC6",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Comparison of results for MUC7",
        "Entity": "Caption"
    },
    {
        "Text": "The joint probability of the two sequences is defined as the product of context probabilities and lexical probabilities over all POS tags:",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Probability estimation tree for the nomi native case of nouns.",
        "Entity": "Caption"
    },
    {
        "Text": "The test 1:ART.Nom checks if the preceding word is a nominative article.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Tagging accuracies on development data in percent.",
        "Entity": "Caption"
    },
    {
        "Text": "Results for 2 and for 10 preceding POS tags as context are reported for our tagger.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Tagging accuracy on development data depending on context size",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Tagging accuracies on test data.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Accuracy on development data depend ing on context size",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 Architecture of the translation approach based on a log-linear modeling approach",
        "Entity": "Caption"
    },
    {
        "Text": "eI = argmax {Pr(eI | f J )} (1)",
        "Entity": "Caption"
    },
    {
        "Text": "Pr(eI | f J ) = p M (eI | f J )",
        "Entity": "Caption"
    },
    {
        "Text": "exp[ M m hm (eI , f J )] m=1 1 1 M I J (3)",
        "Entity": "Caption"
    },
    {
        "Text": "I exp[ m=1 m hm (e 1 , f1 )]",
        "Entity": "Caption"
    },
    {
        "Text": "= argmax M 1 S s=1 ) log p M (es | fs ) (4)",
        "Entity": "Caption"
    },
    {
        "Text": "Pr(f J | eI ) = Pr(f J , aJ | eI ) (5) 1 1 1 1 1 J 1",
        "Entity": "Caption"
    },
    {
        "Text": "Pr(f J , aJ | eI ) = p (f J , aJ | eI ) (6) 1 1 1 1 1 1",
        "Entity": "Caption"
    },
    {
        "Text": "igure 2 Example of a (symmetrized) word alignment (Verbmobil task).",
        "Entity": "Caption"
    },
    {
        "Text": "BP(f J , eI , A) = f j+m , ei+n 1 1 j i : (i , j ) A : j j j + m i i i + n (9) (i , j ) A : j j j + m i i i + n",
        "Entity": "Caption"
    },
    {
        "Text": "= argmax S I n s=1 a l) p (fs , a | es ) (7)",
        "Entity": "Caption"
    },
    {
        "Text": "aJ = argmax p (f J , aJ | eI ) (8) 1 1 1 1 J 1",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Examples of two- to seven-word bilingual phrases obtained by applying the algorithm phrase-extract to the alignment of Figure 2",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4 Examples of alignment templates obtained in training",
        "Entity": "Caption"
    },
    {
        "Text": "p(z = (FJ , EI , A ) J f ) = 1 1 | N(C( f )) (10",
        "Entity": "Caption"
    },
    {
        "Text": "K): J k = fj k 1 +1 , ..",
        "Entity": "Caption"
    },
    {
        "Text": "eI K 1 = e1 , ek = eik 1 +1 , .",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5 Example of segmentation of German sentence and its English translation into alignment templates",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6 Dependencies in the alignment template mode",
        "Entity": "Caption"
    },
    {
        "Text": "K hAT(eI , f J , K , zK ) = log n p(zk | f j k ) (13) 1 1 1 1 k=1 j k 1 +1",
        "Entity": "Caption"
    },
    {
        "Text": "hWRD(eI , f J , K , zK ) = log n p(ei | {fj | (i, j) A}, Ei ) (14) 1 1 1 1 i=1",
        "Entity": "Caption"
    },
    {
        "Text": "(e | f , i, j): p(ei | fj , i 1 i =1 [(i , j) A], j 1 j =1 [(i, j ) A]) (15)",
        "Entity": "Caption"
    },
    {
        "Text": "hAL(eI , f J , K , zK ) = |j 1 j | (16) 1 1 1 1 k k=1 k 1",
        "Entity": "Caption"
    },
    {
        "Text": "I+1 hLM(eI , f J , K , zK ) = log n p(ei | ei 2 , e ) (17) 1 1 1 1 i=1 i 1",
        "Entity": "Caption"
    },
    {
        "Text": "I+1 hCLM(eI , f J , K , zK ) = log n p(C(ei ) | C(ei 4 ), ..",
        "Entity": "Caption"
    },
    {
        "Text": ", C(e )) (18) 1 1 1 1 i=1 i 1",
        "Entity": "Caption"
    },
    {
        "Text": "hLEX(eI , f J , K , zK ) = #CO-OCCURRENCES (LEX, eI , f J ) (20) 1 1 1 1 1 1",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7 Algorithm for breadth-first search with pruning",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8 Algorithm min-jumps to compute the minimum number of needed jumps D(cJ , j) to complete the translation",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Effect of pruning parameter tp and heuristic function on error rate for direct-translation model (Np = 50,000).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6 Effect of pruning parameter Np and heuristic function on search efficiency for direct-translation model (tp = 10 12 )",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 Effect of pruning parameter Np and heuristic function on error rate for direct-translation model (tp = 10 12 ).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 Effect of the length of the language model history (Unigram/Bigram/Trigram: word-based; CLM: class-based 5-gram).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10 Translation results on the Hansards task",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 Corpus statistics for Hansards task (Words*: words without punctuation marks",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Diacritized particles and pseudo-verbs that, after orthographic normalization, have the equivalent surface form 0 an.",
        "Entity": "Caption"
    },
    {
        "Text": "The distinctions in the ATB are linguistically justified, but complicate parsing.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8a shows that the best model recovers SBAR at only 71.0% F1.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The Stanford parser (Klein and Manning, 2002) is unable to recover the verbal reading of the unvocalized surface form 0 an (Table 1)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Frequency distribution for sentence lengths in the WSJ (sections 2\u201323) and the ATB (p1\u20133).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Gross statistics for several different treebanks.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Dev set frequencies for the two most significant discourse markers in Arabic are skewed toward analysis as a conjunction.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Evaluation of 100 randomly sampled variation nuclei types.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: An ATB sample from the human evaluation. The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a). But the city name Sharm Al- Sheikh is also iDafa, hence the possibility for the incorrect annotation in (b).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Incremental dev set results for the manually annotated grammar (sentences of length \u2264 70).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Test set results. Maamouri et al. (2009b) evaluated the Bikel parser using the same ATB split, but only reported dev set results with gold POS tags for sentences of length \u2264 40. The Bikel GoldPOS configuration only supplies the gold POS tags; it does not force the parser to use them. We are unaware of prior results for the Stanford parser.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Dev set learning curves for sentence lengths \u2264 70. All three curves remain steep at the maximum training set size of 18818 trees.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Per category performance of the Berkeley parser on sentence lengths \u2264 70 (dev set, gold segmentation). (a) Of the high frequency phrasal categories, ADJP and SBAR are the hardest to parse. We showed in \u00a72 that lexical ambiguity explains the underperformance of these categories. (b) POS tagging accuracy is lowest for maSdar verbal nouns (VBG,VN) and adjectives (e.g., JJ). Richer tag sets have been suggested for modeling morphologically complex distinctions (Diab, 2007), but we find that linguistically rich tag sets do not help parsing. (c) Coordination ambiguity is shown in dependency scores by e.g., \u2217SSS R) and \u2217NP NP NP R). \u2217NP NP PP R) and \u2217NP NP ADJP R) are both iDafa attachment.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9: Dev set results for sentences of length \u2264 70.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Accuracy of our system in each period (M = 10)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Rank of correct translation for period Dec 01 \u2013 Dec 15 and Dec 16 \u2013 Dec 31. \u2018Cont. rank\u2019 is the context rank, \u2018Trans. Rank\u2019 is the transliteration rank. \u2018NA\u2019 means the word cannot be transliterated. \u2018insuff\u2019 means the correct translation appears less than 10 times in the English part of the comparable corpus. \u2018comm\u2019 means the correct translation is a word appearing in the dictionary we used or is a stop word. \u2018phrase\u2019 means the correct translation contains multiple English words.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Feature space describing each candidate instance (S indicates the set of seeds for a given class).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Correlation after combining Reddy et al.\u2019s method and our method with Mean for f1 (S TRING S IM MEAN ). The correlation using Reddy et al.\u2019s method is 0.714. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Missing argument examples of biological interactions ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Results for French-English",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Results of 2000 sentences",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Baseline + Word Clustering by Relation +            Re-ranking by Coreference ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Precision of acquired relations (material). L and S denote lenient and strict evaluation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Overview of participating languages and treebank properties. \u2019Sents\u2019 = number of sentences, \u2019Tokens\u2019 = number of raw surface forms. \u2019Lex. size\u2019 and \u2019Avg. Length\u2019 are computed in terms of tagged terminals. \u2018NT\u2019 = non- terminals in constituency treebanks, \u2018Dep Labels\u2019 = dependency labels on the arcs of dependency treebanks. \u2013 A more comprehensive table is available at http://www.spmrl.org/spmrl2013-sharedtask.html/#Prop. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 3. Macro-accuracy for cross-lingual bootstrapping.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 15 Size of training data set and the adaptation results on AS open. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Improvement in (gold mention) RE.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: MEDLDA Fmeausres for 3 feature conditions",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 ESA and input/output data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Size and percentage of overlapping relations between KnowNet versions and WN+XWN ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: A toy instance of lattice construction",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Characteristics of the parallel corpus used for experiments. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Results for ensemble classifier.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6 The initial frequencies of character sequences. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag. Across all languages, high performance can be attained by selecting a single tag per word type. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 1. An example for grouped entity tuples. Entity tuples in big frame are those suitable for the template X direct Y , whereas entity tuples in small frame are those held the same relation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Frequency distribution for sentence lengths in the WSJ (sections 2\u201323) and the ATB (p1\u20133). English parsing evaluations usually report results on sentences up to length 40. Arabic sentences of up to length 63 would need to be evaluated to account for the same fraction of the data. We propose a limit of 70 words for Arabic parsing evaluations. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 17 WSD using predominant senses, training, and testing on all domain combinations (automatically classified corpora). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Three types of transitivity constraint violations.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: MUC, CEAF, and B3 coreference results using system mentions.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Diffs in the course of iteration. All models were with back-off mixing (+BM). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Optimal context size (S) and criteria",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Training, tuning, and test conditions",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Number of clustering decisions made ac- cording to mention type (rows anaphor, columns antecedent) and percentage of wrong decisions. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Translation accuracy (BLEU) results.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Size of co-occurrence databases",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 Words in the MSR gold test set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Illustrative tableau for a simple constraint sys- tem not capturable as a regular relation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Objective values for the different mappings used in our experiments for four languages. Note that the goal of the optimization procedure is to minimize the objective value. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 13 Comparison scores for PK open and CTB open. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Priority Order for First Person ADs",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Resolution accuracy (%)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: The top-ranking feature for each group of features and the classifier of a slot",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Gibbs sampling algorithm for IBM Model 1 (im- plemented in the accompanying software). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Schematic of our proposed method",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 The generic beam-search algorithm. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Two words that differ only in one character, but have different internal structures. The character \u543c \u2018people\u2019 is part of a personal name in tree (a), but is a suffix in (b). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Selected entries from the confusion matrix for parts of speech in German with F-scores for the left-hand- side category. ADJ* (ADJD or ADJA) = adjective; ADV = adverb; ART = determiner; APPR = preposition; NE = proper noun; NN = common noun; PRELS = relative pronoun; VVFIN = finite verb; VVINF = non-finite verb; VAFIN = finite auxiliary verb; VAINF = non-finite auxil- iary verb; VVPP = participle; XY = not a word. We use \u03b1* to denote the set of categories with \u03b1 as a prefix. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Evaluation closed results on all data sets",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Perplexity results for two previous grammar-based language models ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Grammatical relations from S EXTANT",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 11 An example Chinese lexicalized phrase-structure parse tree. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Feature templates used for CRF in our system",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. The noun \u201cbox\u201d in Wordnet: each line lists one synset, the set of synonyms, a definition, an optional example sentence, and the supersense label. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Confidence scores for diese in ex. (1)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 Models with functional features: GENDER, NUMBER, rationality (RAT). F N * = functional feature(s) based on Alkuhlani and Habash (2011); GN = GENDER + NUMBER ; GNR = GENDER + NUMBER + RAT . Statistical significance tested only for CORE 12+. . . models on predicted input, against the CORE 12 baseline. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Adjective Full Inflection",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Feature growth rate: For N-best list i in the table, we have (#NewFt = number of new fea- tures introduced since N-best i \u2212 1) ; (#SoFar = Total number of features defined so far); and (#Ac- tive = number of active features for N-best i). E.g., we extracted 7535 new features from N-best 2; combined with the 3900 from N-best 1, the total features so far is 11435. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2 Two different dependency tree paths (a and b) that are considered paraphrastic because the same words ( John and problem) are used to \ufb01ll the corresponding slots (shown co-indexed) in both the paths. The implied meaning of each dependency path is also shown. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: The best results (per F1 -score of the two meth- ods). The parameters of method 1 included using only those string transformations that occur at least 2 times in the training data, and limiting rule application to a maxi- mum of 2 times within a word, and including a unigram post-filter. Rules were contextually conditioned. For method 2, all the examples (threshold 1) in the training data were used as positive and negative evidence, with- out a unigram filter. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Scores for CityU corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: FDG Analyser\u2019s output example",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 Dev-set results of using the agreement-filter on top of the lexicon-enhanced lattice parser (parser does both segmentation and parsing). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Simple parser vs full parser \u2013 syntactic quality. Trained on first 5,000 sentences of the training set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Effect of different sets of reference translations used during tuning. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 French grammar development. Incremental effects on grammar size and labeled F1 for each of the manual grammar features (development set, sentences \u2264 40 words). The baseline is a parent-annotated grammar. The features tradeoff between maximizing two objectives: overall parsing F1 and MWE F1. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 9 An example of syntactically motivated paraphrastic patterns that can be extracted from the paraphrase corpus constructed by Cohn, Callison-Burch, and Lapata (2008). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7 Word sense disambiguation accuracy for \u201cNP1 V NP2 NP3\u201d frame. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 16: Incorrect resolution example of pronoun resolution module ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Accuracy for Different Part-Of-Speech",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: POS tagging of unknown words using contextual features (accuracy in percent).   \u008d is a classifier that uses only contextual features,   \u008d + baseline is the same classifier with the addition of the baseline feature (\u201cNNP\u201d or \u201cNN\u201d). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Modification of \u201cO\u201d (other labels) to transfer information on a preceding named entity. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Time to read and accept or reject proposals versus their length",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Distribution of Spurious Errors",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Morph features of frequent words and rare words as computed from the WSJ Corpus of Penn Treebank. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: learning curves of the averaged and non- averaged perceptron algorithms ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10: Results for the joint segmentation, tagging, and parsing task using pipeline and joint models. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results on the Arabic GALE Phase 2 evaluation set with one reference translation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Analysis of the number of relevant documents out of the top 10 and the total number of retrieved documents (up to 100) for a sample of queries. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Kendall\u2019s (\u03c4 ) correlation over WMT 2013 (all- en), for the full dataset and also the subset of the data containing a noun compound in both the reference and the MT output ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Question-specific popular topic words and opinion words generated by Opinion HITS ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Example non-coreferent paths: Italicized entities do not generally corefer",
        "Entity": "Caption"
    },
    {
        "Text": "Table 25 Comparison of dependency accuracies between phrase-structure parsing and dependency parsing using CTB5 data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example of the effects of OOV processing for German\u2192English",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10 NW 21 identification results on PK test set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6 Order in which the German source positions are covered for the German-to-English reordering example given in Figure 5. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Notation used in this paper. The convention eIi indicates a subsequence of a length I sequence. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The System architecture1",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 The results of setting 1 (Punctuation and other encoding information are not used; the maximum length is 30). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: POS/morphological feature accuracies on the development sets.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 Performance of existing approaches on MSRVDC Dataset 1. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The NLM-WSD test set and some of its sub- sets. Note that the test set used by (Joshi et al., 2005) comprises the set union of the terms used by (Liu et al., 2004) and (Leroy and Rindflesch, 2005) while the \u201ccom- mon subset\u201d is formed from their intersection. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results for GigaPairs (all numbers in %); re- sults that significantly differ from Full are marked with asterisks (* p<0.05; ** p<0.01). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Precision on full RTE data",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Corpus statistics for Chinese (Zh) character segmentation and English (En)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Percentage of non-projective arcs recovered correctly (number of labels in parentheses)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Domain specific results",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 12: MUC-6: Level Distribution of the Six Facts Combined",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 Generative patterns of ONA, where sij denotes the j-th character of the i-th word of ON (Sun, Zhou and Gao 2003). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 16: Arabic Order-Free Structure",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Numbers of rules in Hiero or phrase-pairs in Moses. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Translation performances (BLEU% and NIST scores) of NIST task: decoder (1-best), rescoring on original 2,400 N-best (RESC1) and 4,000 N-best hypotheses (RESC2), re-decoding (RD), n-gram expansion (NE), confusion network (CN) and combination of all hypotheses (COMB). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Effectiveness of combining the two scor- ing methods. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 19: Phonetic Stress in Russian",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Translation results for French-English",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Active sparse feature templates",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The semantic representations of a word W , its inverse W inv and its negation \u00acW . The domain part of the representation remains un- changed, while the value part will partially be in- verted (inverse), or inverted and scaled (negation) with 0 < \u00b5 < 1. The (separate) functional repre- sentation also remains unchanged. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Comparing similarity measures on D1 and D2. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Results on the unseen plausibility dataset.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Performance results on the SemEval-2010 WSI Task, with rank shown in parentheses. Refer- ence scores of the best submitted systems are shown in the bottom. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Comparison with other systems",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Summary table on the various methods investigated for POS tagging ",
        "Entity": "Caption"
    },
    {
        "Text": "  Figure 5. Results for webpage snippet number. 7.3 Experiment on Multiple Feature Fusion To verify the effectiveness for multiple feature fusion, the test on the feature combination for OOV term translation is implemented. As shown in Table 1, the highest accuracy (the percentage of the correct translations in all the extracted translations) of 83.1367% can be ac- ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Classification results with decision tree on XLE output (Method 3) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 NEA type-insensitive (type-sensitive) performance with the same English NE recognizer (Mallet system) and different Chinese NE recognizers. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example context for WSD S ENSEVAL-2 target word bar (inventory of 21 senses) and extracted features ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Features used in baseline system",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Validation Features for Crosslingual Slot Filling",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Reachability of 1000 training sentences: can they be translated with the model? ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Context Clustering with Spectral-based Clustering technique. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: NIST08 Chinese-English translation BLEU",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Comparison of SWSD systems",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Labeled precision and recall for the three types of labels. The line labeled \u2018Flat*\u2019 is for unlabeled met- rics of flat words, which is effectively the ordinary word segmentation accuracy. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Approximate times in seconds to generate predictions of maximum word sequence length M , on a 1.2GHz processor, for the MEMD model. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Type-level English POS Tag Ranking: We list the top 5 and bottom 5 POS tags in the lexicon and the predictions of our models under the best hyperparameter setting. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Linking FrameNet frames and VerbNet classes. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9: Arabic Clitics - Example 1",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Overview of the results for all baselines for other-anaphora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Results of the French to English system (WMT-2012). The marked system (*) corresponds to the system submitted for manual evaluation. (cs: case-sensitive, ci: case-insensitive) ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 2. Translation performance of EM, Gibbs sampling, and variational Bayes and lower translation performance (Section V-A). after applying alignment combination within and across methods: EM(Co), ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Statistics of three test sets.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Two dendrograms for the graph in Figure 3.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 9: Evaluation of translation to English on out-of-domain test data",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Beam search algorithm for joint tagging and de- pendency parsing of input sentence x with weight vector w and beam parameters b1 and b2 . The symbols h.c, h.s and h.f denote, respectively, the configuration, score and feature representation of a hypothesis h; h.c.A denotes the arc set of h.c. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Error analysis of parser components av- eraged over Arabic, Bulgarian, Danish, Dutch, Japanese, Portuguese, Slovene, Spanish, Swedish and Turkish. N/P: Allow non-projective/Force pro- jective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10 Comparing disagreements between ILP-Global and Greedy-Global against the gold-standard graphs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A verse written in the BAD web application.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: CPU time, memory usage, and uncased BLEU (Papineni et al., 2002) score for single-threaded Moses translating the same test set. We ran each lossy model twice: once with specially-tuned weights and once with weights tuned using an exact model. The difference in BLEU was minor and we report the better result. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. The Roov over the bakeoff-2 data.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Corpus statistics of Chinese side, where Sent., Avg., Lon., and Len. are short for sentence, longest, average, and length respectively. RT RAIN denotes the reachable (given rule table without added rules) subset of T RAIN data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Results of the fill-in-the-blank exercise",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Four ambiguous words, their senses and frequency",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Rhetorical relations in RST Spanish Treebank",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5. Impact of Data Selection (Chinese)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Subgraph of tuned-LP output for \u201cheadache\u201d",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 NW 11 identification results on PK test set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: How the IBM models model the translation process. This is a hypothetical example and not taken from any actual training or decoding logs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 13 Example translations for Chinese\u2013English MT. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. F-scores of UML-DOP compared to      previous models on the same data ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 13 Distribution of Category (VI) error classes (type-insensitive). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 Best F1-measure values for all possible combination. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8 Results of 70 high-frequency two-character CASs. \u2018Voting\u2019 indicates the accuracy of the baseline method that always chooses the more frequent case of a given CAS. \u2018ME\u2019 indicates the accuracy of the maximum-entropy classifier. \u2018VSM\u2019 indicates the accuracy of the method of using VSM for disambiguation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Tagging accuracy on the gold-standard normalizations (OrigP = original punctuation, ModP = modern punctuation, NoP = no punctu- ation) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Results of different systems for pronoun resolution on MUC-6 and MUC-7 (*Here we only list backward feature assigner for pronominal candidates. In RealResolve-1 to RealResolve-4, the backward features for non-pronominal candidates are all found by DTnon\u2212pron .) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: N/P classifier with and without SWSD",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Standards and corpora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Graphical representation of our model. Hyper- parameters, the stickiness factor, and the frame and event initial and transition distributions are not shown for clar- ity. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Summary of devtest results and shared task test results for submitted systems and LIU baseline with hier- archical reordering. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Translation of PCC sample commentary",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: BLC for WN1.6 using all or hyponym relations",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results for morphological processing, German\u2192English ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A pair of comparable, non-parallel documents",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 2. Integration of paradigmatic relations \u2013 recall/precision curves.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3. Results for \u03b1 value setting.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Average min/max hypothesis length pro- ducible by each method (h = 1 for CF). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Translation results for English\u2192German",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7 String-to-tree configurations; each is associated with a feature that counts its occurrences in a derivation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10: Performance of Altavista counts and BNC counts for compound interpretation (data from Lauer 1995) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Accuracy for induced verb classes.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Precision of acquired relations (causality). L and S denote lenient and strict evaluation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Count of relationships in 77 gold standard documents.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 12 The set of new features. The last two columns denote the number and percentage of examples for which the value of the feature is non-zero in examples generated from the 23 gold-standard graphs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Tagging accuracy on development data depending on context size ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 7. Arabic    English BLEU scores o schemes in the 1M-sentence translation task. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Experimental results for Japanese\u2013English",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Features used in our parsing models.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Summary of LINGUA performance",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Word prediction from a partial parse",
        "Entity": "Caption"
    },
    {
        "Text": "Table 27: Performance comparison between original and universal tagsets",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Disambiguation results.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10: An example antecedent of a nominal in- teraction keyword ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Equation 1 settings",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Feature templates of a typical character-based word segmentor. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: User Interface with Arabic Script Dis- play in Java. Mouse clicks on the virtual keyboard or key presses on the physical keyboard are inter- cepted, converted to Arabic Unicode characters, and stored in a buffer, which has a start and an end but no inherent ordering. The Arabic Canvas Object observes the buffer and contains an Ara- bic Scribe object that renders the string of Uni- code characters right-to-left as connected Arabic glyphs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 Architecture of the statistical translation approach based on Bayes\u2019 decision rule. ",
        "Entity": "Caption"
    },
    {
        "Text": "  Table 5. Timings from the word alignments for our SMT evaluation. The values are averaged over both alignment directions. For these experiments we used systems with                  8-core Intel E5-2670 processors running at 2.6 GHz. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Translation results for French\u2192English",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: An example of NE and non-NE",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Feature set for the baseline pronoun resolution system",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Comparison of different methods on ACE 2004 data set. P, R and F stand for precision, recall and F1, respectively. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: F-score on development data",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Expletive \u201cit\u201d compared results",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Examples for disagreement between the two judges.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Best LO and LL configurations scores",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Character tagging with deterministic constraints.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results of the second run (with postprocessing)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Combined systems (English) in cross- validation, best recall in bold. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: English word perplexity (PPL) on the RT04 test set using a unigram LM. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: False negative matches on the Riedel (Riedel et al., 2010) and KBP dataset (Surdeanu et al., 2012). All numbers are on bag (pairs of entities) level. BD* are the numbers before downsampling the negative set to 10% and 5% in Riedel and KBP dataset, respectively. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2 Summariser and VPA Architecture",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results of a 10-fold cross-validation for various machine learning algorithms. ",
        "Entity": "Caption"
    },
    {
        "Text": " Table 1. OBI vs. BI; where the lost of F > 1%,   such as SC-B, is caused by incorrect English segments that will be discussed in the section 4. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: New generated hypotheses through n- gram expansion and one reference. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 BNC results for other-anaphora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Comparison figures on subsets of the Stanford Sentiment Treebank ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: An example of alignment units",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: CTB 10-fold CV POS tagging accuracy using an all-at-once approach ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results for baseline (BAS) system (standard multiclass SVM) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Case frame of \u201chaken (dispatch).\u201d",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Distribution of reasons for false negatives (missed argument mentions) by BInc at K=20. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example sentence and extracted features from the SENSEVAL 2 word church ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Incorporating additional information",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Time comparison between FSAs and FSRAs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: The posterior distribution of our joint model. Because the sequence of words w is deterministic given analyses l and assignments to analyses (tables) z, the joint posterior over all variables P(w, t, l, z|\u03b1t , a, b, \u03b1s , \u03b1 f ) is equal to P(t, l, z|\u03b1t , a, b, \u03b1s , \u03b1 f ) when lzi = wi for all i, and 0 otherwise. We give equations for the non-zero case. ns refer to token counts, ms to table counts. We add two dummy tokens at the start, end, and between sentences to pad the context history. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: The set of types and subtypes of relations used in the 2004 ACE evaluation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Parser performance on WSJ;23, supervised adaptation. All models use Brown;T,H as the out-of-domain treebank. Baseline models are built from the fractions of WSJ;2-21, with no out-of-domain treebank. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: MWE acquisition applied to lexicography",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 Disambiguation results for G2 and X2 . ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Rule evaluation examples and their judgment.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9: Accuracy of Target Candidate Detection",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: normalized Mutual Information values for three graphs and different iterations in %. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5. Summary of results of baseline and tagger on selected subsets of labels: NER categories evaluated on Semcor (upper section), and 5 most frequent verb (middle) and noun (bottom) categories evaluated on Senseval. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: MUC-7: Level Distribution of Each of the Facts",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Segmentation results on different languages. Results are calculated based on word types. For each language we report precision, recall and F1 measure, number of word types in the corpus and number of word types with gold standard segmentation available. For each language we report the segmentation result without and with emission likelihood scaling (without LLS and with LLS respectively). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Outlier detection by comparing distances between nearest neighbors ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5. The number of OAS (types), CAS (types), LUW (types) and EIW (types) for our CWS. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: EM input for our example sentence. j-values follow each lexical candidate. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: BLEU scores achieved with different sets of parallel corpora. All systems are base- line n-code with POS factor models. The follow- ing shorthands are used to denote corpora, : \u201dN\u201d stands for News-Commentary, \u201dE\u201d for Europarl, \u201dC\u201d for CommonCrawl, \u201dU\u201d for UN and (nf) for non filtered corpora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 The top 10 ranked features for country produced by MI, the weighting function employed in the LIN method. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: The increase in performance for successive variants of Bayes and Mixture Model as evaluated by 5-fold cross vali- dation on S ENSEVAL -2 English data ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Syncretism Example 1",
        "Entity": "Caption"
    },
    {
        "Text": "Table 15 The training, development, and test data for English dependency parsing. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Learning curve of BLC20 on SE3",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: F1 per coarse relation type (ACE 2005). SYS is the final model, i.e. last row (PET+PET WC+PET LSA) of Table 5. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: (a) An undirected graph G representing the similarity matrix; (b) The bipartite graph showing three clusters on G; (c) The induced clusters U ; (d) The new graph G1 over clusters U ; (e) The new bipartite graph over G1 ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Number of extracted instances and the sample sizes (P and N indicate positive and neg- ative annotations). ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 3. Interpolation \u2013 recall/precision curves.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Source span lengths",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Cross-domain B3 and MUC results for Reconcile and Sieve with lexical features. Gray cells represent results that are not significantly different from the best results in the column at the 0.05 p-level. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Bidirectional checking of entailment relation (\u2192) of p1 \u2192 p2 and p2 \u2192 p1 . p1 is \u201creduces bone mass\u201d in s1 and p2 is \u201cdecreases the quantity of bone\u201d in s2 . p1 and p2 are exchanged between s1 and s2 to generate corresponding paraphrased sentences s01 and s02 . p1 \u2192 p2 (p2 \u2192 p1 ) is verified if s1 \u2192 s01 (s2 \u2192 s02 ) holds. In this case, both of them hold. English is used for ease of explanation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Our boosted ranker combining monolingual and bilingual features (bottom) compared to three base- lines (top) gives comparable performance to the human- curated upper bound. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: The performances of the transliteration models and their comparison on EMatch. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11 Comparison between a ME framework and the derived model on the same test set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Decoding algorithm for the standard Tree-to-String transducer. lef tw/rightw denote the left/right boundary word of s. c1 , c2 denote the descendants of v, ordered based on RHS of t. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Empirical results for the baseline models as well as BAYE S UM, when all query fields are used. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 13 Large-scale clustering on D3 with n/na/nd/nad/ns-dass. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Texts used for the evaluation",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: The \u2018B, I, E, S\u2019 Tag Set",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 Top 10 features of country by the Bootstrapped feature weighting. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Examples of translations",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Relation types and subtypes in the ACE                   training data ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Single systems (English) in cross- validation, sorted by recall. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Basic Features for CRF-based Segmenter",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Evaluation measures for Chinese word segmenter. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Sample analysis of an English sentence. Input: Do we have to reserve rooms?. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: The most frequent OOV\u2019s (with counts \u2265 10) of the dialectal test sets against the MSA training data.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Replication of the experiment with a corpus of non-native speakers (CEDEL2, Lozano, 2009)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 Performance measures of the training algorithms. Green indicates the best performance, while red the worst. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: P r (f, a\u0303|e) for IBM Models",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Average precisions over the 10 corpora of different window size (3 seeds)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: A wRTG modelling the interdependency constraint for Fig. 1",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: BiTAM models for Bilingual document- and sentence-pairs. A node in the graph represents a random variable, and a hexagon denotes a parameter. Un-shaded nodes are hidden variables. All the plates represent replicates. The outmost plate (M -plate) represents M bilingual document-pairs, while the inner N -plate represents the N repeated choice of topics for each sentence-pairs in the document; the inner J-plate represents J word-pairs within each sentence-pair. (a) BiTAM-1 samples one topic (denoted by z) per sentence-pair; (b) BiTAM-2 utilizes the sentence-level topics for both the translation model (i.e., p(f |e, z)) and the monolingual word distribution (i.e., p(e|z)); (c) BiTAM-3 samples one topic per word-pair. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Performance of Altavista counts and BNC counts for adjective ordering (data from Malouf 2000) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 Split constituents: In this case, a single semantic role label points to multiple nodes in the original treebank tree. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Effect of observation pruning on the translation quality (average over all test sets).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 21 The standard split of CTB2 data for phrase-structure parsing. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2. Percentage of examples of major syntactic classes.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Training time comparison. The training time for each model is calculated from scratch. For example, the training time of IBM Model 4 includes the training time of IBM Model 1, the HMM, and IBM Model 3. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Effects of combination using the confidence measure. Here we used \u03b1 = 0.8 and confidence threshold t = 0.7. The separator \u201c/\u201d divides the results of s1, s2, and s3. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11: German-English Official Test Submis- sion. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Under-sampled system for the task of rela- tion detection. The proportion of positive examples in the training and test corpus is 50.0% and 20.6% respectively. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Impact of the topic cache size",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Results of Multiple Trials and Compari- son to Simulated Annealing ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Meta alternations and their average precision values for the task. The random baseline performs at 0.313 while the frequency baseline ranges from 0.255 to 0.369 with a mean of 0.291. Alternations for which the model outperforms the frequency baseline are in boldface (mean AP: 0.399, standard deviation: 0.119). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: BLEU scores of two open test sets obtained when training by MERT, S-slack-SVM and 1-slack-SVM using four development sets containing 400 sentences randomly se- lecting from WMT-08 dev2006. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 20 Precision of organization name recognition on the MSR test set, using Viterbi iterative training, initialized by four seed sets with different sizes. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Contribution of combining the dynamic and",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Results of voting experiments.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Performance analysis of HRGs, CWU, CWW & HAC for different parameter combinations (Table 2). (A) All combinations of p1 , p2 and p3 = 0.05. (B) All combinations of p1 , p2 and p3 = 0.09. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Statistics for the ACE 2005 corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Statistics of training, development and test data for NIST task. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: MAP(%), under the \u201850 rules, All\u2019 setup, when adding component match scores to Precision (P) or prior- only MAP baselines, and when ranking with allCP or allCP+pr methods but ignoring that component scores. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Complexity Analysis of Algorithm 1.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9: Performance comparison with the literature for compound bracketing ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5. Extracted UW and noun set",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Average F1 using different hypothesized type-specific features. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: ROUGE-SU measures in EM learning",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Multi-class Classification Results with PlusCOMP for SVM, LLDA and MEDLDA for the six ACE 05 categories and NO-REL ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Meta-evaluation results at document level",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 The accuracies of joint segmentation and POS-tagging by 10-fold cross validation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Example of inference rules needed in RTE ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 Performance evaluation on MSRVDC Dataset 2. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Final Performance (Frozen Systems) on SENSEVAL Lexical Sample WSD Test Data",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Inuktitut: Parimunngaujumaniralauqsimanngittunga = \u201cI never said I wanted to go to Paris\u201d ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Latent Dependency coupling for the RE task. The D-C ONNECT factor expresses ternary connection re- lations because the shared head word of the proposed re- lation is unknown. As is convention, variables are repre- sented by circles, factors by rectangles. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Precision for each phrase type (Ev.Ling).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Micro-averaged (across the 5 folds) RE results using gold mentions.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Average three-fold cross-validation accuracies, in percent. Boldface: best performance for a given setting (row). Recall that our baseline results ranged from 50% to 69%. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Effect of Factors",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example of semantic trees",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Sample Minipar parse and extracted gram- matical function features ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Translation Candidates for \u8e81\u9b31\u75c5 (manic- depression) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 (a) A standard PTB parse of Example (1a). (b) The MWE part of speech functions syntactically like the ordinary nominal category, as shown by this paraphrase. (c) We incorporate the presence of the MWE into the syntactic analysis by flattening the tree dominating part of speech and introducing a new non-terminal label multiword noun (MWN) for the resulting span. The new representation classifies an MWE according to a global syntactic type and assigns a POS to each of the internal tokens. It makes no commitment to the internal syntactic structure of the MWE, however. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Polarity classifier with and without SWSD.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: With the English tree and alignment provided by a parser and aligner at test time, the Chinese parser finds the correct dependencies (see \u00a76). A monolingual parser\u2019s incor- rect edges are shown with dashed lines. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: oscillating states in matrix CW for an unweighted graph ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: The 10 best languages for the particle compo- nent of BANNARD using LCS. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: ROUGE-2 measures in k-means learning",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Results of negated event/property detection on gold standard cue and scope annotation ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Morphological Analysis/Generation as a Relation between Analyses and Words ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6 Related research using English as source language",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: RM gain over other optimizers averaged over all test sets. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10 Model accuracy using equal distribution of verb frequencies for the estimation of P(c). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Comparing distributions on D1 and D2. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Empirically-derived classifier similarity",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Baseline performance and n-best oracle scores (UAS/LAS) on the development sets. mate\u2019 uses the prepro- cessing provided by the organizers, the other parsers use the preprocessing described in Section 2. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Experimental results (F-measure).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: BCN x Baseline",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Linking FrameNet frames and VerbNet classes ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Performance of Chinese Word Segmentation",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Performance of Final Translation (BLEU-4).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: POS Tagging of Unknown Word using Contextual and Lexical features in a Sequential Model. The input for capitalized classifier has 2 values and therefore 2 ways to create confusion                                      k\u0097 sets. There are at most \u0089\u0095\u0094 &F\u0096 \u0081 \u0081 +!\u0098 different in- puts for the suffix classifier (26 character + 10 digits + 5 other symbols), therefore suffix may                      k\u0097 emit up to \u0089 \u0094 &R\u0096 \u0081 \u0081 +R\u0098 confusion sets. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Precision for 200 candidates (Ev.Rec).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: The semantic roles of cases beside C-1 verb cluster ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results on Penn (English) Treebank, Wall Street Journal, sentences with 100 words or fewer.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Ten relation instances extracted by our system that did not appear in Freebase.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. NPs in a sample from the Catalan training data (left) and the English translation (right). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Comparison for Head and Tail datasets",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The first set of features in our model. All of them are binary. The final feature set includes two sets: the set here, and a set obtained by its conjunction with the verb\u2019s lemma. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: BLEU scores for each translation direction trained on (\u2190) directional (condition on target and generate source) and (\u2194) symmetrised alignments (grow-diag-final-and). Observe that the plots are on different scales. This means that results cannot directly be compared across plots. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 23 Comparisons against other segmenters: In Column 1, SXX indicates participating sites in the 1st SIGHAN International Chinese Word Segmentation Bakeoff, and CRFs indicates the word segmenter reported in (Peng et al. 2004). In Columns 2 to 5, entries contain the F-measure of each segmenter on different open runs, with the best performance in bold. Column Site-Avg is the average F-measure over the data sets on which a segmenter reported results of open runs, where a bolded entry indicates the segmenter outperforms MSRSeg. Column Our-Avg is the average F-measure of MSRSeg over the same data sets, where a bolded entry indicates that MSRSeg outperforms the other segmenter. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4. The differences of F-measure and ROOV between near-by steps of our CWS. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Topic words extracted from target-side doc-                       uments ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Accuracy for all cases, all excluding sen- tences with quotes, and only sentences with quotes. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 9 Word sense disambiguation accuracy for \u201cNP1 V for NP2 NP3\u201d frame. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Performance on Ar-En with basic (left) and sparse (right) feature sets on MT05 and MT08.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4. Some of the top selected features by Infor-                    mation Gain ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Examples of ambiguous words that are trans- lated incorrectly by the MSA-English system, but cor- rectly by the Dialectal Arabic-English system. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3. Comparison of two augmentation strategies over different sampling strategies in selecting the initial seed set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6 Comparing clustering initializations on D1. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Average scores by cluster: baseline versus LR[0.20,0.95]. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 2. Multilingual bootstrapping.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Result for microblog classification",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Effectiveness of Latent Topic Extraction from Multi-Language Corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3 Back-off lattice with more specific distributions towards the top. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: The BLEU score of self-trained cascaded trans- lation model under five initial training sets. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 1. Structure and data preprocessing of the initial dataset and the cleaned one after preprocessing.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Examples for the effect of the combined lexica.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: System Architecture.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Two-list implementation of a DP-based search algorithm for statistical MT. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Overview of Morph Resolution",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Error reduction as a function of vocabulary size",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Examples and number of them in Semcor, for sense approach and for class approach ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Relation Extraction Results. Models using hidden constituency syntax provide significant gains over the syntactically-uniformed baseline model in both languages, but the advantages of the latent syntax were mitigated on the smaller Chinese data set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Results of Uryupina\u2019s discourse new clas- sifier ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Chinese character usage in 3 corpora. The   numbers in brackets indicate the percentage of  characters that are shared by at least 2 corpora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Clusters for transitive, unaccusative, and ditransitive",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Causes of Error for FPs",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 Number of learned splits per NT-category after five split-merge cycles. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: The effect on caseframe coverage of adding in-domain and out-of-domain documents. The difference between adding in-domain and out- of-domain text is significant p < 10\u22123 (Study 3). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Results for the classification task. S TRING S IM MEAN is our method using Mean for f1",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Evaluation of the three anaphoric resolvers discussed by Ng and Cardie. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Learning curves on the development dataset of the Beijing Univ. corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Pearson\u2019s r and Kendall\u2019s \u03c4 (absolute) between adequacy and automatic evaluation measures on different levels of the MATR MT06 data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Smoothed precision curves over the five corpora.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. A pruned phrase tokenization lattice. Edges are tokenizations of phrases, e.g. e5 represents tokenizing \u8d28\u7591 \u2018question\u2019 into a word and e7 represents tokenizing \u7591\u4ed6 \u2018doubt him\u2019 into a partial word \u7591 \u2018doubt\u2019 followed by a word \u4ed6 \u2018him\u2019. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5. SA classification results",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Agreement counts in morphological annotation compared between the baseline system and the oracle system using gold syntax. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Features and functions used in clustering algorithm",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Part-of-speech tagging accuracies for various baselines and oracles, as well as our approach. \u201cAvg\u201d denotes macro-average across the eight languages. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Comparison our closed results with the top three in all test sets",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 DP-TSG notation. For consistency, we largely follow the notation of Liang, Jordan, and Klein (2010). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Many-to-1 accuracy across a range of languages, comparing our model with mkcls and the best published result (? Berg-Kirkpatrick et al. (2010) and \u2020 Lee et al. (2010)). This data was taken from the CoNLL-X shared task training sets, resulting in listed corpus sizes. Fine PoS tags were used for evaluation except for items marked with c , which used the coarse tags. For each language the systems were trained to produce the same number of tags as the gold standard. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: A text segment from MUC-6 data set",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Salience grading for candidate antecedents",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Gender classification performance (%)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Recall and precision of the patterns.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6 Most frequent semantic roles for each syntactic position. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Dataset statistics: development (dev) and test.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Time consumption of transduction.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Example of similar document pairs.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Memory-based learner results",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Pipeline architecture for dialogue act recognition and re-ranking component. Here, the input is a list of dialogue acts with confidence scores, and the output is the same list of dialogue acts but with recomputed confidence scores. A dialogue act is represented as DialogueActType(attribute-value pairs). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. F1-measures with \uf061 in [0 3]",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Evaluation of topic identification",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4 Impact of corpus size (measured in number of running words in the corpus) on vocabulary size (measured in number of different full-form words found in the corpus) for the German part of the Verbmobil corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5. Templates for feedback.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Segmentation performance on words that have the same final suffix as their preceding words. The F1 scores are computed based on all boundaries within the words, but the accuracies are obtained using only the final suffixes. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Values obtained for Precision, Recall and F- scores with method 1 by changing the minimum fre- quency of the correspondences to construct rules for foma. The rest of the options are the same in all three experiments: only one rule is applied within a word. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Translation results in lower-case BLEU. CN for confusion network and CF for confusion forest with different vertical (v) and horizontal (h) Markovization order. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Comparison results with TAC 2008 Three Top Ranked Systems (system 1-3 demonstrate top 3 systems in TAC) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7: Percent of query language documents for which",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3 The lattice for the Hebrew sequence !\u202b( \u05d1\u05e6\u200c\u05dc\u05dd \u05d4\u05e0\u200c\u05e2\u05d9\u05dd\u202csee footnote 19). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Average ratings and Pearson correlation for rules from the personal stories corpus. Lower ratings are better; see Fig. 2. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Overview of the system.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Most frequent phrase dependencies in DE\u2192EN data, shown with their counts and attachment directions. Child phrases point to their parents. To focus on interesting phrase dependencies, we only show those in which one phrase has at least two tokens and neither phrase is entirely punctuation. The words forming the longest lexical dependency in each extracted phrase dependency are shown in bold; these are used for back-off features. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 NEA type-insensitive (type-sensitive) performance with the same Chinese NE recognizer (Wu\u2019s system) and different English NE recognizers. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Total annotation time, portion spent se- lecting annotation type, and absolute improve- ment for rapid mode. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results on Penn (Chinese) Treebank.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Comparison of MERT, PRO, and MIRA on tuning Urdu-English SBMT systems, and test results at every iteration. PRO performs comparably to MERT and MIRA. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 10 Number of processed arcs for the pseudotranslation task as a function of the input sentence length J (y-axis is given in log scale). The complexity for the four different reordering constraints MON, GE, EG, and S3 is given. The complexity of the S3 constraint is close to J4 . ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Coreference Resolution Performance",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Translation Model (IBM Model 4)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Conditioning features for the probabilistic CFG used in the reported empirical trials ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Vocabulary size of IWSLT task (40K)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Examples of similar syntactic structures across different relation types. The head words of the first and the second arguments are shown in italic and bold, respectively. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: DRS and corresponding DRG (in tuples and in graph format) for \u201cA customer did not pay.\u201d",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Examples of unigram and bigram features extracted from Figure 1. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Effectiveness of Extracting Common Topics",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: English MWEs and their components with their translation in Persian. Direct matches between the trans- lation of a MWE and its components are shown in bold; partial matches are underlined. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Experiment results (as F1 scores) where IM is identification of mentions and S - Setting. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Comparison of Min-, Simple-, Full-and Dynamic-Expansions: More Examples",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: The total costs for the three MTurk subtasks in- volved with the creation of our Dialectal Arabic-English parallel corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Word Alignment Accuracy (F-measure) and Machine Translation Quality for BiTAM Models, comparing with IBM Models, and HMMs with a training scheme of 18 h7 43 on the Treebank data listed in Table 1. For each column, the highlighted alignment (the best one under that model setting) is picked up to further evaluate the translation quality. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Distribution of Class Labels in the WSJ Section of the Penn TreeBank. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Basic Travel Expression Corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 Illustration of the paraphrase degree calculation.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. A Morphophonological Rule",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Training instances obtained from Verb- Net (upper) and VerbNet+SemLink (lower) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Activity detection: Activities are detected on the Santa Barbara Corpus (SBC) and the meet- ing database (meet) either without clustering the activities (all) or clustering them according to their interactivity (interactive) (see Sec. 2 for details). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Mechanical evaluation of translation",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Impact of scaling techinques (ILP\u2212 /ILPscale ).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Translation results for English-French",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2. PAT-Tree Instantiation for Figure 1. In the extraction process, the PAT-tree is ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Segmentation results by the pure subword-based IOB tagging. The separator \u201c/\u201d divides the results by three lexicon sizes as illustrated in Table 3. The first is character-based (s1), while the other two are subword-based with different lexicons (s2/s3). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Cross-source Comparable Data Example (each morph and target pair is shown in the same color) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Performance of WSD system using various combinations of learning algorithms and features.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: LL results tested against gs-swaco",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Syntactic features. h and ld mark features from the head and the left-most daughter, dir is a binary fea- ture marking the direction of the head with respect to the current token. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Weights learned for discount features. Nega- tive weights indicate bonuses; positive weights indicate penalties. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: The pronoun resolution algorithm by incorporating coreferential information of can- didates ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example dependency tree.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results of the first run (without postprocessing)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Nouns and verbs supersense labels, and short description (from the Wordnet documentation).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example of 1-to-n word alignments be- tween English words and Chinese characters ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Derivation by Means of Adding a Suffix",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3 Accuracy for words with high confidence measure. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Comparison of our system with other best-reported systems on the ACE corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4 Precision by Named Entity Class",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 The top 20 most similar words for country (and their ranks) in the similarity list of LIN, followed by the next four words in the similarity list that were judged as entailing at least in one direction. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Mixed-case TER and BLEU, and lower-case METEOR scores on Arabic NIST MT03+MT04. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: The demographics of RWSData. No, RW, RA, SbL, WbL, TS, and TD are labeled as (N)umber (o)f, (R)elated (W)orks, (R)eferenced (A)rticles, (S)entence-(b)ased (L)ength of, (W)ord- (b)ased (L)ength of, (T)ree (S)ize, and (T)ree (D)epth, respectively. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 Initial NE recognition type-insensitive (type-sensitive) performance across various domains. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results of segmentation of entry titles (F-score (precision/recall)).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 12 Results of human evaluation performed via Amazon Mechanical Turk. The percentages represent the portion of sentences for which one system had more preference judgments than the other system. If a sentence had an equal number of judgments for the two systems, it was counted in the final row (\u201cneither preferred\u201d). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: An non-regular OT approximation.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Final results on 25 corpora in 20 languages, with the number of induced classes equal to the number of gold standard tags in all cases. k-means and SVD2 models could not produce a clustering in the Czech CoNLL corpus due its size. Best published results are from \u2217 Christodoulopoulos et al. (2010), \u2020 Berg-Kirkpatrick et al. (2010) and \u2021 Lee et al. (2010). The latter two papers do not report VM scores. No best published results are shown for the MULTEXT languages; Christodoulopoulos et al. (2010) report results based on 45 tags suggesting that clark performs best on these corpora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10. MRRs for the phonetic transliteration 2",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Part of a sample headline cluster, with aligned paraphrases ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4 The path of Selection. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 24 Accuracies of our phrase-structure parser on CTB5 using gold-standard and automatically assigned POS-tags. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: The animacy data set from Talbanken05; number of noun lemmas (Types) and tokens in each class. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Individual basic classifiers\u2019 contribution to the final classifier combination performance.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Ten highest-scoring matches for the Xin- hua corpus for 8/13/01. The final column is the \u2212log P estimate for the transliteration. Starred entries are incorrect. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Smoothed recall curves over the five corpora.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example entries for the Transfer of a Message - levels 1 and 2 classes",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Graph for a neo-Davidsonian structure.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Breakdown of results by supersense",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Related research integrating context into word-based SMT (WB-SMT) models",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Results by relation",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Improvement in (predicted mention) RE.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: LO cosine sentence configuration scores",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Number of unique entries in training and    test sets, categorized by semantic attributes ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Removal and reduction of constituents using dependencies",
        "Entity": "Caption"
    },
    {
        "Text": "Table 21: Arabic Vocalization Problem",
        "Entity": "Caption"
    },
    {
        "Text": "Table 13 Transition-based feature templates for the dependency parser. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Evaluation of coarse-grained POS tagging on test data ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: F-score of two segmenters, with (\u2212) and without (+) word token/type features. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Examples for correct templates that were learned by TEASE for input templates. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: A relative comparison of running a se- lection of regular expressions and scripts against other finite-state toolkits. The first and second en- tries are short regular expressions that exhibit ex- ponential behavior. The second results in a FSM with 221 states and 222 arcs. The others are scripts that can be run on both Xerox/PARC and Foma. The file lexicon.lex is a LEXC format English dic- tionary with 38418 entries. North Sami is a large lexicon (lexc file) for the North Sami language available from http://divvun.no. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 Experiments on the threshold\u2013partial recall relationship of the large corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Number of extracted paraphrases.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Test corpora statistics.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Improvements of different tree setups",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Estimated precision on human-evaluation experiments of the highest-ranked 100 and 1000 results per relation, using stratified samples. \u2018Average\u2019 gives the mean precision of the 10 relations. Key: Syn = syntactic features only. Lex = lexical features only. We use stratified samples because of the overabundance of location-contains instances among our high-confidence results. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: The whole process of re-training the upper case NER. Q signifies that the text is converted to upper case before processing. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: InvR scores ranked by difference, Giga- word to Web Corpus ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Results when using unsupervised dependency parsers. Cells contain averaged % BLEU on the three test sets and % BLEU on tuning data (MT03) in parentheses. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: TV show types: The distribution of show types in a large database of TV shows (1067 shows) that has been recorded over the period of a couple of months until April 2000 in Pittsburgh, PA ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Contribution of features",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Corpus Excerpt with Dialogue Act Annotation",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Subgraph of Local\u22171 output for\u201cheadache\u201d",
        "Entity": "Caption"
    },
    {
        "Text": "Table 17 Example translations for the translation direction French to English using the S3 reordering constraint. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Violation permutation transducer.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3:  Recall for morphological hasXY() descriptions ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Unbalanced vs. balanced combining. All runs ignored the context. Evaluated on the Test data set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Test accuracy with unsupervised training methods",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7: Percentage of obtaining two clusters when applying CW on n-bipartite cliques ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Performance comparison on the ACE 2003/2003 data over both 5 major types (the numbers outside parentheses) and 24 subtypes (the numbers in parentheses) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Tagging accuracies on test data.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Syntactic Seeding Heuristics",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7: Accuracy of underlying segment hypotheses.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 \u2013 Pk for C99 corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Classification results with decision tree on joined feature set (Method 5) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Parser performance on Brown;E, baselines. Note that the Gildea results are for sentences \u2264 40 words in length. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Rhetorical pattern of C-Question",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Evolution of \u03c4A means relative to the length of the n-best sequence ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Cluster features ordered by importance.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Multi-reference word error rate (mWER)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 9: MUC-5: Level Distribution of Each of the Five Facts",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Event descriptions spread across two sentences ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 \u2013 Error rates for Le Monde corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Lattice representation of the sentence bclm hneim. Double-circles denote token boundaries. Lattice arcs correspond to different segments of the token, each lattice path encodes a possible reading of the sentence. Notice how the token bclm have analyses which include segments which are not directly present in the unsegmented form, such as the definite article h (1-3) and the pronominal suffix which is expanded to the sequence fl hm (\u201cof them\u201d, 2-4, 4-5). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Final PARSEVAL F1 scores for constituents on the test set for the predicted setting. ST Baseline denotes the best baseline (out of 2) provided by the Shared Task organizers. Our submission is underlined. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 \u2013 Precision/recall for Le Monde corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Part-of-speech annotations of the three-    character strings \u7d30\u67f3\u71df xi liu ying \u2018Little Willow military camp\u2019 and \u65b0\u8c50\u5e02 xin feng shi  \u2018Xinfeng city\u2019. Both are \u2018strings with internal structures\u2019, with nested structures that perfectly   match at all three levels. They are the noun phrases that end both verses in the couplet \u5ffd\u904e               \u65b0\u8c50\u5e02, \u9084\u6b78\u7d30\u67f3\u71df. ",
        "Entity": "Caption"
    },
    {
        "Text": "  Table 1. Data sets used for our alignment quality experiments. The total number of sentences in the respective corpora are given along with the number of sentences and    gold-standard (S)ure and (P)ossible alignment links in the corresponding test set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Some words extracted from the large corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Graph of the one-to-one accuracy of our full model (+FEATS) under the best hyperparameter setting by iteration (see Section 5). Performance typically stabi- lizes across languages after only a few number of itera- tions. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: The highest ranked phrasal verb candidates from our full system that do not appear in either Wiktionary set. Candidates are presented in decreasing rank; \u201cpat on\u201d is the second highest ranked candidate. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Comparison of the structured feature and the flat features extracted from parse trees ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Structure of a three-pass machine translation system with the new regeneration pass. The original N-best translations list (N- best1) is expanded to generate a new N-best translations list (N-best2) before the rescoring pass. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Coverage of caseframes in summaries with respect to the source text. The model aver- age is statistically significantly different from all the other conditions p < 10\u22128 (Study 3). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: For each domain the percentage of target domain words (types) that are unseen in the source together with the most frequent OOV words. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 Feature templates used for CRF in our experiments",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: The sizes of error models as automata",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Summary for graphs and test datasets obtained from each seed pair ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 17 Training the Easy-First Parser on gold and predicted tags, accuracy by gold attachment type (selected): subject, object, modification (of a verb or a noun) by a noun, modification (of a verb or a noun) by a preposition, idafa, and overall results (repeated). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2 Examples of dependency trees with word alignment. Arrows are drawn from children to parents. A child word is a modifier of its parent. Each word has exactly one parent and $ is a special \u201cwall\u201d symbol that serves as the parent of all root words in the tree (i.e., those with no other parent). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Feature space growing curve. The horizontal scope X[i:j] denotes the introduction of different tem- plates. X[0:5]: Cn (n = \u22122..2); X[5:9]: Cn Cn+1 (n = \u22122..1); X[9:10]: C\u22121 C1 ; X[10:15]: C0 Cn (n = \u22122..2); X[15:19]: C0 Cn Cn+1 (n = \u22122..1); X[19:20]: C0 C\u22121 C1 ; X[20:21]: W0 ; X[21:22]: W\u22121 W0 . W0 de- notes the current considering word, while W\u22121 denotes the word in front of W0 . All the data are collected from the training procedure on MSR corpus of SIGHAN bake- off 2. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Evaluation of the GUITAR improvement - summarization ratio: 15%.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Learning curves showing the effects of increas- ing the size of dialectal training data, when combined with the 150M-word MSA parallel corpus, and when used alone. Adding the MSA training data is only use- ful when the dialectal data is scarce (200k words). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Ablation study of the web (w), query- log (q) and table (t) features (bold letters indicate whole feature families). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Accuracy scores for the CoNLL 2009 shared task test sets. Rows 1\u20132: Top performing systems in the shared CoNLL Shared Task 2009; Gesmundo et al. (2009) was placed first in the shared task; for Bohnet (2010), we include the updated scores later reported due to some improvements of the parser. Rows 3\u20134: Baseline (k = 1) and best settings for k and \u03b1 on development set. Rows 5\u20136: Wider beam (b1 = 80) and added graph features (G) and cluster features (C). Second beam parameter b2 fixed at 4 in all cases. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7: A comparison between QA semantic parsing approaches[12]",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 10: Networks Illustrating Steps 2 and 3 of the Compile-Replace Algorithm ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Performance on T2 using a pre-defined tree structure. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: NIST BLEU scores on the German-English (de- en) and French-English (fr-en) Europarl test2008 set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Comparison to onlySL and onlyGraph.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6 Comparison between three different decoders for word segmentation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Contingency table for the children of  liquid  in the object position of drink. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Numbers of relations on the ACE RDC 2004: break down by relation types and subtypes ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3. Accuracy and recall as functions of the number of monthly query logs used to train the language model ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Rhetorical pattern of C-Colon",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: good#a#15 gloss and examples.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Effect of dictionary scale",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8: a) Parsing of input sentence[78]",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Experimental results with individual features, compared against Moses and the moses-chart baseline. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Results on WMT-2013 (blindtest)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Sample of extracted entailment rules.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Signature caseframe densities for differ- ent sets of summarizers, for the initial and update guided summarization tasks (Study 2). \u2217 : p < 0.005. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Selected document statistics for three JDPA Corpus document sources.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 10 Lattice dependency parsing using an arc-factored dependency model. Lone indices like p and i denote nodes in the lattice, and an ordered pair like (i, j) denotes the lattice edge from node i to node j. S TART is the single start node in the lattice and F INAL is a set of final nodes. We use edgeScore(i, j) to denote the model score of crossing lattice edge (i, j), which only includes the phrase-based features h 0 . We use arcScore((i, j), (l, m)) to denote the score of building the dependency arc from lattice edge (i, j) to its parent (l, m); arcScore only includes the QPD features h 00 . ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Feature templates for POS tagging. wi is the ith word in the sentence, ti is its POS tag. For a word w, cj (w) is its j th character, c\u2212j (w) is the last j th character, and l(w) is its length. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Frequency of Major Relation SubTypes in the ACE training and devtest corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The relationship extraction system.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Verb classes (see Section 3.1), their Levin class numbers, and the number of experimental verbs in each (see Section 3.2). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Comparison of raw input and constrained input.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Topics with MWEs",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A portion of the local co-occurrence graph for \u201cmouse\u201d from the SemEval-2010 Task 14 corpus ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Name Pairs Mined Using Previous Methods",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6 A paraphrase generation lattice for the sentence He ate lunch at a cafe near Paris. Alternate paths between various nodes represent phrasal replacements. The probability values associated with each edge are not shown for the sake of clarity. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: The parts of taxonomic names",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Comparison of the effect of morphological segmentation when translating MSA web text and Dialectal Arabic web text. The morphological segmentation uniformly improves translation quality, but the improvements are more dramatic for MSA than for Dialectal Arabic when comparing similarly-sized training corpora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 4 The dependency parse tree of the English sentence Can you play my favourite old record? and the dependency features extracted from it for the SMT phrase play my favourite ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6. Size of the test data",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Segmentation performance presented in previous work and of our combination model. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 14: An example result of BioAR",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Multiple Bayesian learning runs (using anneal- ing with temperature decreasing from 2 to 0.08) for POS tagging. Each point represents one run; the y-axis is tag- ging accuracy and the x-axis is the \u2212 log P(derivation) of the final sample. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7. Performance Comparison of Combined              Model and KLD Model ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: micro-average F1 and AUC for the algorithms.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Overview of the tasks investigated in this paper (n: size of n-gram; POS: parts of speech; Ling: linguistic knowledge; Type: type of task) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Anaphora resolution preferences.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example of the non-relation Same-Unit",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Related research integrating context into alternative SMT models",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Development results for POS+MORPH tagging. Given are training times in minutes (TT) and accuracies (ACC). Best baseline results are underlined and the overall best results bold. * indicates a significant difference between the best baseline and a PCRF model. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Corpus of complex news stories.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Comparison of size of k-best list for cube decoding with various feature sets. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Evaluation of Correction and Inference Mechanisms",
        "Entity": "Caption"
    },
    {
        "Text": "  Figure 3. High TF/ITF words in \u201cCom-Com\u201d (Numbers are TF/ITF score, frequency in the collec-  tion (TF), frequency in the corpus (TF) and word) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Illustration of a worsening filter for morpheme boundaries. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results \u2014 Evaluation A.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: HeiST baseline, cross-lingual projection, SVM.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: 50-document corpora averages",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Using Basic Features to Filter Answers",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Results of 1000 sentences",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: The average number of source text sen- tences needed to cover a summary sentence. The model average is statistically significantly differ- ent from all the other conditions p < 10\u22127 (Study 1). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Resampling probabilities for alternations, after 1000 iterations. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Size of the vocabularies for the \u201cNo LP\u201d and \u201cWith LP\u201d models for which we can impose constraints. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 NER type-insensitive (type-sensitive) performance of different English NE recognizers. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 19 English NE recognition on test data after semi-supervised learning. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Example of a word lattice",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 6 Distances found between phrase boundaries with linked modifier words and with parent words",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Overall results Coverage/Accuracy",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Evaluation of the Russian n-gram model.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Global features in the entity kernel for reranking. These features are anchored for each entity instance and adapted to entity categories. For example, the entity string (first feature) of the entity \u201cUnited Nations\u201d with entity type \u201cORG\u201d is \u201cORG United Nations\u201d. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Result of our topic similarity model in terms of BLEU and speed (words per second), comparing with the traditional hierarchical system (\u201cBaseline\u201d) and the topic-specific lexicon translation method (\u201cTopicLex\u201d). \u201cSimSrc\u201d and \u201cSimTgt\u201d denote similarity by source-side and target-side rule-distribution respectively, while \u201cSim+Sen\u201d acti- vates the two similarity and two sensitivity features. \u201cAvg\u201d is the average B LEU score on the two test sets. Scores marked in bold mean significantly (Koehn, 2004) better than Baseline (p < 0.01). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 18 Type-sensitive improvement for Chinese/English NER. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: space distribution of most reliable",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Feature blending of translation models",
        "Entity": "Caption"
    },
    {
        "Text": "Table 14 Top four worst-case statistics of features for NE boundary errors. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Adapting a parser to a new annotation style. We learn to parse in a \u201ctarget\u201d style (wide column label) given some number (narrow column label) of supervised target-style training sentences. As a font of additional features, all training and test sentences have already been augmented with parses in some \u201csource\u201d style (row label): either gold-standard parses (an oracle experiment) or else the output of a parser trained on 18k source trees (more realistic). If we have 0 training sentences, we simply output the source-style parse. But with 10 or 100 target-style training sentences, each off-diagonal block learns to adapt, mostly closing the gap with the diagonal block in the same column. In the diagonal blocks, source and target styles match, and the QG parser degrades performance when acting as a \u201cstacked\u201d parser. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Dependency structure of a sentence.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Senses found by our algorithm from first order cooccurrences (LM-1 and LAT-1)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: The effect of language and gender in-",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: ANC pronoun resolution accuracy for varying SVM-thresholds. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Word-to-word alignment.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: The chunking results for the six systems associated with the project (shared task CoNLL- 2000). The baseline results have been obtained by selecting the most frequent chunk tag associ- ated with each part-of-speech tag. The best results at CoNLL-2000 were obtained by Support Vector Machines. A majority vote of the six LCG sys- tems does not perform much worse than this best result. A majority vote of the five best systems outperforms the best result slightly (     error re- duction). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Log-likelihood of samples plotted against iter- ations. Dark lines show the average over five runs, grey lines in the back show the real samples. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: DRS for the sentence \u201cI saw nothing suspi- cious\u201d ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Statistics for the ACE corpus.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: The 7 speakers from ICSI-MRDA dataset used in our experiments. The table lists: the Speaker ID, orig- inal speaker tag, the type of meeting selected for this speaker, the number of meetings this speaker participated and the total number of dialogue acts by this speaker. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Discovered metaphorical associations",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Parsing scores of the various systems",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 12: The Malay fst After the Application of Compile-Replace to the Lower-Side Language",
        "Entity": "Caption"
    },
    {
        "Text": "Table 12 Experiments combining dependency relations, words and part-of-speech",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Automatically generated training set examples.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Unlabeled TedEval scores (accuracy/exact match) for the test sets in the predicted segmentation set- ting. Only sentences of length \u2264 70 are evaluated. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: A comparison of the effectiveness of performing Levantine-to-MSA mapping before translating into English, versus translating directly from Levantine into English. The mapping from Levantine to MSA was done manually, so it is an optimistic estimate of what might be done automatically. Although initially helpful to the MSA baseline system, the usefulness of pivoting through MSA drops as more dialectal data is added, eventually hurting performance. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Comparison of results for MUC-6",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Different tree span categories with SPT (dotted circle) and an ex- ample of the dynamic context-sensitive tree span (solid circle) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Performance of TL-NE, BL and BL-A as the number of seed instances S of the target type increases. (H = 500. \u03bbT\u00b5 was set to 104 and 102 ). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: MUC-7: Level Distribution of Each of the Facts",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Salient features for fire and the violence cluster",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: A protein domain-referring phrase example",
        "Entity": "Caption"
    },
    {
        "Text": "Table 12 Large-scale clustering on D2. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Illustration of search in statistical trans- lation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results for German-English",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Detailed DIFF results",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Results changing beam width k of the tree",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Stemmed results on 3,138-utterance test set. Asterisked results are significantly better than the baseline (p \u2264 0.05) using 1,000 iterations of paired bootstrap re-sampling (Koehn, 2004). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Screenshot of ConAno",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Corpus characteristics for translation task. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The screenshot of our web-based system shows a simple quantitative analysis of the frequency of two terms in news articles over time. While in the 90s the term Friedensmission (peace operation) was predominant a reverse tendency can be observed since 2001 with Auslandseinsatz (foreign intervention) being now frequently used. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Plurality language families across 20 clusters. The columns indicate portion of lan- guages in the plurality family, number of lan- guages, and entropy over families. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Overlaid bilingual embeddings: English words are plotted in yellow boxes, and Chinese words in green; reference translations to English are provided in boxes with green borders directly below the original word. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8. EDT and mention detection results.",
        "Entity": "Caption"
    },
    {
        "Text": "  Figure 2. Automatically detected posture points (H = headDepth, M = midTorsoDepth, L = lowerTorsoDepth) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Examples for the effect of equivalence classes resulting from dropping morpho-syntactic tags not relevant for translation. First the translation using the original representation, then the new representation, its reduced form and the resulting translation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Overview of the WEBRE algorithm (Illustrated with examples sampled from experiment results). The tables and rec- tangles with a database sign show knowledge sources, shaded rectangles show the 2 phases, and the dotted shapes show the sys- tem output, a set of Type A relations and a set of Type B relations. The orange arrows denote resources used in phase 1 and the green arrows show the resources used in phase 2. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Results for different user simulations. Numbers give % reductions in keystrokes. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Summary of supersense tagging accuracies",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 Japanese-to-English Display of NICT- ATR Speech-to-Speech Translation System ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 17: Six Accepted Word Orders in Russian",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Feature set for coreference resolution (Feature 22, 23 and features involving Cj are not used in the single-candidate model) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: PubMed Results. The curve represents the Pareto Frontier of all results collected after multiple runs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Comparison against Stevenson and Joanis (2003)\u2019s result on T1 (using similar features). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Individual feature type contribution to perfor- mance. Fields marked with \u00a3 indicate that the difference in performance was not statistically significant at a     level (paired McNemar test). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Examples of learned pronoun probabilities.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. System performance on the is-a relation on the CHEM dataset. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Examples of non-phonetic translations.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Learning curves of systems with different features",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Language-dependent lexical features. A word list can be collected to encode different ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 13: Correlation between manual and automatic scores for German-English",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Parallel topics extracted by the bLSA model. Top words on the Chinese side are translated into English for illustration purpose. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Learning curves in terms of word predic- tion accuracy on deciding between the confusable pair there, their, and they\u2019re, by IGT REE trained on TRAIN - REUTERS, and tested on REUTERS, AL - ICE, and BROWN . The top graphs are accuracies at- tained by the confusable expert; the bottom graphs are attained by the all-words predictor trained on TRAIN - REUTERS until 130 million examples, and on TRAIN - NYT beyond (marked by the vertical bar). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4 Word sense disambiguation accuracy for \u201cNP1 V NP2 to NP3\u201d frame. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results on MUC-4 entity extraction. C&J 2011 +granularity refers to their experiment in which they mapped one of their templates to five learned clusters rather than one. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2. System Pipeline (Test Procedure)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Example of creating a confusion net- work from the word alignments, and new hy- potheses generated through the confusion net- work. The sentence in bold is the alignment ref- erence. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6 Word sense disambiguation accuracy for \u201cNP1 V NP2\u201d frame. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Our segmentation results by the dictionary- based approach for the closed test of Bakeoff 2005, very low R-oov rates due to no OOV recognition applied. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Tagging and segmentation results on Estonian Multext-East corpus (Learned seg and Learned tag) com- pared to the semisupervised setting where segmentations are fixed to gold standard (Fixed seg) and tags are fixed to gold standard (Fixed tag). Finally the segmentatation results from Morfessor system for comparison are pre- sented. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: The BLEU score of self-trained h4 translitera- tion models under four selection strategies. nt (n=1..5) stands for the n-th iteration. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 15 Training the MaltParser on gold tags, accuracy by gold attachment type (selected): subject, object, modification (of a verb or a noun) by a noun, modification (of a verb or a noun) by a preposition, idafa, and overall results (repeated). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Comparison of paraphrase generators. Top: the MOSES baseline; middle and bold: the \u201ctrue-score\u201d MCPG; down: the \u201ctranslator\u201d MCPG. The use of \u201ctrue-score\u201d improves the MCPG per- formances. MCPG reaches MOSES performance level. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Official BakeOff2005 results. Keys: F - Regular Tagging only, all training data are used P1 - Regular Tagging only, 90% of training data are used P2 - Regular Tagging only, 70% of training data are used S - Regular and Correctional Tagging, Separated Mode I - Regular and Correctional Tagging, Integrated Mode ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 7. Example of fuzzy divisive clustering.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Final UAS/LAS scores for dependencies on the test sets for the predicted setting. Other denotes the highest scoring other participant in the Shared Task. ST Baseline denotes the MaltParser baseline provided by the Shared Task organizers. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Statistics of paraphrase pairs retrieved from MSRPC. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Translation outputs for the EN con- nective as, which was translated more correctly by SYSTEM 2 thanks to the disambiguating sense tags compared to the BASELINE that often just produces the prepositional as \u2013 jako. The erro- neous translations are marked in bold. The PDTB sense tags indicate the meaning of the CZ trans- lations and are encoded as follows: Synchrony (Sy), Asynchrony (Asy), Contingency (Co), Cause (Ca). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5. Sample poster scores.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Single-threaded speed and memory use on the perplexity task. The P ROBING model is fastest by a sub- stantial margin but generally uses more memory. T RIE is faster than competing packages and uses less memory than non-lossy competitors. The timing basis for Queries/ms in- cludes kernel and user time but excludes loading time; we also subtracted time to run a program that just reads the query file. Peak virtual memory is reported; final resident memory is similar except for BerkeleyLM. We tried both aggressive reading and lazy memory mapping where appli- cable, but results were much the same. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Evaluation of Turkish predictive text input.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 Corpus statistics for Hansards task (Words*: words without punctuation marks). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. The F-measure improvement between the BMM-based CWS and it with WSM in the MSR_C track (OOV is 0.034) using a, b, and c system dictionary. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Example of a MUC-4 template",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Dependency structure of text. Tree skeleton in bold ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Two STs composing a STN",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Example of trellis of the modified Viterbi search",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Disambiguation results in % dependent on frequency ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Mixed Membership MEDLDA",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Normalization accuracy after training on n tokens and evaluating on 1,000 tokens (average of 10 random training and evaluation sets), compared to the \u201cbaseline\u201d score of the full text without any normalization ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Contribution of feature sets (causality).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Performance of different relation types and major subtypes in the test data",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Impact of Three Subtasks on Coreference Resolution Performance. A score marked with a * indicates that a 0.5 threshold was used because threshold selection from the training data resulted in an extreme version of the system, i.e. one that places all CEs into a single coreference chain. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The role of the standard Basque (Batua) ana- lyzer in filtering out unwanted output candidates created by the induced rule set produced by method 1. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Coverage of verb association features by grammar/window resources.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Performance of Japanese Word Segmentation",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A correct tree (tree1) and an incorrect tree (tree2) for \u201cBCLM HNEIM\u201d, indexed by terminal boundaries. Erroneous nodes in the parse hypothesis are marked in italics. Missing nodes from the hypothesis are marked in bold. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 20 Feature templates for the phrase-structure parser. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Precision and recall for articles.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Distribution of domain labels of predom- inant senses for 38 polysemous words ranked using the SPORTS and FINANCE corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results for different feature sets, with corresponding feature size and train/test BLEU/PER. All multitask features give statistically significant improvements over the baselines (boldfaced), e.g. Shared Subspace: 29.1 BLEU vs Baseline: 28.6 BLEU. Combinations of multitask features with high frequency features also give significant improvements over the high frequency features alone. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Association frequencies for target verb.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Decoding as lattice parsing, with the highest-scoring translation denoted by black lattice arcs (others are grayed out) and thicker blue arcs forming a dependency tree over them. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: 5-fold cross-validation results on training data.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 18: Arabic idafa Construct",
        "Entity": "Caption"
    },
    {
        "Text": "Table 15: Experimental results of test corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Comparison of three statistical translation approaches (test on text input: 251 sentences = 2197 words + 430 punctuation marks). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Some of the words extracted from the small corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 13 The accuracy/speed tradeoff graph for the phrase-structure parser. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Translation quality results (BLEU-4 [%]) for newswire (nw) sets. Avg is the weighted averaged (by number of sentences) of the individual test set gains. All improvements are statistically significant at p \u2264 0.01. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 1. System architecture overview",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: English Eve corpus results. Standard deviations are in parentheses; \u2217 denotes a significant difference from the M ORTAG model. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Character- and word-based features of a possi- ble word wi over the input character sequence c. Suppose that wi = ci0 ci1 ci2 , and its preceding and following char- acters are cl and cr respectively. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Predicted (P) vs Observed (O) scores.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Substitution/deletion/insertion costs for /g/.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: the largest clusters from partitioning the second order graph with CW. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example candidate space of dimensionality 2. Note: I = {1, 2}, J(1) = J(2) = {1, 2, 3}. We also show a local scoring function hw (i, j) (where w = [\u22122, 1]) and a local gold scoring function g(i, j). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Comparison of our method (FEATS) to state-of-the-art methods. Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): The KM model uses a variety of orthographic features and employs the EM or LBFGS optimization algorithm; Posterior regulariation model (Grac\u0327a et al., 2009): The G10 model uses the posterior regular- ization approach to ensure tag sparsity constraint. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Average number of Pareto points",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Dependency accuracy on 13 languages. Unlabeled (UA) and Labeled Accuracy (LA). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Relative recall evaluation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: The effect of language detection",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: BS on NIST task",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4 The extended generic beam-search algorithm with multiple beams. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Distribution of Pronoun Mentions and Fre- quency of c-command Features ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3. Another derivation yielding same tree",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Bayesian dialogue act recognisers show- ing the impact of ASR N-best information. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. The scored results of our CWS in the MSR_C track (OOV is 0.034) for 3rd bakeoff. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10: Arabic Clitics - Example 2",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Bootstrapping for Name Tagging",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Dissimilarity of temporal distributions of \u2018WTO\u2019 in English and Chinese corpora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 The results of setting 4 (Punctuation and other encoding information are used; the maximum length is 30). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Average Precision, Recall and F1 at dif- ferent top K rule cutoff points. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Post-hoc analysis on the models built by the DAC system: some of the top features with corresponding feature weights in parentheses, for each individual tagger. (POS tags are capitalized; BOS stands for Beginning Of Sentence) ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 6. Algorithm for fuzzy divisive clustering based on nouns.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Feature interpolation of translation models: A=ICTCLAS, B=dict-hybrid, C=dict-PKU-LDC, D=dict-CITYU, E=CRF-AS",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A simplified version in Foma source code of the regular expressions and transducers used to bypass several sources of distortion within the DTs in order to parse variations of unseen input DTs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: German\u2013English results for hierarchical and syntactic models, in % BLEU ",
        "Entity": "Caption"
    },
    {
        "Text": "ACE-value; ECM-F: Entity-constrained Mention F-measure. MP uses                                                                      & & Table 3: Coreference results on true mentions: MP \u2013 mention-pair model;                                                                           EM                                                                             \u2013 entity-me                                                                             features wh features. None of the ECM-F differences between MP and EM is statistically significant at ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3 FSA for the pattern hit a e . ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5 NP agreement violations that were caught by the agreement filter system. (a) Noun-compound case that was correctly handled. (b) Case involving conjunction that was correctly handled. (c) A case where fixing the agreement violation introduces a PP-attachment mistake. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: F-score curves on the MSR, CU, and PKU datasets: ADF learning vs. SGD and LBFGS training methods.",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 5. Czech English BLEU scores of various al EM(Co), GS(Co), EM(Co)+GS(Co), and VB(Co). ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 6. F-measure for the objective and subjective classes for multilingual bootstrapping (versus cross-lingual framework).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Statistics of the Verbmobil test corpus for German-to-English translation. Unknowns are word forms not contained in the training corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4. Experimental result of total unknown",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Segmentation, Parsing and Tagging Results us- ing the Setup of (Cohen and Smith, 2007) (sentence length \u2264 40). The Models\u2019 are Ordered by Performance. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Encoding schemes (d = dependent, h = syntactic head, p = path; n = number of dependency types)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8. Related noun group and sum of Bayesian",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 shows the result of varying the number of samplers and iterations for all",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Comparison of different configurations.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5 The binary tree of Selection. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Details of the corpora. W.T. represents word types; C.T. represents character types; S.C. represents simpli\ufb01ed Chinese; T.C. represents traditional Chinese. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 13 Experiments applying individual features in English-to-Hindi translation",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Results of CT when MP is less than 0.875",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Accuracy of 5-fold cross-validation with sta-            tistics-based semantic features ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Corpora statistics of Bakeoff 2005",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Clustering-based stratified seed sampling",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Examples of improvement in MT output when training on our Dialectal Arabic-English parallel corpus instead of an MSA-English parallel corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11 Precision and partial recall of word lengths two to four of the first experiment on IT and AV. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Precision, Recall, and F1-score of Baseline, Engkoo, Google, and Ours over test sets Ti",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Effect of matching skip (F-score (precision/recall)).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Sample pairs of similar caseframes by relation type, and the similarity score assigned to them by our distributional model. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Performance on Zh-En with basic (left) and sparse (right) feature sets on MT03 and MT05.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Weights learned for generating syntactic nodes of various types anywhere in the English translation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Statistics for Verbmobil task: training corpus (Train), conventional dictionary (Lex), development corpus (Dev), test corpus (Test) (Words*: words without punctuation marks). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Dev-set results when using lattice parsing on top of an external lexicon/analyzer. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Precision-recall curve for rescoring",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: WS: word-segmentation.           Baseline: language-independent features. LexFeat: plus lex- ical features. Numbers are averaged over the 10 ex- periments in Figure 2. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Results on unseen test set for models which performed best on dev set \u2013 predicted input. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6 Dev-set results of using the agreement-filter on top of the lexicon-enhanced parser (starting from gold segmentation). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Development Sets Results.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Training and Test perplexities us- ing different contextual information and different thresholds \u008a . The reference perplexities obtained with the basic translation model 5 are TrainPP = 10.38 and TestPP = 13.22. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Evaluation results within sets",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8 Quasi-synchronous tree-to-tree configurations from Smith and Eisner (2006). There are additional configurations involving NULL alignments and an \u201cother\u201d category for those that do not fit into any of the named categories. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Number of recall errors according to mention type (rows anaphor, columns antecedent). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Few examples of the untranslatable tokens in forum posts ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 9: Wikipedia topics (T=400).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3 Training with scarce resources. \u201cRestructuring,\u201d \u201clearn phrases,\u201d and \u201cannotation\u201d all require morpho-syntactic analysis of the transformed sentences. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Effect of language and error models to speed (time in seconds per 10,000 word forms) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: BLEU scores after discriminative hypergraph- reranking. Only the language model (LM) or the transla- tion model (TM) or both (LM+TM) may be discrimina- tively trained to prefer the oracle-best hypotheses. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Part-of-speech tags of the Penn Chinese   Treebank that are referenced in this paper.      Please see (Xia, 2000) for the full list. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Thread length distribution.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Contribution of each feture. ALL: all features, PER: perceptron model, WLM: word language model, PLM: POS language model, GPR: generating model, LPR: labelling model, LEN: word count penalty. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. The initial performance of applying various sampling strategies to selecting the initial ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Sorted frequency of tags for WSJ. The gold standard distribution follows a steep exponential curve while the induced model distributions are more uniform. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Performance of the mention detection sys- tem using lexical features only. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: An example showing how to compute the target side position of a semantic role by using the median of its aligning points. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Lexical features. Top part: Adding each feature separately; difference from CORE 12 (predicted). Bottom part: Greedily adding best features from previous part. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Propagation: All items",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Results on the standard 14 CSSC data sets",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Graphical model of synonym pair gen- erative process ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Final accuracy with drift detection",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 \u2013 Pk for Le Monde corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Performance of Altavista counts and BNC counts for candidate selection for MT (data from Prescher et al. 2000) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Translation results in terms of BLEU score and translation edit rate (TER) estimated on newstest2010 with the NIST scoring script. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Outline of the segmentation process",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Classification of corpus token by type",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Performance of our system in the compe- tition ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 15 Most frequent SFC labels for all senses of polysemous words in WordNet, by part of speech. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: BLEU scores of English to Russian ma- chine translation system evaluated on tst2012 and tst2013 using baseline GIZA++ alignment and transliteration augmented-GIZA++ alignment and post-processed the output by transliterating OOVs. Human evaluation in WMT13 is performed on TA-GIZA++ tested on tst2013 (marked with *) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Baseline and oracle-best 4-gram BLEU scores with 4 references for NIST Chinese-English MT datasets. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7. Impacts of the mined semantic lexicons and the use of PubMed",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Impact of Joint Bilingual Name Tagging on Word Alignment (%). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: The 10 best languages for the verb component of BANNARD using LCS. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: General architecture of LINGUA",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: F-measure for the objective and subjective classes for multilingual bootstrapping (versus cross-lingual framework) ",
        "Entity": "Caption"
    },
    {
        "Text": "                     #name tokens/#all tokens(%) Figure 3: Word alignment gains according to the percentage of name words in each sentence. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Top-7 Chinese long-form candidates for the En- glish acronym TAA, according to the LH score. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Learning curves on the development dataset of the HK City Univ. corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Classification results with 5-gram and fre- quency threshold 4 (Method 2) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: AER results. IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM. We choose t = 1, 5, and 30 for the fertility HMM. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Statistical Information of Corpora",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Translated fragments, according to the lexicon.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Examples of DTs and their ICD-codes.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: The incompleteness of Freebase (* are must- have attributes for a person). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7: Contribution of feature sets (prevention).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Plate diagram depicting the joint model. Hyper- parameters have been omitted for clarity. The L-shaped plate contains the tokens, while the square plates contain the morphological analyses. The t are latent tags, zi is an assignment to a morphological analysis lk = (sk , fk ), and wi is the observed word. T is the number of distinct tags, and Kt the number of tables used by tag type t. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Examples of the three top candidates in the transliteration of English/Arabic, English/Hindi and English/Chinese. The second column is the rank. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results of UniGraph, BiGraph, and Bi- Graph*. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Dependency graph for Czech sentence from the Prague Dependency Treebank1",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: MRRs on the augmented candidate list.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: CoreLex\u2019s basic types with their corresponding WordNet anchors. CAM adopts these as meta senses.",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 2. Cell phone experiment result (17 aspects)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 9 Transition-based feature context for the dependency parser. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 4. Integration of confidence measures and interpolation \u2013 recall/precision curves.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Visualisation examples. Top: named en- tity recognition, middle: dependency syntax, bot- tom: verb frames. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Percentage of overlapping relations between KnowNet versions",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: General Knowledge Sources",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Single-threaded time and memory consumption of Moses translating 3003 sentences. Where applicable, models were loaded with lazy memory mapping (-L), prefaulting (-P), and normal reading (-R); results differ by at most than 0.6 minute. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2 Taxonomy of morphologically derived words (MDWs) in MSRSeg. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Time comparison between FSAs and FSRAs. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Distribution of isolated vs. initial posi- tion for the most frequent lexical items ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Dirichlet-Tree prior of depth two.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Comparison of training log likelihood of English LSA models bootstrapped from a Chinese LSA and from a flat monolingual English LSA. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results obtained by applying different types of features in isolation to the Baseline system.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Graph of words for the target word paper. Numbers inside vertices correspond to their degree. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Example context for the spelling confusion set {piece,peace} and extracted features ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Relation weights (Method 2)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Summary Results on the 2004 ACE Evaluation Data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 11 The four types of changes. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 Results per concept for the ILP-Global. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Cross-domain B3 (Bagga and Baldwin, 1998) results for Reconcile with its general feature set. The Paired Permutation test (Pesarin, 2001) was used for statistical significance testing and gray cells represent results that are not significantly different from the best result. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Results when tuning for performance over the development set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Examples of aggregated instances.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Accuracy of our system in each period (M = 10) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Development results for POS tagging. Given are training times in minutes (TT) and accuracies (ACC). Best baseline results are underlined and the overall best results bold. * indicates a significant difference (positive or negative) between the best baseline and a PCRF model. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Rule types in SSTb and HeiST",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: The classifier refining algorithm",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: An example CCG parse obtained from [60]",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: A chain-structured DCRF as our intra- sentential parsing model. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Gender detection accuracies (%) using a 4-gram language model for the letter sequence of          the source name in Latin script. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9: Scale-up to 160K on IWSLT data sets",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7: Corpus size vs. KL-divergence",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Feature templates for parsing, where X can be word, first and last character of word, first and last character bigram of word, POS tag. Xl+a /Xr\u2212a denotes the first/last ath X in the span, while Xl\u2212a /Xr+a denotes the ath X left/right to span. Xm is the first X of right child, and Xm\u22121 is the last X of the left child. len, lenl , lenr denote the length of the span, left child and right child respectively. wl is the length of word. ROOT/LEAF means the template can only generate the features for the root/initial span. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10 Simplified prevalence score, evaluation on SemCor, polysemous words only. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 1. The polarity classification (positive and negative) based on product aspect framework",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results obtained by adding different types of features incrementally to the Baseline system.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Turning distributional similarity into a weighted inference rule ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Example questions correctly answered by CCG-Distributional.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Table showing the number of pairs of different occurrences of the same token sequence, where one occurrence is given a certain label and the other occurrence is given a certain label. We show these counts both within documents, as well as over the whole corpus. As we would expect, most pairs of the same entity sequence are labeled the same(i.e. the diagonal has most of the density) at both the document and corpus levels. These statistics are from the CoNLL 2003 English training set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Snapshot of the supersense-annotated data. The 7 article titles (translated) in each domain, with total counts of sentences, tokens, and supersense mentions. Overall, there are 2,219 sentences with 65,452 tokens and 23,239 mentions (1.3 tokens/mention on average). Counts exclude sentences marked as problematic and mentions marked ?. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: A real translation example",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: A summary of the parsing and evaluation sce- narios. X depicts gold information, \u2013 depicts unknown information, to be predicted by the system. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Histogram of token movement size ver- sus its occurrences performed by the model Neu- big on the source english data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results for different predictor configura- tions. Numbers give % reductions in keystrokes. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Single systems (Basque) in cross- validation, sorted by recall. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6. Perplexity Comparison of Different               Pruning Methods ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 Confusion matrix for argument labels, with ArgM labels collapsed into one category. Entries are a fraction of total annotations; true zeros are omitted, while other entries are rounded to zero. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10: Overall transliteration performance",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 %BLEU on tune and test sets for ZH\u2192EN translation, showing the contribution of feature sets in our QPD model. Both QPD models are significantly better than the best Moses numbers on test sets 1 and 2, but not on test set 3. The full QPD model is significantly better than the version with only T GT T REE features on test set 1 but statistically indistinguishable on the other two test sets. Hiero is significantly better than the full QPD model on test set 2 but not on the other two. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7: A Path in a Transducer for English",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Evaluation against the monosemous (Pred.) and pol- ysemous (Multiple) gold standards. The figures in parentheses are results of evaluation on randomly polysemous data + sig- nificance of the actual figure. Results were obtained with fine- grained SCFs (including prepositions). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 12 Distribution of various error categories (type-insensitive). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Basic Statistics of DUC2007 Update Data Set",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Effectiveness of score propagation.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Descriptive statistics for Web scores and BNC scores for other-anaphora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 1. Integration of confidence measures \u2013 recall/precision curves (figures in the legend correspond to resp. \u03b41 and \u03b42 ).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 Most frequent syntactic positions for each semantic role. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 Semantic roles of verbs\u2019 subjects, for the verb classes of Merlo and Stevenson (2001). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: MUC-7: Level Distribution of Each of the Facts",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: the influence of features. (F: F-measure. Feature numbers are from Table 1) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Sample seeds used for each semantic relation and sample outputs from Espresso. The number in the parentheses for each relation denotes the total number of seeds. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Text normalization for FR-EN.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: GC examples.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Processed Data Statistics",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7 Algorithm for breadth-first search with pruning. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Final results on CTB-6 and CTB-7",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Listing of all seeds used for KEdis and KEpat , as well as the top-10 entities discovered by ES-all on one of our test folds. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Coverage/precision with various rule collections",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 5. Macro-accuracy for multilingual bootstrapping (versus cross-lingual framework).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8 A hierarchical summary of propositions involving nausea as an argument, such as headache is related to nausea, acupuncture helps with nausea, and Lorazepam treats nausea. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: \u201cAcceptance radius\u201d of an outlier within the training set (left) and a more \u201cnormal\u201d training set object (right) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Precision statistics for pronouns. Rows are pronoun surfaces, columns number of cluster- ing decisions and percentage of wrong decisions for all and only anaphoric pronouns respectively. ",
        "Entity": "Caption"
    },
    {
        "Text": " Table 2. Performance of English system with perfect mentions and perfect relations ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: BLEU scores for pre-ordering experi- ments with a n-code system and the approach pro- posed by (Neubig et al., 2012) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Discourse tree for two sentences in RST-DT. Each of the sentences contains three EDUs. The second sentence has a well-formed discourse tree, but the first sentence does not have one. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example of semantic trees",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7 The adjusted frequencies of character sequences. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Overall steps of proposed method",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8: A Network with a Regular-Expression Substring on the Lower Side ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: CTB 10-fold CV word segmentation F- measure using an all-at-once approach ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 DP-based algorithm for solving traveling-salesman problems due to Held and Karp. The outermost loop is over the cardinality of subsets of already visited cities. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results on Named Entity Recognition",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Overall results in experiments with au- tomatic features compared to gold standard fea- tures, expressed as unlabeled and labeled attach- ment scores. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2. F1-measure with \uf062 in [0,1]",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Average Metric Rank in NIST Metrics MATR 2008 Official Results",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11 Feature ablation experiments for UR\u2192EN translation with string-to-tree features, showing the drop in BLEU when separately removing word (W ORD), cluster (C LUST), and configuration (C FG) feature sets. \u2217 = significantly worse than T GT T REE. Removing word features causes no significant difference. Removing cluster features results in a significant difference on both test sets, and removing configuration features results in a significant difference on test 2 only. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Plate diagram representation of the trigram HMM. The indexes i and j range over the set of tags and k ranges over the set of characters. Hyper-parameters have been omitted from the figure for clarity. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Network Schema of Morph-Related Het- erogeneous Information Network ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: CCG derivation as generated by the C&C tools",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Spelling correction precision (%), impact of adding parse features SVM trained on 1G words of news text, tested on 9-months of NYT data. *: Improvement of (NG+)LEX+PAR vs. (NG+)LEX is statistically significant. \u03b1: Improvement of NG+LEX+PAR vs. NG is statistically significant. &: Relative increase or decrease of error rate compared to \u201dNG+LEX\u201d #: As in Bergsma et al. (2009; 2010) no morphological variants of the words are used in evaluation ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: P, R and F1 fine-grained results for the resources evaluated at Senseval-3, English Lexical Sample Task ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results from the empirical evaluation, including the Bayesian model without PoS tags (Base- line), the alternating alignment-annotation algorithm (AAA), the corresponding method but with super- vised PoS taggers for both languages (Supervised), and comparable previous results on the same data. The number of alignment links |A|, of which |A \u2229 S| are considered (S)ure, and |A \u2229 P | (P)ossible, are reported. For convenience, precision (P ), recall (R), F1 score (F ) and Alignment Error Rate (AER) are also given. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Number of learned splits per POS category after five split-merge cycles. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: The decision tree (Nwire) for the system using the single semantic relatedness feature ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Positive and negative examples for entailment in the training set. The direction of entailment is from the left template to the right template. ",
        "Entity": "Caption"
    },
    {
        "Text": "          A u to m a tic M e tr ic s H u m a n E v a lu a tio n Figure 2: Scores based on Automatic Metrics and Human Evaluation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Performance of TL-comb and TL-auto as H changes. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Results for two kinds of headlines",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Question tracking interface to a summa- rization system. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results for the acquisition of subcategori- sation frames. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Acceptance rates for a noun phrase in the course of iteration. All models were with back-off mix- ing (+BM). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: An example where syntactic features help to link the PRO mention \u00d1\u00eb (hm) with its antecedent, the NAM                   ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results from WSD system applied to various sections of the NLM-WSD data set using a variety of fea- tures and machine learning algorithms. Results from baseline and previously published approaches are included for comparison. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Structure of the out-of-vocabulary word \u623d\u4282 \u483d\u543c \u2018English People\u2019. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Upper bound for combination. The error reduction (ER) rate is a comparison between the F-score produced by the oracle combination sys- tem and the character-based system (see Tab. 1). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Training Data Sizes for Common ESL Confused Words ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Graphical model for the Bayesian Query-Focused Summarization Model. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: NIST scores per Levenshtein distance",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Information Flow in the Xerox Arabic Demo. Input words from the user interface are transmitted across the Internet (dotted lines) and analyzed by a server, typically producing multi- ple analysis strings. Each analysis string is then generated in fully voweled form, combined with English glosses and then reformatted as HTML before being sent back across the Internet to the user\u2019s browser for display. The analyzer and gen- erator finite-state transducers (FSTs) are identical except that the lower side language of the genera- tor is limited to contain only fully-voweled words. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Contribution of different features over 43          relation subtypes in the test data ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Consistently formatted term translation                       pairs ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Number of features used according to different cut-off threshold. In the second column of the table are shown the number of features used when only the English context is considered. The third column correspond to English, German and Word-Classes contexts. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Bilingual training size vs. BLEU score (mid- dle line, left axis) and phrase table composition (top line, right axis) on Arabic Development Set. The baseline BLEU score (bottom line) is included for comparison. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: CCG derivation and unresolved semantics for the sentence \u201cI saw nothing suspicious\u201d",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: An example word which has very complex structures. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Two examples from the all-en dataset. Each example shows a reference translation, and the outputs of two machine translation systems. In each case, the output of MT system 1 is annotated as the better translation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Statistics of datasets.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Example of DIRT algorithm output. Most confident paraphrases of X put emphasis on Y ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Illustration of bottom-to-top search.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Dependency tree for the sentence \u201cPROT1 contains a sequence motif binds to PROT2.\u201d ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: a) An example of a document from Tu\u0308Ba-D/Z, b) an abbreviated entity grid representation of it, and c) the feature vector representation of the abbreviated entity grid for transitions of length two. Mentions of the entity Frauen are underlined. nom: nominative, acc: accusative, oth: dative, oblique, and other arguments ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Coreference relations in our dictionary.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2. Modified Viterbi search \u2013 stop-word treatment",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Key definitions for our model. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Types of features extracted for edge e from h to n",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Feature set for our pronoun resolution system(*ed feature is only for the single-candidate model while **ed feature is only for the twin-candidate mode) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Tuning Results.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 13 Evaluating predominant sense information for polysemous nouns on the Senseval-2 all-words task data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: F-measure for the objective and subjective classes for cross-lingual bootstrapping ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example of a term construction rule as a branch in a decision tree.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: German\u2013English translation results. Results are cumulative. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Example of a so-called semi-formal text, where one can see that here more time points are available, and that those can be complemen- tary to the time points to be extracted from formal texts. So, already at this level, a unification or merging of extracted time points is necessary. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: A Portion of the Syntactic Tree.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Experimental results for the two methods on the five corpora. PRE denotes precision, REC denotes recall, and F1 denotes F1-Measure. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10: Performance of Two Categories",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Counts of the number of times multiple occurrences of a token sequence is labeled as different entity types in the same document. Taken from the CoNLL training set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Categories of multi-character words that      are considered \u2018strings without internal  structures\u2019 (see Section 4.1). Each category is  illustrated with one example from our corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7. System Performance Comparison.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3:     Accuracies (%) for Leave-One- Out (LOO) and Only-One Word-Extraction-Rule Evaluation. none includes all words and serves for comparison. Important words reduce accuracy for LOO, but rank high when used as only rule. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Translation results for English-German",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: The System Performance Based on Each Single Feature Set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Accuracy results for binary decisions.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Clustering evaluation for the experiment with Named Entities ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11 Demonstration of the combination of the two pruning thresholds tC = 5.0 and tc = 12.5 to speed up the search process for the two reordering constraints GE and S3 (no = 50). The translation performance is shown in terms of mWER on the TEST-331 test set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Distribution of relations in ACE 2005.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Experimental results of CityU corpus measured in F-measure. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Check feature templates: G f\u0087&@j         ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Performance of different FS machines in terms of the percentage of unclassified entries. All the classified entries were correctly classified, yielding, as a result, a precision of 100%. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10: F-measures for different systems",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Devtest Set Statistics by Language",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9: ROUGE-SU in empirical approach",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Results of system combination on Dev7 (development) corpus and Test09,             the o\ufb03cial test corpus of IWSLT\u201909 evaluation campaign. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Supersense evaluation results. Values are the percentage of correctly assigned supersenses. k indicates the number of nearest neighbours considered. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11 Distribution of antecedent NP types for definite NP anaphora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Learning curves of word prediction accu- racies of IGT REE trained on TRAIN - REUTERS, and tested on REUTERS, ALICE, and BROWN. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: MT06 Dev. Optimization & Test Set Spearman Correlation Results",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: BLEU scores for CWS schemes",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: B3 results for baselines and lexicalized feature sets across four domains.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Evaluation results for all combinations of mixture adapted language and translation models: Baseline(bl) scores are italicized, best scores are in bold ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Precision and recall for different values of",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11 Accuracy of semantic-role prediction (in percentages) for unknown boundaries (the system must identify the correct constituents as arguments and give them the correct roles). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: 2 billion word corpus statistics",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Information access hierarchy: Oral com- munications take place in very different formats and the first step in the search is to determine the database (or sub-database) of the rejoinder. The next step is to find the specific rejoinder. Since re- joinders can be very long the rejoinder has to seg- mented and a segment has to be selected. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Evaluation of speech recognition",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results for morphological processing, English\u2192German ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: AER comparison (en\u2192cn)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Performance of various clustering-based seed sampling strategies on the held-out test data with the optimal cluster number for each clustering algorithm ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Multiple EM restarts for POS tagging. Each point represents one random restart; the y-axis is tag- ging accuracy and the x-axis is EM\u2019s objective function, \u2212 log P(data). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Three different vocabulary sizes used in subword- based tagging. s1 contains all the characters. s2 and s3 contains some common words. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Posterior likelihood at each of the first 100 iter- ations, from 4 runs (with different random seeds) on 10% of the Morphochallenge dataset (\u03b1i6=j = 0.001, \u03b1i=j = 100, \u03b2 = 0.1), indicating convergence within the first 15 iterations. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 Weights of different log-linear features of the CCG\u00b11 system",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: New training and testing procedures",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Example segmentations (\u201e|\u201f indicates the       separator between adjacent snippets) ",
        "Entity": "Caption"
    },
    {
        "Text": " Table 4. Performance of English system with system mentions and system relations ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Segmentation algorithm.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Possibility combination of neighboring        tokens within the corpus for PER ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. English Name Tagger",
        "Entity": "Caption"
    },
    {
        "Text": "                 sentence length Figure 6: Time consumption of the various change types in ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2 Disambiguation of conventional dictionaries. \u201cLearn phrases,\u201d \u201canalyze,\u201d and \u201cannotation\u201d require morpho-syntactic analysis of the transformed sentences. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 17 Type-insensitive improvement for Chinese/English NER. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Segmentation, tagging and parsing results on the Standard dev/train Split, for all Sentences",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 Comparing clustering initializations on D2. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: An example of the label consistency problem. Here we would like our model to encourage entities Albert Einstein and Einstein to get the same label, so as to improve the chance that both are labeled PERSON. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Statistics on the Italian EVALITA 2009       and English CoNLL 2003 corpora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Translation Performance (%).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Systems whose F-measures are not signif- icantly different from Alice-ME at the 0.10 signifi- cance level with 0.99 confidence ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Counts for the POS tags mentioned in Table 5.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Architecture of the Structured Output Layer Neural Network language model. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Number and ratio of statistically signifi- cant distinction between system performance. Au- tomatic scores are computed on a larger tested than manual scores (3064 sentences vs. 300\u2013400 sen- tences). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Example translations from the different methods. Boldface indicates correct translations. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4. Active Learning with Large Corpora",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Final system results (as F1 scores) where IM is identification of mentions and S - Setting. For more details cf. (Recasens et al., 2010). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9: Four examples showing the translation obtained with the Model 4 and the ME model for a given German source sentence. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 16 Experimental results on the WMT 2010 test set",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Accuracy of string slots on the TST3 and TST4 test set ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: String-to-tree configurations; each is associated with a feature that counts its occurrences in a derivation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8: MUC-4: Level Distribution of the Five Facts Combined",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Results on a truly independent test set, consisting of data harvested from Egyptian Facebook pages that are entirely distinct from the our dialectal training set. The improvements over the MSA baseline are still considerable: +2.9 BLEU points when no Facebook data is available for tuning and +2.7 with a Facebook tuning set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: SMT performance results",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: An example for the \u201cBMES\u201d representa- tion. The sentence is \u201c\u6211\u7231\u5317\u4eac\u5929\u5b89\u95e8\u201d (I love Bei- jing Tian-an-men square), which consists of 4 Chi- nese words: \u201c\u6211\u201d (I), \u201c\u7231\u201d (love), \u201c\u5317\u4eac\u201d (Beijing), and \u201c\u5929\u5b89\u95e8\u201d (Tian-an-men square). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 LIN (MI) weighting: The top 10 common features for country\u2013state and country\u2013party, along with their corresponding ranks in each of the two feature vectors. The features are sorted by the sum of their feature weights with both words. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. five different tree kernel setups on the ACE 2003 five major types using the parse tree structure information only (regardless of any entity-related information) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 Web results for other-anaphora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: The STTS tags PDAT and ART, their rep- resentation in the Annotation Model and linking with the Reference Model. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: An ATB sample from the human evaluation. The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a). But the city name Sharm Al- Sheikh is also iDafa, hence the possibility for the incorrect annotation in (b). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Feature templates and instances. Suppose we are considering the third character \u201dU\u201d in \u201de U /\u00a1\u201d.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Hypegraph size measured by the average number of hyperedges (h = 1 for CF). \u201clattice\u201d is the average number of edges in the original CN. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: An Initial Learning Curve for Confusable                   Disambiguation ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Parameters of Measures (Section 3) which, combined with particular WSMs, achieved the highest average correlation in TrValD. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Performance with noisy relevance judg- ments. The X-axis is the R-precision of the IR engine and the Y-axis is the summarization per- formance in MAP. Solid lines are BAYE S UM, dot- ted lines are KL-Rel. Blue/stars indicate title only, red/circles indicated title+description+summary and black/pluses indicate all fields. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Comparison to previous work on the 7 re- lations of ACE 2004. K: kernel-based; F: feature- based; yes/no: models argument order explicitly. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 10 Dutch-to-English Learning curves (left-hand side graphs) and difference curves (right-hand side graphs) comparing the Moses baseline against four context-informed models (PR, OE, POS\u00b12 and Word\u00b12). These curves are plotted with scores obtained using three evaluation metrics: BLEU (top), METEOR (centre) and TER (bottom) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Sizes of the automata.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Illustration of Pareto Frontier. Ten hypotheses are plotted by their scores in two metrics. Hypotheses indicated by a circle (o) are pareto-optimal, while those indicated by a plus (+) are not. The line shows the convex hull, which attains only a subset of pareto-optimal points. The triangle (4) is a point that is weakly pareto-optimal but not pareto-optimal. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Tradeoffs of precision and recall values in the experiments with method 1 using various different pa- rameters. When the unigram filter is applied the precision is much better, but the recall drops. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A typed narrative chain. The four top arguments are given. The ordering O is not shown. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: The contribution of MMVC in a rank-based classi- fier combination on S ENSEVAL -1 and S ENSEVAL -2 English as computed by 5-fold cross validation over training data ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Automatic role labeling results (%) using the HMM and Maxent classifiers. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Corpus statistics: Verbmobil training. Singletons are types occurring only once in train- ing. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11 Training, development, and test data from CTB5 for joint word segmentation and POS-tagging. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Comparison of performance across the five PPI corpora",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Word distribution in the extended Cilin",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Glue Semantics proof for (86), English Way Construction (means interpretation)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6 Conventional dictionary used to complement the training corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Examples of positive and negative words. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 Overview of the results for the best algorithms for other-anaphora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 IV and OOV recall in   (Zhang et al., 2006a) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Devoicing transducer compiled through a rule.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 12 NWI results on HK and AS corpora, NWI as post-processor versus unified approach. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Multiple Analyses for suis",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3. Accuracy of the proposed system on a set which contains misspelled queries that the users had reformulated ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Part of a sample headline cluster, with sub-clusters ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Feature sets for the dependency ranker for each language. default denotes the default ranker feature set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4. Examples of the top-3 candidates in the       transliteration of English \u2013 Chinese ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Experiments on the threshold\u2013precision relationship of the small corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Synonyms for chain",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: BS on IWSLT 2006 task",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Architecture of NILER system.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4 Distribution of domain labels of predominant senses for 38 polysemous nouns ranked using the SPORTS and FINANCE corpora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: BLEU scores as a function of development data size. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11: BS on IWSLT data sets using MTTK",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. The F-score over the bakeoff-2 data.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Evaluation of the manual annotation improvement - summarization ratio: 15%.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 10: Example of parser error. Tree (a) is correct, and (b) is the wrong result by our parser. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 14 Comparison scores for HK open and AS open. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Diff results tested against gs-swaco",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Bayesian Alignment Results.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Running example of graph creation",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Topics, along with associated regression coefficient \u03b7 from a learned 25-topic model on German-English (left) and German-Chinese (right) documents. Notice that theme-related topics have regression parameter near zero, topics discussing the number of pages have negative regression parameters, topics with \u201cgood,\u201d \u201cgreat,\u201d \u201cha\u030co\u201d (good) and \u201cu\u0308berzeugt\u201d (convinced) have positive regression parameters. For the German-Chinese corpus, note the presence of \u201cgut\u201d (good) in one of the negative sentiment topics, showing the difficulty of learning collocations. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Definitions for the top four senses of \u201claw\u201d according to WordNet ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Evaluation of the GUITAR system without DN detection over a hand-annotated treebank ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Sizes of rule application test set for each learned rule-set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Individual Performance of KSs for Disasters",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Network of relations. Edges indicate that the relations have a non-empty support inter- section, and edge labels show the size of the inter- section. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: The sizes of dictionaries as automata",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Example of morphological analyses.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: BLEU scores in translation experiments. E: En- glish, T: Turkish, C: Czech, A: Arabic. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6 Average Precision and Recall",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: F1 scores and speed (in sentences per sec.) of SegTagDep on CTB-5c-1 w.r.t. the beam size. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Some examples for MEDLINE tagset: Number of lex. entries per tag and sample words. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Lexical entailment precision values for top-n similar words by the Bootstrapped LIN and the original LIN method. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 17 Training, development, and test data for Chinese dependency parsing. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Accuracies (%) for Word-Extraction Us- Litkowski and Hargraves (2007) selected exam- ing MALT Parser or Heuristics.                  ples based on a search for governors8 , most anno- ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 12 SemCor results for Nouns using jcn. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 14 Graph-based feature templates for the dependency parser. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2 A latent layered POS tag representation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 14 Summary of results for unknown-boundary condition. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Procedure for scoring agreement for each hy- pothesis generated during the search algorithm of Fig. 4. In the extended hypothesis eI1 , the index n + 1 indicates the start of the new attachment. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Evaluation of context-sensitive convolution tree kernels using SPT on the major relation types of the ACE RDC 2003 (inside the parentheses) and 2004 (outside the parentheses) corpora. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Creation of a Lexical Transducer. The .o. operator represents the composition operation.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8 The hierarchical form of a result. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Performance of WSD system over individual ab- breviations in three reduced corpora ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Dendrogram of the participants cluster based on their feedback profile ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Hand-coded rules for supersense guessing",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: each line of the 6-best translations and BLEU scores with 1-best translation selected by the current param- eter \u03b1 ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Lexical Caseframe Expectations",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: A partially scaled and inverted identity matrix J\u00b5 . Such a matrix can be used to trans- form a vector storing a domain and value repre- sentation into one containing the same domain but a partially inverted value, such as W and \u00acW de- scribed in Figure 1. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Evaluation results of the methods.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Coverage of summary text caseframes in source text (Study 3).",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Classification results with decision tree on vectors of frequency of rarest n-grams (Method 4) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Baseline performance.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Results using 5-fold cross validation on S ENSEVAL- 1 training data (English) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10 Occurrences of error types for the best other-anaphora algorithm algoWebv4 . ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Evaluation results for ReWoS variants and baselines.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Average polysemy on SE2 and SE3",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Learning curves of bootstrapping meth- ods for semantic classification on TS1. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Gibbs sampling for Bayesian inference outperforms both EM and Variational Bayesian EM. \u2217 The output of EM alignment was used as the gold standard. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Results for verbs",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Participants in the shared task. Not all groups participated in all translation directions.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: F-measure(%) Breakdown by Mention Type: NAM(e), NOM(inal), PRE(modifier) and PRO(noun). Chinese data does not have the PRE type. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Open test, in percentages (%)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Macro-accuracy for multilingual bootstrapping (versus cross-lingual framework) ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 4. Simpli\ufb01ed Lesk algorithm [21].",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Monolingual and Crosslingual Baseline Slot Filling Pipelines ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: The semantic roles of cases beside C-3 verb cluster ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Properties of the manually aligned corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Trivial and single-feature baselines (using SVM- acc unless noted otherwise) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Correlations of resolution class scores with respect to the average. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10 Experiments with words and parts-of-speech as contextual features",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Comparing the best F-measure obtained by At-Least-N Voting with Majority Voting, Summing and the single best classifier. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Architecture of Name-aware Machine Translation System.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: ROUGE-W in empirical approach",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Distribution of elicited ratings for High and Low similarity items ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Incremental results for the four sieves using our dictionary on the development set. Baseline is the Stanford system without the WordNet sieves. Scores are on gold mentions. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: TreeTagger and RFTagger outputs. Starred word forms are modified during preprocessing.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Effect of supplementing recasing model training data with the test set source. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 13 Accuracy of semantic-role prediction for unknown boundaries (the system must identify the correct constituents as arguments and give them the correct roles). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: string insertion operation for right-to-left decoding method. A string e0 was prepended before the partial output string, e, and the first word in e 0 was aligned from f j . ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Sample of subjective features appearing in the top 100 discriminant attributes selected with Information Gain on the 3rd fold training data at iteration 10. The words in italics in the multilingual features represent equivalent translations in English and Romanian. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Impact of Syntactic Features on English Sys- tem After Taking out Distance Features. Numbers are F-measures(%). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Three kinds of tree kernels.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 18 Precision of person name recognition on the MSR test set, using Viterbi iterative training, initialized by four seed sets with different sizes. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Evaluation results for links",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Example coreferent paths: Italicized entities generally corefer.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Topics sorted by number of words assigned.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A Generic Morphological Analyzer as a Black Box ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Filtered 5-gram dataset statistics.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: The MCPG algorithm.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Accuracy of various instantiations of the system",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: The values of AP, Spearman (\u03c1) and Kendall (\u03c4 ) correlations between the LSA-based and PMI-based model respectively and the Gold data with regards to the expression type. Every zero value in the table corresponds to the theoretically achieved mean value of correlation calculated from the infinite number of correlation values between the ranking of scores assigned by the annotators and the rankings of scores being obtained by a random number genarator. Reddy-WSM stands for the best performing WSM in the DISCO task (Reddy et al., 2011b). StatMix stands for the best performing system based upon association measures (Chakraborty et al., 2011). Only \u03c1-All and \u03c4 -All are available for the models explored by Reddy et al. (2011b) and Chakraborty et al. (2011). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9: Comparison of our approach with using only the Gigaword corpus",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Intercoder agreement for activities: The meeting dialogues and Santa Barbara corpus have been annotated by a semi-naive coder and the first author of the paper. The \u03ba-coefficient is determined as in Carletta et al. (1997) and mutual information measures how much one label \u201cinforms\u201d the other (see Sec. 3). For CallHome Spanish 3 dialogues were coded for activities by two coders and the result seems to indicate that the task was easier. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: This log-log plot shows that there are many rare features and few common features. The probability that a feature occurs in x number of N- best lists behaves according to the power-law x\u2212\u03b1 , where \u03b1 = 2.28. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: SyntSem tagged corpus extract.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 The comparison between DLG, AV, BE, and ESA. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Mutual information between feature subset and class label with f req based feature ranking. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Impact of removing individual measures and us- ing a tuned weighting scheme (all numbers in %); results that significantly differ from Full are marked with aster- isks (* p<0.05; ** p<0.01). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 13 Descriptive statistics for WordNet hyp/syn relations on the coreference data set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4. System performance on the part-of relation on the TREC-9 dataset. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Confusion matrix among subtypes of ArgM, defined in Table 1. Entries are fraction of all ArgM labels. Entries are a fraction of all ArgM labels; true zeros are omitted, while other entries are rounded to zero. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: NIL expression forms based on word formation.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Performance vs. log start penalty",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 Effect of alignment template length on translation quality. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: An iterative algorithm for minimizing our ob- jective in Eq. (7). For simplicity we assume that all the weights \u03b1i and \u03bb are equal to one. It can be shown that the objective monotonically decreases in every iteration. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Pseudo code of our clustering algorithm",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Examples of zero anaphora",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Accuracy (recall) of systems on the two bench- marks. The systems are divided into three groups. Group 1 uses 10-fold cross-validation; groups 2 and 3 use the in- dependent test set. Groups 1 and 2 measure accuracy of logical form; group 3 measures accuracy of the answer; but there is very small difference between the two as seen from the Kwiatkowski et al. (2010) numbers. Our best system improves substantially over past work, despite us- ing no logical forms as training data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Semantic drift in CELL (n=20, m=20)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3. The porposed TSHAC algorithm.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8: Examples of the MT outputs with and without SRFs. The first and second example shows that SRFs improve the completeness and the ordering of the MT outputs respectively, the third example shows that SRFs improve both properties. The subscripts of each Chinese phrase show their aligned words in English. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Training corpus statistics (* without punctuation marks). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Baseline Pipeline Results",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Automatic evaluation with 50% of Freebase relation data held out and 50% used in training on the 102 largest relations we use. Precision for three different feature sets (lexical features, syntactic features, and both) is reported at recall levels from 10 to 100,000. At the 100,000 recall level, we classify most of the instances into three relations: 60% as location-contains, 13% as person-place-of-birth, and 10% as person-nationality. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Contingency table for the children of  canine  in the subject position of run. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Individuals for accusative and sin- gular in the TIGER Annotation Model ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Filtering out objective phrases",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Performance of Altavista counts and BNC counts for compound bracketing (data from Lauer 1995) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Example of word-dependent substitution costs.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 18 Test accuracies of various dependency parsers on CTB5 data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Chinese-English Results (% BLEU).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Relation Feature Spaces of the Example Sentence \u201c\u2026\u2026 to stop the merger of an estimated",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: A context modeling example.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results for sentence-based predicte alignment in the three benchmark settings MTC, Leagues and MSR (all numbers in %); results that significantly differ from Full are marked with asterisks (* p<0.05; ** p<0.01). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Simple parser vs full parser \u2013 morphological quality. The parsing models were trained on the first 5,000 sentences of the training data, the morphological tagger was trained on the full training set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6 Performance of proposed system on MSRVDC Dataset 1. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Parsing performance with each POS tag set, on gold and predicted input. L AS = labeled attachment accuracy (dependency + relation). U AS = unlabeled attachment accuracy (dependency only). L S = relation label prediction accuracy. L AS diff = difference between labeled attachment accuracy on gold and predicted input. POS acc = POS tag prediction accuracy. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: The effect of syntactic features when predicting morphology using lexicons. * mark statistically signifi- cantly better models compared to our baseline (sentence- based t-test with \u03b1 = 0.05). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A fragment of an entailment graph (a), its SCC graph (b) and its reduced graph (c). Nodes are predicates with typed variables (see Section 5), which are omitted in (b) and (c) for compactness. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7 The accuracy/speed tradeoff graph for the joint segmentor and POS-taggers and the two-stage baseline. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Lexicon-based phrase labeling",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: NIL expression forms based on POS attribute.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Manual analysis of suggested corrections on CLC data. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 25: Arabic Tokenization Schemes",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Vocabulary size of NIST task (40K)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. F-scores of U-DOP, UML-DOP and a  supervised treebank PCFG (ML-PCFG) for a   random 90/10 split of WSJ10 and WSJ40. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Results tested against gs-so",
        "Entity": "Caption"
    },
    {
        "Text": "Table 24: Splitting Compounds in Russian",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Results for feature combination.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: MUC-7: Level Distribution of the Facts Combined",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Estimation of model parameters. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 9: After the Application of Compile- Replace ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: An example confusion network construc- tion ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Annotation tool for manual judgement of adequacy and fluency of the system output. Translations from 5 randomly selected systems for a randomly selected sentence is presented. No additional information beyond the instructions on this page are given to the judges. The tool tracks and reports annotation speed. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5. Some subtrees from trees in figure 4",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11 French MWE identification per category and overall results (test set, sentences \u2264 40 words). MWI and MWCL do not occur in the test set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Effect of discriminatively learned penalties for OOV words. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Example nouns and their supersenses",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Effect of two-level lexicon combination. For the baseline we used the conventional one-level full form lexicon. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: a) A related work section extracted from (Wu and Oard, 2008); b) An associated topic hierar- chy tree of a); c) An associated topic tree, annotated with key words/phrases. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 FT detection results on the MSR gold test set. The \u2018All\u2019 column shows the results of detecting all 10 types of factoids, as described in Table 1, which amount to 6630 factoids, as shown in Table 3. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Usefulness of syntactic information: (black) dash-dotted line \u2013 word boundaries only, (red) dashed line \u2013 POS info, and (blue) solid line \u2013 full parse trees. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Multiple Bayesian learning runs (using averag- ing) for POS tagging. Each point represents one run; the y-axis is tagging accuracy and the x-axis is the average \u2212 log P(derivation) over all samples after burn-in. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10 Top 20 most similar words for country and their ranks in the similarity list by the Bootstrapped LIN measure. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3 Using Chinese translations as the distributional elements to extract a set of English paraphrastic patterns from a large English corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Word segmentation results.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: An example dependency parse tree rep- resentation. The subgraph represents a dependency relation feature between arg 1 \u201cPalestinians\u201d and \u201cof\u201d. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Voting under hand-invented schemes.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Translation performance of baseline and bLSA-Adapted Chinese-English SMT systems on manual transcriptions and 1-best ASR hypotheses ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Sizes of our comparable corpora",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Phrase pairs extracted from a document pair               with an economic topic ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Accuracy with different sizes of labeled data",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Effect of pruning parameter tp and heuristic function on error rate for direct-translation model (Np = 50,000). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Brown clusters in tree kernels (cf. Fig 2).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: The search graph on development set of IWSLT task ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 Precision of existing and proposed approaches. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 16 FSRA* for Arabic nominative definite nouns. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Comparison with previous work.",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 6. German English BLEU scores of various al EM(Co), GS(Co), EM(Co)+GS(Co), and VB(Co). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Learning Curve on RIBES: comparing single- objective optimization and PMO. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8 FSRA-2 for Arabic nominative definite and indefinite nouns. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Corpus size vs. accuracy",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7. Precision at top 200",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2. Example patterns in student discussion                    threads ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 A focused entailment graph. For clarity, edges that can be inferred by transitivity are omitted. The single strongly connected component is surrounded by a dashed line. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9: Example patterns of nominal interaction keywords ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Chunk-based translation model. The words in bold are head words.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Plate diagram of the basic model with a single feature per token (the observed variable f ). M , Z, and nj are the number of word types, syntactic classes z, and features (= tokens) per word type, respectively. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Comparison of the performance of the bootstrapped SVM method from (Zhang, 2004) and LP method with 100 seed labeled examples for relation type classification task. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Rhetorical pattern of C-Exclamation",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Numbers of expressions of all the differ- ent types from the DISCO and Reddy datasets. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Language-pair datasets.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Categories of multi-character words that  are considered \u2018strings with internal structures\u2019   (see Section 4.2). Each category is illustrated    with an example from our corpus. Both the   individual characters and the compound they              form receive a POS tag. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Results for the systems and original headline: \u2020 and \u2021 stand for significantly better than Unsupervised and Our system at 95% confidence, respectively ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Error analysis of confidence measure with and without EIV tag",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Analysis of results of segmentation on LDC training and test data for all CWS schemes",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Distribution of activity types: Both databases contain a lot of discussing, informing and story-telling activities however the meeting data contains a lot more planning and advising. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 %BLEU on tune and test sets for UR\u2192EN translation, using our unsupervised Urdu parser to incorporate source syntactic features. The two QPD rows are statistically indistinguishable on both test sets. Both are significantly better than all Moses results, but Hiero is significantly better than all others. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Outline of word segmentation process",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: S/O classifier with learned SWSD integration",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Results of the mapping algorithm",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Examples of DCS trees that use the aggregate relation (\u03a3) to (a) compute the cardinality of a set and (b) take the average over a set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Distribution of annotated data.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 The empirical formulae for the prediction (linear model). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Effect of language and error models to quality (recall, proportion of suggestion sets containing a cor- rectly suggested word) ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 1 Examples of ambiguity for the English word play, together with different translations depending on the context ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Accuracy for three classes on a general purpose list of 2,000 words. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Simulation comparing the expected table count (solid lines) versus the approximation under Eq. 3 (dashed lines) for various values of a. This data was gen- erated from a single PYP with b = 1, P0 (i) = 14 and n = 100 customers which all share the same tag. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Ungrammatical Arabic output of Google Trans- late for the English input The car goes quickly. The subject should agree with the verb in both gender and number, but the verb has masculine inflection. For clarity, the Arabic tokens are arranged left-to-right. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Segmentation precision/recall relative to gold word length in training data.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3. Voting Among Classifiers",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2. Description of the unrestricted     corpora used in the evaluation. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 Test-set results of the best-performing models. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4. Accuracy of 5-fold cross-validation with self-              extracted semantic features ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Illustration on temporality",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 Bootstrapped weighting: top 10 common features for country\u2013state and country\u2013party along with their corresponding ranks in the two (sorted) feature vectors. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 The comparison between NPYLM and ESA. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results for the submitted runs",
        "Entity": "Caption"
    },
    {
        "Text": "Table 28: Arabic POS Studies with Different Tagsets",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Feature set for the baseline pronoun res- olution system ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1. Counts of different (mis)spellings of Albert Einstein\u2019s name in a web query log. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Baseline results for human word lists. Data: 700 positive and 700 negative reviews.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Plate diagram of our model.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Attributive demonstrative pronouns (PDAT) in the STTS Annotation Model ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Dynamic-Expansion Tree Span Scheme",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. The RCM structure",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2. The pseudo code of Algorithm 1.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Size of Seed Lexicons",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Sense disambiguated TS for airport#n#1 obtained from BNC using InfoMap and SSI-Dijkstra ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Distribution of SCs in the ACE corpus.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: WSI and WSD Pipeline",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: The combined sequence and parse tree representation of the relation instance \u201cleader of a minority government.\u201d The non-essential nodes for \u201ca\u201d and for \u201cminority\u201d are removed based on the algorithm from Qian et al. (2008). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Avg. runtime per sentence of FindPareto",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9:      Top ranked sentences using the LR[0.20,0.95] system on the question \u201cWhat caused the Kursk to sink?\u201d ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Are the single most probable words for a given",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Shallow parsing: chunking (Extracted from: http://kontext.fraunhofer.de)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 Procedural description to compute the set Succ of successor hypotheses by which to extend a partial hypothesis (S, C, j). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Analysis of context length",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Biography Text Evaluations.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2 Words (excluding multiwords) in WordNet 1.7.1 and the BNC without any data in SemCor. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: An example of MOD feature extraction. An oval in the dependency tree denotes a bunsetsu. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Screenshot of Annis Linguistic Database",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Illustration of the alignment of steps.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Translation extraction from comparable corpora using cross-lingual WSI and WSD.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Syntactic tree kernel (STK).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Average sentence cover size: the average number of sentences needed to generate the case- frames in a summary sentence (Study 1). Model summaries are shown in darker bars. Peer system numbers that we focus on are in bold. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Comparison of tree span schemes with antecedents in different sentences apart ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Structure of a typical two-pass ma- chine translation system. N-best translations are generated by the decoder and the 1-best transla- tion is returned after rescored with additional feature functions. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Precision, Recall, and F1-score of Engkoo, Google, and Ours with head and tail datasets",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: The Effects of Temporal Constraint",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 Semantic roles for different frame sets of kick. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Automatic evaluation and sentence Levenshtein scores",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Evaluation of topic segmentation for the French corpus (Pk and WD as percentages) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Our approach for detecting parallel fragments. The lower part of the figure shows the source and target sentence together with their alignment. Above are displayed the initial signal and the filtered signal. The circles indicate which fragments of the target sentence are selected by the procedure. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6. System performance on the succession relation on the TREC-9 dataset. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Dev set learning curves for sentence lengths \u2264 70. All three curves remain steep at the maximum training set size of 18818 trees. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: PTB vs. Wiktionary type coverage across sec- tions of the Brown corpus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7: Evaluation of translation to English on in-domain test data",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results of the baseline model: best guess",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4 A comparison between ILP-Global and ILP-Local for two fragments of the test-set concept seizure. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6. The performance on the set of unknown",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2. Stratefied Sampling for initial seeds",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Realistic Scenario: Tedeval Unlabeled Accuracy and Exact Match for the Raw scenario. Top upper part refers to constituency results, the lower part refers to dependency results. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Data about our evaluation corpora",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Results of IE Experiment",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: One of the 69 test documents, containing 10 narrative events. The protagonist is President Bush. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Ten most frequent classes using equal distribution of verb frequencies. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Dependency parse with dependency path from \u2018Edwin Hubble\u2019 to \u2018Marshfield\u2019 highlighted in boldface. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Example of first and second order features using a predefined n-gram size of 2.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9 Evaluation on SemCor, polysemous words only. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Average results of Reweighting among all 7 speakers when the amount of speaker specific data is 0, 500, 2000 ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Multi-lingual Results: We report token-level one-to-one and many-to-one accuracy on a variety of languages under several experimental settings (Section 5). For each language and setting, we report one-to-one (1-1) and many- to-one (m-1) accuracies. For each cell, the first row corresponds to the result using the best hyperparameter choice, where best is defined by the 1-1 metric. The second row represents the performance of the median hyperparameter setting. Model components cascade, so the row corresponding to +FEATS also includes the PRIOR component (see Section 3). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: (a) RM and large margin solution comparison and (b) the spread of the projections given by each. RM and large margin solutions are shown with a darker dotted line and a darker solid line, respectively. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Inferred Dirichlet transition hyperparameters for bigram CLUST on three-way classification task with four latent clusters. Row gives starting state, column gives target state. Size of red blobs are proportional to magnitude of corresponding hyperparameters. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Extent of generalization for different values of \u03b1 and sample sizes. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: An example showing the combination of the se- mantic role sequences of the states. Above/middle is the state information before/after applying the TTS template, and bot- tom is the used TTS template and the triggered SRFs during the combination. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Input sentences.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Graphical view of an unordered schema automatically built from the verb \u2018convict\u2019. Each node shape is a chain in the schema. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Experiment 3: Results by training set size, \u03b8 = 1.0 ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Results with Coref Rules Alone",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 8 Average number of target phrase distribution sizes for source phrases for TRIBL and IGTree com- pared to the Moses baseline ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Results for development and test set for the two languages by ME1 ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 10 The comparison of overall accuracies of various joint segmentor and POS-taggers by 10-fold cross validation using CTB. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Multi-threaded time and memory consumption of Moses translating 3003 sentences on eight cores. Our code supports lazy memory mapping (-L) and prefault- ing (-P) with MAP POPULATE, the default. IRST is not threadsafe. Time for Moses itself to load, including load- ing the language model and phrase table, is included. Along with locking and background kernel operations such as prefaulting, this explains why wall time is not one-eighth that of the single-threaded case. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 14 %BLEU on tune and test sets for UR\u2192EN translation, comparing several settings for maximum dependency lengths in the decoder (\u03c9x is for the source side and \u03c9y is for the target side). The upper table shows Moses BLEU scores for comparison. The lower table compares two max dependency length settings during tuning, and several for decoding on the test sets, showing both BLEU scores and average decoding times per sentence. See text for discussion. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: Log probability of the sampler state over 1000 iterations on Languages A and B. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Opinion PageRank Performance with varying parameter \u00b5 (\u03bb = 0.2) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Upper triangle of the sentence-similarity matrix.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Speed of oracle extraction from hypergraphs. The basic dynamic program (Sec. 2.1) improves signifi- cantly by collapsing equivalent oracle states (Sec. 2.2). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: A Dictionary based Word Graph",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Effects of combination using the confidence measure. The upper numbers and the lower numbers are of the character-based and the subword-based, respec- tively ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 12 Interdigitation FSRA \u2013 general. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: BLEU score for those 25% utterances which resulted in different translations after bLSA adaptation (manual transcriptions) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 9: Dev set results for sentences of length \u2264 70. Cov- erage indicates the fraction of hypotheses in which the char- acter yield exactly matched the reference. Each model was able to produce hypotheses for all input sentences. In these experiments, the input lacks segmentation markers, hence the slightly different dev set baseline than in Table 6. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Results on the MT06 and MT08 test sets",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Performance on Bilingual Lexicon Extraction",
        "Entity": "Caption"
    },
    {
        "Text": "Table 12: Arabic Pronoun Dropping",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7: Proposed output for the new Chinese word seg- mentation paradigm. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Context-sensitive spelling correction (* denotes also using 60% WSJ, 5% corrupted) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Perplexity measured on nt08 with the baseline LM (std), with the LM estimated on the sampled texts (generated texts), and with the inter- polation of both. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Comparison of the existing efforts on ACE RDC task.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Models with lexical morpho-semantic features. Top: Adding all lexical features together on top of the CORE 12 baseline. Center: Adding each feature separately. Bottom: Greedily adding best features from previous part, on predicted input. Statistical significance tested only on predicted (non-gold) input, against the CORE 12 baseline. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Oracle lower-case BLEU",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Word segmentation on NIST data sets",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Parameter estimation from Q/A pairs.",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 2 Example of CCG supertags. CCG supertags are combined under the operations of forward and backward applications into a parse tree ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Example derivation for the query \u2018how many people visit the public library of new york annu- ally.\u2019 Underspecified constants are labelled with the words from the query that they are associated with for readability. Constants from O, written in typeset, are introduced in step (c). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4. Performance for different p values",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 Unknown word model features for Arabic and French. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Comparison of average per-document ter- comTER with invWER on the EVAL07 GALE Newswire (\u201cNW\u201d) and Weblogs (\u201cWB\u201d) data sets. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5 Word sense disambiguation accuracy for \u201cNP1 V NP2 for NP3\u201d frame. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7 Grammatical contexts used for acquiring the BNC thesaurus. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Number of Sentences for bilingual training, de- velopment and test and monolingual forum data sets ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: F-Score of the RF and SVM, GIZA++ and Levenshtein distance-based classifier on the second order dataset ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2. A derivation for Mary likes Susan",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: Distribution of errors",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Performance relationship between WMEB and BASILISK on Sgold UNION ",
        "Entity": "Caption"
    },
    {
        "Text": "                                       0 Figure 1: The computation of   DKL (Pv(e i)                                             kPe0i ) using a toy corpus, for e = looking forward to. Note that the sec- ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: POS annotations of a couplet, i.e., a pair of two verses, in a classical Chinese poem. See     Table 1 for the meaning of the POS tags. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2 The effect of varying the number of samples (k) on accuracy. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4. Pairwise precision/recall/F1 of WEBRE and SNE.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Identified metaphorical expressions for the mappings FEELING IS FIRE and CRIME IS A DISEASE ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Performance comparison of two SLU systems through weakly supervised and super- vised training on the three test sets (TER: Topic Error Rate; SER: Slot Error Rate) ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 11: MUC-6: Level Distribution of Each of the Six Facts",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Dataset characteristics including the number of documents, annotated CEs, coreference chains, annotated CEs per chain (average), and number of documents in the train/test split. We use st to indicate a standard train/test split. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Computation of probabilities using the language model.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Examples of correct (above) and incorrect (below) alignments ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 Results of all distributional similarity measures when tuning K over the development set. We encode the description of the measures presented in Table 2 in the following manner\u2014 h = health-care corpus; R = RCV1 corpus; b = binary templates; u = unary templates; L = Lin similarity measure; B = BInc similarity measure; pCt = pair of CUI tuples representation; pC = pair of CUIs representation; Ct = CUI tuple representation; C = CUI representation; Lin & Pantel = similarity lists learned by Lin and Pantel. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5 Most frequent Brown cluster phrase dependencies extracted from DE\u2192EN data, shown with their counts. As in Table 4, we only show those in which one phrase has at least two tokens and neither phrase is entirely punctuation. Each cluster is shown as a set of words large enough to cover 95% of the token counts in the cluster, up to a maximum of four words. It is characteristic of Brown clustering that very frequent tokens (e.g., function words) often receive their own clusters. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6. Impact of Confidence Measures",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Association overlap for target verbs.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: This table shows the performance achieved by the different systems, shown in accuracy (%). The Number of cases denotes the number of instances in the testset. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: ROUGE-2 measures in EM learning",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3 Coreference factors for name recognition",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 11: Two Paths in the Initial Malay Transducer Defined via Concatenation",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Correlation between cohesion-driven functions.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2. Correctness evaluation result",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: accuracy using non-averaged and averaged perceptron.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Effect of Arabic stemming features on coref- erence resolution. The row marked with \u201cTruth\u201d represents the results with \u201ctrue\u201d mentions while the row marked with \u201cSystem\u201d represents that mentions are detected by the system. Numbers under \u201cECM- F\u201d are Entity-Constrained-Mention F-measure and numbers under \u201cACE-Val\u201d are ACE-values. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Total number of synsets classified by sentiment.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: MRR, Precision, Recall, and F1-score",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1 Architecture of the translation approach based on a log-linear modeling approach. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7 An example showing the generalization of the word lattice (a) into a slotted lattice (b). The word lattice is produced by aligning seven sentences. Nodes having in-degrees > 1 occur in more than one sentence. Nodes with thick incoming edges occur in all sentences. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Details of the PKU data",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Show type detection: Using the neural net- work described in Sec. 2 the show type was detected. If there is a number in the word column the word feature is being used. The number indicates how many word/part of speech pairs are in the vocabu- lary additionally to the parts of speech. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Example initial lexical entries",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Performance 12 of the baseline and using different cluster features with PC4 over the 7 types. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Examining the learned hidden representation for SRL. In this example the syntactic dependency arcs derived from gold standard syntactic annotations (left) are entirely disjoint from the correct predicate/arguments pairs (shown in the heatmaps by the squares outlined in black), and the observed syntax model fails to recover any of the correct predictions. In contrast, the hidden model structure (right) learns a representation that closely parallels the desired end task predictions, helping it recover three of the four correct SRL predictions (shaded arcs: red corresponds to a correct prediction, with true labels GA, KARA, etc.), and providing some evidence towards the fourth. The dependency tree corresponding to the hidden structure is derived by edge-factored decoding: dependency variables whose beliefs > 0.5 are classified as true (though some arcs not relevant to the SRL predictions are omitted for clarity). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7 Summaries Recall and Precision",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11 Corpus statistics for Chinese\u2013English corpora\u2014large data track (Words*: words without punctuation marks). ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 3. Procedure to find semantic types for antecedent candidates",
        "Entity": "Caption"
    },
    {
        "Text": "Table 7: Results for OOV-processing and MBR, German\u2192English. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Example Stimuli with High and Low similarity landmarks ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Minimum distances from airport#n#1",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2. Extracted NE pair instances and context",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Context Clustering with Spectral-based Clustering technique. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: A CRF as a multi-sentential parsing model.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 13 Example translations for the translation direction English to German using three different reordering constraints: MON, EG, and S3. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7: Two possible DTs for three sentences.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Probability of # of boundaries f10 (m\u2032 ; 3).",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Examples of viterbi chunking and chunk alignment for English-to-Japanese translation model. Chunks are bracketed and the words with \u2217 to the left are head words. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11: Performance comparison with the literature for compound interpretation ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Frequencies of patterns in the evaluation data (causation). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 16 Overview of the results for all BNC algorithms for coreference. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Evaluation results from DSO-WSJ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Mention Detection Results",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Test verbs and their monosemous/polysemic gold standard senses",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8: Squares represent the proportion of tokens in each language assigned to a topic. The left topic, world ski km won,",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: In-domain (first column) and out-of-domain performance (columns two to four) on ACE 2005. PET and BOW are abbreviated by P and B, respectively. If not specified BOW is marked. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Diacritized particles and pseudo-verbs that, after orthographic normalization, have the equivalent surface form    an. The distinctions in the ATB are linguistically justified, but complicate parsing. Table 8a shows that the best model recovers SBAR at only 71.0% F1. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Feature templates used in R-phase. Ex- ample used is \u201c32 ddd\u201d. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: AER comparison (cn \u2192en)",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: MAP results for the two split Lin test- sets. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8 Model accuracy using equal distribution of verb frequencies for the estimation of P(c). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Particles and prepositions allowed in phrasal verbs gathered from Wiktionary. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Framework for MWE acquisition from corpora",
        "Entity": "Caption"
    },
    {
        "Text": "Table 8: Translation quality.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Summary of the results obtained by our algorithm in comparison to Word 2007",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Comparison of word segmentation F- measure for SIGHAN bakeoff3 tasks ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: English-French translation results in terms of BLEU score and TER estimated on newstest2010 with the NIST script. All means that the translation model is trained on news-commentary, Europarl, and the whole GigaWord. The rows upper quartile and median corre- spond to the use of a filtered version of the GigaWord. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Additional notation and signatures for CAM",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Example input and best output found",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3. Semantic Role learning curve",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: RST Spanish Treebank statistics",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: MTO is not sensitive to the number of random substitutes sampled per word token. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 An example of some discovered paraphrases.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Description of feature sets. \u2217 Glob only uses the same set of similarity measures when combined with other semantic features. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Average precision (AP) and coverage (Cov) results for our proposed system ES-all and the baselines. \u2021 indicates AP statistical signifi- cance at the 0.95 level wrt all baselines. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7 Word reordering for the translation direction English to German: The reordering is restricted to the English verb group. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Sample of experimental items for the meta alternation anm-fod. (Abbreviations are listed in Table 2.)",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Topic transfer in bilingual LSA model.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Average word accuracy for transduced sentences.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 11: Cross Framework Evaluation: Unlabeled TedEval on generalized gold trees in gold scenario, trained on 5k sentences and tested on 5k terminals. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Precision, Recall and F1 Results (%) for Coarse-Grained Classification. Comparison to O\u2019Hara and Wiebe (2009). Classes ordered by frequency ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Length distribution of entities in the train- ing set of the shared task in 2004 JNLPBA ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Opinion HITS model",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1 Distribution of antecedent NP types in the other-anaphora data set. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 The results of setting 3 (Punctuation is used; the maximum length is 30). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4. Contribution of every feature",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Pearson\u2019s (r) correlation results over the WMT all-en dataset, and the subset of the dataset that contains noun compounds ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Distribution of Error Types",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Sample of subjective features appearing in the top 100 discriminant attributes selected with Information Gain on the 3rd fold training data at iteration 10. The words in italics in the multilingual features represent equivalent translations in English and Romanian. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Example outputs of matching implementation of Finnish OT. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: MTO is not sensitive to the number of partitions used to discretize the substitute vector space within our experimental range. ",
        "Entity": "Caption"
    },
    {
        "Text": "Fig. 5 The semantic graph of an English sentence and the semantic features extracted from it for an SMT phrase ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 8: Rate of obtaining two clusters for mix- tures of SW-graphs dependent on merge rate r. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Language families in our data set. The Other category includes 9 language isolates and 21 language family singletons. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Overview of the method",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Discourse-new prediction results by Bean and Riloff ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Incremental evaluations, by incrementally adding new features (word features and high dimensional edge features), new word detection, and ADF training (replacing SGD training with ADF training). Number of passes is decided by empirical convergence of the training methods. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Examples of SMT errors due to MWEs.",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Processing time for POS tagging of known words using contextual features (In CPU seconds). Train: training time over     + sentences. Brill\u2019s learner was interrupted after 12 days of train- ing (default threshold was used). Test: average number of seconds to evaluate a single sentence. All runs were done on the same machine. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 5: Parameters of WSMs (Section 2) which, combined with particular Measures, achieved the highest average correlation in TrValD. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4: Comparison to SVM.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: F1 scores (in %) of SegTagDep on CTB- 5c-1 w.r.t. the training epoch (x-axis) and parsing feature weights (in legend). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Combinatorial Search Problems in Decoding",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 5: Opinion PageRank Performance with varying parameter \u03bb (\u00b5 = 0.5) ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Mapping algorithm - refining step.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 3: Encoding local word order.",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 2: Skeleton of basic lexicon transducer in LEXC generated from BAMA lexicons. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Statistical Information of Corpora",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1: Bell tree representation for three mentions: numbers in [] denote a partial entity. In-focus entities are marked on the solid arrows, and active mentions are marked by *. Solid arrows signify that a mention is linked with an in-focus partial entity while dashed arrows indicate starting of a new entity. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 4 Most frequent sense analysis for Senseval-2 and -3 polysemous lemmas occurring more than once in a document (adverb data is only from Senseval-2). ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: Priority Order for Second Person ADs",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 6: Creation of a Lexical Transducer",
        "Entity": "Caption"
    },
    {
        "Text": "Table 22 Accuracies of various phrase-structure parsers on CTB2 with gold-standard POS-tags. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 4: 10 most used verbs (lemma) in indirect speech. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Performance comparsion of the rule- based robust semantic parser, the reversed two- stage classification system and our SLU systems (TER: Topic Error Rate; SER: Slot Error Rate; ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 3: Eq. 9: Log-likelihood. Eq. 10: Pseudolikelihood. In both cases we maximize w.r.t. \u03b8. Eqs. 11\u201313: Recursive DP equations for summing over t and a. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 6: POS tagging with deterministic constraints. The maximum in each column is bold. ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 7 Accuracy of different methods in predicting OOV words polarity. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 1: Example of topic-to-topic correspondence. The last line shows the correspondence probability. Each col- umn means a topic represented by its top-10 topical word- s. The first column is a target-side topic, while the rest three columns are source-side topics. ",
        "Entity": "Caption"
    },
    {
        "Text": "Table 2: F-measure on SIGHAN bakeoff 2. SIGHAN best: best scores SIGHAN reported on the four corpus, cited from Zhang and Clark (2007). ",
        "Entity": "Caption"
    },
    {
        "Text": "Figure 1. Language Model Pruning Algorithm",
        "Entity": "Caption"
    }
]