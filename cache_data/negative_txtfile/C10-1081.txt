
	
		We propose semantic role features for a Tree-to-String transducer to model the reordering/deletion of source-side semantic roles.
		These semantic features, as well as the Tree-to-String templates, are trained based on a conditional log-linear model and are shown to significantly outperform systems trained based on Max-Likelihood and EM.
		We also show significant improvement in sentence fluency by using the semantic role features in the log-linear model, based on manual evaluation.
	
	
			Syntax-based statistical machine translation (SSMT) has achieved significant progress during recent years (Galley et al., 2006; May and Knight, 2007; Liu et al., 2006; Huang et al., 2006), showing that deep linguistic knowledge, if used properly, can improve MT performance.
			Semantics-based SMT, as a natural extension to SSMT, has begun to receive more attention from researchers (Liu and Gildea, 2008; Wu and Fung, 2009).
			Semantic structures have two major advantages over syntactic structures in terms of helping machine translation.
			First of all, semantic roles tend to agree better between two languages than syntactic constituents (Fung et al., 2006).
			This property motivates the approach of using the consistency of semantic roles to select MT outputs (Wu and Fung, 2009).
			Secondly, the set of semantic roles of a predicate models the skeleton of a sentence, which is crucial to the readability of MT output.
			By skeleton, we mean the main structure of a sentence including the verbs and their arguments.
			In spite of the theoretical potential of the semantic roles, there has not been much success in using them to improve SMT systems.
			Liu and Gildea (2008) proposed a semantic role based Tree-to-String (TTS) transducer by adding semantic roles to the TTS templates.
			Their approach did not differentiate the semantic roles of different predicates, and did not always improve the TTS transducer s performance.
			Wu and Fung (2009) took the output of a phrase-based SMT system Moses (Koehn et al., 2007), and kept permuting the semantic roles of the MT output until they best matched the semantic roles in the source sentence.
			This approach shows the positive effect of applying semantic role constraints, but it requires re-tagging semantic roles for every permuted MT output and does not scale well to longer sentences.
			This paper explores ways of tightly integrating semantic role features (SRFs) into an MT system, rather than using them in post-processing or n- best re-ranking.
			Semantic role labeling (SRL) systems usually use sentence-wide features (Xue and Palmer, 2004; Pradhan et al., 2004; Toutanova et al., 2005); thus it is difficult to compute target- side semantic roles incrementally during decoding.
			Noticing that the source side semantic roles are easy to compute, we apply a compromise approach, where the target side semantic roles are generated by projecting the source side semantic roles using the word alignments between the source and target sentences.
			Since this approach does not perform true SRL on the target string, it cannot fully evaluate whether the source and target semantic structures are consistent.
			However, the approach does capture the semantic-level reordering of the sentences.
			We assume here that the MT system is capable of providing word alignment (or equivalent) information during decoding, which is generally true for current statistical MT systems.
			Specifically, two types of semantic role features are proposed in this paper: a semantic role reordering feature designed to capture the skeleton- level permutation, and a semantic role deletion fea 716 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 716 724, Beijing, August 2010 ture designed to penalize missing semantic roles in the target sentence.
			To use these features during decoding, we need to keep track of the semantic role sequences (SRS) for partial translations, which can be generated based on the source-side semantic role sequence and the corresponding word alignments.
			Since the SRL system and the MT system are separate, a translation rule (e.g., a phrase pair in phrase-based SMT) could cover two partial source-side semantic roles.
			In such cases partial SRSs must be recorded in such a way that they can be combined later with other partial SRSs.
			Dealing with this problem will increase the complexity of the decoding algorithm.
			Fortunately, Tree-to- String transducer based MT systems (Liu et al., 2006; Huang et al., 2006) can avoid this problem by using the same syntax tree for both SRL and MT. Such an arrangement guarantees that a TTS template either covers parts of one source-side semantic role, or a few complete semantic roles.
			This advantage motivates us to use a TTS transducer as the MT system with which to demonstrate the use of the proposed semantic role features.
			Since it is hard to design a generative model to combine both the semantic role features and the TTS templates, we use a log-linear model to estimate the feature weights, by maximizing the conditional probabilities of the target strings given the source syntax trees.
			The log-linear model with latent variables has been discussed by Blunsom et al.
			(2008); we apply this technique to combine the TTS templates and the semantic role features.
			The remainder of the paper is organized as follows: Section 2 describes the semantic role features proposed for machine translation; Section 3 describes how semantic role features are used and trained in a TTS transducer; Section 4 presents the experimental results; and Section 5 gives the conclusion.
	
	
			Translation 2.1 Defining Semantic Roles.
			There are two semantic standards with publicly available training data: PropBank (Palmer et al., 2005) and FrameNet (Johnson et al., 2002).
			Prop- Bank defines a set of semantic roles for the verbs in the Penn TreeBank using numbered roles.
			These roles are defined individually for each verb.
			For example, for the verb disappoint, the role name arg1 means experiencer, but for the verb wonder, role name arg1 means cause.
			FrameNet is motivated by the idea that a certain type of verbs can be gathered together to form a frame, and in the same frame, a set of semantic roles is defined and shared among the verbs.
			For example, the verbs boil, bake, and steam will be in frame apply heat, and they have the semantic roles of cook, food, and heating instrument.
			Of these two semantic standards, we choose PropBank over FrameNet for the following reasons: 1.
			PropBank has a simpler semantic definition.
			than FrameNet and thus is easier for automatic labeling.
			2.
			PropBank is built upon the Penn TreeBank.
			and is more consistent with statistical parsers, most of which are trained on the Penn Tree- Bank.
			3.
			PropBank is a larger corpus than FrameNet.
			Note that the semantic standard/corpus is not cru cial in this paper.
			Any training corpus that can be used to automatically obtain the set of semantic roles of a verb could be used in our approach.
			2.2 Semantic Role Features.
			Ideally, we want to use features based on the true semantic roles of the MT candidates.
			Considering there is no efficient way of integrating SRL and MT, accurate target-side semantic roles can only be used in post-processing and re-ranking the MT outputs, where a limited number of MT candidates are considered.
			On the other hand, it is much easier to obtain reliable semantic roles for the source sentences.
			This paper uses a compromise approach, where the target-side semantic roles are projected from the source-side semantic roles using the word alignment derived from the translation process.
			More specifically, we define two types of semantic role features: 1.
			Semantic Role Reordering (SRR) This fea-.
			ture describes reordering of the source-side semantic roles (including the predicate) in the target side.
			It takes the following form: arg0 arg neg arg1 arg1 arg0 I did not see the book you borrowed SrcP red : SrcRole1, ..., SrcRolen   T arRole1, ..., T arRolen SRR:             where SrcP red and SrcRole denotes the central verb and semantic roles in the source side, and T arRole denotes the target-side roles.
			The source/target SRSs do not need be continuous, but there should be a one-to-one alignment between the roles in the two sides.
			Compared to the general reordering models used in statistical MT systems, this type of feature is capable of modeling skeleton-level reordering, which is crucial to the fluency of MT output.
			Because a predicate can have different semantic role sequences in different voices, passive/active are tagged for each occurrence of the verbs based on their POS and preceding words.
			                                           
			2.
			Deleted Roles (DR) are the individual source-.
			side semantic roles which are deleted in the MT outputs, taking the form of: SrcP red : SrcRole   deleted DR is meant to penalize the deletion of the semantic roles.
			Though most statistical MT systems have penalties for word deletion, it is still useful to make separate features for the deletion of semantic roles, which is considered more harmful than the deletion of non-core components (e.g., modifiers) and deserves more serious penalty.
			                                                           
			Both types of features can be made non-lexicalized by removing the actual verb but retaining its voice information in the features.
			Non-lexicalized features are used in the system to alleviate the problem of sparse verbs.
			see active: arg neg verb   arg neg verb borrowed active: arg1 arg0   arg0 arg1 borrowed active: arg1 verb   verb arg1 borrowed active: arg0 verb   arg0 verb borrowed active: arg1 arg0 verb   arg0 verb arg1 DR: see active: arg0   deleted                                                                                                                             
			We first briefly describe the basic Tree-to-String translation model used in our experiments, and then describe how to modify it to incorporate the semantic role features.
			3.1 Basic Tree-to-String Transducer.
			A Tree-to-String transducer receives a syntax tree as its input and, by recursively applying TTS templates, generates the target string.
			A TTS template is composed of a left-hand side (LHS) and a right-hand side (RHS), where the LHS is a sub- tree pattern and the RHS is a sequence of variables and translated words.
			The variables in the RHS of a template correspond to the bottom level non- terminals in the LHS s subtree pattern, and their relative order indicates the permutation desired at the point where the template is applied to translate one language to another.
			The variables are further transformed, and the recursive process goes on until there are no variables left.
			The formal description of a TTS transducer is given by Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined by Huang et al.
			(2006).
			For a given derivation (decomposition into templates) of a syntax tree, the translation probability is computed as the product of the templates which generate both the source syntax trees and the target translations.
			n
	
	
			Machine Translation P r(S | T , D ) = t D  P r(t) This section describes how to use the proposed semantic role features in a Tree-to-String transducer, Here, S denotes the target sentence, T denotes the source syntax tree, and D  denotes the derivation of T . In addition to the translation model, the function DECODE(T ) for tree node v of T in bottom-up order do for template t applicable at v do {c1 , c2 }=match(v, t); s.lef tw = c1 .lef tw; s.rightw = c2 .rightw; s.val = c1 .val   c2 .val; s.val  = P r(t); s.val  = P r(c2 .lef tw|c1 .rightw); add s to v s beam;                                                                         
			lef tw/rightw denote the left/right boundary word of s. c1 , c2 denote the descendants of v, ordered based on RHS of t. VBG [giving: verb] giving VBG [giving: verb] giving VP NP [giving: arg2] VP [giving: arg2 arg1] NP [giving: arg2] NP [giving: arg1] NP [giving: arg1] TTS system includes a trigram language model, a deletion penalty, and an insertion bonus.
			                                                                                
			To incorporate the n-gram language model, states in the algorithm denote a tree node s best translations with different left and right boundary words.
			We use standard beam-pruning to narrow the search space.
			                                                                                                                                
			It is straightforward to generalize the algorithm for larger n-gram models and TTS templates with any number of children in the bottom using target-side binarized combination (Huang et al., 2006).
			TTS template: (VP (VBG giving ) NP#1 NP#2 )   NP#1 NP#2 Triggered SRR: giving active: arg2 arg1   arg2 arg1 Triggered DR: giving active: verb   deleted                                                                                           
			Above/middle is the state information before/after applying the TTS template, and bottom is the used TTS template and the triggered SRFs during the combination.
			be easily derived.
			Now we show how to incorporate the two types of semantic role features into a TTS transducer.
			To use the semantic role reordering feature SRR, the states in the decoding algorithm need to be expanded to encode the target-side SRSs.
			The SRSs are initially attached to the translation states of the source tree con 3.2 Modified Tree-to-String Transducer with.
			Semantic Role Features Semantic role features can be used as an auxiliary translation model in the TTS transducer, which focuses more on the skeleton-level permutation.
			The model score, depending on not only the input source tree and the derivation of the tree, but also the semantic roles of the source tree, can be formulated as: VBZ [bring: verb] VP NP [bring: arg1] NNP NN new test PP [bring: arg3] P r(S | T , D ) = n f  F (S,T .role,D ) P r(f ) where T denotes the source syntax tree with semantic roles, T .role denotes the semantic role sequence in the source side and F (S.role, T .role, D ) denotes the set of defined semantic role features over T .role and the target side semantic role sequence S.role.
			Note that given T .role and the derivation D , S.role can Median = 3 ar^g1 Combined SRS arg3 verb arg1                                                                                                                                    
			stituents which are labeled as semantic roles for some predicate.
			These semantic roles are then accumulated with reordering and deletion operations specified by the TTS templates as the decoding process goes bottom-up.
			                                                                     
			The model component corresponding to the feature SRR is computed when combining two translation states.
			I.e., the probabilities of the SRR features composed based on the semantic roles of function DECODE(T ) for tree node v of T in bottom-up order do for template t applicable at v do {c1 , c2 }=match(v, t); s.lef tw = c1 .lef tw; s.rightw = c2 .rightw; s.role = concatenate(c1 .role, c2 .role); if v is a semantic role then set s.role to v.role; s.val = c1 .val   c2 .val; s.val  = P r(t); s.val  = P r(c2 .lef tw|c1 .rightw); t> Compute the probabilities associated with semantic roles the two combining states will be added into the = Q f  Sema(c1 .role,c2 .role,t) add s to v s beam; P r(f ); combined state.
			                          
			                                                                                                             
			Sema(c1 .role, c2 .role, t) denotes the triggered semantic ity is O(N M 4(n 1)R(LC C ! )V ),                                                                                                     
			the number of nodes in the source syntax tree, M is the vocabulary size of the target language, n is the order of the n-gram language model, R is the maximum number of TTS templates which can be matched at a tree node, C is the maximum number of roles of a verb, and V is the maximum number of verbs in a sentence.
			In this formula, LC C ! is the number of role sequences obtained by first choosing i out of C possible roles and then permuting the i roles.
			This theoretical upper bound is not reached in practice, because the number of possible TTS templates applicable at a tree node is very limited.
			Furthermore, since we apply beam pruning at each tree node, the running time is controlled by the beam size, and is linear in the size of the tree.
			                                                                                                                                                                                                                                  
			                                                                                                                                                                                                            
			Since we are primarily interested in the relative order of the semantic roles, we approximate each semantic role s target side position by the median of the word positions that is aligned to.
			If more than one semantic role is mapped to the same position in the target side, their source side order will be used as their target side order, i.e., monotonic translation is assumed for those semantic roles.
			                                                                                                 
			The word alignments in the TTS templates are also used to compute the deletion feature DR. Whenever a semantic role is deleted in a TTS template s RHS, the corresponding deletion penalty will be applied.
			3.3 Training.
			We describe two alternative methods for training the weights for the model s features, including both the individual TTS templates and the semantic role features.
			The first method maximizes data likelihood as is standard in EM, while the second method maximizes conditional likelihood for a log- linear model following Blunsom et al.
			(2008).
			3.3.1 Maximizing Data Likelihood The standard way to train a TTS translation model is to extract the minimum TTS templates using GHKM (Galley et al., 2004), and then normalize the frequency of the extracted TTS templates (Galley et al., 2004; Galley et al., 2006; Liu et al., 2006; Huang et al., 2006).
			The probability of the semantic features SRR and DR can be computed similarly, given that SRR and DR can be derived from the paired source/target sentences and the word alignments between them.
			We refer to this model as max-likelihood training and normalize the counts of TTS templates and semantic features based on their roots and predicates respectively.
			We wish to overcome noisy alignments from GIZA++ and learn better TTS rule probabilities by re-aligning the data using EM within the TTS E-step: for all pair of syntax tree T and target string S do for all TTS Template t, semantic features f do PD:t D P r(S,T ,D) function COMPUTE PARTITION(T ) for tree node v of T in bottom-up order do for template t applicable at v do for {s1 , s2 }=Match(v, t) do EC (t) += P ; s.sum += s .sum   s .sum  Dt P r(S,T ,Dt ) PD:f  D P r(S,T ,D) 1 2 exp( t + P f EC (f ) += M-step: PDt P r(S,T ,Dt ) ; f  Sema(s1 ,s2 ,t)   ); s.role = concatenate(s1 .role, s2 .role); add s to v; for all TTS Template t, semantic features f do P r(t) = EC (t) ; Ptt :tt .root=t.root EC (tt ) P r(f ) = EC (f ) ; Pf t :f t .predicate=t.predicate EC (f t )                                                                                                            
			We can estimate the expected counts of the TTS templates and the semantic features by formulating the probability of a pair of source tree and target string as: for state s in root do res += s.sum; return res;                                                                                     
			Sema(s1 , s2 , t) denotes all the semantic role features generated by combining s1 and s2 using t. role features are defined makes it impossible to design a sound generative model to incorporate these features, a log-linear model is also a theoretically better choice than the EM algorithm.
			                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     
			The difficult part of the EM algorithm is the E- step, which computes the expected counts of the TTS templates and the semantic features by summing over all possible derivations of the source trees and target strings.
			The standard inside- outside algorithm (Graehl and Knight, 2004) can be used to compute the expected counts of the TTS templates.
			Similar to the modification made in the TTS decoder, we can add the target-side semantic where the features f include both the TTS templates and the semantic role features.
			The numerator in the formula above can be computed using the same dynamic programming algorithm used to compute the expected counts in the EM algorithm.
			However, the partition function (denominator) requires summing over all possible source trees and target strings, and is infeasible to compute.
			Instead of approximating the partition function using methods such as sampling, we change the objective function from the data likelihood to the conditional likelihood: role sequence to the dynamic programming states D exp i  i fi (S, T , D)of the inside-outside algorithm to compute the ex P r(S | T ) = P P exp P   f (S , T , D ) pected counts of the semantic features.
			This way St  all(T ) Dt i i i each state (associated with a source tree node) represents a target side span and the partial SRSs.
			To speed up the training, a beam is created for each target span and only the top rated SRSs in the beam are kept.
			3.3.2 Maximizing Conditional Likelihood A log-linear model is another way to combine the TTS templates and the semantic features together.
			                                                                                                                                                                                                                                                                                           
			Again, to simplify the illustration, only binary TTS templates are used.
			Using the conditional probability as the objective function not only reduces the computational cost, but also corresponds better to the TTS decoder, where the best MT output is selected only among the possible candidates which can be generated from the input source tree using TTS templates.
			The derivative of the logarithm of the objective function (over the entire training corpus) w.r.t. a feature weight can be computed as: (n-gram language model, TTS templates, SRR, DR) weights of the transducer are tuned based on the development set using a grid-based line search, and the translation results are evaluated based on a single Chinese reference using BLEU4 (Papineni et al., 2002).
			Huang et al.
			(2006) used character- based BLEU as a way of normalizing inconsistent   log Q P r(S | T ) = X EC (fi )   EC t (fi )} Chinese word segmentati on, but we avoid this prob   i { S,T D|S,T S |T lem as the training, developmen t, and test data are from the same source.
			                                                                                                                                                                                                                                                                                                                                                                                           
			With the objective function and its derivatives, a variety of optimization methods can be used to obtain the best feature weights; we use LBFGS (Zhu et al., 1994) in our experiments.
			To prevent the model from overfitting the training data, a weighted Gaussian prior is used with the objective function.
			The variance of the Gaussian prior is tuned based on the development set.
	
	
			We train an English-to-Chinese translation system using the FBIS corpus, where 73,597 sentence pairs are selected as the training data, and 500 sentence pairs with no more than 25 words on the Chinese side are selected for both the development and test data.1 Charniak (2000) s parser, trained on the Penn Treebank, is used to generate the English syntax trees.
			To compute the semantic roles for the source trees, we use an in-house maxent classifier with features following Xue and Palmer (2004) and Pradhan et al.
			(2004).
			The semantic role labeler is trained and tuned based on sections 2 21 and section 24 of PropBank respectively.
			The standard role-based F-score of our semantic role labeler is 88.70%.
			Modified KneserNey trigram models are trained using SRILM (Stolcke, 2002) on the Chinese portion of the training data.
			                                                                                                                                                    s parser.
			The baseline system in our experiments uses the TTS templates generated by using GHKM and the union of the two single-direction alignments generated by GIZA++.
			Unioning the two single-direction alignments yields better performance for the SSMT systems using TTS templates (Fossum et al., 2008) than the two single-direction alignments and the heuristic diagonal combination (Koehn et al., 2003).
			The two single-direction word alignments as well as the union are used to generate the initial TTS template set for both the EM algorithm and the log-linear model.
			The initial TTS templates  probabilities/weights are set to their normalized counts based on the root of the TTS template (Galley et al., 2006).
			To test semantic role features, their initial weights are set to their normalized counts for the EM algorithm and to 0 for the log-linear model.
			                                                     
			We can see that the EM algorithm, based only on TTS templates, is slightly better than the baseline system.
			Adding semantic role features to the EM algorithm actually hurts the performance, which is not surprising since the combination of the TTS templates and semantic role features does not yield a sound generative model.
			The log-linear model based on TTS templates achieves significantly better results than both the baseline system and the EM algorithm.
			Both improvements are significant at p < 0.05 based on 2000 iterations of paired bootstrap re- sampling of the test set (Koehn, 2004).
			Adding semantic role features to the log-linear model further improves the BLEU score.
			One problem in our approach is the sparseness of the verbs, which makes it difficult for the log-linear model to tune the lexicalized semantic role features.
			One way to alleviate this problem is to make features based on verb classes.
			We first tried using the verb TTS Templates + SRF + Verb Class Union 15.6     EM 15.9 15.5 15.6 Log-linear 17.1 17.4 17.6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          
			Unfortu SRF On   123  4   6,7     14,15   9     11,12 SRF Off   123  4    14,15 ,   6,7     11,12   9 Source A1 gratifying2 change3 also4 occurred5 in6 the7 structure8 of9 ethnic10 minority11 cadres12 SRF On     10,11   8  4   5    2   3 nately, VerbNet only covers about 34% of the verb SRF Off           ,                tokens in our training corpus, and does not improve the system s performance.
			We then resorted 1 2 3 4 10,11 8 to automatic clustering based on the aspect model (Hofmann, 1999; Rooth et al., 1999).
			The training corpus used in clustering is the English portion of the selected FBIS corpus.
			Though automatically obtained verb clusters lead to further improvement in BLEU score, the total improvement from the semantic role features is not statistically significant.
			Because BLEU4 is biased towards the adequacy of the MT outputs and may not effectively evaluate their fluency, it is desirable to give a more accurate evaluation of the sentence s fluency, which is the property that semantic role features are supposed to improve.
			To do this, we manually compare the outputs of the two log-linear models with and without the semantic role features.
			Our evaluation focuses on the completeness and ordering of the semantic roles, and better, equal, worse are tagged for each pair of MT outputs indicating the impact of the semantic role features.
			                                                                                                                                                        
			                                                                                                                            .
	
	
			This paper proposes two types of semantic role features for a Tree-to-String transducer: one models the reordering of the source-side semantic role sequence, and the other penalizes the deletion of a source-side semantic role.
			                                                                                  .
			The first and second example shows that SRFs improve the completeness and the ordering of the MT outputs respectively, the third example shows that SRFs improve both properties.
			The subscripts of each Chinese phrase show their aligned words in English.
			and the Tree-to-String templates, trained based on a conditional log-linear model, are shown to significantly improve a basic TTS transducer s performance in terms of BLEU4.
			To avoid BLEU s bias towards the adequacy of the MT outputs, manual evaluation is conducted for sentence fluency and significant improvement is shown by using the semantic role features in the log-linear model.
			Considering our semantic features are the most basic ones, using more sophisticated features (e.g., the head words and their translations of the source- side semantic roles) provides a possible direction for further experimentation.
			Acknowledgments This work was funded by NSF IIS0546554 and IIS0910611.
	
