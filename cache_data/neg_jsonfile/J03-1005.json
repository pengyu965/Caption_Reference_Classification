[
    {
        "Text": "\n\t\n\t\tIn this article, we describe an efficient beam search algorithm for statistical machine translation based on dynamic programming (DP).",
        "Entity": "Normal"
    },
    {
        "Text": "The search algorithm uses the translation model presented in Brown et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(1993).",
        "Entity": "Normal"
    },
    {
        "Text": "Starting from a DP-based solution to the traveling-salesman problem, we present a novel technique to restrict the possible word reorderings between source and target language in order to achieve an efficient search algorithm.",
        "Entity": "Normal"
    },
    {
        "Text": "Word reordering restrictions especially useful for the translation direction German to English are presented.",
        "Entity": "Normal"
    },
    {
        "Text": "The restrictions are generalized, and a set of four parameters to control the word reordering is introduced, which then can easily be adopted to new translation directions.",
        "Entity": "Normal"
    },
    {
        "Text": "The beam search procedure has been successfully tested on the Verbmobil task (German to English, 8,000-word vocabulary) and on the Canadian Hansards task (French to English, 100,000-word vocabulary).",
        "Entity": "Normal"
    },
    {
        "Text": "For the medium-sized Verbmobil task, a sentence can be translated in a few seconds, only a small number of search errors occur, and there is no performance degradation as measured by the word error criterion used in this article.",
        "Entity": "Normal"
    },
    {
        "Text": "This article is about a search procedure for statistical machine translation (MT).",
        "Entity": "Normal"
    },
    {
        "Text": "The task of the search procedure is to find the most likely translation given a source sentence and a set of model parameters.",
        "Entity": "Normal"
    },
    {
        "Text": "Here, we will use a trigram language model and the translation model presented in Brown et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(1993).",
        "Entity": "Normal"
    },
    {
        "Text": "Since the number of possible translations of a given source sentence is enormous, we must find the best output without actually generating the set of all possible translations; instead we would like to focus on the most likely translation hypotheses during the search process.",
        "Entity": "Normal"
    },
    {
        "Text": "For this purpose, we present a data-driven beam search algorithm similar to the one used in speech recognition search algorithms (Ney et al.",
        "Entity": "Normal"
    },
    {
        "Text": "1992).",
        "Entity": "Normal"
    },
    {
        "Text": "The major difference between the search problem in speech recognition and statistical MT is that MT must take into account the different word order for the source and the target language, which does not enter into speech recognition.",
        "Entity": "Normal"
    },
    {
        "Text": "Tillmann, Vogel, Ney, and Zubiaga (1997) proposes a dynamic programming (DP) based search algorithm for statistical MT that monotonically translates the input sentence from left to right.",
        "Entity": "Normal"
    },
    {
        "Text": "The word order difference is dealt with using a suitable preprocessing step.",
        "Entity": "Normal"
    },
    {
        "Text": "Although the resulting search procedure is very fast, the preprocessing is language specific and requires a lot of manual   IBM T. J. Watson Research Center, Yorktown Heights, NY 10598.",
        "Entity": "Normal"
    },
    {
        "Text": "Email: ctill@us.ibm.com.",
        "Entity": "Normal"
    },
    {
        "Text": "The research reported here was carried out while the author was with Lehrstuhl fu  r Informatik VI, Computer Science Department, RWTH Aachen.",
        "Entity": "Normal"
    },
    {
        "Text": "Lehrstuhl fu  r Informatik VI, Computer Science Department, RWTH Aachen, D-52056 Aachen, Germany.",
        "Entity": "Normal"
    },
    {
        "Text": "Email: ney@informatik.rwthaachen.de.",
        "Entity": "Normal"
    },
    {
        "Text": "Oc 2003 Association for Computational Linguistics work.",
        "Entity": "Normal"
    },
    {
        "Text": "Currently, most search algorithms for statistical MT proposed in the literature are based on the A  concept (Nilsson 1971).",
        "Entity": "Normal"
    },
    {
        "Text": "Here, the word reordering can be easily included in the search procedure, since the input sentence positions can be processed in any order.",
        "Entity": "Normal"
    },
    {
        "Text": "The work presented in Berger et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(1996) that is based on the A  concept, however, introduces word reordering restrictions in order to reduce the overall search space.",
        "Entity": "Normal"
    },
    {
        "Text": "The search procedure presented in this article is based on a DP algorithm to solve the traveling-salesman problem (TSP).",
        "Entity": "Normal"
    },
    {
        "Text": "A data-driven beam search approach is presented on the basis of this DP-based algorithm.",
        "Entity": "Normal"
    },
    {
        "Text": "The cities in the TSP correspond to source positions of the input sentence.",
        "Entity": "Normal"
    },
    {
        "Text": "By imposing constraints on the possible word reorderings similar to that described in Berger et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(1996), the DP-based approach becomes more effective: when the constraints are applied, the number of word re- orderings is greatly reduced.",
        "Entity": "Normal"
    },
    {
        "Text": "The original reordering constraint in Berger et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(1996) is shown to be a special case of a more general restriction scheme in which the word reordering constraints are expressed in terms of simple combinatorical restrictions on the processed sets of source sentence positions.1 A set of four parameters is given to control the word reordering.",
        "Entity": "Normal"
    },
    {
        "Text": "Additionally, a set of four states is introduced to deal with grammatical reordering restrictions (e.g., for the translation direction German to English, the word order difference between the two languages is mainly due to the German verb group.",
        "Entity": "Normal"
    },
    {
        "Text": "In combination with the reordering restrictions, a data-driven beam search organization for the search procedure is proposed.",
        "Entity": "Normal"
    },
    {
        "Text": "A beam search pruning technique is conceived that jointly processes partial hypotheses according to two criteria: (1) The partial hypotheses cover the same set of source sentence positions,and (2) the partial hypotheses cover sets C of source sentence positions of equal car dinality.",
        "Entity": "Normal"
    },
    {
        "Text": "A partial hypothesis is said to cover a set of source sentence positions when exactly the positions in the set have already been processed in the search process.",
        "Entity": "Normal"
    },
    {
        "Text": "To verify the effectiveness of the proposed techniques, we report and analyze results for two translation tasks: the German to English Verbmobil task and French to English Canadian Hansards task.The article is structured as follows.",
        "Entity": "Normal"
    },
    {
        "Text": "Section 2 gives a short introduction to the trans lation model used and reports on other approaches to the search problem in statistical MT.",
        "Entity": "Normal"
    },
    {
        "Text": "In Section 3, a DP-based search approach is presented, along with appropriate pruning techniques that yield an efficient beam search algorithm.",
        "Entity": "Normal"
    },
    {
        "Text": "Section 4 reports and analyzes translation results for the different translation directions.",
        "Entity": "Normal"
    },
    {
        "Text": "In Section 5, we conclude with a discussion of the achieved results.",
        "Entity": "Normal"
    },
    {
        "Text": "2.1 IBM Translation Approach.",
        "Entity": "Normal"
    },
    {
        "Text": "In this article, we use the translation model presented in Brown et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(1993), and the mathematical notation we use here is taken from that paper as well: a source string 1 = f1     fj     fJ is to be translated into a target string eI = e1     ei     eI .",
        "Entity": "Normal"
    },
    {
        "Text": "Here, I is the length of the target string, and J is the length of the source string.",
        "Entity": "Normal"
    },
    {
        "Text": "Among all possible target strings, we will choose the string with the highest probability as given by Bayes  1 The word reordering restriction used in the search procedure described in Berger et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(1996) is not.",
        "Entity": "Normal"
    },
    {
        "Text": "mentioned in Brown et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(1993), although exactly the translation model described there is used.",
        "Entity": "Normal"
    },
    {
        "Text": "Equivalently, we use exactly the translation model described in Brown et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(1993) but try different reordering restrictions for the DP-based search procedure.",
        "Entity": "Normal"
    },
    {
        "Text": "98                                                                                             \n\t\t\tdecision rule:  eI = arg max{Pr (eI | f J )} 1 I 1 1 1 = arg max{Pr (eI )   Pr(f J | eI )} (1) I 1 1 1 1 Pr (eI ) is the language model of the target language, whereas Pr (f J | eI ) is the string 1 1 1translation model.",
        "Entity": "Normal"
    },
    {
        "Text": "The language model probability is computed using a trigram lan guage model.",
        "Entity": "Normal"
    },
    {
        "Text": "The string translation probability Pr (f J | eI ) is modeled using a series of 1 1five models of increasing complexity in training.",
        "Entity": "Normal"
    },
    {
        "Text": "Here, the model used for the trans lation experiments is the IBM4 model.",
        "Entity": "Normal"
    },
    {
        "Text": "This model uses the same parameter set as the IBM5 model, which in preliminary experiments did not yield better translationresults.",
        "Entity": "Normal"
    },
    {
        "Text": "The actual implementation used during the experiments is described in Al Onaizan et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(1999) and in Och and Ney (2000).",
        "Entity": "Normal"
    },
    {
        "Text": "The argmax operation denotes the search problem (i.e., the generation of the output sentence in the target language).",
        "Entity": "Normal"
    },
    {
        "Text": "The transformations may range from simple word categorization to more complex preprocessing steps that require someparsing of the source string.",
        "Entity": "Normal"
    },
    {
        "Text": "In this article, however, we will use only word catego 99 rization as an explicit transformation step.",
        "Entity": "Normal"
    },
    {
        "Text": "In the search procedure both the language and the translation model are applied after the text transformation steps.",
        "Entity": "Normal"
    },
    {
        "Text": "The following  types  of parameters are used for the IBM4 translation model: Lexicon probabilities: We use the lexicon probability p(f | e) for translating the single target word e as the single source word f .",
        "Entity": "Normal"
    },
    {
        "Text": "A source word f may be translated by the  null  word e0 (i.e., it does not produce any target word e).",
        "Entity": "Normal"
    },
    {
        "Text": "A translation probability p(f | e0 ) is trained along with the regular translation probabilities.",
        "Entity": "Normal"
    },
    {
        "Text": "Fertilities: A single target word e may be aligned to n = 0, 1 or more source words.",
        "Entity": "Normal"
    },
    {
        "Text": "This is explicitly modeled by the fertility parameter  (n | e): the probability that the target word e is translated by n source words is  (n | e).",
        "Entity": "Normal"
    },
    {
        "Text": "The fertility for the  null  word is treated specially (for details see Brown et al.",
        "Entity": "Normal"
    },
    {
        "Text": "[1993]).",
        "Entity": "Normal"
    },
    {
        "Text": "Berger et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(1996) describes the extension of a partial hypothesis by a pair of target words (e1, e), where e1 is not connected to any source word f .",
        "Entity": "Normal"
    },
    {
        "Text": "In this case, the so-called spontaneous target word e1 is accounted for with the fertility.",
        "Entity": "Normal"
    },
    {
        "Text": "Here, the translation probability  (0 | e1) and no- translation probability p(f | e1).",
        "Entity": "Normal"
    },
    {
        "Text": "Class-based distortion probabilities: When covering a source sentence position j, we use distortion probabilities that depend on the previously covered source sentence positions (we say that a source sentence position j is covered for a partial hypothesis when it is taken account of in the translation process by generating a target word or the  null  word e0 ).",
        "Entity": "Normal"
    },
    {
        "Text": "In Brown et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(1993), two types of distortion probabilities are distinguished: (1) the leftmost word of a set of source words f aligned to the same target word e (which is called the  head ) is placed, and (2) the remaining source words are placed.",
        "Entity": "Normal"
    },
    {
        "Text": "Two separate distributions are used for these two cases.",
        "Entity": "Normal"
    },
    {
        "Text": "For placing the  head  the center function center (i) (Brown et al.",
        "Entity": "Normal"
    },
    {
        "Text": "[1993] uses the notation 8i ) is used: the average position of the source words with which the target word ei 1 is aligned.",
        "Entity": "Normal"
    },
    {
        "Text": "The distortion probabilities are class-based: They depend on the word class F (f ) of a covered source word f as well as on the word class E (e) of the previously generated target word e. The classes are automatically trained (Brown et al.",
        "Entity": "Normal"
    },
    {
        "Text": "1992).",
        "Entity": "Normal"
    },
    {
        "Text": "When the IBM4 model parameters are used during search, an input sentence can be processed one source position at a time in a certain order primarily determined by the distortion probabilities.",
        "Entity": "Normal"
    },
    {
        "Text": "We will use the following simplified set of translation model parameters: lexicon probabilities p(f | e) and distortion probabilities p(j | j1, J).",
        "Entity": "Normal"
    },
    {
        "Text": "Here, j is the currently covered input sentence position and j1 is the previously covered input sentence position.",
        "Entity": "Normal"
    },
    {
        "Text": "The input sentence length J is included, since we would like to think of the distortion probability as normalized according to J.",
        "Entity": "Normal"
    },
    {
        "Text": "No fertility probabilities or  null  word probabilities are used; thus each source word f is translated as exactly one target word e and each target word e is translated as exactly one source word f .",
        "Entity": "Normal"
    },
    {
        "Text": "The simplified notation will help us to focus on the most relevant details of the DP-based search procedure.",
        "Entity": "Normal"
    },
    {
        "Text": "The simplified set of parameters leads to an unrealistic assumption about the length of the source and target sentence, namely, I = J.",
        "Entity": "Normal"
    },
    {
        "Text": "During the translation experiments we will, of course, not make this assumption.",
        "Entity": "Normal"
    },
    {
        "Text": "The implementation details for using the full set of IBM4 model parameters are given in Section 3.9.2.",
        "Entity": "Normal"
    },
    {
        "Text": "100 2.2 Search Algorithms for Statistical Machine Translation.",
        "Entity": "Normal"
    },
    {
        "Text": "In this section, we give a short overview of search procedures used in statistical MT: Brown et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(1990) and Brown et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(1993) describe a statistical MT system that is based on the same statistical principles as those used in most speech recognition systems (Jelinek 1976).",
        "Entity": "Normal"
    },
    {
        "Text": "Berger et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(1994) describes the French-to-English Candide translation system, which uses the translation model proposed in Brown et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(1993).",
        "Entity": "Normal"
    },
    {
        "Text": "A detailed description of the decoder used in that system is given in Berger et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(1996) but has never been published in a paper: Throughout the search process, partial hypotheses are maintained in a set of priority queues.",
        "Entity": "Normal"
    },
    {
        "Text": "There is a single priority queue for each subset of covered positions in the source string.",
        "Entity": "Normal"
    },
    {
        "Text": "In practice, the priority queues areinitialized only on demand; far fewer than the full number of queues possible are actu ally used.",
        "Entity": "Normal"
    },
    {
        "Text": "The priority queues are limited in size, and only the 1,000 hypotheses with the highest probability are maintained.",
        "Entity": "Normal"
    },
    {
        "Text": "Each priority queue is assigned a threshold to select the hypotheses that are going to be extended, and the process of assigning these thresholds is rather complicated.",
        "Entity": "Normal"
    },
    {
        "Text": "A restriction on the possible word reorderings, which is described in Section 3.6, is applied.",
        "Entity": "Normal"
    },
    {
        "Text": "Wang and Waibel (1997) presents a search algorithm for the IBM2 translation model based on the A  concept and multiple stacks.",
        "Entity": "Normal"
    },
    {
        "Text": "An extension of this algorithm is demonstrated in Wang and Waibel (1998).",
        "Entity": "Normal"
    },
    {
        "Text": "Here, a reshuffling step on top of the original decoder is used to handle more complex translation models (e.g., the IBM3 model is added).",
        "Entity": "Normal"
    },
    {
        "Text": "Translation approaches that use the IBM2 model parameters but are based on DP are presented in Garc  a-Varea, Casacuberta, and Ney (1998) and Niessen et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(1998).",
        "Entity": "Normal"
    },
    {
        "Text": "An approach based on the hidden Markov model alignments as used in speech recognition is presented in Tillmann, Vogel, Ney, and Zubiaga (1997) and Tillmann, Vogel, Ney, Zubiaga, and Sawaf (1997).",
        "Entity": "Normal"
    },
    {
        "Text": "This approach assumes that source and target language have the same word order, and word order differences are dealt with in a preprocessing stage.",
        "Entity": "Normal"
    },
    {
        "Text": "The work by Wu (1996) also uses the original IBM model parameters and obtains an efficient search algorithm by restricting the possible word reorderings using the so-called stochastic bracketing transduction grammar.",
        "Entity": "Normal"
    },
    {
        "Text": "Three different decoders for the IBM4 translation model are compared in Germann et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(2001).",
        "Entity": "Normal"
    },
    {
        "Text": "The first is a reimplementation of the stack-based decoder described in Berger et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(1996).",
        "Entity": "Normal"
    },
    {
        "Text": "The second is a greedy decoder that starts with an approximate solution and then iteratively improves this first rough solution.",
        "Entity": "Normal"
    },
    {
        "Text": "The third converts the decoding problem into an integer program (IP), and a standard software package for solving IP is used.",
        "Entity": "Normal"
    },
    {
        "Text": "Although the last approach is guaranteed to find the optimal solution, it is tested only for input sentences of length eight or shorter.",
        "Entity": "Normal"
    },
    {
        "Text": "This article will present a DP-based beam search decoder for the IBM4 translation model.",
        "Entity": "Normal"
    },
    {
        "Text": "The decoder is designed to carry out an almost full search with a small number of search errors and with little performance degradation as measured by the word error criterion.",
        "Entity": "Normal"
    },
    {
        "Text": "A preliminary version of the work presented here was published in Tillmann and Ney (2000).",
        "Entity": "Normal"
    },
    {
        "Text": "3.1 Inverted Alignment Concept.",
        "Entity": "Normal"
    },
    {
        "Text": "To explicitly describe the word order difference between source and target language, Brown et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(1993) introduced an alignment concept, in which a source position j is mapped to exactly one target position i: regular alignment: j   i = aj 101 .",
        "Entity": "Normal"
    },
    {
        "Text": "May of fourth the on you visit not can colleague my case this In I d F k m K S a v M n b .",
        "Entity": "Normal"
    },
    {
        "Text": "n i a a e o i m i a i e          e l n s l n e m i l e e i c s n l r h u e t t c g e h e n e n Regular alignment example for the translation direction German to English.",
        "Entity": "Normal"
    },
    {
        "Text": "For each German source word there is exactly one English target word on the alignment path.",
        "Entity": "Normal"
    },
    {
        "Text": "An example for this kind of alignment is given in Figure 2, in which each German source position j is mapped to an English target position i.",
        "Entity": "Normal"
    },
    {
        "Text": "In Brown et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(1993), this alignment concept is used for model IBM1 through model IBM5.",
        "Entity": "Normal"
    },
    {
        "Text": "For search purposes, we use the inverted alignment concept as introduced in Niessen et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(1998) and Ney et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(2000).",
        "Entity": "Normal"
    },
    {
        "Text": "An inverted alignment is defined as follows: inverted alignment: i   j = bi Here, a target position i is mapped to a source position j.",
        "Entity": "Normal"
    },
    {
        "Text": "The coverage constraint for an inverted alignment is not expressed by the notation: Each source position j should be  hit  exactly once by the path of the inverted alignment bI = b1     bi     bI .",
        "Entity": "Normal"
    },
    {
        "Text": "The advantage of the inverted alignment concept is that we can construct target sentence hypotheses from bottom to top along the positions of the target sentence.",
        "Entity": "Normal"
    },
    {
        "Text": "Using the inverted alignments in the maximum approximation, we rewrite equation (1) to obtain the following search criterion, in which we are looking for the most likely target 102 Figure 3 Illustration of the transitions in the regular and in the inverted alignment model.",
        "Entity": "Normal"
    },
    {
        "Text": "The regular alignment model (left figure) is used to generate the sentence from left to right; the inverted alignment model (right figure) is used to generate the sentence from bottom to top.",
        "Entity": "Normal"
    },
    {
        "Text": "sentence eI of length I = J for an observed source sentence f J of length J: 1 1 max p(J | I)   max{p(eI )   p(f J | eI )} (2) I I 1 1 1 1 I I  = max I p(J | I)   max e n p(ei | ei 1 , ei 2 )   max n[p(bi | bi 1 , J)   p(fbi | ei )] b 1 i=1 I 1 i=1 = max p(J | I)   max n[p(ei | ei 1 , ei 2 )   p(bi | bi 1 , J)   p(fbi | ei )] I eI I 1 ,b1 i=1 The following notation is used: ei 1 , ei 2 are the immediate predecessor target words, ei is the word to be hypothesized, p(ei | ei 1 , ei 2 ) denotes the trigram language model probability, p(fbi | ei ) denotes the lexicon probability for translating the target word ei as source word fbi , and p(bi | bi 1 , J) is the distortion probability for covering source position bi after source position bi 1 .",
        "Entity": "Normal"
    },
    {
        "Text": "Note that in equation (2) two products over i are merged into a single product over i.",
        "Entity": "Normal"
    },
    {
        "Text": "The translation probability p(f J | eI ) is computed in 1 1 the maximum approximation using the distortion and the lexicon probabilities.",
        "Entity": "Normal"
    },
    {
        "Text": "Finally, p(J | I) is the sentence length model, which will be dropped in the following (it is not used in the IBM4 translation model).",
        "Entity": "Normal"
    },
    {
        "Text": "For each source sentence f J to be translated, we are searching for the unknown mapping that optimizes equation (2): i   (bi , ei ) In Section 3.3, we will introduce an auxiliary quantity that can be evaluated recursively using DP to find this unknown mapping.",
        "Entity": "Normal"
    },
    {
        "Text": "We will explicitly take care of the coverage constraint by introducing a coverage set C of source sentence positions that have already been processed.",
        "Entity": "Normal"
    },
    {
        "Text": "Figure 3 illustrates the concept of the search algorithm using inverted alignments: Partial hypotheses are constructed from bottom to top along the positions of the target sentence.",
        "Entity": "Normal"
    },
    {
        "Text": "Partial hypotheses of length i 1 are extended to obtain partial hypotheses of the length i.",
        "Entity": "Normal"
    },
    {
        "Text": "Extending a partial hypothesis means covering a source sentence position j that has not yet been covered.",
        "Entity": "Normal"
    },
    {
        "Text": "For a given grid point in the 103 Table 1 DP-based algorithm for solving traveling-salesman problems due to Held and Karp.",
        "Entity": "Normal"
    },
    {
        "Text": "The outermost loop is over the cardinality of subsets of already visited cities.",
        "Entity": "Normal"
    },
    {
        "Text": "input: cities j = 1, ...\n\t\t\t, J with distance matrix djj 1 initialization: D({k}, k) := d1k for each path length c = 2, ...\n\t\t\t, J do for each pair (C, j), where C   {2, ...\n\t\t\t, J} and j  C and |C| = c do D(C, j) = min {djj 1 + D(C\\{j}, j )} traceback: j1  C\\{j}   find shortest tour: D  = min [D({2, ...\n\t\t\t, J}, k)+ dk1 ] k {2,...,J}   recover optimal sequence of cities translation lattice, the unknown target word sequence can be obtained by tracing back the translation decisions to the partial hypothesis at stage i = 1.",
        "Entity": "Normal"
    },
    {
        "Text": "The grid points are defined in Section 3.3.",
        "Entity": "Normal"
    },
    {
        "Text": "In the left part of the figure the regular alignment concept is shown for comparison purposes.",
        "Entity": "Normal"
    },
    {
        "Text": "3.2 Held and Karp Algorithm for Traveling-Salesman Problem.",
        "Entity": "Normal"
    },
    {
        "Text": "Held and Karp (1962) presents a DP approach to solve the TSP, an optimization problem that is defined as follows: Given are a set of cities {1, ...\n\t\t\t, J} and for each pair of cities j, j1 the cost djj 1 > 0 for traveling from city j to city j1.",
        "Entity": "Normal"
    },
    {
        "Text": "We are looking for the shortest tour, starting and ending in city 1, that visits all cities in the set of cities exactly once.",
        "Entity": "Normal"
    },
    {
        "Text": "We are using the notation C for the set of cities, since it corresponds to a coverage set of processed source positions in MT.",
        "Entity": "Normal"
    },
    {
        "Text": "A straightforward way to find the shortest tour is by trying all possible permutations of the J cities.",
        "Entity": "Normal"
    },
    {
        "Text": "The resulting algorithm has a complexity of O(J!).",
        "Entity": "Normal"
    },
    {
        "Text": "DP can be used, however, to find the shortest tour in O(J2   2J ), which is a much smaller complexity for larger values of J.",
        "Entity": "Normal"
    },
    {
        "Text": "The approach recursively evaluates the quantity D(C, j): D(C, j) := costs of the partial tour starting in city 1, ending in city j, and visiting all cities in C Subsets of cities C of increasing cardinality c are processed.",
        "Entity": "Normal"
    },
    {
        "Text": "The algorithm, shown in Table 1, works because not all permutations of cities have to be considered explicitly.",
        "Entity": "Normal"
    },
    {
        "Text": "During the computation, for a pair (C, j), the order in which the cities in C have been visited can be ignored (except j); only the costs for the best path reaching j has to be stored.",
        "Entity": "Normal"
    },
    {
        "Text": "For the initialization the costs for starting from city 1 are set: D({k}, k) = d1k for each k   {2, ...\n\t\t\t, |C|}.",
        "Entity": "Normal"
    },
    {
        "Text": "Then, subsets C of increasing cardinality are processed.",
        "Entity": "Normal"
    },
    {
        "Text": "Finally, the cost for the optimal tour is obtained in the second-to-last line of the algorithm.",
        "Entity": "Normal"
    },
    {
        "Text": "The optimal tour itself can be found using a back-pointer array in which the optimal decision for each grid point (C, j) is stored.",
        "Entity": "Normal"
    },
    {
        "Text": "Figure 4 illustrates the use of the algorithm by showing the  supergraph  that is searched in the Held and Karp algorithm for a TSP with J = 5 cities.",
        "Entity": "Normal"
    },
    {
        "Text": "When traversing the lattice from left to right following the different possibilities, a partial path to a node j corresponds to the subset C of all cities on that path together with the last visited 104 Figure 4 Illustration of the algorithm by Held and Karp for a traveling salesman problem with J = 5 cities.",
        "Entity": "Normal"
    },
    {
        "Text": "Not all permutations of cities have to be evaluated explicitly.",
        "Entity": "Normal"
    },
    {
        "Text": "For a given subset of cities the order in which the cities have been visited can be ignored.",
        "Entity": "Normal"
    },
    {
        "Text": "city j.",
        "Entity": "Normal"
    },
    {
        "Text": "Of all the different paths merging into the node j, only the partial path with the smallest cost has to be retained for further computation.",
        "Entity": "Normal"
    },
    {
        "Text": "3.3 DP-Based Algorithm for Statistical Machine Translation.",
        "Entity": "Normal"
    },
    {
        "Text": "In this section, the Held and Karp algorithm is applied to statistical MT.",
        "Entity": "Normal"
    },
    {
        "Text": "Using the concept of inverted alignments as introduced in Section 3.1, we explicitly take care of the coverage constraint by introducing a coverage set C of source sentence positions that have already been processed.",
        "Entity": "Normal"
    },
    {
        "Text": "Here, the correspondence is according to the fact that each source sentence position has to be covered exactly once, fulfilling the coverage constraint.",
        "Entity": "Normal"
    },
    {
        "Text": "The cities of the more complex translation TSP correspond roughly to triples (e1, e, j), the notation for which is given below.",
        "Entity": "Normal"
    },
    {
        "Text": "The final path output by the translation algorithm will contain exactly one triple (e1, e, j) for each source position j.",
        "Entity": "Normal"
    },
    {
        "Text": "The algorithm processes subsets of partial hypotheses with coverage sets C of increasing cardinality c. For a trigram language model, the partial hypotheses are of the form (e1, e, C, j), where e1, e are the last two target words, C is a coverage set for the already covered source positions, and j is the last covered position.",
        "Entity": "Normal"
    },
    {
        "Text": "The target word sequence that ends in e1, e is stored as a back pointer to the predecessor partial hypothesis (and recursively to its predecessor hypotheses) and is not shown in the notation.",
        "Entity": "Normal"
    },
    {
        "Text": "Each distance in the TSP now corresponds to the negative logarithm of the product of the translation, distortion, and language model probabilities.",
        "Entity": "Normal"
    },
    {
        "Text": "The following 105 Table 2 DP-based algorithm for statistical MT that consecutively processes subsets C of source sentence positions of increasing cardinality.",
        "Entity": "Normal"
    },
    {
        "Text": "input: source language string f1     fj     fJ initialization for each cardinality c = 1, 2, ...\n\t\t\t, J do for each pair (C, j), where C   {1, ...\n\t\t\t, J} and j  C and |C| = c do for each pair of target words e , e   E traceback: Qe1 (e, , j) = p(fj e) max e11 j1  C\\{j} {p(j | j , J)   p(e | e , e )   Qe11 (e , C\\{j}, j )}   find best end hypothesis: max{p($ | e, e )   Qe1 (e, {1, ...\n\t\t\t, J}, j)}   recover optimal word sequence auxiliary quantity is defined: Qe1 (e, C, j) := probability of the best partial hypothesis (ei , bi ), where 1 1 C = {bk | k = 1, ...\n\t\t\t, i}, bi = j, ei = e, and ei 1 = e1 The above auxiliary quantity satisfies the following recursive DP equation: Qe1 (e, C, j) = p(fj | e)   max p(j | j1, J)   p(e | e1, e11)   Qe11 (e1, C\\{j}, j ) e11 j1  C\\{j} Here, j1 is the previously covered source sentence position and e1, e11 are the predecessor words.",
        "Entity": "Normal"
    },
    {
        "Text": "The DP equation is evaluated recursively for each hypothesis (e1, e, C, j).",
        "Entity": "Normal"
    },
    {
        "Text": "The resulting algorithm is depicted in Table 2.",
        "Entity": "Normal"
    },
    {
        "Text": "Some details concerning the initialization and the finding of the best target language string are presented in Section 3.4.\n\t\t\tp($ | e, e1) is the trigram language probability for predicting the sentence boundary symbol $.",
        "Entity": "Normal"
    },
    {
        "Text": "The complexity of the algorithm is O(E3   J2   2J ), where E is the size of the target language vocabulary.",
        "Entity": "Normal"
    },
    {
        "Text": "3.4 Verb Group Reordering: German to English.",
        "Entity": "Normal"
    },
    {
        "Text": "The above search space is still too large to translate even a medium-length inputsentence.",
        "Entity": "Normal"
    },
    {
        "Text": "On the other hand, only very restricted reorderings are necessary; for ex ample, for the translation direction German to English, the word order difference is mostly restricted to the German verb group.",
        "Entity": "Normal"
    },
    {
        "Text": "The approach presented here assumes a mostly monotonic traversal of the source sentence positions from left to right.2 A small number of positions may be processed sooner than they would be in that monotonic traversal.",
        "Entity": "Normal"
    },
    {
        "Text": "Each source position then generates a certain number of target words.",
        "Entity": "Normal"
    },
    {
        "Text": "The restrictions are fully formalized in Section 3.5.A typical situation is shown in Figure 5.",
        "Entity": "Normal"
    },
    {
        "Text": "When translating the sentence monotoni cally from left to right, the translation of the German finite verb kann, which is the left verbal brace in this case, is skipped until the German noun phrase mein Kollege, which is the subject of the sentence, is translated.",
        "Entity": "Normal"
    },
    {
        "Text": "Then, the right verbal brace is translated: 2 Also, this assumption is necessary for the beam search pruning techniques to work efficiently..\n\t\t\t106 .",
        "Entity": "Normal"
    },
    {
        "Text": "May of fourth the on you visit not can colleague my case this In I d F k m K S a v M n b .",
        "Entity": "Normal"
    },
    {
        "Text": "Figure 5 n i a e l s l e m a e o n i l n n l e g e i m i a i e e e i c s r h u t t c e h n e n Word reordering for the translation direction German to English: The reordering is restricted to the German verb group.",
        "Entity": "Normal"
    },
    {
        "Text": "The infinitive besuchen and the negation particle nicht.",
        "Entity": "Normal"
    },
    {
        "Text": "The following restrictions are used: One position in the source sentence may be skipped for a distance of up to L = 4 source positions, and up to two source positions may be moved for a distance of at most R = 10 source positions (the notation L and R shows the relation to the handling of the left and right verbal brace).",
        "Entity": "Normal"
    },
    {
        "Text": "To formalize the approach, we introduce four verb group states S:   Initial : A contiguous initial block of source positions is covered.",
        "Entity": "Normal"
    },
    {
        "Text": "Skip: One word may be skipped, leaving a  hole  in the monotonic traversal.",
        "Entity": "Normal"
    },
    {
        "Text": "Move : Up to two words may be  moved  from later in the sentence.",
        "Entity": "Normal"
    },
    {
        "Text": "Cover : The sentence is traversed monotonically until the state Initial is reached.",
        "Entity": "Normal"
    },
    {
        "Text": "107 4.\n\t\t\tmein 5.",
        "Entity": "Normal"
    },
    {
        "Text": "Kollege.",
        "Entity": "Normal"
    },
    {
        "Text": "Skip Initial Move Cover 1.",
        "Entity": "Normal"
    },
    {
        "Text": "In.",
        "Entity": "Normal"
    },
    {
        "Text": "6.\n\t\t\tkann 7.\n\t\t\tnicht 9.",
        "Entity": "Normal"
    },
    {
        "Text": "Sie.",
        "Entity": "Normal"
    },
    {
        "Text": "2.\n\t\t\tdiesem 12.",
        "Entity": "Normal"
    },
    {
        "Text": "Mai 8.\n\t\t\tbesuchen 10.\n\t\t\tam 3.",
        "Entity": "Normal"
    },
    {
        "Text": "Fall.",
        "Entity": "Normal"
    },
    {
        "Text": "Figure 6 13.",
        "Entity": "Normal"
    },
    {
        "Text": "11.\n\t\t\tvierten Order in which the German source positions are covered for the German-to-English reordering example given in Figure 5.",
        "Entity": "Normal"
    },
    {
        "Text": "The states Move and Skip both allow a set of upcoming words to be processed sooner than would be the case in the monotonic traversal.",
        "Entity": "Normal"
    },
    {
        "Text": "The state Initial is entered whenever there are no uncovered positions to the left of the rightmost covered position.",
        "Entity": "Normal"
    },
    {
        "Text": "The sequence of states needed to carry out the word reordering example in Figure 5 is given in Figure 6.",
        "Entity": "Normal"
    },
    {
        "Text": "The 13 source sentence words are processed in the order shown.",
        "Entity": "Normal"
    },
    {
        "Text": "A formal specification of the state transitions is given in Section 3.5.",
        "Entity": "Normal"
    },
    {
        "Text": "Any number of consecutive German verb phrases in a sentence can be processed by the algorithm.",
        "Entity": "Normal"
    },
    {
        "Text": "The finite-state control presented here is obtained from a simple analysis of the German- to-English word reordering problem and is not estimated from the training data.",
        "Entity": "Normal"
    },
    {
        "Text": "It can be viewed as an extension of the IBM4 model distortion probabilities.",
        "Entity": "Normal"
    },
    {
        "Text": "Using the above states, we define partial hypothesis extensions of the following type: (S1, C\\{j}, j1)   (S, C, j) Not only the coverage set C and the positions j, j1, but also the verb group states S, S1, are taken into account.",
        "Entity": "Normal"
    },
    {
        "Text": "For the sake of brevity, we have omitted the target language words e, e1 in the notation of the partial hypothesis extension.",
        "Entity": "Normal"
    },
    {
        "Text": "For each extension an uncovered position is added to the coverage set C of the partial hypothesis, and the verb group state S may change.",
        "Entity": "Normal"
    },
    {
        "Text": "A more detailed description of the partial hypothesis extension for a certain state S is given in the next section in a more generalcontext.",
        "Entity": "Normal"
    },
    {
        "Text": "Covering the first uncovered position in the source sentence, we use the lan 108 guage model probability p(e | $, $).",
        "Entity": "Normal"
    },
    {
        "Text": "Here, $ is the sentence boundary symbol, which is thought to be at position 0 in the target sentence.",
        "Entity": "Normal"
    },
    {
        "Text": "The search starts in the hypothesis (Initial , { }, 0).",
        "Entity": "Normal"
    },
    {
        "Text": "{ } denotes the empty set, where no source sentence position is covered.",
        "Entity": "Normal"
    },
    {
        "Text": "The following recursive equation is evaluated: Qe1 (e, S, C, j) (3) = p(fj | e) max e11 ,S1 ,j1 (S1 ,C\\{j},j1 ) (S,C,j) j1  C\\{j} {p(j | j1, J)   p(e | e1, e11)   Qe11 (e1, S1, C\\{j}, j1)} The search ends in the hypotheses (Initial , {1, ...\n\t\t\t, J}, j); the last covered position may be in the range j   {J  L, ...\n\t\t\t, J}, because some source positions may have been skipped at the end of the input sentence.",
        "Entity": "Normal"
    },
    {
        "Text": "{1, ...\n\t\t\t, J} denotes a coverage set including all positions from position 1 to position J.",
        "Entity": "Normal"
    },
    {
        "Text": "The final translation probability QF is QF = max e,e1 j {J L,...,J} p($ | e, e1)   Qe1 (e, Initial , {1, ...\n\t\t\t, J}, j) (4) where p($ | e, e1) denotes the trigram language model, which predicts the sentence boundary $ at the end of the target sentence.",
        "Entity": "Normal"
    },
    {
        "Text": "QF can be obtained using an algorithm very similar to the one given in Table 2.",
        "Entity": "Normal"
    },
    {
        "Text": "The complexity of the verb group reordering for the translation direction German to English is O(E3   J   (R2   L   R)), as shown in Tillmann (2001).",
        "Entity": "Normal"
    },
    {
        "Text": "3.5 Word Reordering: Generalization.",
        "Entity": "Normal"
    },
    {
        "Text": "For the translation direction English to German, the word reordering can be restricted in a similar way as for the translation direction German to English.",
        "Entity": "Normal"
    },
    {
        "Text": "Again, the word order difference between the two languages is mainly due to the German verb group.",
        "Entity": "Normal"
    },
    {
        "Text": "During the translation process, the English verb group is decomposed as shown inFigure 7.",
        "Entity": "Normal"
    },
    {
        "Text": "When the sentence is translated monotonically from left to right, the trans lation of the English finite verb can is moved, and it is translated as the German left verbal brace before the English noun phrase my colleague, which is the subject of the sentence.",
        "Entity": "Normal"
    },
    {
        "Text": "The translations of the infinitive visit and of the negation particle not areskipped until later in the translation process.",
        "Entity": "Normal"
    },
    {
        "Text": "For this translation direction, the trans lation of one source sentence position may be moved for a distance of up to L = 4 source positions, and the translation of up to two source positions may be skipped for a distance of up to R = 10 source positions (we take over the L and R notation from the previous section).",
        "Entity": "Normal"
    },
    {
        "Text": "Thus, the role of the skipping and the moving are simply reversed with respect to their roles in German-to-English translation.",
        "Entity": "Normal"
    },
    {
        "Text": "For the example translation in Figure 7, the order in which the source sentence positions are covered is given in Figure 8.We generalize the two approaches for the different translation directions as fol lows: In both approaches, we assume that the source sentence is mainly processedmonotonically.",
        "Entity": "Normal"
    },
    {
        "Text": "A small number of upcoming source sentence positions may be pro cessed earlier than they would be in the monotonic traversal: The states Skip and Move are used as explained in the preceding section.",
        "Entity": "Normal"
    },
    {
        "Text": "The positions to be processed outside the monotonic traversal are restricted as follows:   The number of positions dealt with in the states Move and Skip is restricted.",
        "Entity": "Normal"
    },
    {
        "Text": "There are distance restrictions on the source positions processed in those states.",
        "Entity": "Normal"
    },
    {
        "Text": "109 .",
        "Entity": "Normal"
    },
    {
        "Text": "besuchen nicht Mai vierten am Sie Kollege mein kann Fall diesem In I t c m c c n v y o t f o M .",
        "Entity": "Normal"
    },
    {
        "Text": "n h a y o a o i o n h o f a Figure 7 i s l n t s u s e l i e t a g u e e u y r t h Word reordering for the translation direction English to German: The reordering is restricted to the English verb group.",
        "Entity": "Normal"
    },
    {
        "Text": "These restrictions will be fully formalized later in this section.",
        "Entity": "Normal"
    },
    {
        "Text": "In the state Move , some source sentence positions are  moved  from later in the sentence to earlier.",
        "Entity": "Normal"
    },
    {
        "Text": "After source sentence positions are moved, they are marked, and the translation of the sentence is continued monotonically, keeping track of the positions already covered.",
        "Entity": "Normal"
    },
    {
        "Text": "To formalize the approach, we introduce four reordering states S:   Initial : A contiguous initial block of source positions is covered.",
        "Entity": "Normal"
    },
    {
        "Text": "Skip: A restricted number of source positions may be skipped, leaving  holes  in the monotonic traversal.",
        "Entity": "Normal"
    },
    {
        "Text": "Move : A restricted number of words may be  moved  from later in the sentence.",
        "Entity": "Normal"
    },
    {
        "Text": "Cover : The sentence is traversed monotonically until the state Initial is reached.",
        "Entity": "Normal"
    },
    {
        "Text": "To formalize the approach, the following notation is introduced: rmax (C) = max c c C 110 7.\n\t\t\tyou 8.\n\t\t\ton 9.\n\t\t\tthe 10.\n\t\t\tfourth 11.\n\t\t\tof 12.",
        "Entity": "Normal"
    },
    {
        "Text": "May.",
        "Entity": "Normal"
    },
    {
        "Text": "13.\n\t\t\tnot 5.\n\t\t\tmy Cover Skip Initial 1.",
        "Entity": "Normal"
    },
    {
        "Text": "In.",
        "Entity": "Normal"
    },
    {
        "Text": "2.\n\t\t\tthis 3.\n\t\t\tcase 6.\n\t\t\tcolleague 14.\n\t\t\tvisit Move Figure 8 15.",
        "Entity": "Normal"
    },
    {
        "Text": "4.\n\t\t\tcan Order in which the English source positions are covered for the English-to-German reordering example given in Figure 7.\n\t\t\tlmin (C) = mi n c c / C u ( C ) m ( C ) w ( C ) = = = car d ({c | c  / C and c < rmax (C) }) car d ({c | c   C an d c > lmi n (C) }) rma x (C)   lmi n (C) rmax (C) is the rightmost covered and lmin (C) is the leftmost uncovered source position.",
        "Entity": "Normal"
    },
    {
        "Text": "u(C) is the number of  skipped  positions, and m(C) is the number of  moved  positions.",
        "Entity": "Normal"
    },
    {
        "Text": "The function card ( ) returns the cardinality of a set of source positions.",
        "Entity": "Normal"
    },
    {
        "Text": "The function w(C) describes the  window  size in which the word reordering takes place.",
        "Entity": "Normal"
    },
    {
        "Text": "A procedural description for the computation of the set of successor hypotheses for a given partial hypothesis (S, C, j) is given in Table 3.",
        "Entity": "Normal"
    },
    {
        "Text": "There are restrictions on the possible successor states: A partial hypothesis in state Skip cannot be expanded into a partial hypothesis in state Move and vice versa.",
        "Entity": "Normal"
    },
    {
        "Text": "If the coverage set for the newly generated hypothesis covers a contiguous initial block of source positions, the state Initial is entered.",
        "Entity": "Normal"
    },
    {
        "Text": "No other state S is considered as a successor state in this case (hence the use of the continue statement in the procedural description).",
        "Entity": "Normal"
    },
    {
        "Text": "The set of successor hypotheses Succ by which to extend the partial hypothesis (S, C, j) is computed using the constraints defined by the values for numskip, widthskip, nummove , and widthmove , as explained in the Appendix.",
        "Entity": "Normal"
    },
    {
        "Text": "In particular, a source position k is discarded for extension if the  window  restrictions are violated.",
        "Entity": "Normal"
    },
    {
        "Text": "Within the restrictions all possible successors are computed.",
        "Entity": "Normal"
    },
    {
        "Text": "It can be observed that the set of successors, as computed in Table 3, is never empty.",
        "Entity": "Normal"
    },
    {
        "Text": "111 Table 3 Procedural description to compute the set Succ of successor hypotheses by which to extend a partial hypothesis (S, C, j).",
        "Entity": "Normal"
    },
    {
        "Text": "input: partial hypothesis (S, C, j) Succ := { } for each k  / C do Set C = C  {k} if u(C ) = 0 Succ := Succ   (Initial, C , k) continue if (S = Initial) or (S = Skip) if w(C )   widthskip and u(C )   numskip Succ := Succ   (Skip, C , k) if (S = Initial) or (S = Move) if k I= lmin (C ) and w(C )   widthmove and m(C )   nummove Succ := Succ   (Move, C , k) if (S = Move) or (S = Cover) if (lmin (C ) = k) Succ := Succ   (Cover, C , k) output: set Succ of successor hypotheses There is an asymmetry between the two reordering states Move and Skip: While in state Move , the algorithm is not allowed to cover the position lmin (C).",
        "Entity": "Normal"
    },
    {
        "Text": "It must first enter the state Cover to do so.",
        "Entity": "Normal"
    },
    {
        "Text": "In contrast, for the state Skip, the newly generated hypothesis always remains in the state Skip (until the state Initial is entered.)",
        "Entity": "Normal"
    },
    {
        "Text": "This is motivated by the word reordering for the German verb group.",
        "Entity": "Normal"
    },
    {
        "Text": "After the right verbal brace has been processed, no source words may be moved into the verbal brace from later in the sentence.",
        "Entity": "Normal"
    },
    {
        "Text": "There is a redundancy in the reorderings: The same reordering might be carried out using either the state Skip or Move , especially if widthskip and widthmove are about the same.",
        "Entity": "Normal"
    },
    {
        "Text": "The additional computational burden is alleviated somewhat by the fact that the pruning, as introduced in Section 3.8, does not distinguish hypotheses according to the states.",
        "Entity": "Normal"
    },
    {
        "Text": "A complexity analysis for different reordering constraints is given in Tillmann (2001).",
        "Entity": "Normal"
    },
    {
        "Text": "3.6 Word Reordering: IBM-Style Restrictions.",
        "Entity": "Normal"
    },
    {
        "Text": "We now compare the new word reordering approach with the approach used in Berger et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(1996).",
        "Entity": "Normal"
    },
    {
        "Text": "In the approach presented in this article, source sentence words are aligned with hypothesized target sentence words.3 When a source sentence word is aligned, we say its position is covered.",
        "Entity": "Normal"
    },
    {
        "Text": "During the search process, a partial hypothesis is extended by choosing an uncovered source sentence position, and this choice is restricted.",
        "Entity": "Normal"
    },
    {
        "Text": "Only one of the first n uncovered positions in a coverage set may be chosen, where n is set to 4.",
        "Entity": "Normal"
    },
    {
        "Text": "This choice is illustrated in Figure 9.",
        "Entity": "Normal"
    },
    {
        "Text": "In the figure, covered positions are marked by a filled circle, and uncovered positions are marked by an unfilled circle.",
        "Entity": "Normal"
    },
    {
        "Text": "Positions that may be covered next are marked by an unfilled square.",
        "Entity": "Normal"
    },
    {
        "Text": "The restrictions for a coverage set C can be expressed in terms of the expression u(C) defined in the previous section: The number of uncovered source sentence positions to the left of the rightmost covered position.",
        "Entity": "Normal"
    },
    {
        "Text": "Demanding u(C)   3, we obtain the S3 restriction 3 In Berger et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(1996), a morphological analysis is carried out and word morphemes are processed.",
        "Entity": "Normal"
    },
    {
        "Text": "during the search.",
        "Entity": "Normal"
    },
    {
        "Text": "Here, we process only full-form words.",
        "Entity": "Normal"
    },
    {
        "Text": "112 uncovered position covered position uncovered position for extension 1 j J Figure 9 Illustration of the IBM-style reordering constraint.",
        "Entity": "Normal"
    },
    {
        "Text": "introduced in the Appendix.",
        "Entity": "Normal"
    },
    {
        "Text": "An upper bound of O(E3   J4 ) for the word reordering complexity is given in Tillmann (2001).",
        "Entity": "Normal"
    },
    {
        "Text": "3.7 Empirical Complexity Calculations.",
        "Entity": "Normal"
    },
    {
        "Text": "In order to demonstrate the complexity of the proposed reordering constraints, wehave modified our translation algorithm to show, for the different reordering con straints, the overall number of successor states generated by the algorithm given inTable 3.",
        "Entity": "Normal"
    },
    {
        "Text": "The number of successors shown in Figure 10 is counted for a pseudotransla tion task in which a pseudo source word x is translated into the identically pseudo  target word x.",
        "Entity": "Normal"
    },
    {
        "Text": "No actual optimization is carried out; the total number of successors is simply counted as the algorithm proceeds through subsets of increasing cardinality.",
        "Entity": "Normal"
    },
    {
        "Text": "The complexity differences for the different reordering constraints result from the different number of coverage subsets C and corresponding reordering states S allowed.",
        "Entity": "Normal"
    },
    {
        "Text": "For the different reordering constraints we obtain the following results (the abbreviations MON, GE, EG, and S3 are taken from the Appendix):   MON: For this reordering restriction, a partial hypothesis is always extended by the position lmin (C), hence the number of processed arcs is J.   GE, EG: These two reordering constraints are very similar in terms of complexity: The number of word reorderings is heavily restricted in each.",
        "Entity": "Normal"
    },
    {
        "Text": "Actually, since the distance restrictions (expressed by the variables widthskip and widthmove ) apply, the complexity is linear in the length of the input sentence J.   S3: The S3 reordering constraint has a complexity close to J4 .",
        "Entity": "Normal"
    },
    {
        "Text": "Since no distance restrictions for the skipped positions apply, the overall search space is significantly larger than for the GE or EG restriction.",
        "Entity": "Normal"
    },
    {
        "Text": "113 1e+07 1e+06 \" J 4 \" \" S 3 \" \" E G \" \" G E \" \" M O N \" 100000 10000 1000 100 10 1 0 5 10 15 20 25 30 35 40 45 50 Figure 10 Number of processed arcs for the pseudotranslation task as a function of the input sentence length J (y-axis is given in log scale).",
        "Entity": "Normal"
    },
    {
        "Text": "The complexity for the four different reordering constraints MON, GE, EG, and S3 is given.",
        "Entity": "Normal"
    },
    {
        "Text": "The complexity of the S3 constraint is close to J4 .",
        "Entity": "Normal"
    },
    {
        "Text": "3.8 Beam Search Pruning Techniques.",
        "Entity": "Normal"
    },
    {
        "Text": "To speed up the search, a beam search strategy is used.",
        "Entity": "Normal"
    },
    {
        "Text": "There is a direct analogy to the data-driven search organization used in continuous-speech recognition (Ney et al.",
        "Entity": "Normal"
    },
    {
        "Text": "1992).",
        "Entity": "Normal"
    },
    {
        "Text": "The full DP search algorithm proceeds cardinality-synchronously over subsets of source sentence positions of increasing cardinality.",
        "Entity": "Normal"
    },
    {
        "Text": "Using the beam search concept, the search can be focused on the most likely hypotheses.",
        "Entity": "Normal"
    },
    {
        "Text": "The hypotheses Qe1 (e, C, j) are distinguished according to the coverage set C, with two kinds of pruning based on this coverage set: 1.",
        "Entity": "Normal"
    },
    {
        "Text": "The coverage pruning is carried out separately for each coverage set C..\n\t\t\t2.",
        "Entity": "Normal"
    },
    {
        "Text": "The cardinality pruning is carried out jointly for all coverage sets C with.",
        "Entity": "Normal"
    },
    {
        "Text": "the same cardinality c = c(C).",
        "Entity": "Normal"
    },
    {
        "Text": "After the pruning is carried out, we retain for further consideration only hypotheses with a probability close to the maximum probability.",
        "Entity": "Normal"
    },
    {
        "Text": "The number of surviving hypotheses is controlled by four kinds of thresholds:   the coverage pruning threshold tC   the coverage histogram threshold nC   the cardinality pruning threshold tc   the cardinality histogram threshold nc For the coverage and the cardinality pruning, the probability Qe1 (e, C, j) is adjusted to take into account the uncovered source sentence positions   = {1, ...\n\t\t\t, J}\\C.",
        "Entity": "Normal"
    },
    {
        "Text": "To make 114 this adjustment, for a source word f at an uncovered source position, we precompute an upper bound  p(f ) for the product of language model and lexicon probability:  p(f ) = max p(e e1, e11) p(f e) e11 ,e1 ,e The above optimization is carried out only over the word trigrams (e, e1, e11) that have actually been seen in the training data.",
        "Entity": "Normal"
    },
    {
        "Text": "Additionally, the observation pruning described below is applied to the possible translations e of a source word f .",
        "Entity": "Normal"
    },
    {
        "Text": "The upper bound is used in the beam search concept to increase the comparability between hypotheses covering different coverage sets.",
        "Entity": "Normal"
    },
    {
        "Text": "Even more benefit from the upper bound  p(f ) can be expected if the distortion and the fertility probabilities are taken into account (Tillmann 2001).",
        "Entity": "Normal"
    },
    {
        "Text": "Using the definition of  p(f ), the following modified probability Q  e1 (e, C, j) is used to replace the original probability Qe1 (e, C, j), and all pruning is applied to the new probability: Q  e1 (e, C, j) = Qe1 (e, C, j)   n  p(fj ) j    For the translation experiments, equation (3) is recursively evaluated over subsets of source positions of equal cardinality.",
        "Entity": "Normal"
    },
    {
        "Text": "For reasons of brevity, we omit the state descrip tion S in equation (3), since no separate pruning according to the states S is carried out.",
        "Entity": "Normal"
    },
    {
        "Text": "The set of surviving hypotheses for each cardinality c is referred to as the beam.",
        "Entity": "Normal"
    },
    {
        "Text": "The size of the beam for cardinality c depends on the ambiguity of the translation task for that cardinality.",
        "Entity": "Normal"
    },
    {
        "Text": "To fully exploit the speedup of the DP beam search, the search space is dynamically constructed as described in Tillmann, Vogel, Ney, Zubiaga, and Sawaf (1997), rather than using a static search space.",
        "Entity": "Normal"
    },
    {
        "Text": "To carry out the pruning, the maximum probabilities with respect to each coverage set C and cardinality c are computed:   Coverage pruning: Hypotheses are distinguished according to the subset of covered positions C. The probability Q  (C) is defined: Q  ( ) = max Q  e1 (e, , j) e,e1 ,j   Cardinality pruning: Hypotheses are distinguished according to the cardinality c(C) of subsets C of covered positions.",
        "Entity": "Normal"
    },
    {
        "Text": "The probability Q  (c) is defined for all hypotheses with c(C) = c: Q  (c) = max Q  ( ) C c(C)=c The coverage pruning threshold tC and the cardinality pruning threshold tc are used to prune active hypotheses.",
        "Entity": "Normal"
    },
    {
        "Text": "We call this pruning translation pruning.",
        "Entity": "Normal"
    },
    {
        "Text": "Hypotheses are pruned according to their translation probability: Q  e1 (e, C, j) < tC   Q  (C) Q  e1 (e, C, j) < tc   Q  (c) For the translation experiments presented in Section 4, the negative logarithms of the actual pruning thresholds tc and tC are reported.",
        "Entity": "Normal"
    },
    {
        "Text": "A hypothesis (e1, e, C, j) is discarded if its probability is below the corresponding threshold.",
        "Entity": "Normal"
    },
    {
        "Text": "For the current experiments, the 115 coverage and the cardinality threshold are constant for different coverage sets C and cardinalities c. Together with the translation pruning, histogram pruning is carried out: The overall number N(C) of active hypotheses for the coverage set C and the overall number N(c) of active hypotheses for all subsets of a given cardinality may not exceed a given number; again, different numbers are used for coverage and cardinality pruning.",
        "Entity": "Normal"
    },
    {
        "Text": "The coverage histogram pruning is denoted by nC , and the cardinality histogram pruning is denoted by nc : N(C) > nC N(c) > nc If the numbers of active hypotheses for each coverage set C and cardinality c, N(C) and N(c), exceed the above thresholds, only the partial hypotheses with the highest translation probabilities are retained (e.g., we may use nC = 1,000 for the coverage histogram pruning).",
        "Entity": "Normal"
    },
    {
        "Text": "The third type of pruning conducted observation pruning: The number of words that may be produced by a source word f is limited.",
        "Entity": "Normal"
    },
    {
        "Text": "For each source language word f the list of its possible translations e is sorted according to p(f | e)   puni (e) where puni (e) is the unigram probability of the target language word e. Only the best no target words e are hypothesized during the search process (e.g., during the experiments to hypothesize, the best no = 50 words was sufficient.",
        "Entity": "Normal"
    },
    {
        "Text": "3.9 Beam Search Implementation.",
        "Entity": "Normal"
    },
    {
        "Text": "In this section, we describe the implementation of the beam search algorithm presented in the previous sections and show how it is applied to the full set of IBM4 model parameters.",
        "Entity": "Normal"
    },
    {
        "Text": "3.9.1 Baseline DP Implementation.",
        "Entity": "Normal"
    },
    {
        "Text": "The implementation described here is similar to that used in beam search speech recognition systems, as presented in Ney et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(1992).",
        "Entity": "Normal"
    },
    {
        "Text": "The similarities are given mainly in the following:   The implementation is data driven.",
        "Entity": "Normal"
    },
    {
        "Text": "Both its time and memory requirements are strictly linear in the number of path hypotheses (disregarding the sorting steps explained in this section).",
        "Entity": "Normal"
    },
    {
        "Text": "The search procedure is developed to work most efficiently when the input sentences are processed mainly monotonically from left to right.",
        "Entity": "Normal"
    },
    {
        "Text": "The algorithm works cardinality-synchronously, meaning that all the hypotheses that are processed cover subsets of source sentence positions of equal cardinality c.   Since full search is prohibitive, we use a beam search concept, as in speech recognition.",
        "Entity": "Normal"
    },
    {
        "Text": "We use appropriate pruning techniques in connection with our cardinality-synchronous search procedure.",
        "Entity": "Normal"
    },
    {
        "Text": "Table 4 shows a two-list implementation of the search algorithm given in Table 2 in which the beam pruning is included.",
        "Entity": "Normal"
    },
    {
        "Text": "The two lists are referred to as S and Snew : S is the list of hypotheses that are currently expanded, and Snew is the list of newly 116 Table 4 Two-list implementation of a DP-based search algorithm for statistical MT.",
        "Entity": "Normal"
    },
    {
        "Text": "input: source string f1     fj     fJ initial hypothesis lists: S = {($, $, { }, 0)} for each cardinality c = 1, 2, ...\n\t\t\t, J do Snew = { } for each hypothesis (e , e, C, j )   S, where j  C and |C| = c do Expand (e , e, C, j ) using probabilities p(fj | e)   p(j | j , J)   p(e | e , e ) Look up and add or update expanded hypothesis in Snew Sort hypotheses in Snew according to translation score Carry out cardinality pruning Sort hypotheses in Snew according to coverage set C and translation score Carry out coverage pruning Bookkeeping of surviving hypotheses in Snew S := Snew output: get best target word sequence eI from bookkeeping array generated hypotheses.",
        "Entity": "Normal"
    },
    {
        "Text": "The search procedure processes subsets of covered source sentence positions of increasing cardinality.",
        "Entity": "Normal"
    },
    {
        "Text": "The search starts with S = {($, $, { }, 0)}, where $ denotes the sentence start symbol for the immediate two predecessor words and { } denotes the empty coverage set, in which no source position is covered yet.",
        "Entity": "Normal"
    },
    {
        "Text": "For the initial search state, the position last covered is set to 0.",
        "Entity": "Normal"
    },
    {
        "Text": "A set S of active hypotheses is expanded for each cardinality c using lexicon model, language model, and distortion model probabilities.",
        "Entity": "Normal"
    },
    {
        "Text": "The newly generated hypotheses are added to the hypothesis set Snew ; for hypotheses that are not distinguished according to our DP approach, only the best partial hypothesis is retained for further consideration.",
        "Entity": "Normal"
    },
    {
        "Text": "This so-called recombination is implemented as a set of simple lookup and update operations on the set Snew of partial hypotheses.",
        "Entity": "Normal"
    },
    {
        "Text": "During the partial hypothesis extensions, an anticipated pruning is carried out: Hypotheses are discarded before they are considered for recombination and are never added to Snew .",
        "Entity": "Normal"
    },
    {
        "Text": "(The anticipated pruning is not shown in Table 4.",
        "Entity": "Normal"
    },
    {
        "Text": "It is based on the pruning thresholds described in Section 3.8.)",
        "Entity": "Normal"
    },
    {
        "Text": "After the extension of all partial hypotheses in S, a pruning step is carried out for the hypotheses in the newly generated set Snew .",
        "Entity": "Normal"
    },
    {
        "Text": "The pruning is based on two simple sorting steps on the list of partial hypotheses Snew .",
        "Entity": "Normal"
    },
    {
        "Text": "(Instead of sorting the partial hypothe ses, we might have used hashing.)",
        "Entity": "Normal"
    },
    {
        "Text": "First, the partial hypotheses are sorted according to their translation scores (within the implementation, all probabilities are converted into translation scores by taking the negative logarithm   log()).",
        "Entity": "Normal"
    },
    {
        "Text": "Cardinality prun ing can then be carried out simply by running down the list of hypotheses, starting with the maximum-probability hypothesis, and applying the cardinality thresholds.",
        "Entity": "Normal"
    },
    {
        "Text": "Then, the partial hypotheses are sorted a second time according to their coverage set C and their translation score.",
        "Entity": "Normal"
    },
    {
        "Text": "After this sorting step, all partial hypotheses that cover the same subset of source sentence positions are located in consecutive fragments in the overall list of partial hypotheses.",
        "Entity": "Normal"
    },
    {
        "Text": "Coverage pruning is carried out in a single run over the list of partial hypotheses: For each fragment corresponding to the same coverage set C, the coverage pruning threshold is applied.",
        "Entity": "Normal"
    },
    {
        "Text": "The partial hypotheses that survive the two pruning stages are then written into the so-called bookkeeping array (Ney et al.",
        "Entity": "Normal"
    },
    {
        "Text": "1992).",
        "Entity": "Normal"
    },
    {
        "Text": "For the next expansion step, the set S is set to the newly generated list of hypotheses.",
        "Entity": "Normal"
    },
    {
        "Text": "Finally, the target translation is constructed from the bookkeeping array.",
        "Entity": "Normal"
    },
    {
        "Text": "117 3.9.2 Details for IBM4 Model.",
        "Entity": "Normal"
    },
    {
        "Text": "In this section, we outline how the DP-based beam search approach can be carried out using the full set of IBM4 parameters.",
        "Entity": "Normal"
    },
    {
        "Text": "(More details can be found in Tillmann [2001] or in the cited papers.)",
        "Entity": "Normal"
    },
    {
        "Text": "First, the full set of IBM4 parameters does not make the simplifying assumption given in Section 3.1, namely, that source and target sentences are of equal length: Either a target word e may be aligned with several source words (its fertility is greater than one) or a single source word may produce zero, one, or two target words, as described in Berger et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(1996), or both.",
        "Entity": "Normal"
    },
    {
        "Text": "Zero target words are generated if f is aligned to the  null  word e0 .",
        "Entity": "Normal"
    },
    {
        "Text": "Generating a single target word e is the regular case.",
        "Entity": "Normal"
    },
    {
        "Text": "Two target words (e1, e11) may be generated.",
        "Entity": "Normal"
    },
    {
        "Text": "The costs for generating the target word e1 are given by its fertility  (0 | e1) and the language model probability; no lexicon probability is used.",
        "Entity": "Normal"
    },
    {
        "Text": "During the experiments, we restrict ourselves to triples of target words (e, e1, e11) actually seen in the training data.",
        "Entity": "Normal"
    },
    {
        "Text": "This approach is used for the French-to-English translation experiments presented in this article.",
        "Entity": "Normal"
    },
    {
        "Text": "Another approach for mapping a single source language word to several target language words involves preprocessing by the word-joining algorithm given in Till- mann (2001), which is similar to the approach presented in Och, Tillmann, and Ney (1999).",
        "Entity": "Normal"
    },
    {
        "Text": "Target words are joined during a training phase, and several joined target language words are dealt with as a new lexicon entry.",
        "Entity": "Normal"
    },
    {
        "Text": "This approach is used for the German-to-English translation experiments presented in this article.",
        "Entity": "Normal"
    },
    {
        "Text": "In order to deal with the IBM4 fertility parameters within the DP-based concept, we adopt the distinction between open and closed hypotheses given in Berger et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(1996).",
        "Entity": "Normal"
    },
    {
        "Text": "A hypothesis is said to be open if it is to be aligned with more source positions than it currently is (i.e., at least two).",
        "Entity": "Normal"
    },
    {
        "Text": "Otherwise it is called closed.",
        "Entity": "Normal"
    },
    {
        "Text": "The difference between open and closed is used to process the input sentence one position a time (for details see Tillmann 2001).",
        "Entity": "Normal"
    },
    {
        "Text": "The word reordering restrictions and the beam search pruning techniques are directly carried over to the full set of IBM4 parameters, since they are based on restrictions on the coverage vectors C only.",
        "Entity": "Normal"
    },
    {
        "Text": "To ensure its correctness, the implementation was tested by carrying out forced alignments on 500 German-to-English training sentence pairs.",
        "Entity": "Normal"
    },
    {
        "Text": "In a forced alignment,the source sentence f J and the target sentence eI are kept fixed, and a full search with 1 1 out reordering restrictions is carried out only over the unknown alignment aJ .",
        "Entity": "Normal"
    },
    {
        "Text": "The language model probability is divided out, and the resulting probability is compared to the Viterbi probability as obtained by the training procedure.",
        "Entity": "Normal"
    },
    {
        "Text": "For 499 training sentencesthe Viterbi alignment probability as obtained by the forced-alignment search was exactly the same as the one produced by the training procedure.",
        "Entity": "Normal"
    },
    {
        "Text": "In one case the forced alignment search did obtain a better Viterbi probability than the training procedure.",
        "Entity": "Normal"
    },
    {
        "Text": "Translation experiments are carried out for the translation directions German to English and English to German (Verbmobil task) and for the translation directions French to English and English to French (Canadian Hansards task).",
        "Entity": "Normal"
    },
    {
        "Text": "Section 4.1 reports on the performance measures used.",
        "Entity": "Normal"
    },
    {
        "Text": "Section 4.2 shows translation results for the Verbmobil task.",
        "Entity": "Normal"
    },
    {
        "Text": "Sections 4.2.1 and 4.2.2 describe that task and the preprocessing steps applied.",
        "Entity": "Normal"
    },
    {
        "Text": "In Sections 4.2.3 through 4.2.5, the efficiency of the beam search pruning techniques is shown for German-to-English translation, as the most detailed experiments are conducted for that direction.",
        "Entity": "Normal"
    },
    {
        "Text": "Section 4.2.6 gives translation results for the translation direction English to German.",
        "Entity": "Normal"
    },
    {
        "Text": "In Section 4.3, translation results for the Canadian Hansards task are reported.",
        "Entity": "Normal"
    },
    {
        "Text": "118 4.1 Performance Measures for Translation Experiments.",
        "Entity": "Normal"
    },
    {
        "Text": "To measure the performance of the translation methods, we use three types of au tomatic and easy-to-use measures of the translation errors.",
        "Entity": "Normal"
    },
    {
        "Text": "Additionally, a subjective evaluation involving human judges is carried out (Niessen et al.",
        "Entity": "Normal"
    },
    {
        "Text": "2000).",
        "Entity": "Normal"
    },
    {
        "Text": "The following evaluation criteria are employed:   WER (word error rate): The WER is computed as the minimum number of substitution, insertion, and deletion operations that have to be performed to convert the generated string into the reference target string.",
        "Entity": "Normal"
    },
    {
        "Text": "This performance criterion is widely used in speech recognition.",
        "Entity": "Normal"
    },
    {
        "Text": "The minimum is computed using a DP algorithm and is typically referred to as edit or Levenshtein distance.",
        "Entity": "Normal"
    },
    {
        "Text": "mWER (multireference WER): We use the Levenshtein distance between the automatic translation and several reference translations as a measure of the translation errors.",
        "Entity": "Normal"
    },
    {
        "Text": "For example, on the Verbmobil TEST-331 test set, an average of six reference translations per automatic translation are available.",
        "Entity": "Normal"
    },
    {
        "Text": "The Levenshtein distance between the automatic translation and each of the reference translations is computed, and the minimum Levenshtein distance is taken.",
        "Entity": "Normal"
    },
    {
        "Text": "The resulting measure, the mWER, is more robust than the WER, which takes into account only a single reference translation.",
        "Entity": "Normal"
    },
    {
        "Text": "PER (position-independent word error rate): In the case in which only a single reference translation per sentence is available, we introduce as an additional measure the position-independent word error rate (PER).",
        "Entity": "Normal"
    },
    {
        "Text": "This measure compares the words in the two sentences without taking the word order into account.",
        "Entity": "Normal"
    },
    {
        "Text": "Words in the reference translation that have no counterpart in the translated sentence are counted as substitution errors.",
        "Entity": "Normal"
    },
    {
        "Text": "Depending on whether the translated sentence is longer or shorter than the reference translation, the remaining words result in either insertion (if the translated sentence is longer) or deletion (if the translated sentence is shorter) errors.",
        "Entity": "Normal"
    },
    {
        "Text": "The PER is guaranteed to be less than or equal to the WER.",
        "Entity": "Normal"
    },
    {
        "Text": "The PER is more robust than the WER since it ignores translation errors due to different word order in the translated and reference sentences.",
        "Entity": "Normal"
    },
    {
        "Text": "SSER (subjective sentence error rate): For a more fine-grained evaluation of the translation results and to check the validity of the automatic evaluation measures subjective judgments by test persons are carried out (Niessen et al.",
        "Entity": "Normal"
    },
    {
        "Text": "2000).",
        "Entity": "Normal"
    },
    {
        "Text": "The following scale for the error count per sentence is used in these subjective evaluations: 0.0 : semantically correct and syntactically correct     :     0.5 : semantically correct and syntactically wrong     :     1.0 : semantically wrong (independent of syntax) Each translated sentence is judged by a human examiner according to the above error scale; several human judges may be involved in judging the same translated sentence.",
        "Entity": "Normal"
    },
    {
        "Text": "Subjective evaluation is carried out only for the Verbmobil TEST-147 test set.",
        "Entity": "Normal"
    },
    {
        "Text": "119 Table 5 Training and test conditions for the German-to-English Verbmobil corpus (*number of words without punctuation).",
        "Entity": "Normal"
    },
    {
        "Text": "German English Trai nin g: Se nte nc es W or ds 58,0 5 1 9 , 5 2 3 73 54 9, 92 1 W or ds* 4 1 8 , 9 7 9 45 3, 63 2 Voc abu lary : Siz e 7 , 9 1 1 4 , 6 4 8 Sin gle to ns 3 , 4 5 3 1 , 6 9 9 TEST 331: Se nte nc es W or ds 33 5 , 5 9 1 1 6 , 2 7 9 Bigram/Trigram Perplexity 84.0/68.2 49.3/38.3 TEST-147: Sentences 147 Words 1,968 2,173 Bigram/Trigram Perplexity   34.6/28.1 4.2 Verbmobil Translation Experiments.",
        "Entity": "Normal"
    },
    {
        "Text": "4.2.1 The Task and the Corpus.",
        "Entity": "Normal"
    },
    {
        "Text": "The translation system is tested on the Verbmobil task(Wahlster 2000).",
        "Entity": "Normal"
    },
    {
        "Text": "In that task, the goal is the translation of spontaneous speech in face to-face situations for an appointment scheduling domain.",
        "Entity": "Normal"
    },
    {
        "Text": "We carry out experiments for both translation directions: German to English and English to German.",
        "Entity": "Normal"
    },
    {
        "Text": "Although the Verbmobil task is still a limited-domain task, it is rather difficult in terms of vocabulary size, namely, about 5,000 words or more for each of the two languages; second, the syntactic structures of the sentences are rather unrestricted.",
        "Entity": "Normal"
    },
    {
        "Text": "Although the ultimate goal of the Verbmobil project is the translation of spoken language, the input used for the translation experiments reported on in this article is mainly the (more or less) correct orthographic transcription of the spoken sentences.",
        "Entity": "Normal"
    },
    {
        "Text": "Thus, the effects of spontaneous speech are present in the corpus; the effect of speech recognition errors, however, is not covered.",
        "Entity": "Normal"
    },
    {
        "Text": "The corpus consists of 58,073 training pairs; its characteristics are given in Table 5.",
        "Entity": "Normal"
    },
    {
        "Text": "For the translation experiments, a trigram language model with a perplexity of 28.1 is used.",
        "Entity": "Normal"
    },
    {
        "Text": "The following two test corpora are used for the translation experiments: TEST-331: This test set consists of 331 test sentences.",
        "Entity": "Normal"
    },
    {
        "Text": "Only automatic evaluation is carried out on this test corpus: The WER and the mWER are computed.",
        "Entity": "Normal"
    },
    {
        "Text": "For each test sentence in the source language there is a range of acceptable reference translations (six on average) provided by a human translator, who is asked to produce word-to-word translations wherever it is possible.",
        "Entity": "Normal"
    },
    {
        "Text": "Part of the reference sentences are obtained by correcting automatic translations of the test sentences that are produced using the approach presented in this article with different reordering constraints.",
        "Entity": "Normal"
    },
    {
        "Text": "The other part is produced from the source sentences without looking at any of their translations.",
        "Entity": "Normal"
    },
    {
        "Text": "The TEST-331 test set is used as held-out data for parameter optimization (for the language mode scaling factor and for the distortion model scaling factor).",
        "Entity": "Normal"
    },
    {
        "Text": "Furthermore, the beam search experiments in which the effect of the different pruning thresholds is demonstrated are carried out on the TEST-331 test set.",
        "Entity": "Normal"
    },
    {
        "Text": "TEST-147: The second, separate test set consists of 147 test sentences.",
        "Entity": "Normal"
    },
    {
        "Text": "Translation results are given in terms of mWER and SSER.",
        "Entity": "Normal"
    },
    {
        "Text": "No parameter optimization 120 is carried out on the TEST-147 test set; the parameter values as obtained from the experiments on the TEST-331 test set are used.",
        "Entity": "Normal"
    },
    {
        "Text": "4.2.2 Preprocessing Steps.",
        "Entity": "Normal"
    },
    {
        "Text": "To improve the translation performance the following preprocessing steps are carried out: Categorization: We use some categorization, which consists of replacing a single word by a category.",
        "Entity": "Normal"
    },
    {
        "Text": "The only words that are replaced by a category label are proper nouns denoting German cities.",
        "Entity": "Normal"
    },
    {
        "Text": "Using the new labeled corpus, all probability models are trained anew.",
        "Entity": "Normal"
    },
    {
        "Text": "To produce translations in the  normal  language, the categories are translated by rule and are inserted into the target sentence.",
        "Entity": "Normal"
    },
    {
        "Text": "Word joining: Target language words are joined using a method similar to the one described in Och, Tillmann, and Ney (1999).",
        "Entity": "Normal"
    },
    {
        "Text": "Words are joined to handle cases like the German compound noun  Zahnarzttermin  for the English  dentist s appointment,  because a single word has to be mapped to two or more target words.",
        "Entity": "Normal"
    },
    {
        "Text": "The word joining is applied only to the target language words; the source language sentences remain unchanged.",
        "Entity": "Normal"
    },
    {
        "Text": "During the search process several joined target language words may be generated by a single source language word.",
        "Entity": "Normal"
    },
    {
        "Text": "Manual lexicon: To account for unseen words in the test sentences and to obtain a greater number of focused translation probabilities p(f | e), we use a bilin gual GermanEnglish dictionary.",
        "Entity": "Normal"
    },
    {
        "Text": "For each word e in the target vocabulary, we create a list of source translations f according to this dictionary.",
        "Entity": "Normal"
    },
    {
        "Text": "The translation probability pdic (f | e) for the dictionary entry (f , e) is defined as   1 pdic (f | e) =   Ne if (f , e) is in dictionary  0 otherwise where Ne is the number of source words listed as translations of the target word e. The dictionary probability pdic (f | e) is linearly combined with the automatically trained translation probabilities paut (f | e) to obtain smoothed probabilities p(f | e): p(f | e) = (1    )   pdic (f | e)+     paut (f | e) For the translation experiments, the value of the interpolation parameter is fixed at   = 0.5.",
        "Entity": "Normal"
    },
    {
        "Text": "4.2.3 Effect of the Scaling Factors.",
        "Entity": "Normal"
    },
    {
        "Text": "In speech recognition, in which Bayes  decision rule is applied, a language model scaling factor  LM is used; a typical value is  LM   15.",
        "Entity": "Normal"
    },
    {
        "Text": "This scaling factor is employed because the language model probabilities are more reliably estimated than the acoustic probabilities.",
        "Entity": "Normal"
    },
    {
        "Text": "Following this use of a language model scaling factor in speech recognition, such a factor is introduced into statistical MT, too.",
        "Entity": "Normal"
    },
    {
        "Text": "The optimization criterion in equation (1) is modified as follows:  eI = arg max{p(eI ) LM   p(f J | eI )} 1 I 1 1 1 1 where p(eI ) is the language model probability of the target language sentence.",
        "Entity": "Normal"
    },
    {
        "Text": "In the experiments presented here, a trigram language model is used to compute p(eI ).",
        "Entity": "Normal"
    },
    {
        "Text": "The 121 Table 6 Computing time, mWER, and SSER for three different reordering constraints on the TEST-147 test set.",
        "Entity": "Normal"
    },
    {
        "Text": "During the translation experiments, reordered words are not allowed to cross punctuation marks.",
        "Entity": "Normal"
    },
    {
        "Text": "Re or de rin g co nst rai nt C P U ti m e [ s e c ] m W E R [ % ] S S E R [ % ] MO N 0 .",
        "Entity": "Normal"
    },
    {
        "Text": "2 4 0 .",
        "Entity": "Normal"
    },
    {
        "Text": "6 28 .6 GE 5 .",
        "Entity": "Normal"
    },
    {
        "Text": "2 3 3 .",
        "Entity": "Normal"
    },
    {
        "Text": "3 21 .0 S3 1 3 .",
        "Entity": "Normal"
    },
    {
        "Text": "7 3 4 .",
        "Entity": "Normal"
    },
    {
        "Text": "4 19 .9 effect of the language model scaling factor  LM is studied on the TEST-331 test set.",
        "Entity": "Normal"
    },
    {
        "Text": "A minimum mWER is obtained for  LM = 0.8, as reported in Tillmann (2001).",
        "Entity": "Normal"
    },
    {
        "Text": "Unlike in speech recognition, the translation model probabilities seem to be estimated as reliably as the language model probabilities in statistical MT.",
        "Entity": "Normal"
    },
    {
        "Text": "A second scaling factor  D is introduced for the distortion model probabilities p(j | j1, J).",
        "Entity": "Normal"
    },
    {
        "Text": "A minimum mWER is obtained for  D = 0.4, as reported in Tillmann (2001).",
        "Entity": "Normal"
    },
    {
        "Text": "The WER and mWER on the TEST-331 test set increase significantly, if no distortion probability is used, for the case  D = 0.0.",
        "Entity": "Normal"
    },
    {
        "Text": "The benefit of a distortion probability scaling factor of  D = 0.4 comes from the fact that otherwise, a low distortion probability might suppress long-distant word reordering that is important for German-to-English verb group reordering.",
        "Entity": "Normal"
    },
    {
        "Text": "The setting  LM = 0.8 and  D = 0.4 is used for all subsequent translation results (including the translation direction English to German).",
        "Entity": "Normal"
    },
    {
        "Text": "4.2.4 Effect of the Word Reordering Constraints.",
        "Entity": "Normal"
    },
    {
        "Text": "Table 6 shows the computing time, mWER, and SSER on the TEST-147 test set as a function of three reordering constraints: MON, GE, and S3 (as discussed in the Appendix).",
        "Entity": "Normal"
    },
    {
        "Text": "The computing time is given in terms of central processing unit (CPU) time per sentence (on a 450 MHz Pentium III personal computer).",
        "Entity": "Normal"
    },
    {
        "Text": "For the SSER, it turns out that restricting the word reordering such that it may not cross punctuation marks improves translation performance significantly.",
        "Entity": "Normal"
    },
    {
        "Text": "The average length of the sentence fragments that are separated by punctuation marks is rather small: 4.5 words per fragment.",
        "Entity": "Normal"
    },
    {
        "Text": "A coverage pruning threshold 4 of tC = 5.0 and an observation pruning of no = 50 are applied during the experiments.",
        "Entity": "Normal"
    },
    {
        "Text": "No other type of pruning is used.5 The MON constraint performs worst in terms of both mWER and SSER.",
        "Entity": "Normal"
    },
    {
        "Text": "The computing time is small, since no reordering is carried out.",
        "Entity": "Normal"
    },
    {
        "Text": "Constraints GE and S3 perform nearly identically in terms of both mWER and SSER.",
        "Entity": "Normal"
    },
    {
        "Text": "The GE constraint, however, works about three times as fast as the S3 constraint.",
        "Entity": "Normal"
    },
    {
        "Text": "Table 7 shows example translations obtained under the three different reordering constraints.",
        "Entity": "Normal"
    },
    {
        "Text": "Again, the MON reordering constraint performs worst.",
        "Entity": "Normal"
    },
    {
        "Text": "In the second and third translation examples, the S3 word reordering constraint performs worse than the GE reordering constraint, since it cannot take the word reordering due to the German verb group properly into account.",
        "Entity": "Normal"
    },
    {
        "Text": "The German finite verbs bin (second example) and ko nnten (third example) are too far away from the personal pronouns ich and Sie (six 4 For the translation experiments, the negative logarithm of the actual pruning thresholds tc and tC is.",
        "Entity": "Normal"
    },
    {
        "Text": "reported; for simplicity reasons we do not change the notation.",
        "Entity": "Normal"
    },
    {
        "Text": "5 In a speech-to-speech demo system, we would use the GE reordering restriction and a slightly sharper.",
        "Entity": "Normal"
    },
    {
        "Text": "pruning in order to achieve translation times of about one second per sentence.",
        "Entity": "Normal"
    },
    {
        "Text": "122 Table 7 Example translations for the translation direction German to English using three different reordering constraints: MON, GE, and S3.",
        "Entity": "Normal"
    },
    {
        "Text": "Input: Ja, wunderbar.",
        "Entity": "Normal"
    },
    {
        "Text": "Ko  nnen wir machen.",
        "Entity": "Normal"
    },
    {
        "Text": "MON: Yes, wonderful.",
        "Entity": "Normal"
    },
    {
        "Text": "Can we do.",
        "Entity": "Normal"
    },
    {
        "Text": "GE: Yes, wonderful.",
        "Entity": "Normal"
    },
    {
        "Text": "We can do that.",
        "Entity": "Normal"
    },
    {
        "Text": "S3: Yes, wonderful.",
        "Entity": "Normal"
    },
    {
        "Text": "We can do that.",
        "Entity": "Normal"
    },
    {
        "Text": "Input: Das ist zu knapp , weil ich ab dem dritten in Kaiserslautern bin.",
        "Entity": "Normal"
    },
    {
        "Text": "Genaugenommen nur am dritten.",
        "Entity": "Normal"
    },
    {
        "Text": "Wie wa  re es denn am a  hm Samstag, dem zehnten Februar?",
        "Entity": "Normal"
    },
    {
        "Text": "MON: That is too tight , because I from the third in Kaiserslautern.",
        "Entity": "Normal"
    },
    {
        "Text": "In fact only on the third.",
        "Entity": "Normal"
    },
    {
        "Text": "How about a  hm Saturday , the tenth of February?",
        "Entity": "Normal"
    },
    {
        "Text": "GE: That is too tight, because I am from the third in Kaiserslautern.",
        "Entity": "Normal"
    },
    {
        "Text": "In fact only on the third.",
        "Entity": "Normal"
    },
    {
        "Text": "A  hm how about Saturday, February the tenth?",
        "Entity": "Normal"
    },
    {
        "Text": "S3: That is too tight, from the third because I will be in Kaiserslautern.",
        "Entity": "Normal"
    },
    {
        "Text": "In fact only on the third.",
        "Entity": "Normal"
    },
    {
        "Text": "A  hm how about Saturday, February the tenth?",
        "Entity": "Normal"
    },
    {
        "Text": "Input: Wenn Sie dann noch den siebzehnten ko  nnten, wa  re das toll, ja.",
        "Entity": "Normal"
    },
    {
        "Text": "MON: If you then also the seventeenth could, would be the great, yes.",
        "Entity": "Normal"
    },
    {
        "Text": "GE: If you could then also the seventeenth, that would be great, yes.",
        "Entity": "Normal"
    },
    {
        "Text": "S3: Then if you could even take seventeenth, that would be great, yes.",
        "Entity": "Normal"
    },
    {
        "Text": "Input: Ja, das kommt mir sehr gelegen.",
        "Entity": "Normal"
    },
    {
        "Text": "Machen wir es dann am besten so.",
        "Entity": "Normal"
    },
    {
        "Text": "MON: Yes, that suits me perfectly.",
        "Entity": "Normal"
    },
    {
        "Text": "Do we should best like that.",
        "Entity": "Normal"
    },
    {
        "Text": "GE: Yes, that suits me fine.",
        "Entity": "Normal"
    },
    {
        "Text": "We do it like that then best.",
        "Entity": "Normal"
    },
    {
        "Text": "S3: Yes, that suits me fine.",
        "Entity": "Normal"
    },
    {
        "Text": "We should best do it like that.",
        "Entity": "Normal"
    },
    {
        "Text": "and five source sentence positions, respectively) to be reordered properly.",
        "Entity": "Normal"
    },
    {
        "Text": "In the last example, the less restrictive S3 reordering constraint leads to a better translation; the GE translation is still acceptable, though.",
        "Entity": "Normal"
    },
    {
        "Text": "4.2.5 Effect of the Beam Search Pruning Thresholds.",
        "Entity": "Normal"
    },
    {
        "Text": "In this section, the effect of the beam search pruning is demonstrated.",
        "Entity": "Normal"
    },
    {
        "Text": "Translation results on the TEST-331 test set are presented to evaluate the effectiveness of the pruning techniques.6 The quality of the search algorithm with respect to the GE and S3 reordering constraints is evaluated using two criteria: 1.",
        "Entity": "Normal"
    },
    {
        "Text": "The number of search errors for a certain combination of pruning.",
        "Entity": "Normal"
    },
    {
        "Text": "thresholds is counted.",
        "Entity": "Normal"
    },
    {
        "Text": "A search error occurs for a test sentence if the final translation probability QF for a candidate translation eI as given in equation (4) is smaller than a reference probability for that test sentence.",
        "Entity": "Normal"
    },
    {
        "Text": "We will compute reference probabilities two ways, as explained below.",
        "Entity": "Normal"
    },
    {
        "Text": "2.",
        "Entity": "Normal"
    },
    {
        "Text": "The mWER performance measure is computed as a function of the.",
        "Entity": "Normal"
    },
    {
        "Text": "pruning thresholds used.",
        "Entity": "Normal"
    },
    {
        "Text": "Generally, decreasing the pruning threshold 6 The CPU times on the TEST-331 set are higher, since the average fragment length is greater than for the.",
        "Entity": "Normal"
    },
    {
        "Text": "TEST-147 set.",
        "Entity": "Normal"
    },
    {
        "Text": "123 Effect of the coverage pruning threshold tC on the number of search errors and mWER on the TEST-331 test set (no cardinality pruning carried out: tc =  ).",
        "Entity": "Normal"
    },
    {
        "Text": "A cardinality histogram pruning of 200,000 is applied to restrict the maximum overall size of the search space.",
        "Entity": "Normal"
    },
    {
        "Text": "The negative logarithm of tC is reported.",
        "Entity": "Normal"
    },
    {
        "Text": "Re or de rin g co nst rai nt t C C P U ti m e [ s e c ] S e a r c h e r r o r s Qref > QF QF  > QF m W E R [ % ] GE 0.",
        "Entity": "Normal"
    },
    {
        "Text": "0 1 0 .",
        "Entity": "Normal"
    },
    {
        "Text": "2 1 3 1 8 3 2 3 7 3 .",
        "Entity": "Normal"
    },
    {
        "Text": "5 0.",
        "Entity": "Normal"
    },
    {
        "Text": "1 0 .",
        "Entity": "Normal"
    },
    {
        "Text": "4 3 2 3 1 3 0 1 5 3 .",
        "Entity": "Normal"
    },
    {
        "Text": "1 1.",
        "Entity": "Normal"
    },
    {
        "Text": "0 1 .",
        "Entity": "Normal"
    },
    {
        "Text": "4 3 1 0 2 2 6 3 0 .",
        "Entity": "Normal"
    },
    {
        "Text": "3 2.",
        "Entity": "Normal"
    },
    {
        "Text": "5 4 .",
        "Entity": "Normal"
    },
    {
        "Text": "7 5 5 1 4 2 2 5 .",
        "Entity": "Normal"
    },
    {
        "Text": "8 5.",
        "Entity": "Normal"
    },
    {
        "Text": "0 2 9 .",
        "Entity": "Normal"
    },
    {
        "Text": "6   3 5 2 4 .",
        "Entity": "Normal"
    },
    {
        "Text": "6 7.",
        "Entity": "Normal"
    },
    {
        "Text": "5 1 5 6   2 2 4 .",
        "Entity": "Normal"
    },
    {
        "Text": "9 10.",
        "Entity": "Normal"
    },
    {
        "Text": "0 6 3 0     2 4 .",
        "Entity": "Normal"
    },
    {
        "Text": "9 12.",
        "Entity": "Normal"
    },
    {
        "Text": "5 1 3 0 0     2 4 .",
        "Entity": "Normal"
    },
    {
        "Text": "9 S3 0.",
        "Entity": "Normal"
    },
    {
        "Text": "0 1 5 .",
        "Entity": "Normal"
    },
    {
        "Text": "4 8 3 1 4 3 2 4 7 0 .",
        "Entity": "Normal"
    },
    {
        "Text": "0 0.",
        "Entity": "Normal"
    },
    {
        "Text": "1 9 .",
        "Entity": "Normal"
    },
    {
        "Text": "2 1 2 2 5 3 0 3 5 0 .",
        "Entity": "Normal"
    },
    {
        "Text": "9 1.",
        "Entity": "Normal"
    },
    {
        "Text": "0 4 6 .",
        "Entity": "Normal"
    },
    {
        "Text": "2 4 2 2 3 3 1 .",
        "Entity": "Normal"
    },
    {
        "Text": "6 2.",
        "Entity": "Normal"
    },
    {
        "Text": "5 1 9 0   1 2 9 2 8 .",
        "Entity": "Normal"
    },
    {
        "Text": "4 5.",
        "Entity": "Normal"
    },
    {
        "Text": "0 8 3 0     2 8 .",
        "Entity": "Normal"
    },
    {
        "Text": "3 leads to a higher word error rate, since the optimal path through the translation lattice is missed, resulting in translation errors.",
        "Entity": "Normal"
    },
    {
        "Text": "Two automatically generated reference probabilities are used.",
        "Entity": "Normal"
    },
    {
        "Text": "These probabilities are computed separately for the reordering constraints GE and S3 (the difference is not shown in the notation, but will be clear from the context): Qref : A forced alignment is carried out between each of the test sentences and its corresponding reference translation; only a single reference translation for each test sentence is used.",
        "Entity": "Normal"
    },
    {
        "Text": "The probability obtained for the reference translation is denoted by Qref .",
        "Entity": "Normal"
    },
    {
        "Text": "QF  : A translation is carried out with conservatively large pruning thresholds, yielding a translation close to the one with the maximum translation probability.",
        "Entity": "Normal"
    },
    {
        "Text": "The translation probability for that translation is denoted by QF  .",
        "Entity": "Normal"
    },
    {
        "Text": "First, in a series of experiments we study the effect of the coverage and cardinality pruning for the reordering constraints GE and S3.",
        "Entity": "Normal"
    },
    {
        "Text": "(When we report on the different pruning thresholds, we will show the negative logarithm of those pruning thresholds.)",
        "Entity": "Normal"
    },
    {
        "Text": "The experiments are carried out on two different pruning  dimensions : 1.",
        "Entity": "Normal"
    },
    {
        "Text": "In Table 8, only coverage pruning using threshold tC is carried out; no.",
        "Entity": "Normal"
    },
    {
        "Text": "cardinality pruning is applied: tc =  .",
        "Entity": "Normal"
    },
    {
        "Text": "2.",
        "Entity": "Normal"
    },
    {
        "Text": "In Table 9, only cardinality pruning using threshold tc is carried out; no.",
        "Entity": "Normal"
    },
    {
        "Text": "coverage pruning is applied: tC =  .",
        "Entity": "Normal"
    },
    {
        "Text": "Both tables use an observation pruning of no = 50.",
        "Entity": "Normal"
    },
    {
        "Text": "The effect of the coverage pruning threshold tC is demonstrated in Table 8.",
        "Entity": "Normal"
    },
    {
        "Text": "For the translation experiments reportedin this table, the cardinality pruning threshold is set to tc =  ; thus, no compari son between partial hypotheses that do not cover the same set C of source sentence 124 Effect of the cardinality pruning threshold tc on the number of search errors and mWER on the TEST-331 test set (no coverage pruning is carried out: tC =  ).",
        "Entity": "Normal"
    },
    {
        "Text": "A coverage histogram pruning of 1,000 is applied to restrict the overall size of the search space.",
        "Entity": "Normal"
    },
    {
        "Text": "The negative logarithm of tc is shown.",
        "Entity": "Normal"
    },
    {
        "Text": "Re or de rin g co nst rai nt t c C P U ti m e [ s e c ] S e a r c Qre f > QF h error s QF   > QF m W E R [ % ] GE 1.",
        "Entity": "Normal"
    },
    {
        "Text": "0 0 .",
        "Entity": "Normal"
    },
    {
        "Text": "0 3 4 5 2 8 7 4 8 .",
        "Entity": "Normal"
    },
    {
        "Text": "5 2.",
        "Entity": "Normal"
    },
    {
        "Text": "0 0 .",
        "Entity": "Normal"
    },
    {
        "Text": "0 6 2 0 2 7 7 4 1 .",
        "Entity": "Normal"
    },
    {
        "Text": "9 3.",
        "Entity": "Normal"
    },
    {
        "Text": "0 0 .",
        "Entity": "Normal"
    },
    {
        "Text": "1 3 1 6 2 6 6 3 7 .",
        "Entity": "Normal"
    },
    {
        "Text": "7 4.",
        "Entity": "Normal"
    },
    {
        "Text": "0 0 .",
        "Entity": "Normal"
    },
    {
        "Text": "3 0 6 2 3 9 3 4 .",
        "Entity": "Normal"
    },
    {
        "Text": "1 5.",
        "Entity": "Normal"
    },
    {
        "Text": "0 0 .",
        "Entity": "Normal"
    },
    {
        "Text": "5 5 2 2 1 2 3 0 .",
        "Entity": "Normal"
    },
    {
        "Text": "5 7.",
        "Entity": "Normal"
    },
    {
        "Text": "5 3 .",
        "Entity": "Normal"
    },
    {
        "Text": "2   1 0 6 2 6 .",
        "Entity": "Normal"
    },
    {
        "Text": "6 10.",
        "Entity": "Normal"
    },
    {
        "Text": "0 1 4 .",
        "Entity": "Normal"
    },
    {
        "Text": "2   3 2 2 5 .",
        "Entity": "Normal"
    },
    {
        "Text": "1 12.",
        "Entity": "Normal"
    },
    {
        "Text": "5 4 2 .",
        "Entity": "Normal"
    },
    {
        "Text": "2   5 2 4 .",
        "Entity": "Normal"
    },
    {
        "Text": "9 15.",
        "Entity": "Normal"
    },
    {
        "Text": "0 9 3 .",
        "Entity": "Normal"
    },
    {
        "Text": "9     2 4 .",
        "Entity": "Normal"
    },
    {
        "Text": "9 17.",
        "Entity": "Normal"
    },
    {
        "Text": "5 1 7 6 .",
        "Entity": "Normal"
    },
    {
        "Text": "7     2 4 .",
        "Entity": "Normal"
    },
    {
        "Text": "9 S3 1.",
        "Entity": "Normal"
    },
    {
        "Text": "0 0 .",
        "Entity": "Normal"
    },
    {
        "Text": "0 2 1 0 3 3 1 5 1 .",
        "Entity": "Normal"
    },
    {
        "Text": "4 2.",
        "Entity": "Normal"
    },
    {
        "Text": "0 0 .",
        "Entity": "Normal"
    },
    {
        "Text": "0 5 1 2 8 3 4 6 .",
        "Entity": "Normal"
    },
    {
        "Text": "2 3.",
        "Entity": "Normal"
    },
    {
        "Text": "0 0 .",
        "Entity": "Normal"
    },
    {
        "Text": "1 0 1 2 7 4 4 3 .",
        "Entity": "Normal"
    },
    {
        "Text": "3 4.",
        "Entity": "Normal"
    },
    {
        "Text": "0 0 .",
        "Entity": "Normal"
    },
    {
        "Text": "2 2   2 5 1 4 0 .",
        "Entity": "Normal"
    },
    {
        "Text": "2 5.",
        "Entity": "Normal"
    },
    {
        "Text": "0 0 .",
        "Entity": "Normal"
    },
    {
        "Text": "5 0   2 2 7 3 7 .",
        "Entity": "Normal"
    },
    {
        "Text": "5 7.",
        "Entity": "Normal"
    },
    {
        "Text": "5 4 .",
        "Entity": "Normal"
    },
    {
        "Text": "3   1 7 1 3 2 .",
        "Entity": "Normal"
    },
    {
        "Text": "9 10.",
        "Entity": "Normal"
    },
    {
        "Text": "0 2 6 .",
        "Entity": "Normal"
    },
    {
        "Text": "8   9 9 3 0 .",
        "Entity": "Normal"
    },
    {
        "Text": "8 12.",
        "Entity": "Normal"
    },
    {
        "Text": "5 1 2 3 .",
        "Entity": "Normal"
    },
    {
        "Text": "3   4 9 2 8 .",
        "Entity": "Normal"
    },
    {
        "Text": "9 15.",
        "Entity": "Normal"
    },
    {
        "Text": "0 4 3 0     2 8 .",
        "Entity": "Normal"
    },
    {
        "Text": "2 positions is carried out.",
        "Entity": "Normal"
    },
    {
        "Text": "To restrict the overall size of the search space in terms of CPU time and memory requirements, a cardinality pruning of nc = 200,000 is applied.",
        "Entity": "Normal"
    },
    {
        "Text": "As can be seen from Table 8, mWER and the number of search errors decrease significantly as the coverage pruning threshold tC increases.",
        "Entity": "Normal"
    },
    {
        "Text": "For the GE reordering constraint, mWER decreases from 73.5% to 24.9%.",
        "Entity": "Normal"
    },
    {
        "Text": "For a coverage pruning threshold tC   5.0, mWER remains nearly constant at 25.0%, although search errors still occur.",
        "Entity": "Normal"
    },
    {
        "Text": "For the S3 reordering constraint, mWER decreases from 70.0% to 28.3%.",
        "Entity": "Normal"
    },
    {
        "Text": "The largest coverage threshold tested for the S3 constraint is tC = 5.0, since for larger threshold values tC , the search procedure cannot be carried out because of memory and time restrictions.",
        "Entity": "Normal"
    },
    {
        "Text": "The number of search errors is reduced as the coverage pruning threshold is increased.",
        "Entity": "Normal"
    },
    {
        "Text": "It turns out to be difficult to verify search errors by looking at the reference translation probabilities Qref alone.",
        "Entity": "Normal"
    },
    {
        "Text": "The translation with the maximum translation probability seems to be quite narrowly defined.",
        "Entity": "Normal"
    },
    {
        "Text": "The coverage pruning is more effective for the GE constraint than for the S3 constraint, since the overall search space for the GE reordering is smaller.",
        "Entity": "Normal"
    },
    {
        "Text": "Table 9 shows the effect of the cardinality pruning threshold tc on mWER when no coverage pruning is carried out (a histogram coverage pruning of 1,000 is applied to restrict the overall size of the search space).",
        "Entity": "Normal"
    },
    {
        "Text": "The cardinality threshold tc has a strong effect on mWER, which decreases significantly as the cardinality threshold tc increases.",
        "Entity": "Normal"
    },
    {
        "Text": "For the GE reordering constraint, mWER decreases from 48.5% to 24.9%; for the S3 reordering constraint, mWER decreases from 51.4% to 28.2%.",
        "Entity": "Normal"
    },
    {
        "Text": "For the coverage threshold t = 15.0, the GE constraint works about four times as fast as the S3 constraint, since the overall search space for the S3 constraint is much larger.",
        "Entity": "Normal"
    },
    {
        "Text": "Although the overall search space is much larger for the S3 constraint, for smaller values of the coverage 125 Effect of observation pruning on the number of search errors and mWER on the TEST-331 test set (parameter setting: tc =  , tC = 10.0 ).",
        "Entity": "Normal"
    },
    {
        "Text": "No histogram pruning is applied.",
        "Entity": "Normal"
    },
    {
        "Text": "The results are reported for the GE constraint.",
        "Entity": "Normal"
    },
    {
        "Text": "Ob ser vat ion pr un in g no C P U ti m e [ s e c ] S e a r c h e r r o r s Qref > QF QF  > QF m W E R [ % ] 1 2 .",
        "Entity": "Normal"
    },
    {
        "Text": "0 1 3 2 8 4 2 9 .",
        "Entity": "Normal"
    },
    {
        "Text": "3 2 5 .",
        "Entity": "Normal"
    },
    {
        "Text": "9 6 2 3 9 2 6 .",
        "Entity": "Normal"
    },
    {
        "Text": "9 3 1 0 .",
        "Entity": "Normal"
    },
    {
        "Text": "8 2 1 9 6 2 5 .",
        "Entity": "Normal"
    },
    {
        "Text": "7 5 2 3 .",
        "Entity": "Normal"
    },
    {
        "Text": "6 2 1 4 0 2 5 .",
        "Entity": "Normal"
    },
    {
        "Text": "3 10 6 2 .",
        "Entity": "Normal"
    },
    {
        "Text": "9   9 9 2 4 .",
        "Entity": "Normal"
    },
    {
        "Text": "8 25 2 3 8   4 4 2 4 .",
        "Entity": "Normal"
    },
    {
        "Text": "5 50 6 3 0     2 4 .",
        "Entity": "Normal"
    },
    {
        "Text": "9 threshold tC   5.0, the S3 constraint works as fast as the GE constraint or even faster, because only a very small portion of the overall search space is searched for small values of the cardinality pruning threshold tc .",
        "Entity": "Normal"
    },
    {
        "Text": "There is some computational overhead in expanding a partial hypothesis for the GE constraint because the finite-state control has to be handled.",
        "Entity": "Normal"
    },
    {
        "Text": "No results are obtained for the S3 constraint and the coverage threshold tc = 17.5 because of memory restrictions.",
        "Entity": "Normal"
    },
    {
        "Text": "The number of search errors is reduced as the cardinality pruning threshold is increased.",
        "Entity": "Normal"
    },
    {
        "Text": "Again, it is difficult to verify search errors by looking at the reference translation probabilities alone.",
        "Entity": "Normal"
    },
    {
        "Text": "Both coverage and cardinality pruning are more efficient for the GE reordering constraint than for the S3 reordering constraint.",
        "Entity": "Normal"
    },
    {
        "Text": "For the S3 constraint, no translation results are obtained for a coverage threshold tc > 5.0 without cardinality pruning applied because of memory and computing time restrictions.",
        "Entity": "Normal"
    },
    {
        "Text": "For the GE constraint virtually a full search can be carried out where only observation pruning is applied: Identical target translations and translation probabilities are produced for the hypoth esis files for the two cases (1) tC = 10.0, tc =  , and (2) tC =  , tc = 15.0.",
        "Entity": "Normal"
    },
    {
        "Text": "(Actually, for one test sentence in the TEST-331 test set, the translations are different, although the translation probabilities are exactly the same.)",
        "Entity": "Normal"
    },
    {
        "Text": "Since the pruning is carried out independently on two different pruning dimensions, no search errors will occur if the thresholds are further increased.",
        "Entity": "Normal"
    },
    {
        "Text": "Table 10 shows the effect of the observation pruning parameter no on mWER for the reordering constraint GE.",
        "Entity": "Normal"
    },
    {
        "Text": "mWER is significantly reduced by hypothesizing up to the best 50 target words e for a source language word f .",
        "Entity": "Normal"
    },
    {
        "Text": "mWER increases from 24.9% to 29.3% when the number of hypothesized words is decreased to only a single word.Table 11 demonstrates the effect of the combination of the coverage pruning thresh old tC = 5.0 and the cardinality pruning threshold tc = 12.5, where the actual values are found in informal experiments: In a typical setting of the two parameters tc should be at least twice as big as tC .",
        "Entity": "Normal"
    },
    {
        "Text": "For the GE reordering constraint, the average computing time is about seven seconds per sentence without any loss in translation performance as measured in terms of mWER.",
        "Entity": "Normal"
    },
    {
        "Text": "For the S3 reordering constraint, the average computing time per sentence is 27 seconds.",
        "Entity": "Normal"
    },
    {
        "Text": "Again, the combination of coverage and cardinality pruning works more efficiently for the GE constraint.",
        "Entity": "Normal"
    },
    {
        "Text": "The memory requirement for the algorithm is about 100 MB.",
        "Entity": "Normal"
    },
    {
        "Text": "4.2.6 English-to-German Translation Experiments.",
        "Entity": "Normal"
    },
    {
        "Text": "A series of translation experiments for the translation direction English to German are also carried out.",
        "Entity": "Normal"
    },
    {
        "Text": "The results, given 126 Demonstration of the combination of the two pruning thresholds tC = 5.0 and tc = 12.5 to speed up the search process for the two reordering constraints GE and S3 (no = 50).",
        "Entity": "Normal"
    },
    {
        "Text": "The translation performance is shown in terms of mWER on the TEST-331 test set.",
        "Entity": "Normal"
    },
    {
        "Text": "Reordering tC tc CPU time Search errors mWER con stra int [s e c] Q ref > Q F QF  > QF [ % ] GE 5.",
        "Entity": "Normal"
    },
    {
        "Text": "0 12.",
        "Entity": "Normal"
    },
    {
        "Text": "5 6 .",
        "Entity": "Normal"
    },
    {
        "Text": "9 0 3 8 24 .7 S3 5.",
        "Entity": "Normal"
    },
    {
        "Text": "0 12.",
        "Entity": "Normal"
    },
    {
        "Text": "5 2 6 .",
        "Entity": "Normal"
    },
    {
        "Text": "9 0 6 5 29 .2 Table 12 Translation results for the translation direction English to German on the TEST-331 test set.",
        "Entity": "Normal"
    },
    {
        "Text": "The results are given in terms of computing time, WER, and PER for three different reordering constraints: MON, EG, and S3.",
        "Entity": "Normal"
    },
    {
        "Text": "Re or de rin g co nst rai nt C P U ti m e [ s e c ] W E R [ % ] P E R [ % ] MO N 0 .",
        "Entity": "Normal"
    },
    {
        "Text": "5 70 .6 57.",
        "Entity": "Normal"
    },
    {
        "Text": "0 EG 1 0 .",
        "Entity": "Normal"
    },
    {
        "Text": "1 70 .1 55.",
        "Entity": "Normal"
    },
    {
        "Text": "9 S3 5 3 .",
        "Entity": "Normal"
    },
    {
        "Text": "2 70 .1 55.",
        "Entity": "Normal"
    },
    {
        "Text": "8 in terms of WER and PER, are shown in Table 12.",
        "Entity": "Normal"
    },
    {
        "Text": "For the English-to-German translation direction, a single reference translation for each test sentence is used to carry out the automatic evaluation.",
        "Entity": "Normal"
    },
    {
        "Text": "The translation task for the translation direction English to German is more difficult than for the translation direction German to English; the trigram language model perplexity increases from 38.3 to 68.2 on the TEST-331 test set, as can be seen in Table 5.",
        "Entity": "Normal"
    },
    {
        "Text": "No parameter optimization is carried out for this translation direction; the parameter settings are carried over from the results obtained in Table 11.",
        "Entity": "Normal"
    },
    {
        "Text": "The word error rates for the translation direction English to German are significantly higher than those for the translation direction German to English.",
        "Entity": "Normal"
    },
    {
        "Text": "There are several reasons for this: German vocabulary and perplexity are significantly larger than those for English, and only a single reference translation per test sentence is available for English-to-German translation.",
        "Entity": "Normal"
    },
    {
        "Text": "There is only a very small difference in terms of word error rates for the reordering constraints EG and S3; in particular, WER is 70.1% for both.",
        "Entity": "Normal"
    },
    {
        "Text": "The reordering constraint MON performs slightly worse: WER increases to 70.6%, and PER increases to 57.0%.",
        "Entity": "Normal"
    },
    {
        "Text": "Table 13 shows translation examples for the translation direction English to German.",
        "Entity": "Normal"
    },
    {
        "Text": "The MON constraint performs worst; there is no significant difference in quality of translations produced under the EG and the S3 constraints.",
        "Entity": "Normal"
    },
    {
        "Text": "4.3 Canadian Hansards Translation Experiments.",
        "Entity": "Normal"
    },
    {
        "Text": "4.3.1 The Task and the Corpus.",
        "Entity": "Normal"
    },
    {
        "Text": "The second corpus on which we perform translationexperiments is the Hansard corpus.",
        "Entity": "Normal"
    },
    {
        "Text": "By law, the proceedings of the Canadian parliament are recorded in both French and English.",
        "Entity": "Normal"
    },
    {
        "Text": "(For historical reasons, these proceed ings are called  Hansards. )",
        "Entity": "Normal"
    },
    {
        "Text": "The remarks of the parliament members are written down in whichever of the two languages they use.",
        "Entity": "Normal"
    },
    {
        "Text": "They are then translated into the other language to produce complete sets of the proceedings, one in French and the other in English.",
        "Entity": "Normal"
    },
    {
        "Text": "The resulting bilingual data have been sentence-aligned using statistical methods (Brown et al.",
        "Entity": "Normal"
    },
    {
        "Text": "1990).",
        "Entity": "Normal"
    },
    {
        "Text": "Originally, about three million sentences were selected.",
        "Entity": "Normal"
    },
    {
        "Text": "Here, we use a subset of the original training data; the details regarding this subset 127 Example translations for the translation direction English to German using three different reordering constraints: MON, EG, and S3.",
        "Entity": "Normal"
    },
    {
        "Text": "Input: Yeah , that wouldn t be bad.",
        "Entity": "Normal"
    },
    {
        "Text": "Do you have any ideas where I could stay?",
        "Entity": "Normal"
    },
    {
        "Text": "MON: Ja, das wa  re schade.",
        "Entity": "Normal"
    },
    {
        "Text": "Haben Sie irgendwelche Ideen wo ich ko  nnten u  bernachten?",
        "Entity": "Normal"
    },
    {
        "Text": "EG: Ja, das wa  re nicht schlecht.",
        "Entity": "Normal"
    },
    {
        "Text": "Haben Sie irgendwelche Ideen wo wir wohnen ko  nnten?",
        "Entity": "Normal"
    },
    {
        "Text": "S3: Ja, das wa  re nicht schlecht.",
        "Entity": "Normal"
    },
    {
        "Text": "Haben Sie irgendwelche Ideen wo wir wohnen ko  nnten?",
        "Entity": "Normal"
    },
    {
        "Text": "Input: Oh, that sounds great .",
        "Entity": "Normal"
    },
    {
        "Text": "Could you arrange a suite for me?",
        "Entity": "Normal"
    },
    {
        "Text": "MON: Oh, das klingt gut.",
        "Entity": "Normal"
    },
    {
        "Text": "Ko  nnten Sie unbedingt ein Suite bei mir?",
        "Entity": "Normal"
    },
    {
        "Text": "EG: Oh, das klingt gut.",
        "Entity": "Normal"
    },
    {
        "Text": "Ko  nnten Sie einen Suite ausmachen fu  r mich?",
        "Entity": "Normal"
    },
    {
        "Text": "S3: Oh, das klingt gut.",
        "Entity": "Normal"
    },
    {
        "Text": "Ko  nnten Sie mir einen Suite ausmachen?",
        "Entity": "Normal"
    },
    {
        "Text": "Input: Well, I still need your signature here and then I will check with your company.",
        "Entity": "Normal"
    },
    {
        "Text": "MON: Also, ich konnte Arbeitskraft Unterschrift hier und ich werde nachsehen mit Ihrer Firma.",
        "Entity": "Normal"
    },
    {
        "Text": "EG: Also, ich bra  uchte noch Ihre Unterschrift und dann gucke ich hier mit Ihrer Firma.",
        "Entity": "Normal"
    },
    {
        "Text": "S3: Also, ich brauche hier noch Ihre Unterschrift und dann werde ich veranlassen mit Ihrer Firma.",
        "Entity": "Normal"
    },
    {
        "Text": "Table 14 Training and test conditions for the Hansards task (*number of words without punctuation).",
        "Entity": "Normal"
    },
    {
        "Text": "French English Train: Sentences 1,470,473 W or ds 2 4 , 3 3 8 , 1 9 5 22 ,1 63 ,0 92 W or ds* 2 2 , 1 7 5 , 0 6 9 20 ,0 63 ,3 78 Voc abu lary : Siz e 1 0 0 , 2 6 9 7 8 , 3 3 2 Sin gle to ns 4 0 , 1 9 9 3 1 , 3 1 9 Test: Sentences 5,432 Words 97,646 80,559 Bigr./Tri.",
        "Entity": "Normal"
    },
    {
        "Text": "Perplexity 196.9/121.8 269.9/179.8 are given in Table 14.",
        "Entity": "Normal"
    },
    {
        "Text": "The Hansards corpus presents by far a more difficult task than the Verbmobil corpus in terms of vocabulary size and number of training sentences.",
        "Entity": "Normal"
    },
    {
        "Text": "The training and test sentences are less restrictive than for the Verbmobil task.",
        "Entity": "Normal"
    },
    {
        "Text": "For the translation experiments on the Hansards corpus, no word joining is carried out.",
        "Entity": "Normal"
    },
    {
        "Text": "Two target words can be produced by a single source word, as described in Section 3.9.2.",
        "Entity": "Normal"
    },
    {
        "Text": "4.3.2 Translation Results.",
        "Entity": "Normal"
    },
    {
        "Text": "As can be seen in Table 15 for the translation direction French to English and in Table 16 for the translation direction English to French, the word error rates are rather high compared to those for the Verbmobil task.",
        "Entity": "Normal"
    },
    {
        "Text": "The reason for the higher error rates is that, as noted in the previous section, the Hansards task is by far less restrictive than the Verbmobil task, and the vocabulary size is much 128 Computing time, WER, and PER for the translation direction French to English using the two reordering constraints MON and S3.",
        "Entity": "Normal"
    },
    {
        "Text": "An almost  full  search is carried out.",
        "Entity": "Normal"
    },
    {
        "Text": "Reordering CPU time WER PER con stra int [ s e c ] [ % ] [ % ] MO N 2 .",
        "Entity": "Normal"
    },
    {
        "Text": "5 6 5.",
        "Entity": "Normal"
    },
    {
        "Text": "5 53.",
        "Entity": "Normal"
    },
    {
        "Text": "0 S3 5 8 0 .",
        "Entity": "Normal"
    },
    {
        "Text": "0 6 4.",
        "Entity": "Normal"
    },
    {
        "Text": "9 51.",
        "Entity": "Normal"
    },
    {
        "Text": "4 Table 16 Computing time, WER, and PER for the translation direction English to French using the two reordering constraints MON and S3.",
        "Entity": "Normal"
    },
    {
        "Text": "An almost  full  search is carried out.",
        "Entity": "Normal"
    },
    {
        "Text": "Reordering CPU time WER PER con stra int [ s e c ] [ % ] [ % ] MO N 2 .",
        "Entity": "Normal"
    },
    {
        "Text": "2 6 6.",
        "Entity": "Normal"
    },
    {
        "Text": "6 56.",
        "Entity": "Normal"
    },
    {
        "Text": "3 S3 1 8 9 .",
        "Entity": "Normal"
    },
    {
        "Text": "1 6 6.",
        "Entity": "Normal"
    },
    {
        "Text": "0 54.",
        "Entity": "Normal"
    },
    {
        "Text": "4 larger.",
        "Entity": "Normal"
    },
    {
        "Text": "There is only a slight difference in performance between the MON and the S3 reordering constraints on the Hansards task.",
        "Entity": "Normal"
    },
    {
        "Text": "The computation time is also rather high compared to the Verbmobil task: For the S3 constraint, the average translation time is about 3 minutes per sentence for the translation direction English to French and about 10 minutes per sentence for the translation direction French to English.",
        "Entity": "Normal"
    },
    {
        "Text": "The following parameter setting is used for the experiment conducted here: tC = 5.0, tc = 10.0, nC = 250, and to = 12.",
        "Entity": "Normal"
    },
    {
        "Text": "(The actual parameters are chosen in informal experiments to obtain reasonable CPU times while permitting only a small number of search errors.)",
        "Entity": "Normal"
    },
    {
        "Text": "No cardinality histogram pruning is carried out.",
        "Entity": "Normal"
    },
    {
        "Text": "As for the German- to-English translation experiments, word reordering is restricted so that it may not cross punctuation boundaries.",
        "Entity": "Normal"
    },
    {
        "Text": "The resulting fragment lengths are much larger for the translation direction English to French, and still larger for the translation direction French to English, when compared to the fragment lengths for the translation direction German to English, hence the high CPU times.",
        "Entity": "Normal"
    },
    {
        "Text": "In an additional experiment for the translation direction French to English and the reordering constraint S3, we find we can speed up the translation time to about 18 seconds per sentence by using the following parameter setting: tC = 3.0, tc = 7.5, nC = 20, nc = 400, and no = 5.",
        "Entity": "Normal"
    },
    {
        "Text": "For the resulting hypotheses file, PER increases only slightly, from 51.4% to 51.6%.",
        "Entity": "Normal"
    },
    {
        "Text": "Translation examples for the translation direction French to English under the S3 reordering constraint are given in Table 17.",
        "Entity": "Normal"
    },
    {
        "Text": "The French input sentences show some preprocessing that is carried out beforehand to simplify the translation task (e.g., des is transformed into de les and l est is transformed into le est).",
        "Entity": "Normal"
    },
    {
        "Text": "The translations produced are rather approximative in some cases, although the general meaning is often preserved.",
        "Entity": "Normal"
    },
    {
        "Text": "We have presented a DP-based beam search algorithm for the IBM4 translation model.",
        "Entity": "Normal"
    },
    {
        "Text": "The approach is based on a DP solution to the TSP, and it gains efficiency by imposing constraints on the allowed word reorderings between source and target language.",
        "Entity": "Normal"
    },
    {
        "Text": "A data-driven search organization in conjunction with appropriate pruning techniques 129 Example translations for the translation direction French to English using the S3 reordering constraint.",
        "Entity": "Normal"
    },
    {
        "Text": "Input Je crois que cela donne une bonne ide  e de les principes a` retenir et de ce que devraient e  tre nos responsabilite  s. S3 I think it is a good idea of the principles and to what should be our responsibility.",
        "Entity": "Normal"
    },
    {
        "Text": "Input Je pense que, inde  pendamment de notre parti, nous trouvons tous cela inacceptable.",
        "Entity": "Normal"
    },
    {
        "Text": "S3 I think, regardless of our party, we find that unacceptable.",
        "Entity": "Normal"
    },
    {
        "Text": "Input Je ai le intention de parler surtout aujourd  hui de les nombreuses ame  liorations apporte  es a` les programmes de pensions de tous les Canadiens.",
        "Entity": "Normal"
    },
    {
        "Text": "S3 I have the intention of speaking today about the many improvements in pensions for all Canadians especially those programs.",
        "Entity": "Normal"
    },
    {
        "Text": "Input Chacun en lui - me  me est tre` s complexe et le lien entre les deux le est encore davantage de sorte que pour beaucoup la situation pre  sente est confuse.",
        "Entity": "Normal"
    },
    {
        "Text": "S3 Each in itself is very complex and the relationship between the two is more so much for the present situation is confused.",
        "Entity": "Normal"
    },
    {
        "Text": "is proposed.",
        "Entity": "Normal"
    },
    {
        "Text": "For the medium-sized Verbmobil task, a sentence can be translated in a few seconds on average, with a small number of search errors and no performance degradation as measured by the word error criterion used.",
        "Entity": "Normal"
    },
    {
        "Text": "Word reordering is parameterized using a set of four parameters, in such a way that it can easily be adopted to new translation directions.",
        "Entity": "Normal"
    },
    {
        "Text": "A finite-state control is added, and its usefulness is demonstrated for the translation direction German to English, in which the word order difference between the two languages is mainly due to the German verb group.",
        "Entity": "Normal"
    },
    {
        "Text": "Future work might aim at a tighter integration of the IBM4 model distortion probabilities and the finite-state control; the finite-state control itself may be learned from training data.",
        "Entity": "Normal"
    },
    {
        "Text": "The applicability of the algorithm applied in the experiments in this article is not restricted to the IBM translation models or to the simplified translation model used in the description of the algorithm in Section 3.",
        "Entity": "Normal"
    },
    {
        "Text": "Since the efficiency of the beam search approach is based on restrictions on the allowed coverage vectors C alone, the approach may be used for different types of translation models as well (e.g., for the multiword-based translation model proposed in Och, Tillmann, and Ney [1999]).",
        "Entity": "Normal"
    },
    {
        "Text": "On the other hand, since the decoding problem for the IBM4 translation model is provably NP-complete, as shown in Knight (1999) and Germann et al.",
        "Entity": "Normal"
    },
    {
        "Text": "(2001), wordreordering restrictions as introduced in this article are essential for obtaining an effi cient search algorithm that guarantees that a solution close to the optimal one will be found.",
        "Entity": "Normal"
    },
    {
        "Text": "Appendix: Quantification of Reordering Restrictions To quantify the reordering restrictions in Section 3.5, the four non-negative numbers numskip, widthskip, nummove , and widthmove are used (width skip corresponds to L, widthmove corresponds to R in Section 3.4; here, we use a more intuitive notation).",
        "Entity": "Normal"
    },
    {
        "Text": "Within the implementation of the DP search, the restrictions are provided to the 130 algorithm as an input parameter of the following type: S numskip widthskip M nummove widthmove The meaning of the reordering string is as follows: The two numbers following S that are separated by an underscore describe the way words may be skipped; the two numbers following M that are separated by an underscore describe the way words may be moved during word reordering.",
        "Entity": "Normal"
    },
    {
        "Text": "The first number after S and M denotes the number of positions that may be skipped or moved, respectively (e.g., for the translation direction German to English [GE in the chart below], one position may be skipped and two positions may be moved).",
        "Entity": "Normal"
    },
    {
        "Text": "The second number after S and M restricts the distance a word may be skipped or moved, respectively.",
        "Entity": "Normal"
    },
    {
        "Text": "These  width  parameters restrict the word reordering to take place within a  window  of a certain size, established by the distance between the positions lmin (C) and rmax (C) as defined in Section 3.5.",
        "Entity": "Normal"
    },
    {
        "Text": "In the notation, either the substring headed by S or that headed by M (or both) may be omitted altogether to indicate that the corresponding reordering is not allowed.",
        "Entity": "Normal"
    },
    {
        "Text": "Any numerical value in the string may be set to INF, denoting that an arbitrary number of positions may be skipped/moved or that the moving or skipping distance may be arbitrarily large.",
        "Entity": "Normal"
    },
    {
        "Text": "The following reordering strings are used in this article: Word reordering Description string E The empty string denotes the reordering restriction in which (short: MON) no reordering is allowed.",
        "Entity": "Normal"
    },
    {
        "Text": "S 01 04 M 02 10 This string describes the German-to-English word reordering.",
        "Entity": "Normal"
    },
    {
        "Text": "(short: GE) Up to one word may be skipped for at most 4 positions, and up to two words may be moved up to 10 positions.",
        "Entity": "Normal"
    },
    {
        "Text": "S 02 10 M 01 04 This string describes the English-to-German word reordering.",
        "Entity": "Normal"
    },
    {
        "Text": "(short: EG) Up to two words may be skipped for at most 10 positions and up to one word may be moved for up to 4 positions.",
        "Entity": "Normal"
    },
    {
        "Text": "S 03 INF This string describes the IBM-style word reordering (short: S3) given in Section 3.6.",
        "Entity": "Normal"
    },
    {
        "Text": "Up to three words may be skipped for an unrestricted number of positions.",
        "Entity": "Normal"
    },
    {
        "Text": "No words may be moved.",
        "Entity": "Normal"
    },
    {
        "Text": "S INF INF or These strings denote the word reordering without M INF INF restrictions.",
        "Entity": "Normal"
    },
    {
        "Text": "(short: NO) The word reordering strings can be directly used as input parameters to the DP-based search procedure to test different reordering restrictions within a single implementation.",
        "Entity": "Normal"
    },
    {
        "Text": "This work has been supported as part of the Verbmobil project (contract number 01 IV 601 A) by the German Federal.",
        "Entity": "Normal"
    },
    {
        "Text": "Ministry of Education, Science, Research and Technology and as part of the Eutrans 131 project (ESPRIT project number 30268) by the European Community.",
        "Entity": "Normal"
    },
    {
        "Text": "Some of the experiments on the Canadian Hansards task have been carried out by Nicola Ueffing using the existing implementation of the search algorithm (Och, Ueffing, and Ney [2001]).",
        "Entity": "Normal"
    },
    {
        "Text": "We would like to thank the anonymous reviewers for their detailed comments on an earlier version of this article.",
        "Entity": "Normal"
    },
    {
        "Text": "Also, we would like to thank Niyu Ge, Scott McCarley, Salim Roukos, Nicola Ueffing, and Todd Ward for their valuable remarks.",
        "Entity": "Normal"
    }
]