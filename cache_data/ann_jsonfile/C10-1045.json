[
    {
        "Code": "T1",
        "Entity": "Caption",
        "Span": [
            [
                "5148",
                "5274"
            ]
        ],
        "Text": "Table 1: Diacritized particles and pseudo-verbs that, after orthographic normalization, have the equivalent surface form 0 an.",
        "Type": "Table",
        "Num": "1"
    },
    {
        "Code": "T2",
        "Entity": "Caption",
        "Span": [
            [
                "5306",
                "5387"
            ]
        ],
        "Text": "The distinctions in the ATB are linguistically justified, but complicate parsing.",
        "Type": "Table",
        "Num": "1"
    },
    {
        "Code": "T3",
        "Entity": "Caption",
        "Span": [
            [
                "5419",
                "5485"
            ]
        ],
        "Text": "Table 8a shows that the best model recovers SBAR at only 71.0% F1.",
        "Type": "Table",
        "Num": "1"
    },
    {
        "Code": "T4",
        "Entity": "Reference",
        "Span": [
            [
                "5419",
                "5485"
            ]
        ],
        "Text": "Table 8a shows that the best model recovers SBAR at only 71.0% F1.",
        "RefType": "Direct",
        "Type": "Table",
        "Num": "8"
    },
    {
        "Code": "T5",
        "Entity": "Reference",
        "Span": [
            [
                "6594",
                "6721"
            ]
        ],
        "Text": "This is especially true in the case of quotations\u2014which are common in the ATB\u2014where (1) will follow a verb like (2) (Figure 1).",
        "RefType": "Direct",
        "Type": "Table",
        "Num": "1"
    },
    {
        "Code": "T6",
        "Entity": "Caption",
        "Span": [
            [
                "8324",
                "8466"
            ]
        ],
        "Text": "Figure 1: The Stanford parser (Klein and Manning, 2002) is unable to recover the verbal reading of the unvocalized surface form 0 an (Table 1)",
        "Type": "Figure",
        "Num": "1"
    },
    {
        "Code": "T7",
        "Entity": "Reference",
        "Span": [
            [
                "8334",
                "8467"
            ]
        ],
        "Text": "The Stanford parser (Klein and Manning, 2002) is unable to recover the verbal reading of the unvocalized surface form 0 an (Table 1).",
        "RefType": "Direct",
        "Type": "Table",
        "Num": "1"
    },
    {
        "Code": "T8",
        "Entity": "Reference",
        "Span": [
            [
                "9200",
                "9291"
            ]
        ],
        "Text": "Figure 4 shows a constituent headed by a process nominal with an embedded adjective phrase.",
        "RefType": "Direct",
        "Type": "Figure",
        "Num": "4"
    },
    {
        "Code": "T9",
        "Entity": "Reference",
        "Span": [
            [
                "5986",
                "6010"
            ]
        ],
        "Text": "Table 1 shows four words",
        "RefType": "Direct",
        "Type": "Table",
        "Num": "1"
    },
    {
        "Code": "T10",
        "Entity": "Reference",
        "Span": [
            [
                "6038",
                "6097"
            ]
        ],
        "Text": "whose unvocalized surface forms 0 an are indistinguishable.",
        "RefType": "Direct",
        "Type": "Table",
        "Num": "1"
    },
    {
        "Code": "T11",
        "Entity": "Caption",
        "Span": [
            [
                "10185",
                "10284"
            ]
        ],
        "Text": "Table 2: Frequency distribution for sentence lengths in the WSJ (sections 2\u201323) and the ATB (p1\u20133).",
        "Type": "Table",
        "Num": "2"
    },
    {
        "Code": "T12",
        "Entity": "Caption",
        "Span": [
            [
                "10910",
                "10968"
            ]
        ],
        "Text": "Table 4: Gross statistics for several different treebanks.",
        "Type": "Table",
        "Num": "4"
    },
    {
        "Code": "T13",
        "Entity": "Caption",
        "Span": [
            [
                "11234",
                "11364"
            ]
        ],
        "Text": "Table 3: Dev set frequencies for the two most significant discourse markers in Arabic are skewed toward analysis as a conjunction.",
        "Type": "Table",
        "Num": "3"
    },
    {
        "Code": "T14",
        "Entity": "Reference",
        "Span": [
            [
                "9998",
                "10082"
            ]
        ],
        "Text": "As a result, Arabic sentences are usually long relative to English, especially after",
        "RefType": "Direct",
        "Type": "Table",
        "Num": "2"
    },
    {
        "Code": "T15",
        "Entity": "Reference",
        "Span": [
            [
                "11397",
                "11420"
            ]
        ],
        "Text": "segmentation (Table 2).",
        "RefType": "Direct",
        "Type": "Table",
        "Num": "2"
    },
    {
        "Code": "T16",
        "Entity": "Reference",
        "Span": [
            [
                "11586",
                "11716"
            ]
        ],
        "Text": "But it conflates the coordinating and discourse separator functions of wa (<..4.b \ufffd \ufffd) into one analysis: conjunction(Table 3).",
        "RefType": "Direct",
        "Type": "Table",
        "Num": "3"
    },
    {
        "Code": "T17",
        "Entity": "Reference",
        "Span": [
            [
                "11932",
                "12061"
            ]
        ],
        "Text": "We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c).",
        "RefType": "Direct",
        "Type": "Table",
        "Num": "8"
    },
    {
        "Code": "T18",
        "Entity": "Reference",
        "Span": [
            [
                "12494",
                "12593"
            ]
        ],
        "Text": "We compared the ATB5 to tree- banks for Chinese (CTB6), German (Negra), and English (WSJ) (Table 4)",
        "RefType": "Direct",
        "Type": "Table",
        "Num": "4"
    },
    {
        "Code": "T19",
        "Entity": "Caption",
        "Span": [
            [
                "15036",
                "15103"
            ]
        ],
        "Text": "Table 5: Evaluation of 100 randomly sampled variation nuclei types.",
        "Type": "Table",
        "Num": "5"
    },
    {
        "Code": "T20",
        "Entity": "Reference",
        "Span": [
            [
                "16118",
                "16182"
            ]
        ],
        "Text": "Table 5 shows type- and token-level error rates for each corpus.",
        "RefType": "Direct",
        "Type": "Table",
        "Num": "5"
    },
    {
        "Code": "T21",
        "Entity": "Caption",
        "Span": [
            [
                "17449",
                "17503"
            ],
            [
                "17507",
                "17632"
            ],
            [
                "17636",
                "17770"
            ]
        ],
        "Text": "Figure 2: An ATB sample from the human evaluation. The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a). But the city name Sharm Al- Sheikh is also iDafa, hence the possibility for the incorrect annotation in (b).",
        "Type": "Figure",
        "Num": "2"
    },
    {
        "Code": "T22",
        "Entity": "Reference",
        "Span": [
            [
                "18197",
                "18412"
            ]
        ],
        "Text": "A simple lexicalized PCFG with second order Markovization gives relatively poor performance: 75.95% F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline (Table 7).",
        "RefType": "Direct",
        "Type": "Table",
        "Num": "7"
    },
    {
        "Code": "T23",
        "Entity": "Caption",
        "Span": [
            [
                "20844",
                "20943"
            ]
        ],
        "Text": "Table 6: Incremental dev set results for the manually annotated grammar (sentences of length \u2264 70).",
        "Type": "Table",
        "Num": "6"
    },
    {
        "Code": "T24",
        "Entity": "Reference",
        "Span": [
            [
                "24779",
                "24943"
            ]
        ],
        "Text": "To differentiate between the coordinating and discourse separator functions of conjunctions (Table 3), we mark each CC with the label of its right sister (splitCC).",
        "RefType": "Direct",
        "Type": "Table",
        "Num": "3"
    },
    {
        "Code": "T25",
        "Entity": "Reference",
        "Span": [
            [
                "24205",
                "24263"
            ]
        ],
        "Text": "In Table 7 we give results for several evaluation metrics.",
        "RefType": "Direct",
        "Type": "Table",
        "Num": "7"
    },
    {
        "Code": "T26",
        "Entity": "Caption",
        "Span": [
            [
                "28599",
                "28629"
            ],
            [
                "28633",
                "28678"
            ],
            [
                "28682",
                "28855"
            ],
            [
                "28859",
                "28995"
            ],
            [
                "28999",
                "29081"
            ]
        ],
        "Text": "Table 7: Test set results. Maamouri et al. (2009b) evaluated the Bikel parser using the same ATB split, but only reported dev set results with gold POS tags for sentences of length \u2264 40. The Bikel GoldPOS configuration only supplies the gold POS tags; it does not force the parser to use them. We are unaware of prior results for the Stanford parser.",
        "Type": "Table",
        "Num": "7"
    },
    {
        "Code": "T27",
        "Entity": "Caption",
        "Span": [
            [
                "29217",
                "29281"
            ],
            [
                "29285",
                "29389"
            ]
        ],
        "Text": "Figure 3: Dev set learning curves for sentence lengths \u2264 70. All three curves remain steep at the maximum training set size of 18818 trees.",
        "Type": "Figure",
        "Num": "3"
    },
    {
        "Code": "T28",
        "Entity": "Reference",
        "Span": [
            [
                "33406",
                "33491"
            ]
        ],
        "Text": "Table 8b shows that verbal nouns are the hardest pre-terminal categories to identify.",
        "RefType": "Direct",
        "Type": "Table",
        "Num": "8"
    },
    {
        "Code": "T29",
        "Entity": "Reference",
        "Span": [
            [
                "34189",
                "34298"
            ]
        ],
        "Text": "We include a list of per-category results for selected phrasal labels, POS tags, and dependencies in Table 8.",
        "RefType": "Direct",
        "Type": "Table",
        "Num": "8"
    },
    {
        "Code": "T30",
        "Entity": "Reference",
        "Span": [
            [
                "33671",
                "33850"
            ]
        ],
        "Text": "However, the learning curves in Figure 3 show that the Berkeley parser does not exceed our manual grammar by as wide a margin as has been shown for other languages (Petrov, 2009).",
        "RefType": "Direct",
        "Type": "Figure",
        "Num": "3"
    },
    {
        "Code": "T31",
        "Entity": "Reference",
        "Span": [
            [
                "34084",
                "34155"
            ]
        ],
        "Text": "In Figure 4 we show an example of variation between the parsing models.",
        "RefType": "Direct",
        "Type": "Figure",
        "Num": "4"
    },
    {
        "Code": "T32",
        "Entity": "Caption",
        "Span": [
            [
                "37968",
                "38083"
            ],
            [
                "38087",
                "38202"
            ],
            [
                "38206",
                "38325"
            ],
            [
                "38329",
                "38453"
            ],
            [
                "38457",
                "38653"
            ],
            [
                "38657",
                "38778"
            ],
            [
                "38782",
                "38866"
            ]
        ],
        "Text": "Table 8: Per category performance of the Berkeley parser on sentence lengths \u2264 70 (dev set, gold segmentation). (a) Of the high frequency phrasal categories, ADJP and SBAR are the hardest to parse. We showed in \u00a72 that lexical ambiguity explains the underperformance of these categories. (b) POS tagging accuracy is lowest for maSdar verbal nouns (VBG,VN) and adjectives (e.g., JJ). Richer tag sets have been suggested for modeling morphologically complex distinctions (Diab, 2007), but we find that linguistically rich tag sets do not help parsing. (c) Coordination ambiguity is shown in dependency scores by e.g., \u2217SSS R) and \u2217NP NP NP R). \u2217NP NP PP R) and \u2217NP NP ADJP R) are both iDafa attachment.",
        "Type": "Table",
        "Num": "8"
    },
    {
        "Code": "T33",
        "Entity": "Reference",
        "Span": [
            [
                "39739",
                "39882"
            ]
        ],
        "Text": "Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1.",
        "RefType": "Direct",
        "Type": "Table",
        "Num": "9"
    },
    {
        "Code": "T34",
        "Entity": "Caption",
        "Span": [
            [
                "40756",
                "40810"
            ]
        ],
        "Text": "Table 9: Dev set results for sentences of length \u2264 70.",
        "Type": "Table",
        "Num": "9"
    },
    {
        "Code": "T35",
        "Entity": "Reference",
        "Span": [
            [
                "41086",
                "41208"
            ]
        ],
        "Text": "In these experiments, the input lacks segmentation markers, hence the slightly different dev set baseline than in Table 6.",
        "RefType": "Direct",
        "Type": "Table",
        "Num": "6"
    }
]