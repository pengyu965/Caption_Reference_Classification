Table 3 . Committee-Based Unsupervised Learning Figure 1 : Two fragments of a hierarchy over word class distributions Table 7 : Test set results . Maamouri et al . ( 2009b ) evaluated the Bikel parser using the same ATB split , but only reported dev set results with gold POS tags for sentences of length ≤ 40 . The Bikel GoldPOS configuration only supplies the gold POS tags ; it does not force the parser to use them . We are unaware of prior results for the Stanford parser . Table 1 : Accuracy in Lexical Sample Tasks Table 7 . Features with Set Values Figure 1 : Average Precision , Recall and F1 at dif- ferent top K rule cutoff points . Table 3 : Evaluation of topic segmentation for the French corpus ( Pk and WD as percentages ) Figure 3 : Result of synthetic data learning experiment for MERT and PRO , with and without added noise . As the dimensionality increases MERT is unable to learn the original weights but PRO still performs adequately . Figure 14 : Intermediate Result . Table 8 Top 10 features of country by the Bootstrapped feature weighting . Figure 3 : Example of the Hybrid Method Figure 2 : Example training run of a pruned 1st -order model on German showing the fraction of pruned gold se- quences ( = sentences ) during training for training ( train ) and development sets ( dev ) . Figure 16 FSRA* for Arabic nominative definite nouns . Table 2 : Best results : For English , name lists are used . For German , part-of-speech tags are used Table 6 : Derivation by Means of Adding a Suffix Figure 5 Word sense disambiguation accuracy for “ NP1 V NP2 for NP3 ” frame . Table 2 : Disambiguation scores on nine confusable set , attained by the all-words prediction classifier trained on 30 million examples of TRAIN - REUTERS , and by confusable experts on the same training set . The second column displays the number of exam- ples of each confusable set in the 30-million word training set ; the list is ordered on this column . Figure 1 : An example of MOD feature extraction . An oval in the dependency tree denotes a bunsetsu . Table 1 : Properties of the manually aligned corpus Table 13 Macro-average recall , precision , and F1 on the development set and test set using the parameters that maximize F1 of the learned edges over the development set . Figure 3 : An example CCG parse obtained from [ 60 ] Table 3 : F1 scores and speed ( in sentences per sec . ) of SegTagDep on CTB-5c-1 w.r.t . the beam size . Figure 3 : Example of a word alignment and of ex- tracted alignment templates . Table 4 : Accuracy of all slots on the TST3 and TST4 test set Figure 2 : Using over 10,000 word-context features leads to overfitting , but its detrimental effects are modest . Scores on the tuning set were obtained from the 1-best output of the online learning algorithm , whereas scores on the test set were obtained using averaged weights . Figure 1 : The first sense heuristic compared with the SENSEVAL -2 English all-words task results Figure 7 Word sense disambiguation accuracy for “ NP1 V NP2 NP3 ” frame . Figure 1 : A graphical representation of the HMM ap- proach for speaker role labeling . This is a simple first order HMM . Table 3 Unknown word model features for Arabic and French . Figure 1 : Source , transformed and extracted trees given headline British soldier killed in Afghanistan Figure 1 : Four synchronous rules with topic distributions . Each sub-graph shows a rule with its topic distribution , where the X-axis means topic index and the Y-axis means the topic probability . Notably , the rule ( b ) and rule ( c ) shares the same source Chinese string , but they have different topic distributions due to the different English translations . Table 5 . Sample poster scores . Table 5 : Effect of language and error models to quality ( recall , proportion of suggestion sets containing a cor- rectly suggested word ) Table 1 : Results on MUC-4 entity extraction . C & J 2011 +granularity refers to their experiment in which they mapped one of their templates to five learned clusters rather than one . Table 4 . Examples of the top-3 candidates in the transliteration of English – Chinese Table 2 : Performance using FBIS training corpus ( top ) and NIST corpus ( bottom ) . Improvements are significant at the p < 0.05 level , except where indicated ( ns ) . Table 4 : Development Sets Results . Table 2 : F-score on development data Table 4 Most frequent phrase dependencies in DE→EN data , shown with their counts and attachment directions . Child phrases point to their parents . To focus on interesting phrase dependencies , we only show those in which one phrase has at least two tokens and neither phrase is entirely punctuation . The words forming the longest lexical dependency in each extracted phrase dependency are shown in bold ; these are used for back-off features . Table 8 : Comparison of f-scores when changing the size of labeled data . ( 1/10 , 1/4 , 1/2 and all labeled data . The size of unlabeled data is ﬁxed as 5 million characters . ) Table 5 : Weights learned for generating syntactic nodes of various types anywhere in the English translation . Table 5 Descriptive statistics for Web scores and BNC scores for other-anaphora . Table 2 Parsing performance with each POS tag set , on gold and predicted input . L AS = labeled attachment accuracy ( dependency + relation ) . U AS = unlabeled attachment accuracy ( dependency only ) . L S = relation label prediction accuracy . L AS diff = difference between labeled attachment accuracy on gold and predicted input . POS acc = POS tag prediction accuracy . Table 3 – Pk for C99 corpus Table 1 . Categories of spurious relation mentions in fp1 ( on a sample of 10 % of relation mentions ) , ranked by the percentage of the examples in each category . In the sample text , red text ( also marked with dotted underlines ) shows head words of the first arguments and the underlined text shows head words of the second arguments . Figure 2 : A partial frame learned by P RO F INDER from the MUC-4 data set , with the most probable emissions for each event and slot . Labels are assigned by the authors for readability . Figure 1 : A translation forest which is the running example throughout this paper . The reference translation is “ the gunman was killed by the police ” . ( 1 ) Solid hyperedges denote a “ reference ” derivation tree t1 which exactly yields the reference translation . ( 2 ) Replacing e3 in t1 with e4 results a competing non-reference derivation t2 , which fails to swap the order of X3,4 . ( 3 ) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally , this is done by deleting a node X0,1 . Table 6 : Search errors [ % ] . Table 9 Initial NE recognition type-insensitive ( type-sensitive ) performance across various domains . Table 2 . MAP of different IR systems with differ- ent segmenters . Figure 6 Distribution of probabilities given by the classiﬁer over all node pairs of the test-set graphs . Table 3 Performance of our method for paraphrase acquisition . Table 2 : Results of the second run ( with postprocessing ) Table 1 : A protein domain-referring phrase example Table 2 : Results of our alignment quality experiments . All timing and accuracy figures use means from five independently initial- ized runs . Note that lower is better for AER , higher is better for F0.5 . All experiments are run on a system with two Intel Xeon E5645 CPUs running at 2.4 GHz , in total 12 physical ( 24 virtual ) cores . Figure 2 : R-iv and R-oov varing as the confidence threshold , t . Table 2 : Chinese character usage in 3 corpora . The numbers in brackets indicate the percentage of characters that are shared by at least 2 corpora . Pr ( eI | f J ) = p M ( eI | f J ) Table 5 : Official BakeOff2005 results . Keys : F - Regular Tagging only , all training data are used P1 - Regular Tagging only , 90 % of training data are used P2 - Regular Tagging only , 70 % of training data are used S - Regular and Correctional Tagging , Separated Mode I - Regular and Correctional Tagging , Integrated Mode Table 8 Numbers of parse-tree nodes in the 1-best parses of the development set that triggered gender or number agreement checks , and the results of these checks . Figure 1 : An example graph modeling relations between mentions . Table 5 Most frequent sense analysis for all polysemous lemmas in the Senseval-2 and -3 test data , broken down by their frequencies of occurrence in SemCor ( adverb data is only from Senseval-2 ) . Table 4 . Gold standard length distribution . Table 1 Space comparison between FSAs and FSRAs . Figure 5 Example of segmentation of German sentence and its English translation into alignment templates . Table 6 : Post-hoc analysis on the models built by the DAC system : some of the top features with corresponding feature weights in parentheses , for each individual tagger . ( POS tags are capitalized ; BOS stands for Beginning Of Sentence ) Figure 1 : Accuracy Trends on MicroWnOp Corpus . Figure 10 : MUC-5 : Level Distribution of the Five Facts Combined Table 4 : Comparison of difference systems on the performs the state-of-the-art Collins and Duffy ’ s con- ACE RDC 2003 corpus over both 5 types ( outside the volution tree kernel . It also shows that feature-based parentheses ) and 24 subtypes ( inside the parentheses ) Table 10 : Performance of Altavista counts and BNC counts for compound interpretation ( data from Lauer 1995 ) Table 3 : Performance of WSD system over individual ab- breviations in three reduced corpora Figure 2 . Incremental alignment with TERp resulting in a confusion network . Table 4 : Individual Performance of KSs for Terrorism Figure 1 : A typed narrative chain . The four top arguments are given . The ordering O is not shown . Figure 10 : Example of parser error . Tree ( a ) is correct , and ( b ) is the wrong result by our parser . Table 12 Common values ( in percentages ) for parse tree path in PropBank data , using gold-standard parses . Figure 2 : Dependency between amount of training data for syntactic parser and quality of morphological prediction . Table 2 : Frequency distribution for sentence lengths in the WSJ ( sections 2–23 ) and the ATB ( p1–3 ) . The distinctions in the ATB are linguistically justified , but complicate parsing . Table 5 : Effectiveness of Latent Topic Extraction from Multi-Language Corpus Table 10 : English to German Final System Re- sults . Table 1 : Examples of similar syntactic structures across different relation types . The head words of the first and the second arguments are shown in italic and bold , respectively . Table 5 Models with lexical morpho-semantic features . Top : Adding all lexical features together on top of the CORE 12 baseline . Center : Adding each feature separately . Bottom : Greedily adding best features from previous part , on predicted input . Statistical significance tested only on predicted ( non-gold ) input , against the CORE 12 baseline . Table 4 : Precision , Recall , and F1-score of Engkoo , Google , and Ours with head and tail datasets Table 1 : Phrase pairs extracted from a document pair with an economic topic Figure 4 Examples of alignment templates obtained in training Table 3 : Results using 5-fold cross validation on S ENSEVAL- 1 training data ( English ) Figure 1 : Caseframe Network Examples Table 13 : An annotation example for the necessity of species information Table 3 : Mutual information between feature subset and class label with χ2 based feature ranking . Figure 1 : EM input for our example sentence . j-values follow each lexical candidate . Table 3 : POS Tagging of known words using con- textual features ( accuracy in percent ) . one-vs-all denotes training where example ` serves as positive example to the true tag and as negative example to all the other tags . SM| ¨R© denotes training where 2 example ` serves as positive example to the true tag Table 1 : English-French translation results in terms of BLEU score and TER estimated on newstest2010 with the NIST script . All means that the translation model is trained on news-commentary , Europarl , and the whole GigaWord . The rows upper quartile and median corre- spond to the use of a filtered version of the GigaWord . Figure 1 . The Lattice of the 8 Patterns . Table 4 : Hypegraph size measured by the average number of hyperedges ( h = 1 for CF ) . “ lattice ” is the average number of edges in the original CN . Fig . 7 . Example of fuzzy divisive clustering . Table 3 : Additional RE features . Table 8 : ROUGE-W in empirical approach Table 7 : Number of translations generated by each method in the final translation output of system COMB : decoder ( Orig . ) , re-decoding ( RD ) , n-gram expansion ( NE ) and confusion network ( CN ) . “ Tot. ” is the size of the dev/test set . Table 4 : Results for the systems and original headline : † and ‡ stand for significantly better than Unsupervised and Our system at 95 % confidence , respectively Table 3 : MAP ( % ) , under the ‘ 50 rules , All ’ setup , when adding component match scores to Precision ( P ) or prior- only MAP baselines , and when ranking with allCP or allCP+pr methods but ignoring that component scores . Table 2 : Statistics about the training/tuning/test datasets used in our experiments . The token counts are calculated before MADA segmentation . Table 1 : Texts used for the evaluation Figure 2 : Semantic drift in CELL ( n=20 , m=20 ) ý Figure 7 ( a ) A Chinese OAS . ( b ) Two sentences in the training set , which contain t whose OASs have been replaced with the single tokens < OAS > . ( Li et al . 2003 ) . Table 3 : N/P classifier with and without SWSD Table 6 : Rule types in SSTb and HeiST Table 6 Effect of pruning parameter Np and heuristic function on search efficiency for direct-translation model ( tp = 10 12 ) Figure 2 : Comparing F-measure , precision , and recall of different voting schemes for Chinese relation extraction . Figure 7 String-to-tree configurations ; each is associated with a feature that counts its occurrences in a derivation . Figure 7 The accuracy/speed tradeoff graph for the joint segmentor and POS-taggers and the two-stage baseline . Table 1 : a ) An example of a document from TüBa-D/Z , b ) an abbreviated entity grid representation of it , and c ) the feature vector representation of the abbreviated entity grid for transitions of length two . Mentions of the entity Frauen are underlined . nom : nominative , acc : accusative , oth : dative , oblique , and other arguments Table 3 : Statistics of the Verbmobil test corpus for German-to-English translation . Unknowns are word forms not contained in the training corpus . Figure 3 : An excerpt from SemLink Table 5 : Comparison of accuracy scores across linguistic levels . Figure 3 : The middle node gets the grey or the black class . Small numbers denote edge weights . Table 5 : The highest ranked phrasal verb candidates from our full system that do not appear in either Wiktionary set . Candidates are presented in decreasing rank ; “ pat on ” is the second highest ranked candidate . Table 2 : Illustrative tableau for a simple constraint sys- tem not capturable as a regular relation . Table 2 : Evaluation of the manual annotation improvement - summarization ratio : 30 % . Figure 2 : Precision at rank for the different sys- tems on the Athletes class . Figure 4 : Translated fragments , according to the lexicon . Table 3 : Feature sets used for learning relationships . The size of a set is the number of features in that set . Figure 1 : Illustration on temporality Table 1 Sources of conﬂict in cross-lingual subjectivity transfer . Definitions and synonyms of the fourth sense of the noun argument , the fourth sense of verb decide , and the first sense of adjective free as provided by the English and Romanian WordNets ; for Romanian we also provide the manual translation into English . Table 4 : Tagging accuracies on test data . Table 1 . Comparison with other approaches Figure 4 : Devoicing transducer compiled through a rule . Figure 1 : The Ensemble Semantics framework for information extraction . Table 1 : Relation weights ( Method 2 ) Table 6 : Final results on CTB-6 and CTB-7 Figure 5 : oscillating states in matrix CW for an unweighted graph Table 7 : Final PARSEVAL F1 scores for constituents on the test set for the predicted setting . ST Baseline denotes the best baseline ( out of 2 ) provided by the Shared Task organizers . Our submission is underlined . Table 8 Best F1-measure values for all possible combination . Table 3 : Our boosted ranker combining monolingual and bilingual features ( bottom ) compared to three base- lines ( top ) gives comparable performance to the human- curated upper bound . Table 2 : False negative matches on the Riedel ( Riedel et al. , 2010 ) and KBP dataset ( Surdeanu et al. , 2012 ) . All numbers are on bag ( pairs of entities ) level . BD* are the numbers before downsampling the negative set to 10 % and 5 % in Riedel and KBP dataset , respectively . Fig . 6 . German English BLEU scores of various al EM ( Co ) , GS ( Co ) , EM ( Co ) +GS ( Co ) , and VB ( Co ) . Table 9 Comparing selectional preference slot definitions . Figure 2 : Network after incremental TER alignment . Table 1 : The parts of taxonomic names Figure 5 : Example outputs of matching implementation of Finnish OT . Table 8 : Character tagging with deterministic constraints . Table 3 : F-measure after successive addition of each global feature group Table 1 : Conditioning features for the probabilistic CFG used in the reported empirical trials Table 4 : WCN x Baseline Table 2 : Result of our topic similarity model in terms of BLEU and speed ( words per second ) , comparing with the traditional hierarchical system ( “ Baseline ” ) and the topic-specific lexicon translation method ( “ TopicLex ” ) . “ SimSrc ” and “ SimTgt ” denote similarity by source-side and target-side rule-distribution respectively , while “ Sim+Sen ” acti- vates the two similarity and two sensitivity features . “ Avg ” is the average B LEU score on the two test sets . Scores marked in bold mean significantly ( Koehn , 2004 ) better than Baseline ( p < 0.01 ) . Table 1 : Most frequent BLC–20 semantic classes on WordNet 3.0 I+1 hLM ( eI , f J , K , zK ) = log n p ( ei | ei 2 , e ) ( 17 ) 1 1 1 1 i=1 i 1 Table 3 : Human Assessment of Errors Table 8a shows that the best model recovers SBAR at only 71.0 % F1 . Table 6 : Parameters used in our system . Figure 2 : Learning curves on the development dataset of the HK City Univ . corpus . Table 4 : Comparison of the performance of the bootstrapped SVM method from ( Zhang , 2004 ) and LP method with 100 seed labeled examples for relation type classification task . Figure 1 : Graphical model of HM-BiTAM Table 2 : The average number of source text sen- tences needed to cover a summary sentence . The model average is statistically significantly differ- ent from all the other conditions p < 10−7 ( Study 1 ) . Figure 2 : A partially scaled and inverted identity matrix Jµ . Such a matrix can be used to trans- form a vector storing a domain and value repre- sentation into one containing the same domain but a partially inverted value , such as W and ¬W de- scribed in Figure 1 . Figure 4 : Definition of the operations used to transform the structure of the underspecified logical form l0 to match the ontology O . The function type ( c ) calculates a constant c ’ s type . The function freev ( lf ) returns the set of variables that are free in lf ( not bound by a lambda term or quantifier ) . The function subexps ( lf ) generates the set of all subexpressions of the lambda calculus expression lf . Table 2 : Filters applied to candidate pair ( H , S ) Table 2 : Results of semantic classification . Table 3 : Performance comparison of two SLU systems through weakly supervised and super- vised training on the three test sets ( TER : Topic Error Rate ; SER : Slot Error Rate ) Table 1 Contingency table for the children of canine in the subject position of run . Table 5 : RM gain over other optimizers averaged over all test sets . Table 5 : Effectiveness of score propagation . Table 11 French MWE identification per category and overall results ( test set , sentences ≤ 40 words ) . MWI and MWCL do not occur in the test set . Table 2 : Parsing results of different models using manual ( gold ) segmentation . Performances significantly superior to HILDA ( with p < 7.1e-05 ) are denoted by * . Significant differences between TSP 1-1 and TSP SW ( with p < 0.01 ) are denoted by † . Figure 1 : How the IBM models model the translation process . This is a hypothetical example and not taken from any actual training or decoding logs . Table 3 : Training phase : effect of question bias ( d ) on Ave. MRR and TRDR . Table 9 : Results of the combined model for classify- ing unknown words into major and medium catego- ries : best guess Table 3 : Experimental results ( F-measure ) . Table 3 : Baseline and oracle-best 4-gram BLEU scores with 4 references for NIST Chinese-English MT datasets . Figure 2 : A sample CCG parse . K ) : J k = fj k 1 +1 , .. Fig . 4 . Examples of rules used during decoding . Table 1 : The regular expressions available in Foma from highest to lower precedence . Horizontal lines separate precedence classes . Table 1 : Confidence scores for diese in ex . ( 1 ) Figure 8 An example Chinese dependency tree . Table 5 : BLEU scores for the French-to-English translation task measured on nt10 with systems tuned on development sets selected according to their original language ( adapted tuning ) . Table 5 : POS tagging accuracy on texts without punctuation and capitalization , for tagging on the original data , the gold-standard normalization , and automatic normalizations using the first n tokens as training data Figure 2 : Modified query . Table 3 The top 10 ranked features for country produced by MI , the weighting function employed in the LIN method . Figure 1 : Probability of # of boundaries f10 ( m′ ; 3 ) . Figure 3 : A WSD example that shows the influence of syntactic , collocational and long-distance context features , the ® probability estimates used by Naïve Bayes and MM and their associated weights ( ) , and the posterior probabilities of the true sense as computed by the two models . Table 4 : Results of task # 17 Table 3 : Effect of discriminatively learned penalties for OOV words . Figure 1 : Plate diagram representation of the model . ti - s , wi -s and si -s denote the tags , words and segmentations respectively . G-s are various DP-s in the model , Ej -s and βj -s are the tag-specific emission distributions and their respective Dirichlet prior parameters . H is Gamma base distribution . S is the base distribution over segments . Coupled DP concetrations parameters have been omitted for clarity . Table 3 : Values obtained for Precision , Recall and F- score with method 1 by changing the threshold frequency of the correspondences and applying a post-filter . Figure 2 : Average sentence cover size : the average number of sentences needed to generate the case- frames in a summary sentence ( Study 1 ) . Model summaries are shown in darker bars . Peer system numbers that we focus on are in bold . Table 9 : Example patterns of nominal interaction keywords Figure 1 : The Stanford parser ( Klein and Manning , 2002 ) is unable to recover the verbal reading of the unvocalized surface form an ( Table 1 ) . Table 5 : the accuracies over the first SIGHAN bake- off data . Figure 2 : Opinion HITS model Fig . 5 . Two real translation examples , Figure 2 : Two STs composing a STN Table 3 : Distribution of reasons for false negatives ( missed argument mentions ) by BInc at K=20 . Figure 2 : Sample Minipar parse and extracted gram- matical function features Table 3 : The 10 most important features and their respective category and values for the English word “ which ” . f Table 5 : Parameters of WSMs ( Section 2 ) which , combined with particular Measures , achieved the highest average correlation in TrValD . Table 6 : Corpus characteristics for perplexity quality experiments . Table 9 French standard parsing experiments ( test set , sentences ≤ 40 words ) . FactLex uses basic POS tags predicted by the parser and morphological analyses from Morfette . FactLex* uses gold morphological analyses . Figure 2 : Clustering an 11-nodes graph with CW in two iterations Figure 3 : The parse tree for This car is not blue , highlighting the limited scope of the negation . Figure 4 : Distribution of the lexical items Table 2 : Number of extracted paraphrases . Table 9 Corpus statistics for Hansards task ( Words* : words without punctuation marks ) . Figure 1 : The Lattice for the Hebrew Phrase bclm hneim Figure 1 : Example queries for abbreviation “ BSA ” Table 1 : Feature set for the baseline pronoun res- olution system Table 2 : System Pairwise Agreement Table 3 : Size of the vocabularies for the “ No LP ” and “ With LP ” models for which we can impose constraints . Table 4 The accuracies of various word segmentors over the first SIGHAN bakeoff data . Table 4 : The effect of syntactic features when predicting morphology using lexicons . * mark statistically signifi- cantly better models compared to our baseline ( sentence- based t-test with α = 0.05 ) . Table 3 . Average precisions over the 10 corpora of different window size ( 3 seeds ) Table 2 : Possibility combination of neighboring tokens within the corpus for PER Table 2 Mechanical evaluation of translation Table 1 : Sample of extracted entailment rules . Figure 6 Order in which the German source positions are covered for the German-to-English reordering example given in Figure 5 . Figure 11 Participle-forming combinations in German . Figure 1 : Illustration of the alignment of steps . Table 1 DP-based algorithm for solving traveling-salesman problems due to Held and Karp . The outermost loop is over the cardinality of subsets of already visited cities . Figure 7 A scenario where ILP-Global makes a mistake , but ILP-Local is correct . Table 1 : Summary table on the various methods investigated for POS tagging Table 3 : Accuracy on seen and unseen tokens . Figure 12 : The Malay fst After the Application of Compile-Replace to the Lower-Side Language Table 3 : Classifier combination accuracy over 5 base classifiers : NB , BR , TBL , DL , MMVC . Best perform- ing methods are shown in bold . Table 23 : A snippet from Russian and Czech tag comparison Table 2 : Frequency of Major Relation SubTypes in the ACE training and devtest corpus . Table 4 : Results for the various experiments ( Exp ) for both the development and test portions of the data , including per- token clitic separation ( tokenization ) accuracy , part-of-speech tagging F1 , affix boundary detection F1 , affix labeling F1 , and both unlabeled and labeled attachment scores . Figure 2 : Unary rule normalization . Nonterminal-yield unary chains are collapsed to single unary rules . Identity unary rules are added to spans that have no unary rule . Table 10 Accuracy of semantic-role prediction ( in percentages ) for known boundaries ( the system is given the constituents to classify ) . Table 2 : Comparison of various groups of parsers . All percentages refer to the share of the total words in test data , attached correctly . The “ single parser ” part shows shares of the data where a single parser is the only one to know how to parse them . The sizes of the shares should correlate with the uniqueness of the individual parsers ’ strategies and with their contributions to the overall success . The “ at least ” rows give clues about what can be got by majority voting ( if the number represents over 50 % of parsers compared ) or by hypothetical oracle selection ( if the number represents 50 % of the parsers or less , an oracle would generally be needed to point to the parsers that know the correct attachment ) . eI K 1 = e1 , ek = eik 1 +1 , . Figure 2 : The whole process of re-training the upper case NER . Q signifies that the text is converted to upper case before processing . Figure 8 : Examples of the MT outputs with and without SRFs . The first and second example shows that SRFs improve the completeness and the ordering of the MT outputs respectively , the third example shows that SRFs improve both properties . The subscripts of each Chinese phrase show their aligned words in English . Table 2 : Examples of features and associated costs . Pseudofeatures are shown in boldface . Exceptional denotes a situation such as the semivowel [ j ] substituting for the affricate [ dZ ] . Substitutions between these two sounds actually occur frequently in second-language error data . Table 7 : Average feature values across best translations of sentences in the MT03 tuning set , both before MERT ( column 2 ) and after ( column 3 ) . “ Same ” versions of tree- to-tree configuration features are shown ; the rarer “ swap ” features showed a similar trend . Table 4 : Impact of feature categories . Numbers after are the standard deviations . * indicates that the result # '' is significantly ( pair-wise t-test ) different from the line above at . Table 12 SemCor results for Nouns using jcn . Figure 4 : “ Acceptance radius ” of an outlier within the training set ( left ) and a more “ normal ” training set object ( right ) Table 14 : An example result of BioAR I+1 hCLM ( eI , f J , K , zK ) = log n p ( C ( ei ) | C ( ei 4 ) , .. Figure 2 . Position of news story boundaries in a CNN news summary in relation to troughs found by the algorithm . Table 7 : Deterministic constraints for POS tagging . Figure 13 : Domain Numbers of MUC-4 , MUC-5 , MUC-6 , and MUC-7 Table 16 Methods of resolving OAs in word segmentation , on the MSR test set . Figure 3 : Smoothed histograms of the probability of the Table 5 : F1 scores of the local CRF and non-local models on the CoNLL 2003 named entity recognition dataset . We also provide the results from Bunescu and Mooney ( 2004 ) for comparison . Figure 1 : Example of semantic trees Table 3 : Reachability of 1000 training sentences : can they be translated with the model ? Table 5 : Experiments with the ILP method using a thresh- old of 1–4 ( times a word-pair is seen ) to trigger rule learn- ing . The figures in parentheses are the same results with the added postprocessing unigram filter that , given sev- eral output candidates of the standard dialect , chooses the most frequent one . Table 1 : WMT10 system combination tuning/testing data Table 6 : LL results tested against gs-swaco Figure 1 : Glue Semantics proof for ( 80 ) , Swedish Directed Motion Construction Pr ( f J | eI ) = Pr ( f J , aJ | eI ) ( 5 ) 1 1 1 1 1 J 1 Table 5 : Resolution accuracy ( % ) Table 4 : Bagging with 50 gold seed sets Table 2 : Overall accuracy of maximum entropy sys- tem using different subsets of features for Penn Chi- nese Treebank words ( manually segmented , part-of- speech-tagged , parsed ) . Table 5 : Results on the NIST MATR 2008 test set for several variations of paraphrase usage . Table 2 : Topic-specific translation lexicons are learned by a 3-topic BiTAM-1 . The third lexicon ( Topic-3 ) prefers to translate the word Korean into ChaoXian ( m : North Korean ) . The co-occurrence ( Cooc ) , IBM-1 & 4 and HMM only prefer to translate into HanGuo ( ¸I : South Korean ) . The two candidate translations may both fade out in the learned translation lexicons . Table 4 : Effects of one-to-one and one-to-many topic pro- jection . Table 2 : Distribution of dialogue acts in our dataset . Table 5 : Show type detection : Using the neural net- work described in Sec . 2 the show type was detected . If there is a number in the word column the word feature is being used . The number indicates how many word/part of speech pairs are in the vocabu- lary additionally to the parts of speech . Table 1 : Results on Chinese Semantic Similarity Figure 3 : Biography Text Evaluations . Table 3 Training , development , and test data for word segmentation on CTB5 . Table 3 : Dependency parsing : LAS scores for full and 5k training sets and for gold and predicted input . Results in bold show the best results per language and setting . Table 10 : Comparison of our approach with the state-of-art systems Table 8 . Subject and Object Agreement Features Table 1 : English MWEs and their components with their translation in Persian . Direct matches between the trans- lation of a MWE and its components are shown in bold ; partial matches are underlined . Table 1 : TreeTagger and RFTagger outputs . Starred word forms are modified during preprocessing . Figure 2 : Tagging accuracy on development data depending on context size Table 7 . Statistics of the ACE corpus . Figure 4 : string insertion operation for right-to-left decoding method . A string e0 was prepended before the partial output string , e , and the first word in e 0 was aligned from f j . Table 2 . The scored results of our CWS in the MSR_C track ( OOV is 0.034 ) for 3rd bakeoff . Table 4 : Results on the standard 14 CSSC data sets Table 9 Corpus statistics for Hansards task ( Words* : words without punctuation marks Table 1 : Some training events for the English word “ which ” . The symbol “ ” is the placeholder of the English word “ which ” in the English context . In the German part the placeholder ( “ ” ) corresponds to the word aligned to “ which ” , in the first example the German word “ die ” , the word “ das ” in the second and the word “ was ” in the third . The considered English and German contexts are separated by the double bar “ p ” .The last number in the rightmost position is the number of occurrences of the event in the whole corpus . Figure 1 . A corpus of two trees p ( z = ( FJ , EI , A ) J f ) = 1 1 | N ( C ( f ) ) ( 10 Table 2 : English Eve corpus results . Standard deviations are in parentheses ; ∗ denotes a significant difference from the M ORTAG model . Table 1 : Basic features used in the maximum entropy model . Table 1 : Features based on the token string Table 2 : The number of vocabularies in the 10k , 50k and 100k data sets . Table 2 : Experimental results using different phrase ta- bles . OutBp : the out-of-domain phrase table . AdapBp : the adapted phrase table . Figure 1 : Lookup of “ is one of ” in a reverse trie . Children of each node are sorted by vocabulary identifier so order is consistent but not alphabetical : “ is ” always appears be- fore “ are ” . Nodes are stored in column-major order . For example , nodes corresponding to these n-grams appear in this order : “ are one ” , “ < s > Australia ” , “ is one of ” , “ are one of ” , “ < s > Australia is ” , and “ Australia is one ” . Figure 1 : Pipeline architecture for dialogue act recognition and re-ranking component . Here , the input is a list of dialogue acts with confidence scores , and the output is the same list of dialogue acts but with recomputed confidence scores . A dialogue act is represented as DialogueActType ( attribute-value pairs ) . Table 2 : good # a # 15 gloss and examples . Table 4 : Accuracy with different sizes of labeled data Table 1 : Parallel Corpus . Table 4 : Results for the three ACE data sets obtained via the B-CUBED scoring program . Table 6 : WS : word-segmentation . Baseline : language-independent features . LexFeat : plus lex- ical features . Numbers are averaged over the 10 ex- periments in Figure 2 . Figure 1 : Learning curve of BLC20 on SE2 Table 14 Graph-based feature templates for the dependency parser . Figure 4 : Size of translation unit n-grams ( % ) seen in test for different n-gram models . Table 4 Performance on Internet data Figure 2 : Our alignment model , represented as a graphi- cal model . Fig . 4 . Arabic English BLEU and TER scores of various a methods : EM ( Co ) , GS ( Co ) , EM ( Co ) +GS ( Co ) , and VB ( Co ) . Table 3 : Single systems ( Basque ) in cross- validation , sorted by recall . Table 4 : Test corpora statistics . Figure 1 : Comparison of a confusion network and a lat- tice . Figure 4 : PTB vs. Wiktionary type coverage across sec- tions of the Brown corpus . Fig . 9 . Performance evaluation of proposed and existing systems . Figure 3 : Accuracy on development data depend ing on context size Table 14 : Arabic Equational Sentences Table 6 : Results for OOV-processing and MBR , English→German . Table 2 : Particles and prepositions allowed in phrasal verbs gathered from Wiktionary . Figure 1 : Sample dataset ( constructed by hand ) : Finnish verbs , with inflection for person and number . Table 3 : Average precision ( AP ) and coverage ( Cov ) results for our proposed system ES-all and the baselines . ‡ indicates AP statistical signifi- cance at the 0.95 level wrt all baselines . Fig . 2 . Schematic ﬂowchart of the workﬂow we followed , regarding the datasets , the training techniques and the operations . Table 3 : The average improvements of BLEU scores on the test08 and news08 ( out-of-domain ) when we trained the paramenters using only 400 development sentences with MERT and SVM-based algorithms four times . Figure 1 A focused entailment graph . For clarity , edges that can be inferred by transitivity are omitted . The single strongly connected component is surrounded by a dashed line . Figure 2 : Contribution of employing the dynamic cache on different test documents Figure 2 . F1-measure with  in [ 0,1 ] Figure 2 : Diffs in the course of iteration . All models were with back-off mixing ( +BM ) . Figure 6 : Metaphors tagged by the system ( in bold ) Table 1 : BLEU scores on the Europarl development test data Table 6 : Precision for each phrase type ( Ev.Ling ) . Table 5 : F1s of some individual FN role classifiers and the overall multiclassifier accuracy ( 454 roles ) . Table 6 : Average scores by cluster : baseline versus LR [ 0.20,0.95 ] . Table 2 : English word perplexity ( PPL ) on the RT04 test set using a unigram LM . Figure 4 : Number and ratio of statistically signifi- cant distinction between system performance . Au- tomatic scores are computed on a larger tested than manual scores ( 3064 sentences vs. 300–400 sen- tences ) . Figure 1 . Bootstrapping for Name Tagging Table 5 : Comparison of difference systems on the ACE RDC 2004 corpus over both 7 types ( outside the parentheses ) and 23 subtypes ( inside the parentheses ) Table 1 : Segmentation , tagging and parsing results on the Standard dev/train Split , for all Sentences Figure 1 : Structure of a term in the original documents Table 1 : Decoder performance on the June 2002 TIDES MT evluation test set with multiple searches from randomized starting points ( MSD=2 , MSSS=5 ) . Table 2 : Matches between MUC and Soderland data at field level Table 4 : Combined systems ( Basque ) in cross- validation , best recall in bold . Only vector ( f ) was used for combination . Table 3 : The results for three systems associ- ated with the project for the NP bracketing task , the shared task at CoNLL-99 . The baseline re- sults have been obtained by finding NP chunks in the text with an algorithm which selects the most frequent chunk tag associated with each part-of- speech tag . The best results at CoNLL-99 was obtained with a bottom-up memory-based learner . An improved version of that system ( MBL ) deliv- ered the best project result . The MDL results have been obtained on a different data set and therefore combination of the three systems was not feasible . Figure 14 : Correlation between manual and automatic scores for English-French A relative comparison of running a selection of regular expressions and scripts against other finite-state toolkits . Figure 2 : Examples of context- free and context-sensitive sub- trees related with Figure 1 ( b ) . Note : the bold node is the root for a sub-tree . Table 2 : All the parameters of Measures for de- termining semantic compositionality described in Section 3 used in our experiments . Table 2 : Example confounders for “ festival ” and “ laws ” and their similarities Figure 3 : Tagging part of log-likelihood plotted against V-measure Table 8 : Results on WMT-2013 ( blindtest ) Table 9 The comparison between DLG , AV , BE , and ESA . Table 13 Accuracy of semantic-role prediction for unknown boundaries ( the system must identify the correct constituents as arguments and give them the correct roles ) . Figure 3 : Learning curves in terms of word predic- tion accuracy on deciding between the confusable pair there , their , and they ’ re , by IGT REE trained on TRAIN - REUTERS , and tested on REUTERS , AL - ICE , and BROWN . The top graphs are accuracies at- tained by the confusable expert ; the bottom graphs are attained by the all-words predictor trained on TRAIN - REUTERS until 130 million examples , and on TRAIN - NYT beyond ( marked by the vertical bar ) . Table 13 : Zero-Copula in Russian Figure 1 : Growth of the Wiktionary over the last three years , showing total number of entries for all languages and for the 9 languages we consider ( left axis ) . We also show the corresponding increase in average accuracy ( right axis ) achieved by our model across the 9 languages ( see details below ) . Figure 1 : A correct tree ( tree1 ) and an incorrect tree ( tree2 ) for “ BCLM HNEIM ” , indexed by terminal boundaries . Erroneous nodes in the parse hypothesis are marked in italics . Missing nodes from the hypothesis are marked in bold . Table 3 : MUC , CEAF , and B3 coreference results using true mentions . Table 1 : Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag . Across all languages , high performance can be attained by selecting a single tag per word type . Table 5 : Translations output by system RESC2 and COMB on IWSLT task ( case-insensitive ) . Table 4 : Results of different systems for pronoun resolution on MUC-6 and MUC-7 ( *Here we only list backward feature assigner for pronominal candidates . In RealResolve-1 to RealResolve-4 , the backward features for non-pronominal candidates are all found by DTnon−pron . ) Table 1 : Summary of results in terms of the MTO and VM scores . Standard errors are given in parentheses when available . Starred entries have been reported in the review paper ( Christodoulopoulos et al. , 2010 ) . Distributional models use only the identity of the target word and its context . The models on the right incorporate orthographic and morphological features . Table 8 Results for Mutiple Document System with additional retrieved texts Table 1 : The animacy data set from Talbanken05 ; number of noun lemmas ( Types ) and tokens in each class . Figure 5 . Performance Comparison of Different Pruning Methods Figure 6 . Perplexity Comparison of Different Pruning Methods Table 3 : Senses found by our algorithm from first order cooccurrences ( LM-1 and LAT-1 ) Figure 1 : Ungrammatical Arabic output of Google Trans- late for the English input The car goes quickly . The subject should agree with the verb in both gender and number , but the verb has masculine inflection . For clarity , the Arabic tokens are arranged left-to-right . Figure 1 Naı̈ve FSA with duplicated paths . Table 2 : MRR of baseline and reinforced matrices Table 3 : Baseline performance and n-best oracle scores ( UAS/LAS ) on the development sets . mate ’ uses the prepro- cessing provided by the organizers , the other parsers use the preprocessing described in Section 2 . Figure 1 . Categories of Message Speech Act . Figure 1 : Graphical model for the Bayesian Query-Focused Summarization Model . Table 2 : New Verb Classes Figure 2 : Bayesian network : α and β are vectors of hy- perparameters , and θ i ( for i ∈ { 1 , . . . , nc } ) and φ are distributions . u is a vector of underlying forms , generated from φ , and si ( for i ∈ nu ) is a set of observed surface forms generated from the hidden variable ui according to θi Table 3 : A real translation example Table 2 : BLEU scores of English to Russian ma- chine translation system evaluated on tst2012 and tst2013 using baseline GIZA++ alignment and transliteration augmented-GIZA++ alignment and post-processed the output by transliterating OOVs . Human evaluation in WMT13 is performed on TA-GIZA++ tested on tst2013 ( marked with * ) Fig . 4 . Integration of confidence measures and interpolation – recall/precision curves . Table 7 : Results on unseen test set for models which performed best on dev set – predicted input . Figure 3 : Example of Learned Name Pairs with Gloss Translations in Parentheses Figure 1 : Plate diagram of the basic model with a single feature per token ( the observed variable f ) . M , Z , and nj are the number of word types , syntactic classes z , and features ( = tokens ) per word type , respectively . Figure 3 . The framework of our approach Table 4 : the influence of features . ( F : F-measure . Feature numbers are from Table 1 ) Figure 2 : An ATB sample from the human evaluation . The ATB annotation guidelines specify that proper nouns should be specified with a flat NP ( a ) . But the city name Sharm Al- Sheikh is also iDafa , hence the possibility for the incorrect annotation in ( b ) . Figure 5 : Example Demonstrating Advantages of Full Parsing Table 4 NER type-insensitive ( type-sensitive ) performance of different English NE recognizers . Figure 1 Japanese-to-English Display of NICT- ATR Speech-to-Speech Translation System Table 6 : Comparison of results for MUC7 Table 6 : Translation results for English-French Fig . 4 . F-measure for the objective and subjective classes for cross-lingual bootstrapping . Table 8 Effect of the length of the language model history ( Unigram/Bigram/Trigram : word-based ; CLM : class-based 5-gram ) . Figure 1 : Turning distributional similarity into a weighted inference rule Table 5 . Another example of some discovered paraphrases . Table 1 : Accuracy and error reduction ( ER ) results ( in percents ) for our model and the MF baseline . Error reduction is computed as M ODEL−M 100−M F F . Results are given for the WSJ and GENIA corpora test sets . The top table is for a model receiving gold standard parses of the test data . The bottom is for a model using ( Charniak and Johnson , 2005 ) state-of-the-art parses of the test data . In the main scenario ( left ) , instances were always mapped to VN classes , while in the OIP one ( right ) it was possible ( during both training and test ) to map instances as not belonging to any existing class . For the latter , no results are displayed for polysemous verbs , since each verb can be mapped both to ‘ other ’ and to at least one class . Figure 5 : Dendrogram of the participants cluster based on their feedback profile Table 2 : Experimental results for Japanese–English Figure 5 : Coverage of summary text caseframes in source text ( Study 3 ) . Table 7 : Accuracy and frequency of the top 5 % for each iteration Figure 1 . F1-measures with  in [ 0 3 ] Figure 8 FSRA-2 for Arabic nominative definite and indefinite nouns . Figure 4 4-tape representation for the Hebrew word htpqdut . Table 4 : Frequencies and scores for each resolution class . Figure 3 : Subgraph of Local∗1 output for “ headache ” Figure 6 : Contribution of feature sets ( causality ) . Table 2 : Sizes of the extracted datasets . Figure 1 : CCG and LTAG supertag sequences . Table 12 Results of human evaluation performed via Amazon Mechanical Turk . The percentages represent the portion of sentences for which one system had more preference judgments than the other system . If a sentence had an equal number of judgments for the two systems , it was counted in the final row ( “ neither preferred ” ) . Figure 6 : CTB 10-fold CV POS tagging accuracy using an all-at-once approach Figure 8 : Extracting sub-trees for S2 . Table 5 Effect of pruning parameter tp and heuristic function on error rate for direct-translation model ( Np = 50,000 ) . Table 10 Numeric-type compounds extracted . Table 10 Experiments with words and parts-of-speech as contextual features Figure 4 : MTO is fairly stable as long as the Z̃ constant 5.4 Morphological and orthographic features is within an order of magnitude of the real Z value . Figure 5 : Decoding algorithm using semantic role features . Sema ( c1 .role , c2 .role , t ) denotes the triggered semantic role features when combining two children states , and ex- amples can be found in Figure 3 . Table 1 : Feature set for our pronoun resolution system ( *ed feature is only for the single-candidate model while **ed feature is only for the twin-candidate mode ) Table 1 : Overall results by Vieira and Poesio Figure 2 : Feature space growing curve . The horizontal scope X [ i : j ] denotes the introduction of different tem- plates . X [ 0:5 ] : Cn ( n = −2..2 ) ; X [ 5:9 ] : Cn Cn+1 ( n = −2..1 ) ; X [ 9:10 ] : C−1 C1 ; X [ 10:15 ] : C0 Cn ( n = −2..2 ) ; X [ 15:19 ] : C0 Cn Cn+1 ( n = −2..1 ) ; X [ 19:20 ] : C0 C−1 C1 ; X [ 20:21 ] : W0 ; X [ 21:22 ] : W−1 W0 . W0 de- notes the current considering word , while W−1 denotes the word in front of W0 . All the data are collected from the training procedure on MSR corpus of SIGHAN bake- off 2 . Figure 3 : Example of feature forest representation of linear chain CRFs . Feature functions are as- signed to “ and ” nodes . Figure 7 Summaries Recall and Precision Table 1 Feature templates used for CRF in our system Figure 4 : A narrative chain and its reverse order . Figure 1 : Overlaid bilingual embeddings : English words are plotted in yellow boxes , and Chinese words in green ; reference translations to English are provided in boxes with green borders directly below the original word . Figure 2 : Comparison of word segmentation F- measure for SIGHAN bakeoff3 tasks Figure 3 : Running example of graph creation Table 1 : Feature growth rate : For N-best list i in the table , we have ( # NewFt = number of new fea- tures introduced since N-best i − 1 ) ; ( # SoFar = Total number of features defined so far ) ; and ( # Ac- tive = number of active features for N-best i ) . E.g. , we extracted 7535 new features from N-best 2 ; combined with the 3900 from N-best 1 , the total features so far is 11435 . Figure 4 : LLDA Fmeausres for 3 feature conditions Figure 2 : Precision and recall for articles . Table 1 : normalized Mutual Information values for three graphs and different iterations in % . Table 3 : Lexical variations creating new rules based on DIRT rule X face threat of Y → X at risk of Y Fig . 2 . Collecting paraphrases using a Paraphrase Recognizer . Table 18 Precision of person name recognition on the MSR test set , using Viterbi iterative training , initialized by four seed sets with different sizes . Table 5 : Results tested against gs-swaco-subjective Table 4 : Translation results for German-English Table 11 Error analysis for false positives and false negatives . Table 3 . GETARUNS pronouns collapsed at structural level Table 2 : Clustering evaluation for the experiment without Named Entities Figure 1 : Example entries for the Transfer of a Message - levels 1 and 2 classes Table 20 : Phonetic Stress in Russian : Fake Homograph Figure 1 : A Motivating Example Figure 5 : Macro-accuracy for multilingual bootstrapping ( versus cross-lingual framework ) Table 1 : Overview of the ACE 2005 data . Figure 1 . An example discussion thread Figure 4 Word sense disambiguation accuracy for “ NP1 V NP2 to NP3 ” frame . Table 3 : Comparison of performance across the five PPI corpora Table 3 Example levels of generalization for different values of α . Table 1 : Part of a sample headline cluster , with sub-clusters Figure 1 : Performance relationship between WMEB and BASILISK on Sgold UNION Table 2 : Number of extracted instances and the sample sizes ( P and N indicate positive and neg- ative annotations ) . Figure 1 : The relationship extraction system . Table 3 : the influence of agenda size . Table 2 : Optimization & Test Set Pearson Correlation Results Figure 2 : An example packed forest representing hy- potheses in Figure 1 ( a ) . Figure 2 : Automatic evaluation with 50 % of Freebase relation data held out and 50 % used in training on the 102 largest relations we use . Precision for three different feature sets ( lexical features , syntactic features , and both ) is reported at recall levels from 10 to 100,000 . At the 100,000 recall level , we classify most of the instances into three relations : 60 % as location-contains , 13 % as person-place-of-birth , and 10 % as person-nationality . Figure 2 : Comparison of Min- , Simple- , Full-and Dynamic-Expansions : More Examples Table 3 . Accuracy of 5-fold cross-validation with sta- tistics-based semantic features The test 1 : ART.Nom checks if the preceding word is a nominative article . Table 19 English NE recognition on test data after semi-supervised learning . Figure 1 : Computation of probabilities using the language model . Table 15 Size of training data set and the adaptation results on AS open . Figure 3 FSA for the pattern hit a e . Table 4 : Effect of language and error models to speed ( time in seconds per 10,000 word forms ) Figure 6 : MUC-7 : Level Distribution of Each of the Facts Table 3 : Results for different user simulations . Numbers give % reductions in keystrokes . Figure 4 : Three types of transitivity constraint violations . C2 Figure 3 : An underspecified d Table 2 : Experiment results ( as F1 scores ) where IM is identification of mentions and S - Setting . Figure 7 An example showing the generalization of the word lattice ( a ) into a slotted lattice ( b ) . The word lattice is produced by aligning seven sentences . Nodes having in-degrees > 1 occur in more than one sentence . Nodes with thick incoming edges occur in all sentences . Table 4 : Effect of two-level lexicon combination . For the baseline we used the conventional one-level full form lexicon . Table 3 : Recall for morphological hasXY ( ) descriptions Table 3 : SC classification accuracies of different methods for the ACE training set and test set . Table 3 : Comparison of raw input and constrained input . Figure 6 : Tune and test curves of five repetitions of the same Urdu-English PBMT baseline feature experiment . PRO is more stable than MERT . Table 4 : ROUGE-2 measures in EM learning Table 2 : Translation results in lower-case BLEU . CN for confusion network and CF for confusion forest with different vertical ( v ) and horizontal ( h ) Markovization order . Figure 2 : Two methods for constructing multilingual distributions over words . On the left , paths to the German word “ wunsch ” in GermaNet are shown . On the right , paths to the English word “ room ” are shown . Both English and German words are shown ; some internal nodes in GermaNet have been omitted for space ( represented by dashed lines ) . Note that different senses are denoted by different internal paths , and that internal paths are distinct from the per-language expression . Table 3 : Results for the three ACE data sets obtained via the MUC scoring program . Table 1 : Priority Order for First Person ADs Table 2 : Results of the syntactic structured fea- tures Table 6 Context model , word classes , class models , and feature functions . Table 15 Overview of the results for all Web algorithms for coreference . Figure 4 . Results for initial ranking manner . Table 2 : Rhetorical relations in RST Spanish Treebank Table 2 : Features used in the naive Bayes Classi- fier for the entity candidate : ws , ws+1 , ... , we . spi is the result of shallow parsing at wi . Fig . 9 BLEU difference curves of four context-informed models using TRIBL Figure 3 : Usefulness of syntactic information : ( black ) dash-dotted line – word boundaries only , ( red ) dashed line – POS info , and ( blue ) solid line – full parse trees . Table 17 Comparison of performance of MSRSeg : The versions that are trained using ( semi- ) supervised iterative training with different initial training sets ( Rows 1 to 8 ) versus the version that is trained on annotated corpus of 20 million words ( Row 9 ) . Table 1 Feature templates for the word segmentor . Figure 2 : A wRTG modelling Fig . 1 Table 2 – Pk for Le Monde corpus Table 1 : Results on the Arabic GALE Phase 2 system combination tuning set with four reference translations . Table 9 : Impact of the improved morphology on the qual- ity of the dependency parser for Czech and German . Table 2 . F-scores of U-DOP , UML-DOP and a supervised treebank PCFG ( ML-PCFG ) for a random 90/10 split of WSJ10 and WSJ40 . Table 7 : LO cosine sentence configuration scores Table 1 : Relation types for ACE 05 corpus Figure 3 : Notation used in this paper . The convention eIi indicates a subsequence of a length I sequence . Table 6 : Results on a truly independent test set , consisting of data harvested from Egyptian Facebook pages that are entirely distinct from the our dialectal training set . The improvements over the MSA baseline are still considerable : +2.9 BLEU points when no Facebook data is available for tuning and +2.7 with a Facebook tuning set . Fig . 3 . Algorithm for Fuzzy Agglomerative Clustering based on verbs . Figure 9 : Confusion matrix for relation labels on the RST-DT test set . Y-axis represents true and X-axis repre- sents predicted relations . The relations are Topic-Change ( T-C ) , Topic-Comment ( T-CM ) , Textual Organization ( T- O ) , Manner-Means ( M-M ) , Comparison ( CMP ) , Evaluation ( EV ) , Summary ( SU ) , Condition ( CND ) , Enablement ( EN ) , Cause ( CA ) , Temporal ( TE ) , Explanation ( EX ) , Background ( BA ) , Contrast ( CO ) , Joint ( JO ) , Same-Unit ( S-U ) , Attribu- tion ( AT ) and Elaboration ( EL ) . Table 16 Accuracy comparisons between various dependency parsers on English data . Table 2 : Examples given in the description of Task 2 . Figure 1 . PoCoS : Core Scheme , Extended Scheme and language-specific instantiations The regular expressions available in Foma from highest to lower precedence . Table 2 : Influence of the n-gram model on the perfor- mance of the statistical approach . Table 2 : Corpus statistics of the MATR MT06 corpus that was used for experimental evaluation of the proposed measures . Table 6 : Results using different parsers Table 2 : F-measure on SIGHAN bakeoff 2 . SIGHAN best : best scores SIGHAN reported on the four corpus , cited from Zhang and Clark ( 2007 ) . Fig . 6 Distances found between phrase boundaries with linked modifier words and with parent words Table 5 : Results for cascading minority-preference DAC system — DACCMP ( consult classifiers in reverse order of frequency of class ) ; “ ER ” refers to error reduction in percent over standard multiclass SVM ( Table 2 ) Figure 1 : Example of a prediction for English to French translation . s is the source sentence , h is the part of its translation that has already been typed , x∗ is what the translator wants to type , and x is the prediction . Table 2 : Translation results in terms of BLEU score and translation edit rate ( TER ) estimated on newstest2010 with the NIST scoring script . Figure 4 : Learning curves showing the effects of increas- ing the size of dialectal training data , when combined with the 150M-word MSA parallel corpus , and when used alone . Adding the MSA training data is only use- ful when the dialectal data is scarce ( 200k words ) . Table 2 : Results for GigaPairs ( all numbers in % ) ; re- sults that significantly differ from Full are marked with asterisks ( * p < 0.05 ; ** p < 0.01 ) . Figure 1 : space distribution by part-of-speech of Table 1 : The total costs for the three MTurk subtasks in- volved with the creation of our Dialectal Arabic-English parallel corpus . Figure 1 : CTB 10-fold CV word segmentation F- measure for our word segmenter Table 14 Experiments applying combinations of features in English-to-Hindi translation Table 7 : List of results in Sighan Bakeoff 2005 Table 5 : Gender detection accuracies ( % ) using a 4-gram language model for the letter sequence of the source name in Latin script . Table 3 : MRR , Precision , Recall , and F1-score Figure 4 The path of Selection . Figure 1 : Our probabilistic model : a question x is mapped to a latent logical form z , which is then evaluated with respect to a world w ( database of facts ) , producing an answer y . We represent logical forms z as labeled trees , induced automatically from ( x , y ) pairs . Table 1 : Rule type distribution of a sample of 200 rules that extracted incorrect mentions . The corre- sponding numbers of incorrect mentions extracted by the sampled rules is shown in parentheses . Pr ( f J , aJ | eI ) = p ( f J , aJ | eI ) ( 6 ) 1 1 1 1 1 1 Figure 5 Example output of our model for Chinese→English translation . The word-segmented Chinese sentence and dependency tree are inputs . Our model ’ s outputs include the English translation , phrase segmentations for each sentence ( a box surrounds each phrase ) , a one-to-one alignment between the English and Chinese phrases , and a projective dependency tree on the English phrases . Note that the Chinese dependency tree is on words whereas the English dependency tree is on phrases . Table 3 : Comparison of the effect of morphological segmentation when translating MSA web text and Dialectal Arabic web text . The morphological segmentation uniformly improves translation quality , but the improvements are more dramatic for MSA than for Dialectal Arabic when comparing similarly-sized training corpora . Figure 3 : Examples for the effect of the combined lexica . Table 1 : Basic Travel Expression Corpus Fig . 2 . Multilingual bootstrapping . Table 5 Results with Coref Rules Alone Table 3 : Summary of features used in experiments in this paper . Figure 1 : Sample pairs of similar caseframes by relation type , and the similarity score assigned to them by our distributional model . Table 3 : The sizes of error models as automata Table 11 : ILP problem size and segmentation speed . Table 17 WSD using predominant senses , training , and testing on all domain combinations ( automatically classified corpora ) . Figure 6 : From DRS to DRG : labelling . Figure 4 : Example DCS trees for utterances in which syntactic and semantic scope diverge . These trees reflect the syntactic structure , which facilitates parsing , but importantly , these trees also precisely encode the correct semantic scope . The main mechanism is using a mark relation ( E , Q , or C ) low in the tree paired with an execute relation ( Xi ) higher up at the desired semantic point . Table 3 : Plurality language families across 20 clusters . The columns indicate portion of lan- guages in the plurality family , number of lan- guages , and entropy over families . Table 1 : Intrinsic evaluation accuracy [ % ] ( development set ) for Arabic segmentation and tagging . Table 2 . Correlation between Perplexity Figure 1 : Example of semantic trees Table 7 . System performance on the reaction relation on the CHEM dataset . hAL ( eI , f J , K , zK ) = |j 1 j | ( 16 ) 1 1 1 1 k k=1 k 1 Figure 2 : Performance of Unsupervised Name Mining Table 3 : Three different vocabulary sizes used in subword- based tagging . s1 contains all the characters . s2 and s3 contains some common words . Figure 8 : Contribution of feature sets ( material ) . K hAT ( eI , f J , K , zK ) = log n p ( zk | f j k ) ( 13 ) 1 1 1 1 k=1 j k 1 +1 Figure 1 : The greedy binding problem . ( a ) The correct binding , ( b ) the greedy binding , ( c ) the result . Figure 1 : Graphical model for PLTM . Figure 3 : learning curves of the averaged and non- averaged perceptron algorithms Table 4 : Classification results with decision tree on vectors of frequency of rarest n-grams ( Method 4 ) Table 6 : BS on IWSLT 2007 task Table 3 : Coreference Resolution Performance Figure 5 A comparison between ILP-Global and Greedy-Global . Parts A1–A3 depict the incremental progress of Greedy Global for a fragment of the headache graph . Part B depicts the corresponding fragment in ILP-Global . Nodes surrounded by a bold oval shape are strongly connected components . Table 2 : F-measure ( % ) Breakdown by Mention Type : NAM ( e ) , NOM ( inal ) , PRE ( modifier ) and PRO ( noun ) . Chinese data does not have the PRE type . Figure 2 : BLEU difference of 1000 bootstrap sam- ples . 95 % confidence interval is [ .15 , .90 ] The proposed approach therefore seems to be a stable method . Table 2 : POS tagging of unknown words using contextual and lexical Features ( accuracy in per- cent ) .  is based only on contextual features , T  is based on contextual and lexical features . SM ( _ # § ) 2 denotes that § follows in the sequential model . 2 Figure 1 : The screenshot of our web-based system shows a simple quantitative analysis of the frequency of two terms in news articles over time . While in the 90s the term Friedensmission ( peace operation ) was predominant a reverse tendency can be observed since 2001 with Auslandseinsatz ( foreign intervention ) being now frequently used . Table 3 : STTS accuracies of the TnT tagger trained on the STTS tagset , the TnT tagger trained on the Tiger tagset , and our tagger trained on the Tiger tagset . Table 1 : Candidates for equivalence classes . Table 1 : Results of the first run ( without postprocessing ) Figure 3 : MUC-7 : Level Distribution of Each of the Facts Fig . 10 Dutch-to-English Learning curves ( left-hand side graphs ) and difference curves ( right-hand side graphs ) comparing the Moses baseline against four context-informed models ( PR , OE , POS±2 and Word±2 ) . These curves are plotted with scores obtained using three evaluation metrics : BLEU ( top ) , METEOR ( centre ) and TER ( bottom ) Figure 1 : Extract of a French-English sentence pair segmented into bilingual units . The original ( org ) French sentence appears at the top of the figure , just above the reordered source s and target t. The pair ( s , t ) decomposes into a sequence of L bilingual units ( tuples ) u1 , ... , uL . Each tuple ui contains a source and a target phrase : si and ti . Figure 4 : Inferred Dirichlet transition hyperparameters for bigram CLUST on three-way classification task with four latent clusters . Row gives starting state , column gives target state . Size of red blobs are proportional to magnitude of corresponding hyperparameters . Table 4 : Causes of Error for FPs Figure 3 : Macro-accuracy for cross-lingual bootstrapping Figure 1 : Finite-state cascades for five natural language problems . Figure 2 : Example context for the spelling confusion set { piece , peace } and extracted features Table 6 : Parameters of Measures ( Section 3 ) which , combined with particular WSMs , achieved the highest average correlation in TrValD . Figure 1 Architecture of the statistical translation approach based on Bayes ’ decision rule . Table 2 The similarity score features used to represent pairs of templates . The columns specify the corpus over which the similarity score was computed , the template representation , the similarity measure employed , and the feature representation ( as described in Section 4.1 ) . Figure 2 : The average results among all 7 speakers when train with different combinations of speaker specific data and other speakers ’ data are displayed . In both Constant adaptation and Reweighted adaptation models the num- ber of speaker specific data are varied from 200 , 500 , 1000 , 1500 to 2000 . In Generic model , only all other speakers ’ data are used for training data . Results for 2 and for 10 preceding POS tags as context are reported for our tagger . Figure 3 . Results obtained in the detection of zero-pronouns . Figure 6 Dependencies in the alignment template mode Figure 1 : Dependency tree for the sentence “ PROT1 contains a sequence motif binds to PROT2 . ” Figure 9 : Example error cases , with associated frequencies , illustrating system output and gold standard references . 5 % of the cases were miscellaneous or otherwise difficult to categorize . Table 5 : An example where syntactic features help to link the PRO mention Ñë ( hm ) with its antecedent , the NAM Table 5 . Templates for feedback . Table 4 : Comparison to Related Approaches Table 9 Bootstrapped weighting : top 10 common features for country–state and country–party along with their corresponding ranks in the two ( sorted ) feature vectors . ( e | f , i , j ) : p ( ei | fj , i 1 i =1 [ ( i , j ) A ] , j 1 j =1 [ ( i , j ) A ] ) ( 15 ) Table 25 Comparison of dependency accuracies between phrase-structure parsing and dependency parsing using CTB5 data . Figure 4 . Impact of Data Size ( Chinese ) Figure 2 : Distribution of domain labels of predom- inant senses for 38 polysemous words ranked using the SPORTS and FINANCE corpus . Table 5 Evaluation of Correction and Inference Mechanisms Table 11 : POS tagging error patterns . # means the error number of the corresponding pattern made by the pipeline tagging model . ↓ and ↑ mean the error number reduced or increased by the joint model . Table 1 : Summary for graphs and test datasets obtained from each seed pair Figure 2 : Glue Semantics proof for ( 83 ) , English Way Construction ( means interpretation ) Table 5 . System performance on the part-of relation on the CHEM dataset . Figure 2 : Performance of TL-comb and TL-auto as H changes . Table 5 : Results on the MUC6 formal test set . Table 1 : Results for sentence-based predicte alignment in the three benchmark settings MTC , Leagues and MSR ( all numbers in % ) ; results that significantly differ from Full are marked with asterisks ( * p < 0.05 ; ** p < 0.01 ) . Table 2 : Segmentation results of dictionary-based segmentation in closed test of Bakeoff 2005 . A “ / ” separates the results of unigram , bigram and trigram . Table 2 : Syntactic features . h and ld mark features from the head and the left-most daughter , dir is a binary fea- ture marking the direction of the head with respect to the current token . Table 1 : Cross-domain B3 ( Bagga and Baldwin , 1998 ) results for Reconcile with its general feature set . The Paired Permutation test ( Pesarin , 2001 ) was used for statistical significance testing and gray cells represent results that are not significantly different from the best result . Table 3 : Pearson ’ s r and Kendall ’ s τ ( absolute ) between adequacy and automatic evaluation measures on different levels of the MATR MT06 data . Table 9 : Comparison of our approach with using only the Gigaword corpus Table 5 : Coverage of caseframes in summaries with respect to the source text . The model aver- age is statistically significantly different from all the other conditions p < 10−8 ( Study 3 ) . Table 2 : Context-sensitive similarity scores ( in bold ) for the Y slots of four rule applications . The components of the score calculation are shown for the topics of Table 1 . For each rule application , the table shows a couple of the topic-biased scores Lint of the rule ( as in Table 1 ) , along with the topic relevance for the given context p ( t|dv , w ) , which weighs the topic-biased scores in the LinW T cal- culation . The context-insensitive Lin score is shown for comparison . Table 12 Overview of the results for all baselines for coreference . Figure 3 : Tuninig test for hyperparameter Q of structural SVM ( fixed λ=1.0 ) by increasing it . Table 9 : Performance of each individual relation type based on 5-fold cross-validation . BP ( f J , eI , A ) = f j+m , ei+n 1 1 j i : ( i , j ) A : j j j + m i i i + n ( 9 ) ( i , j ) A : j j j + m i i i + n Table 4 Relative recall evaluation . Table 7 : Russian to English machine translation system evaluated on WMT-2012 and WMT-2013 . Human evaluation in WMT13 is performed on the system trained using the original corpus with TA- GIZA++ for alignment ( marked with * ) . exp [ M m hm ( eI , f J ) ] m=1 1 1 M I J ( 3 ) Table 7 . Weight of co-occurring words Figure 2 : The conditioning structure of the hierarchical PYP with an embedded character language models . Figure 1 : Translation extraction from comparable corpora using cross-lingual WSI and WSD . Table 1 : Sentiment lexicon description Table 2 : accuracy using non-averaged and averaged perceptron . Table 6 : Number of evaluated English NEs . Table 5 : Results of the character-category association model : best 5 guesses Figure 2 – Automaton for topic shift detection Table 2 : Case frame of “ haken ( dispatch ) . ” Table 5 Comparison of frames . Table 2 : BLEU scores for GBM features . Model parameters were optimized on the Tune set . For PRO and regularized MERT , we optimized with different hyperparameters ( regularization weight , etc . ) , and retained for each experimental condition the model that worked best on Dev . The table shows the performance of these retained models . Table 1 : Coreference Definition Differences for MUC and ACE . ( GPE refers to geo-political entities . ) Table 1 : Approximate times in seconds to generate predictions of maximum word sequence length M , on a 1.2GHz processor , for the MEMD model . Table 3 . 5-fold cross-validation results . All are trained on fp1 ( except the last row showing the unchanged algorithm trained on adj for comparison ) , and tested on adj . McNemar 's test show that the improvement from +purify to +tSVM , and from +tSVM to ADJ are statistically significant ( with p < 0.05 ) . Figure 1 : Three representations of NP modifications : ( a ) the original treebank representation ; ( b ) Selective left-corner representation ; and ( c ) a flat structure that is unambiguously equivalent to ( b ) Table 5 : Incremental results for the four sieves using our dictionary on the development set . Baseline is the Stanford system without the WordNet sieves . Scores are on gold mentions . Table 2 : Precision and recall of direct dependency projection via one-to-one links alone . Table 6 : MRR performance of phonetic translit- eration for 3 corpora using unigram and bigram language models . Figure 1 : Example of a German noun phrase . First and last word agree in number , gender , and case value . Figure 1 : Histogram of token movement size ver- sus its occurrences performed by the model Neu- big on the source english data . Table 4 : Gross statistics for several different treebanks . Test set OOV rate is computed using the following splits : ATB ( Chiang et al. , 2006 ) ; CTB6 ( Huang and Harper , 2009 ) ; Ne- gra ( Dubey and Keller , 2003 ) ; English , sections 2-21 ( train ) and section 23 ( test ) . Figure 3 : Individual Classifier Properties ( cross-validation on SENSEVAL training data ) Table 1 : Number of entries in 3 corpora Table 2 : NIL expression forms based on POS attribute . Table 4 Estimation of F ( v , c ) for the verb feed . , C ( e ) ) ( 18 ) 1 1 1 1 i=1 i 1 Table 2 Sample analysis of an English sentence . Input : Do we have to reserve rooms ? . Table 1 : Language families in our data set . The Other category includes 9 language isolates and 21 language family singletons . Table 1 : Sources of conflict in cross-lingual subjectivity transfer . Definitions and synonyms of the fourth sense of the noun argument , the fourth sense of verb decide , and the first sense of adjective free as provided by the English and Romanian WordNets ; for Romanian we also provide the manual translation into English . Table 5 : Results for verbs Figure 1 : Illustration of entity-relationship graphs Table 7 : Results for OOV-processing and MBR , German→English . Table 1 : Basic Features for CRF-based Segmenter Table 1 : Feature set used in the Stage 2 classifier , and their number for the causal relation experiments . Table 4 : Different Context Window Size Setting Table 7 : Experiments 2 and 3 : Results by the num- ber of senses of a lemma , condition All , θ = 1.0 Figure 2 : The decision tree ( Nwire ) for the system using the single semantic relatedness feature Table 1 : Statistics about the results of our word sense discovery algorithm Table 2 Sample of subjective features appearing in the top 100 discriminant attributes selected with Information Gain on the 3rd fold training data at iteration 10 . The words in italics in the multilingual features represent equivalent translations in English and Romanian . Table 5 : Top 10 POS mistakes made more often by either the CTF-TM with parsing or the CTF-TM without on the ATB part 1 , 2 , and 3 development set . Table 1 : Total annotation time , portion spent se- lecting annotation type , and absolute improve- ment for rapid mode . Table 2 : Statistics of training , development and test data for NIST task . Figure 3 Word sense disambiguation accuracy for “ NP1 V NP2 NP3 ” frame . Figure 1 : POS Tagging of Unknown Word using Contextual and Lexical features in a Sequential Model . The input for capitalized classifier has 2 values and therefore 2 ways to create confusion k sets . There are at most  & F   + !  different in- puts for the suffix classifier ( 26 character + 10 digits + 5 other symbols ) , therefore suffix may k emit up to   & R   +R confusion sets . Table 6 : Results - Evaluation B Figure 2 : Average Precision and Coherence ( κ ) for each meta alternation . Correlation : r = 0.743 ( p < 0.001 ) Table 2 : Impact of role sequence information on the HMM and Maxent classifiers . The combination results of the HMM and Maxent are also provided . Table 5 : Language-dependent lexical features . A word list can be collected to encode different Table 1 : DP algorithm for statistical machine translation . Table 2 : Overall performance of the 3 systems Table 6 . Weight-measure of co-occurring words Figure 4 : Example of Morph-Related Heteroge- neous Information Network Figure 1 : Probability estimation tree for the nomi native case of nouns . Table 4 : Micro-averaged ( across the 5 folds ) RE results using gold mentions . Table 2 : Frequency distribution for sentence lengths in the WSJ ( sections 2–23 ) and the ATB ( p1–3 ) . English parsing evaluations usually report results on sentences up to length 40 . Arabic sentences of up to length 63 would need to be evaluated to account for the same fraction of the data . We propose a limit of 70 words for Arabic parsing evaluations . Table 2 : Results on wide-coverage Question Answer- ing task . CCG-Distributional ranks question/answer pairs by confidence— @ 250 means we evaluate the top 250 of these . It is not possible to give a recall figure , as the total number of correct answers in the corpus is unknown . Table 6 Results of different approach used in our experiments ( White background lines are the results we repeat Zhang‟s methods and they have some trivial difference with Table 1 . ) Table 7 Results for Mutiple Document System Figure 3 : Density of signature caseframes ( Study 2 ) . Table 2 . Accuracy of various instantiations of the system Figure 1 : Example of the non-relation Same-Unit Table 9 The speeds of joint word segmentation and POS-tagging by 10-fold cross validation . Table 2 : Evaluation of coarse-grained POS tagging on test data Table 1 IV and OOV recall in ( Zhang et al. , 2006a ) Table 13 Comparison scores for PK open and CTB open . Table 7 : Results for ODP system using various sources of DA tags Figure 3 : The deductive system for Earley ’ s genera- tion algorithm Table 4 : Examples of the three top candidates in the transliteration of English/Arabic , English/Hindi and English/Chinese . The second column is the rank . Table 3 : BLEU scores for pre-ordering experi- ments with a n-code system and the approach pro- posed by ( Neubig et al. , 2012 ) Table 13 Evaluating predominant sense information for polysemous nouns on the Senseval-2 all-words task data . Table 1 : Total corpus sizes ( in sentences ) and number of ( S ) ure and ( P ) ossible alignment links in their respective evaluation sets . Figure 3 : A decoding trace using improvement caching and tiling ( ICT ) . The search in the second and later iterations is limited to areas where a change has been applied ( marked in bold print ) — note that the number of alignment checked goes down over time . The higher number of alignments checked in the second iteration is due to the insertion of an additional word , which increases the number of possible swap and insertion operations . Decoding without ICT results in the same translation but requires 11 iterations and checks a total of 17701 alignments as opposed to 5 iterations with a total of 4464 alignments with caching . Figure 5 : Results for choosing the correct ordered chain . ( ≥ 10 ) means there were at least 10 pairs of ordered events in the chain . Table 4 : CPU time , memory usage , and uncased BLEU ( Papineni et al. , 2002 ) score for single-threaded Moses translating the same test set . We ran each lossy model twice : once with specially-tuned weights and once with weights tuned using an exact model . The difference in BLEU was minor and we report the better result . Table 2 . Precision and recall for different values of Table 3 : BasicRE gives the performance of our basic RE system on predicting fine-grained relations , obtained by performing 5-fold cross validation on only the news wire corpus of ACE-2004 . Each sub- sequent row +Hier , +Hier+relEntC , +Coref , +Wiki , and +Cluster gives the individual contribution from using each knowledge . The bottom row +ALL gives the performance improvements from adding +Hier+relEntC+Coref+Wiki+Cluster . ∼ indicates no change in score . Table 5 : Results for Positive and Negative Classes . Table 3 : Segmentation performance on words that have the same final suffix as their preceding words . The F1 scores are computed based on all boundaries within the words , but the accuracies are obtained using only the final suffixes . Figure 2 : Selected morphosyntactic categories in the OLiA Reference Model Figure 4 . All binary trees for NNS VBD JJ NNS ( Investors suffered heavy losses ) Table 2 : Factoring of global feature collections g into f . xji denotes hxi , . . . xj i in sequence x = hx1 , . . .i . Table 4 : Different Context Window Size Setting Figure 5 : Run-time in seconds for various −λ values . Figure 1 : WSI and WSD Pipeline Table 2 : Performance of Algorithms Figure 4 : Illustration of search in statistical trans- lation . Table 1 : Examples of disagreement in segmentation guidelines Table 5 . Timings from the word alignments for our SMT evaluation . The values are averaged over both alignment directions . For these experiments we used systems with 8-core Intel E5-2670 processors running at 2.6 GHz . Table 4 : Percentage of non-projective arcs recovered correctly ( number of labels in parentheses ) Figure 6 The initial frequencies of character sequences . eI = argmax { Pr ( eI | f J ) } ( 1 ) Figure 2 . Stratefied Sampling for initial seeds Figure 6 : Hinton diagram comparing most frequent tags and clusters . Table 2 : Mixed-case TER and BLEU , and lower- case METEOR scores on Arabic NIST MT05 . Figure 1 : The clausal and topological field structure of a German sentence . Notice that the subordinate clause receives its own topology . Table 3 : Precision , recall and F-scores for the two classes in MBL-experiments with a general feature space . Table 7 : Results of combining the character-category association and rule-based models : best guess Table 10 : Results for the joint segmentation , tagging , and parsing task using pipeline and joint models . Table 5 : Variation in performance , by number of sentence boundaries ( n ) , and by training corpus size . Table 3 : Sample of Gold Standard entries Table 1 : Processed Data Statistics Table 3 : Backward features used to capture the coreferential information of a candidate Table 4 : Tagging accuracies on test data . Figure 2 : Dependency representation of example ( 2 ) from Talbanken05 . Figure 2 : Segmentation and tagging of the Arabic token AîEñJ . JºJð ‘ and they will write it ’ . This token has four seg- ments with conflicting grammatical features . For example , the number feature is singular for the pronominal object and plural for the verb . Our model segments the raw to- ken , tags each segment with a morpho-syntactic class ( e.g. , “ Pron+Fem+Sg ” ) , and then scores the class sequences . Table 3 : Results for basic DAC system ( per-class feature optimization followed by maximum confidence based choice ) ; “ ER ” refers to error reduction in percent over standard multiclass SVM ( Table 2 ) Figure 3 : Structure of the out-of-vocabulary word 戽䊂 䠽吼 ‘ English People ’ . Figure 6 : Derivation with soft syntax model Table 1 : Automatic role labeling results ( % ) using the HMM and Maxent classifiers . Table 1 : Snapshot of the supersense-annotated data . The 7 article titles ( translated ) in each domain , with total counts of sentences , tokens , and supersense mentions . Overall , there are 2,219 sentences with 65,452 tokens and 23,239 mentions ( 1.3 tokens/mention on average ) . Counts exclude sentences marked as problematic and mentions marked ? . Figure 2 : Automatic Evaluations . Table 7 : Performance of Altavista counts and BNC counts for adjective ordering ( data from Malouf 2000 ) Table 10 Simplified prevalence score , evaluation on SemCor , polysemous words only . Figure 4 . Performance for different p values Table 3 : Example questions correctly answered by CCG-Distributional . Figure 6 : F-measure for the objective and subjective classes for multilingual bootstrapping ( versus cross-lingual framework ) Table 5 : Results for the unsupervised baseline and the supervised system trained on three kinds of feature sets Figure 1 Architecture of the translation approach based on a log-linear modeling approach Figure 1 : An excerpt from the graph for Italian . Three of the Italian vertices are connected to an automatically la- beled English vertex . Label propagation is used to propa- gate these tags inwards and results in tag distributions for the middle word of each Italian trigram . Table 4 : Improvement in f-score through restoring case . Table 1 Examples of two- to seven-word bilingual phrases obtained by applying the algorithm phrase-extract to the alignment of Figure 2 Table 1 : Data examined by the two systems for the ATB Figure 3 : Averaged perceptron learning curves with Non- lexical-target and Lexical-target feature templates . Figure 2 . Extracted NE pair instances and context Table 2 : Comparison with other PPI extraction systems in the AIMed corpus Table 4 : Grammatical relations from S EXTANT Figure 4 : Learning curve of SuperSense on SE3 Figure 1 : Decoding as lattice parsing , with the highest-scoring translation denoted by black lattice arcs ( others are grayed out ) and thicker blue arcs forming a dependency tree over them . Table 16 : Arabic Order-Free Structure Figure 5 : CTB 10-fold CV word segmentation F- measure using an all-at-once approach Table 3 : Gender classification performance ( % ) Figure 1 : Syntactical Variations of “ activate ” Table 11 Training , development , and test data from CTB5 for joint word segmentation and POS-tagging . Table 3 : Non-projective sentences and arcs in PDT and DDT ( NonP = non-projective ) Table 4 . Summary of results for random and first sense baselines and supersense tagger , σ is the standard error computed on the five trials results . Table 2 : Global features in the entity kernel for reranking . These features are anchored for each entity instance and adapted to entity categories . For example , the entity string ( first feature ) of the entity “ United Nations ” with entity type “ ORG ” is “ ORG United Nations ” . Figure 1 : A verse written in the BAD web application . Table 3 . F1 and accuracy of the argument classifiers and the overall multiclassifier for FrameNet semantic roles Table 11 Large-scale clustering on D1 . Figure 2 : Segmentation precision/recall relative to gold word length in training data . Table 2 . Baseline vs . Submitted Results Table 1 : Relationship types and their argument type con- straints . Table 1 : Results for 4-fold site-wise cross-validation us- ing the DP corpus Table 8 : Per category performance of the Berkeley parser on sentence lengths ≤ 70 ( dev set , gold segmentation ) . ( a ) Of the high frequency phrasal categories , ADJP and SBAR are the hardest to parse . We showed in §2 that lexical ambiguity explains the underperformance of these categories . ( b ) POS tagging accuracy is lowest for maSdar verbal nouns ( VBG , VN ) and adjectives ( e.g. , JJ ) . Richer tag sets have been suggested for modeling morphologically complex distinctions ( Diab , 2007 ) , but we find that linguistically rich tag sets do not help parsing . ( c ) Coordination ambiguity is shown in dependency scores by e.g. , ∗SSS R ) and ∗NP NP NP R ) . ∗NP NP PP R ) and ∗NP NP ADJP R ) are both iDafa attachment . Table 6 . System performance on the succession relation on the TREC-9 dataset . Figure 2 : Learning curves of systems with different features Table 2 : Features used in predicting the next parser action Table 1 : Overview of experiments applying WSMs to determine semantic compositionality of word expressions . BNC - British National Corpus , GR - grammatical relations , GNC - German newspaper corpus , TREC - TREC corpus ; SY - substitutability-based methods , CT - component-based methods , CTn - component-based methods comparing WSM neighbors of expressions and their components , CY - compositionality-based methods ; NVAP c. - noun , verb , adjective , adverb combinations , NN - noun-noun , VP - verb-particles , AN - adjective-noun , VO - verb-object , SV - subject-verb , PV - phrasal-verb , PNV - preposition-noun-verb ; dicts . - dictionaries of idioms , WN - Wordnet , MA - use of manually annotated data , S - Spearman correlation , PC - Pearson correlation , CR - Spearman and Kendall correlations , APD - average point difference , CL - classification , P/R - Precision/Recall , P/Rc - Precision/Recall curves , Fm - F measure , R2 - goodness . Table 5 : Average accuracy of three procedures with various settings over 4 datasets . Figure 2 : AER comparison ( cn →en ) Figure 1 : Graphical depiction of our model and summary of latent variables and parameters . The type-level tag assignments T generate features associated with word types W . The tag assignments constrain the HMM emission parameters θ . The tokens w are generated by token-level tags t from an HMM parameterized by the lexicon structure . The hyperparameters α and β represent the concentration parameters of the token- and type-level components of the model respectively . They are set to fixed constants . Table 3 : Accuracies for models with and without oracle pruning . * indicates models significantly worse than the oracle model . Figure 1 : An underspecified discourse structure and its five configurations Table 2 DP-based algorithm for statistical MT that consecutively processes subsets C of source sentence positions of increasing cardinality . Figure 1 : Evolution of τA means relative to the length of the n-best sequence Figure 1 : AER comparison ( en→cn ) Figure 2 : A multiword expression in HeiST Figure 4 : Examples for the effect of equivalence classes resulting from dropping morpho-syntactic tags not relevant for translation . First the translation using the original representation , then the new representation , its reduced form and the resulting translation . Table 1 : Filtered 5-gram dataset statistics . Figure 3 : Results with varying sizes of training data . Year 2003 is not explicitly shown because it has an unusually small number of documents compared to other years . Table 7 : Fact vs. Statistical Cross-Doc Features Figure 1 . Upper triangle of the sentence-similarity matrix . Table 2 Statistics of paraphrase pairs retrieved from MSRPC . Table 1 : Features based on the token string that are based on the probability of each name class during training . Table 2 : Accuracy scores for the CoNLL 2009 shared task test sets . Rows 1–2 : Top performing systems in the shared CoNLL Shared Task 2009 ; Gesmundo et al . ( 2009 ) was placed first in the shared task ; for Bohnet ( 2010 ) , we include the updated scores later reported due to some improvements of the parser . Rows 3–4 : Baseline ( k = 1 ) and best settings for k and α on development set . Rows 5–6 : Wider beam ( b1 = 80 ) and added graph features ( G ) and cluster features ( C ) . Second beam parameter b2 fixed at 4 in all cases . Figure 1 Relation between number of classes and alternations . Table 1 : Adapting a parser to a new annotation style . We learn to parse in a “ target ” style ( wide column label ) given some number ( narrow column label ) of supervised target-style training sentences . As a font of additional features , all training and test sentences have already been augmented with parses in some “ source ” style ( row label ) : either gold-standard parses ( an oracle experiment ) or else the output of a parser trained on 18k source trees ( more realistic ) . If we have 0 training sentences , we simply output the source-style parse . But with 10 or 100 target-style training sentences , each off-diagonal block learns to adapt , mostly closing the gap with the diagonal block in the same column . In the diagonal blocks , source and target styles match , and the QG parser degrades performance when acting as a “ stacked ” parser . Figure 1 : Examples of the semantic role features Table 4 : Sentence error rates of end-to-end evalua- tion ( speech recognizer with WER=25 % ; corpus of 5069 and 4136 dialogue turns for translation Ger- man to English and English to German , respec- tively ) . Figure 5 : MUC-7 : Level Distribution of Each of the Facts Table 7 : Non-anaphoric DNP examples Table 3 . Rank of correct translation for period Dec 01 – Dec 15 and Dec 16 – Dec 31 . ‘ Cont . rank ’ is the context rank , ‘ Trans . Rank ’ is the transliteration rank . ‘ NA ’ means the word can not be transliterated . ‘ insuff ’ means the correct translation appears less than 10 times in the English part of the comparable corpus . ‘ comm ’ means the correct translation is a word appearing in the dictionary we used or is a stop word . ‘ phrase ’ means the correct translation contains multiple English words . Figure 1 : The value of the penalized loss based on the number of iterations : DPLVMs vs. CRFs on the MSR data . Figure 1 : Example candidate space of dimensionality 2 . Note : I = { 1 , 2 } , J ( 1 ) = J ( 2 ) = { 1 , 2 , 3 } . We also show a local scoring function hw ( i , j ) ( where w = [ −2 , 1 ] ) and a local gold scoring function g ( i , j ) . Table 1 . Known topic changes found in 90 generated texts using a block size of six . Figure 1 : Plate diagram of our model . Table 4 : Performance of different relation types and major subtypes in the test data Table 3 : Spanish Ornat corpus results . Standard devia- tions are in parentheses ; ∗ denotes a significant difference from the M ORTAG model . Figure 1 . Corpus Excerpt with Dialogue Act Annotation Table 5 : BS on IWSLT 2006 task Table 1 : Regular expression notation in foma . Table 5 . Extracted UW and noun set Figure 3 : The search graph on development set of IWSLT task Table 2 : Statistics for various corpora utilized in exper- iments . See Section 5 . The English data comes from the WSJ portion of the Penn Treebank and the other lan- guages from the training set of the CoNLL-X multilin- gual dependency parsing shared task . Fig . 1 . The polarity classification ( positive and negative ) based on product aspect framework Figure 6 : Opinion PageRank Performance with varying parameter µ ( λ = 0.2 ) Table 1 : Morph features of frequent words and rare words as computed from the WSJ Corpus of Penn Treebank . Table 3 : Meta-evaluation results at document and system level for submitted metrics Table 5 : Number of affected words by OOV- preprocessing Table 3 : Eq . 9 : Log-likelihood . Eq . 10 : Pseudolikelihood . In both cases we maximize w.r.t . θ. Eqs . 11–13 : Recursive DP equations for summing over t and a . Table 7 Effect of pruning parameter Np and heuristic function on error rate for direct-translation model ( tp = 10−12 ) . Figure 1 : Example of a long jump alignment grid . All possible deletion , insertion , identity and substitution op- erations are depicted . Only long jump edges from the best path are drawn . Figure 2 : Lexical Caseframe Expectations Table 2 : Accuracy for the 14-class task Figure 6 : Performance analysis of HRGs , CWU , CWW & HAC for different parameter combinations ( Table 2 ) . ( A ) All combinations of p1 , p2 and p3 = 0.05 . ( B ) All combinations of p1 , p2 and p3 = 0.09 . Table 3 : Number of unique entries in training and test sets , categorized by semantic attributes Table 1 : Empirical results for the baseline models as well as BAYE S UM , when all query fields are used . Table 5 : Effect on % BLEU of varying number of non-terminals Figure 1 Split constituents : In this case , a single semantic role label points to multiple nodes in the original treebank tree . Table 2 Data similarity measures . Figure 6 : Model accuracy across the Brown cor- pus sections . ST : Stanford tagger , Wik : Wiktionary- tag-set-trained SHMM-ME , PTBD : PTB-tag-set-trained SHMM-ME , PTB : Supervised SHMM-ME . Wik outper- forms PTB and PTBD overall . Table 2 : Feature templates used in R-phase . Ex- ample used is “ 32 ddd ” . Table 4 : Complexity Analysis of Algorithm 1 . Figure 5 . Bootstrapping time for different p values Table 10 . MRRs for the phonetic transliteration 2 Table 2 : Scores for MSRA corpus Figure 5 Example of segmentation of German sentence and its English translation into alignment templates Figure 11 The four types of changes . Figure 3 : Morphological Analysis/Generation as a Relation between Analyses and Words Table 6 Evaluation of Feature and Their Combinations Table 2 : Under-sampled system for the task of rela- tion detection . The proportion of positive examples in the training and test corpus is 50.0 % and 20.6 % respectively . Figure 9 : Chunk - Length and count of glue rules used decoding test set Figure 3 : extracts from the Akkadian project Table 1 The best two performing systems of each type ( according to fine-grained recall ) in Senseval-2 and -3 . Table 9 . MRRs of the phonetic transliteration Table 10 . Speciﬁc Subject and Object Agreement Rules Figure 2 . The procedure of TBL entity track- ing/coreference model Table 7 : Parser performance on WSJ ; 23 , supervised adaptation . All models use Brown ; T , H as the out-of-domain treebank . Baseline models are built from the fractions of WSJ ; 2-21 , with no out-of-domain treebank . Table 2 : Experimental Results . C5.0 is supervised accuracy ; Base is on random clusters . set ; Ling is manually selected subset ; Seed is seed-verb-selected set . See text for further description . Table 3 : Results of the mapping algorithm . hWRD ( eI , f J , K , zK ) = log n p ( ei | { fj | ( i , j ) A } , Ei ) ( 14 ) 1 1 1 1 i=1 Table 10 Translation results on the Hansards task Figure 4 : Relation phrase compliance with semantic/lexical constraints [ 32 ] Table 3 : Results of MET2 under different configurations Figure 2 : The response of the rhyme search engine . Table 7 : Simple parser vs full parser – syntactic quality . Trained on first 5,000 sentences of the training set . Figure 1 : Three narrative events and the six most likely events to include in the same chain . Table 2 : Rule evaluation examples and their judgment . Table 3 : The 10 best languages for the verb component of BANNARD using LCS . Table 2 Accuracy ( % ) of ‘ obscure ’ name recognition Table 8 Comparing feature descriptions . Table 4 : Accuracies ( % ) for Coarse and Fine-Grained PSD , Using MALT and Heuristics . Sorted by preposition . Figure 10 : Evaluation of translation from English on out-of-domain test data Table 1 : The set of types and subtypes of relations used in the 2004 ACE evaluation . Table 1 : Results on Penn ( English ) Treebank , Wall Street Journal , sentences with 100 words or fewer . Figure 5 : Derivation with Hierarchical model # name tokens/ # all tokens ( % ) Figure 3 : Word alignment gains according to the percentage of name words in each sentence . Table 2 : Results obtained by adding different types of features incrementally to the Baseline system . Table 7 : Resolution accuracies for the ACE test set . Figure 5 : BLEU score for those 25 % utterances which resulted in different translations after bLSA adaptation ( manual transcriptions ) Table 12 The set of new features . The last two columns denote the number and percentage of examples for which the value of the feature is non-zero in examples generated from the 23 gold-standard graphs . Figure 5 : Examples of viterbi chunking and chunk alignment for English-to-Japanese translation model . Chunks are bracketed and the words with ∗ to the left are head words . Figure 5 : New generated hypotheses through n- gram expansion and one reference . Table 5 : Impact of the topic cache size Table 2 : Comparison of dynamic context-sensitive tree span with SPT using our context-sensitive convolution tree kernel on the major relation types of the ACE RDC 2003 ( inside the parentheses ) and 2004 ( outside the parentheses ) corpora . 18 % of positive instances in the ACE RDC 2003 test data belong to the predicate-linked category . Figure 2 : Shallow parsing : chunking ( Extracted from : http : //kontext.fraunhofer.de ) Table 1 : Comparison of different context-sensitive Figure 2 . Modified Viterbi search – stop-word treatment Table 4 Results when tuning for performance over the development set . Table 8 : Synonyms for chain Table 6 : Weights learned for word-context features , which fire when English word e is generated aligned to Chinese word f , with Chinese word f−1 to the left or f+1 to the right . Glosses for Chinese words are not part of features . Figure 4 : General Knowledge Sources Figure 1 : Semantic expansion example . Note that the expanded queries that were generated in the first two retrieved texts ( listed under ‘ matched query ’ ) do not contain the original query . Table 3 : Dev set frequencies for the two most significant discourse markers in Arabic are skewed toward analysis as a conjunction . Table 4 : Parser performance on WSJ ; 23 , baselines . Note that the Gildea results are for sentences ≤ 40 words in length . All others include all sentences . Table 5 : Oﬃcial results for the English and Basque lexical tasks ( recall ) . Figure 3 : Screenshot of ConAno Table 6 : F1 scores of the local CRF and non-local models on the CMU Seminar Announcements dataset . We also provide the results from Sutton and McCallum ( 2004 ) for comparison . Table 5 : Results of the fill-in-the-blank exercise Table 2 : DP corpus comparison for OPUS features based on frequent vs. domain-relevant verbs Table 2 Examples of positive and negative words . Table 1 : Features used by paraphrase classifier . Table 2 : Results for the pronoun resolution Table 5 : Topics with MWEs Figure 1 : The combined sequence and parse tree representation of the relation instance “ leader of a minority government. ” The non-essential nodes for “ a ” and for “ minority ” are removed based on the algorithm from Qian et al . ( 2008 ) . Figure 4 : BLEU scores as a function of development data size . Table 10 Alternatives to training on gold-only feature values . Top : Select MaltParser CORE 12+ . . . models re-trained on predicted or gold + predicted feature values . Bottom : Similar models to the top half , with the Easy-First Parser . Statistical significance tested only for CORE 12+ . . . models on predicted input : significance of the MaltParser models from the MaltParser CORE 12 baseline model , and significance of the Easy-First Parser models from the Easy-First Parser CORE 12 baseline . Table 1 : Datasets for the two experimental conditions . Figure 3 : How to get the ordered set B t ( i , j , θ ) Table 2 : Active sparse feature templates Table 1 : Training and test set sources , genres , sizes in terms of numbers of tokens , and unigram and bi- gram coverage ( % ) of the training set on the test sets . Figure 2 : Word alignment based translation model P ( J , A|E ) ( IBM Model 4 ) Table 4 : Evaluation of topic segmentation for the English corpus ( Pk and WD as percentages ) Table 6 : Summary of supersense tagging accuracies Table 12 Accuracy comparisons between various joint segmentors and POS-taggers on CTB5 . Figure 6 : Results on the FQ dataset . Table 4 : Processing steps for the input sentence dire warnings from pentagon over potential defence cuts . Figure 1 : Bell tree representation for three mentions : numbers in [ ] denote a partial entity . In-focus entities are marked on the solid arrows , and active mentions are marked by * . Solid arrows signify that a mention is linked with an in-focus partial entity while dashed arrows indicate starting of a new entity . Table 4 : Dependency Parsing : MWE results Table 2 : Results for different predictor configura- tions . Numbers give % reductions in keystrokes . Table 5 : The contribution of MMVC in a rank-based classi- fier combination on S ENSEVAL -1 and S ENSEVAL -2 English as computed by 5-fold cross validation over training data Table 8 . Comparisons among different strategies on Medstract Figure 1 : ( a ) An undirected graph G representing the similarity matrix ; ( b ) The bipartite graph showing three clusters on G ; ( c ) The induced clusters U ; ( d ) The new graph G1 over clusters U ; ( e ) The new bipartite graph over G1 Table 1 : Test verbs and their monosemous/polysemic gold standard senses Table 1 : Meta-evaluation results at document level Table 3 : BLEU scores for SparseHRM features . Notes in Table 2 also apply here . Table 2 : Distribution of SCs in the ACE corpus . Figure 1 Varying the number of clusters ( evaluation : Randadj ) . Figure 1 : Three kinds of tree kernels . Table 6 : Comparison of the existing efforts on ACE RDC task . Table 6 : Rules for simplifying the morphological complexity for RU . Table 4 : Rhetorical pattern of C-Colon Table 9 Overview of the results for the best algorithms for other-anaphora . Table 1 : Supersense evaluation results . Values are the percentage of correctly assigned supersenses . k indicates the number of nearest neighbours considered . Figure 3 : Size ( in words ) of reorderings ( % ) ob- served in training bi-texts . Table 2 : Performance of the mention detection sys- tem using lexical , syntactic , gazetteer features as well as features obtained by running other named-entity classifiers Table 2 : Few examples of the untranslatable tokens in forum posts Table 1 : Results of different systems on the CoNLL ’ 12 English data sets . Figure 2 : Word prediction speed , in terms of the number of classified test examples per second , mea- sured on the three test sets , with increasing training examples . Both axes have a logarithmic scale . Table 8 : The Effects of Temporal Constraint Table 1 : Statistical Information of Corpora Table 1 : Sense-tagged corpus for the example in Figure 3 Table 21 MSRSeg system results for the MSR test set . Table 1 : Corpus of complex news stories . Table 4 . Performance with SVM trained on a fraction of adj . It shows 5 fold cross validation results . Table 6 : Type-level Results : Each cell report the type- level accuracy computed against the most frequent tag of each word type . The state-to-tag mapping is obtained from the best hyperparameter setting for 1-1 mapping shown in Table 3 . Table 11 : German-English Official Test Submis- sion . Figure 1 : Framework for MWE acquisition from corpora Fig . 2 . Procedure to mine key lexicons for each semantic type igure 2 Example of a ( symmetrized ) word alignment ( Verbmobil task ) . Table 4 : Topics are meaningful within languages but di- Table 5 : Performance of our proposed method ( Spectral- based clustering ) compared with other unsupervised methods : ( ( Hasegawa et al. , 2004 ) ) ’ s clustering method and K-means clustering . Table 1 : The increase in performance for successive variants of Bayes and Mixture Model as evaluated by 5-fold cross vali- dation on S ENSEVAL -2 English data Table 5 : Translation results for English-German Table 3 . System performance on the is-a relation on the CHEM dataset . Figure 1 : A portion of the local co-occurrence graph for “ mouse ” from the SemEval-2010 Task 14 corpus Table 2 : Segmentation results on different languages . Results are calculated based on word types . For each language we report precision , recall and F1 measure , number of word types in the corpus and number of word types with gold standard segmentation available . For each language we report the segmentation result without and with emission likelihood scaling ( without LLS and with LLS respectively ) . Table 3 : The values of AP , Spearman ( ρ ) and Kendall ( τ ) correlations between the LSA-based and PMI-based model respectively and the Gold data with regards to the expression type . Every zero value in the table corresponds to the theoretically achieved mean value of correlation calculated from the infinite number of correlation values between the ranking of scores assigned by the annotators and the rankings of scores being obtained by a random number genarator . Reddy-WSM stands for the best performing WSM in the DISCO task ( Reddy et al. , 2011b ) . StatMix stands for the best performing system based upon association measures ( Chakraborty et al. , 2011 ) . Only ρ-All and τ -All are available for the models explored by Reddy et al . ( 2011b ) and Chakraborty et al . ( 2011 ) . Table 4 : Contribution of individual features to overall performance . Figure 8 Algorithm min-jumps to compute the minimum number of needed jumps D ( cJ , j ) to complete the translation Figure 1 : GC examples . Figure 1 : Segmentation recall relative to gold word frequency . Table 3 Estimation of F ( c , f , v ) and F ( v , c ) . Figure 2 : User Interface with Arabic Script Dis- play in Java . Mouse clicks on the virtual keyboard or key presses on the physical keyboard are inter- cepted , converted to Arabic Unicode characters , and stored in a buffer , which has a start and an end but no inherent ordering . The Arabic Canvas Object observes the buffer and contains an Ara- bic Scribe object that renders the string of Uni- code characters right-to-left as connected Arabic glyphs . Table 2 : Sources of Dictionaries Figure 2 : The solid line shows recall-at-1220 when com- bining the k best-performing bilingual statistics and three monolingual statistics . The dotted line shows the indi- vidual performance of the kth best-performing bilingual statistic , when applied in isolation to rank candidates . Table 2 : Synthetic Data Set from Xinhua News Table 4 : Density of signature caseframes after merging to various threshold for the initial ( Init . ) and update ( Up . ) summarization tasks ( Study 2 ) . Table 1 : Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt . Table 1 : Performance of our system versus a baseline Table 4 : Translation results for English→French Table 1 : Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt . = argmax M 1 S s=1 ) log p M ( es | fs ) ( 4 ) Table 5 : Final results on CTB-5j Table 4 . Comparison of semi-supervised relation classification systems on the ACE RDC 2003 corpus Table 1 : Kendall ’ s ( τ ) correlation over WMT 2013 ( all- en ) , for the full dataset and also the subset of the data containing a noun compound in both the reference and the MT output Figure 7 Algorithm for breadth-first search with pruning . Figure 1 : The density of the F1 -scores with the three approaches . The prior used is a symmetric Dirichlet with α = 0.1 . Figure 8 Quasi-synchronous tree-to-tree configurations from Smith and Eisner ( 2006 ) . There are additional configurations involving NULL alignments and an “ other ” category for those that do not fit into any of the named categories . Table 6 : Performance comparison with the literature for context sensitive spelling correction Figure 5 . Results for webpage snippet number . 7.3 Experiment on Multiple Feature Fusion To verify the effectiveness for multiple feature fusion , the test on the feature combination for OOV term translation is implemented . As shown in Table 1 , the highest accuracy ( the percentage of the correct translations in all the extracted translations ) of 83.1367 % can be ac- Table 4 : F1s of some individual ILC classifiers and the overall multiclassifier accuracy ( 180 classes on PB and 133 on FN ) . Figure 3 : Time to read and accept or reject proposals versus their length Table 2 : Misspellings of receive Figure 12 Interdigitation FSRA – general . Figure 3 : Word type coverage by normalized frequency : words are grouped by word count / highest word count ratio : low [ 0 , 0.01 ) , medium [ 0.01 , 0.1 ) , high [ 0.1 , 1 ] . Figure 8 Results of 70 high-frequency two-character CASs . ‘ Voting ’ indicates the accuracy of the baseline method that always chooses the more frequent case of a given CAS . ‘ ME ’ indicates the accuracy of the maximum-entropy classifier . ‘ VSM ’ indicates the accuracy of the method of using VSM for disambiguation . Table 8 : Translation quality . Table 1 . Usefulness evaluation result Table 3 : Lexical features . Top part : Adding each feature separately ; difference from CORE 12 ( predicted ) . Bottom part : Greedily adding best features from previous part . Table 8 : Per category performance of the Berkeley parser on sentence lengths ≤ 70 ( dev set , gold segmentation ) . ( a ) Of the high frequency phrasal categories , ADJP and SBAR are the hardest to parse . We showed in §2 that lexical ambiguity explains the underperformance of these categories . ( b ) POS tagging accuracy is lowest for maSdar verbal nouns ( VBG , VN ) and adjectives ( e.g. , JJ ) . Richer tag sets have been suggested for modeling morphologically complex distinctions ( Diab , 2007 ) , but we find that linguistically rich tag sets do not help parsing . ( c ) Coordination ambiguity is shown in dependency scores by e.g. , S S S R and NP NP NP R . NP NP PP R and NP NP ADJP R are both iDafa attachment . Table 5 : Precision , Recall and F1 Results ( % ) for Coarse-Grained Classification . Comparison to O ’ Hara and Wiebe ( 2009 ) . Classes ordered by frequency Table 1 : Stemmed results on 3,138-utterance test set . Asterisked results are significantly better than the baseline ( p ≤ 0.05 ) using 1,000 iterations of paired bootstrap re-sampling ( Koehn , 2004 ) . Table 5 . Error distribution of major types on both the 2003 and 2004 data for the compos- ite kernel by polynomial expansion Table 6 . Examples of transformation rules of Table 3 : Weights learned for inserting target English words with rules that lack Chinese words . Figure 2 : Accuracy by sentence length for Method 5 measured on separate grammatical and ungrammat- ical data : Gr = Grammatical , AG = Agreement , RW = Real-Word , EW = Extra Word , MW = Missing Word Table 2 Comparison of the news and reports corpora . Figure 4 : Weather Text Evaluations . Table 8 Models with functional features : GENDER , NUMBER , rationality ( RAT ) . F N * = functional feature ( s ) based on Alkuhlani and Habash ( 2011 ) ; GN = GENDER + NUMBER ; GNR = GENDER + NUMBER + RAT . Statistical significance tested only for CORE 12+ . . . models on predicted input , against the CORE 12 baseline . Table 1 : Performance of the mention detection sys- tem using lexical features only . Table 4 : TV show types : The distribution of show types in a large database of TV shows ( 1067 shows ) that has been recorded over the period of a couple of months until April 2000 in Pittsburgh , PA Figure 2 : Tradeoffs of precision and recall values in the experiments with method 1 using various different pa- rameters . When the unigram filter is applied the precision is much better , but the recall drops . Table 2 : Combined systems ( English ) in cross- validation , best recall in bold . Figure 1 : Re-ordering for the German verbgroup . Table 24 Accuracies of our phrase-structure parser on CTB5 using gold-standard and automatically assigned POS-tags . Table 1 . Data sets used for our alignment quality experiments . The total number of sentences in the respective corpora are given along with the number of sentences and gold-standard ( S ) ure and ( P ) ossible alignment links in the corresponding test set . Table 11 Corpus statistics for Chinese–English corpora—large data track ( Words* : words without punctuation marks ) . Table 15 Training the MaltParser on gold tags , accuracy by gold attachment type ( selected ) : subject , object , modification ( of a verb or a noun ) by a noun , modification ( of a verb or a noun ) by a preposition , idafa , and overall results ( repeated ) . Table 3 : The effect of syntactic features when predicting morphological information . * mark statistically signifi- cantly better models compared to our baseline ( sentence- based t-test with α = 0.05 ) . Table 1 : Recall ( R ) , Precision ( P ) and Mean Average Pre- cision ( MAP ) when only matching template hypotheses directly . Figure 6 : Weather Sentence Evaluations . Table 1 : Part-of-speech tags of the Penn Chinese Treebank that are referenced in this paper . Please see ( Xia , 2000 ) for the full list . Table 3 : Counts of the number of files , sentences ( Sent ) , original space-delimited tokens ( Tok ) , ATB tree tokens ( Tree Toks ) , and affixes in the experimental data . Figure 3 : cumulative distribution of frequency ( CDF ) of the relative ranking of model-predicted probability of being positive for false negatives in a pool mixed of false negatives and true negatives ; and the CDF of the relative ranking of model-predicted probability of being negative for false positives in a pool mixed of false positives and true positives . Figure 1 Percentage of correct entailments within the top 40 candidate pairs of each of the methods , LIN and Bootstrapped LIN ( denoted as LINB in the ﬁgure ) , when using varying numbers of top-ranked features in the feature vector . The value of “ All ” corresponds to the full size of vectors and is typically in the range of 300–400 features . Figure 2 : An ATB sample from the human evaluation . The ATB annotation guidelines specify that proper nouns should be specified with a flat NP ( a ) . But the city name Sharm Al- Sheikh is also iDafa , hence the possibility for the incorrect annotation in ( b ) . Figure 2 : Learning curve with different sizes of labeled data Table 3 : Results for the non-pronoun resolution Figure 1 : Organisation of the hierarchical graph of concepts Figure 2 : Graph of the one-to-one accuracy of our full model ( +FEATS ) under the best hyperparameter setting by iteration ( see Section 5 ) . Performance typically stabi- lizes across languages after only a few number of itera- tions . Figure 1 . The decision tree for ec+mc+zž , learned by C5 . Besides pairwise agreement be- tween the parsers , only morphological case and negativeness matter . Table 2 : Top patterns chosen under different scoring schemes Figure 2 : Example of a word lattice Figure 1 : Dynamic-Expansion Tree Span Scheme Figure 1 : Discourse tree for two sentences in RST-DT . Each of the sentences contains three EDUs . The second sentence has a well-formed discourse tree , but the first sentence does not have one . Table 1 : An example of NE and non-NE Table 2 : Missing argument examples of biological interactions Figure 5 : Distribution over number of hits Table 2 : Comparison of Moses and KIT phrase extraction systems Table 4 : Performance on Ar-En with basic ( left ) and sparse ( right ) feature sets on MT05 and MT08 . Figure 2 : An example of alignment units Table 2 : Syntactic dependency scheme used in this work . Labels that aren ’ t self-explanatory or similar to the labels used by Tratz and Hovy ( 2011 ) for English or CATiB for Arabic ( Habash and Roth , 2009 ) are in bold ( for completely new relations ) or italics ( for similarly named but semantically different relations ) Table 1 : Statistics and Name Distribution of Test Data Sets . Figure 5 : Identified metaphorical expressions for the mappings FEELING IS FIRE and CRIME IS A DISEASE Table 6 : Counts for the POS tags mentioned in Table 5 . Table 3 : Examples of aggregated instances . Figure 1 : Proposed method : data flow . Table 2 : Combination results ( using SVMacc ) Figure 5 : Smoothed F1-Measure curves over the five corpora . Figure 1 : Example of a term construction rule as a branch in a decision tree . Table 2 : Comparison of our system with the best-reported systems on MUC-6 and MUC-7 Table 3 : Three most distinctive topics are displayed . The English words for each topic are ranked according to p ( e|z ) estimated from the topic-specific English sentences weighted by { φdnk } . 33 functional words were removed to highlight the main content of each topic . Topic A is about Us-China economic relationships ; Topic B relates to Chinese companies ’ merging ; Topic C shows the sports of handicapped people . Table 2 : Features Used for Initial Distribution Table 6 : Feature interpolation of translation models : A=ICTCLAS , B=dict-hybrid , C=dict-PKU-LDC , D=dict-CITYU , E=CRF-AS Figure 1 ESA and input/output data . Table 5 : Parsing accuracy ( AS = attachment score , EM = exact match ; U = unlabeled , L = labeled ) Table 4 : Accuracy for induced verb classes . Table 8 Results of all distributional similarity measures when tuning K over the development set . We encode the description of the measures presented in Table 2 in the following manner— h = health-care corpus ; R = RCV1 corpus ; b = binary templates ; u = unary templates ; L = Lin similarity measure ; B = BInc similarity measure ; pCt = pair of CUI tuples representation ; pC = pair of CUIs representation ; Ct = CUI tuple representation ; C = CUI representation ; Lin & Pantel = similarity lists learned by Lin and Pantel . Table 2 : Results on TAC 2010 entity extraction with N - best mapping for N = 1 and N = 5 . Intermediate values of N produce intermediate results , and are not shown for brevity . Table 1 : ROUGE F-scores for different systems Figure 3 : Semantic Caseframe Expectations Table 4 . Performance of English system with system mentions and system relations Table 1 . Nouns and verbs supersense labels , and short description ( from the Wordnet documentation ) . Figure 2 : Participants in the shared task . Not all groups participated in all translation directions . Table 2 : The Performance of SVM and LP algorithm with different sizes of labeled data for relation detection on relation subtypes . The LP algorithm is run with two similarity measures : cosine similarity and JS divergence . Figure 1 : Distribution of isolated vs. initial posi- tion for the most frequent lexical items Table 1 : Summary of the previous work on coreference resolution that employs the learning algorithms , the clustering algorithms , the feature sets , and the training instance creation methods discussed in Section 3.1 . Table 4 Effect of pruning parameter tp and heuristic function on search efficiency for direct-translation model ( Np = 50,000 ) . Table 1 : Arabic Verbal Inflection Table 5 : Results obtained by a combination of the best statistical and knowledge-based configuration . ‘ Best- Single ’ is the best precision or recall obtained by a sin- gle measure . ‘ Union ’ merges the detections of both approaches . ‘ Intersection ’ only detects an error if both methods agree on a detection . Figure 1 : A NE detection window Fig . 6 . Algorithm for fuzzy divisive clustering based on nouns . Figure 4 : Propagation : All items Table 11 Model accuracy using unequal distribution of verb frequencies for the estimation of P ( c ) . Figure 3 : Opinion Question Answering System Figure 5 : MM and MMVC performance by performing 5- fold cross validation on S ENSEVAL -2 data for 4 languages Table 1 Closed test , in percentages ( % ) Figure 3 : Acceptance rates for a noun phrase in the course of iteration . All models were with back-off mix- ing ( +BM ) . Table 5 Comparative performance on MSRPC . Table 20 Experimental results for large-scale English-to-Japanese translation Figure 1 : One possible breakdown of spoken Arabic into dialect groups : Maghrebi , Egyptian , Levantine , Gulf and Iraqi . Habash ( 2010 ) gives a breakdown along mostly the same lines . We used this map as an illustration for annotators in our dialect classification task ( Section 3.1 ) , with Arabic names for the dialects instead of English . Figure 2 : Source span lengths Table 1 : The incompleteness of Freebase ( * are must- have attributes for a person ) . Table 4 : Comparison of our method ( FEATS ) to state-of-the-art methods . Feature-based HMM Model ( Berg- Kirkpatrick et al. , 2010 ) : The KM model uses a variety of orthographic features and employs the EM or LBFGS optimization algorithm ; Posterior regulariation model ( Graça et al. , 2009 ) : The G10 model uses the posterior regular- ization approach to ensure tag sparsity constraint . Table 4 : Lexicon-based phrase labeling Table 4 : Accuracy on Section 1 of the FraCaS suite . Problems are divided into those with one premise sen- tence ( 44 ) and those with multiple premises ( 30 ) . Figure 7 Accuracy of different methods in predicting OOV words polarity . Table 4 Related research integrating context into alternative SMT models Table 4 : A comparison of translation quality of Egyptian , Levantine , and MSA web text , using various training corpora . The highest BLEU scores are achieved using the full set of dialectal data ( which combines Levantine and Egyptian ) , since the Egyptian alone is sparse . For Levantine , adding Egyptian has no effect . In both cases , adding MSA to the dialectal data results in marginally worse translations . Table 4 Confusion matrix among subtypes of ArgM , defined in Table 1 . Entries are fraction of all ArgM labels . Entries are a fraction of all ArgM labels ; true zeros are omitted , while other entries are rounded to zero . Table 10 Comparing selectional preference frame definitions . Table 3 . Number of candidates for each target language . Table 22 Cross-system comparison results . Table 2 : Results on G EO with 250 training and 250 test examples . Our results are averaged over 10 random 250+250 splits taken from our 600 training examples . Of the three systems that do not use logical forms , our two systems yield significant improvements . Our better sys- tem even outperforms the system that uses logical forms . Table 1 : 14 classes used in Joanis et al . ( 2008 ) and their corresponding Levin class numbers Figure 4 : Contribution of combining the dynamic Table 6 . The performance on the set of unknown Table 9 : The effect of gender detection schemes Table 3 : Sizes of rule application test set for each learned rule-set . Figure 1 : Part of a sample headline cluster , with aligned paraphrases Table 1 : Results for morphological processing , English→German Table 1 : A summary of the parsing and evaluation sce- narios . X depicts gold information , – depicts unknown information , to be predicted by the system . Figure 6 : Are the single most probable words for a given Table 5 . The number of OAS ( types ) , CAS ( types ) , LUW ( types ) and EIW ( types ) for our CWS . Fig . 7 BLEU learning curves ( left ) and difference curves ( right ) comparing the Moses baseline against two IGTree ( LTAG±1 and PR ) and TRIBL ( Super-Pair±1 and PR ) classifiers Table 1 . Results of system combination on Dev7 ( development ) corpus and Test09 , the oﬃcial test corpus of IWSLT ’ 09 evaluation campaign . Table 1 : Numbers of expressions of all the differ- ent types from the DISCO and Reddy datasets . Figure 1 : Dependency structure of a sentence . Table 23 Accuracies of various phrase-structure parsers on CTB2 with automatically assigned tags . Table 4 . Comparison of Unsupervised Learning Methods Table 4 Experiments on the threshold–partial recall relationship of the small corpus . Table 2 : Semantic features . Table 5 : Performance comparison on the ACE 2004 data over the 7 relation types . Table 1 : Association frequencies for target verb . Figure 4 : Sorted frequency of tags for WSJ . The gold standard distribution follows a steep exponential curve while the induced model distributions are more uniform . Table 4 : Performance of knowledge-based approach using different relatedness measures . Fig . 6 . F-measure for the objective and subjective classes for multilingual bootstrapping ( versus cross-lingual framework ) . 3 . ( 7 $ % 19 : 6 7 ( Figure 2 Error analysis example . . . . ) 82 mrt ÂyAm ς lý ǍxtfA ’ Alzmyl Almhnd . . . ( ‘ Several days have passed since the disappearance of the colleague the engineer . . . ’ ) , as parsed by the baseline system using only CORE 12 ( left ) and as using the best performing model ( right ) . Bad predictions are marked with < < < . . . > > > . The words in the tree are presented in the Arabic reading direction ( from right to left ) . Table 1 : Entries from the English-Slovene sense cluster inventory . Figure 2 : Screenshot of the main BRAT user-interface , showing a connection being made between the annotations for “ moving ” and “ Citibank ” . Table 3 : Training corpus statistics ( * without punctuation marks ) . Table 6 : The best results ( per F1 -score of the two meth- ods ) . The parameters of method 1 included using only those string transformations that occur at least 2 times in the training data , and limiting rule application to a maxi- mum of 2 times within a word , and including a unigram post-filter . Rules were contextually conditioned . For method 2 , all the examples ( threshold 1 ) in the training data were used as positive and negative evidence , with- out a unigram filter . Table 1 : A text segment from MUC-6 data set Table 6 : Performance on the test set . Scores are on gold mentions . Stars indicate a statistically significant difference with respect to the baseline . Figure 1 : Properties of the training and test sets used in the shared task . The training data is the Europarl cor- pus , from which also the in-domain test set is taken . There is twice as much language modelling data , since training data for the machine translation system is filtered against sentences of length larger than 40 words . Out-of-domain test data is from the Project Syndicate web site , a compendium of political commentary . Figure 1 : Rule expansion with minimal context ( Example 3 ) Table 3 : Oracle lower-case BLEU The joint probability of the two sequences is defined as the product of context probabilities and lexical probabilities over all POS tags : Table 3 : Summary of LINGUA performance Table 7 : Feature blending of translation models Table 5 : Rhetorical pattern of C-Semicolon Figure 4 : Using the type model for disambiguation in the derivation of file a suit . Type distributions are shown after the variable declarations . Both suit and the object of file are lexically ambiguous between different types , but after the β -reduction only one interpretation is likely . If the verb were wear , a different interpretation would be preferred . Table 3 : Evaluation results for all combinations of mixture adapted language and translation models : Baseline ( bl ) scores are italicized , best scores are in bold Table 1 . Accuracy of our system in each period ( M = 10 ) Table 11 The comparative error rates of the pseudo-disambiguation task for the three examined similarity measures , with and without applying the bootstrapped weighting for each of them . Table 5 Some words extracted from the large corpus . Figure 7 . Performance Comparison of Combined Model and KLD Model Table 2 shows the result of varying the number of samplers and iterations for all Table 2 : Most frequent phrase dependencies with at least 2 words in one of the phrases ( dependencies in which one phrase is entirely punctuation are not shown ) . $ indicates the root of the tree . Table 3 : Russian to English machine translation system evaluated on tst2012 and tst2013 . Human evaluation in WMT13 is performed on the system trained using the original corpus with TA-GIZA++ for alignment ( marked with * ) Figure 1 : Example of the effects of OOV processing for German→English Table 8 : Final UAS/LAS scores for dependencies on the test sets for the predicted setting . Other denotes the highest scoring other participant in the Shared Task . ST Baseline denotes the MaltParser baseline provided by the Shared Task organizers . Fig . 3 : NEs after agents-based modification Figure 2 . Growing Algorithm for Language Model Pruning Table 3 : Most frequent monosemic words in BG Table 7 Results per concept for the ILP-Global . Table 3 . The highest performance of applying various sampling strategies in selecting the initial seed set on the ACE RDC 2004 corpus Table 6 : Example Translations for the Verbmobil task . = argmax S I n s=1 a l ) p ( fs , a | es ) ( 7 ) Figure 2 . Automatically detected posture points ( H = headDepth , M = midTorsoDepth , L = lowerTorsoDepth ) Table 3 : Domain specific results Figure 11 : MUC-6 : Level Distribution of Each of the Six Facts Figure 1 : a ) A related work section extracted from ( Wu and Oard , 2008 ) ; b ) An associated topic hierar- chy tree of a ) ; c ) An associated topic tree , annotated with key words/phrases . Figure 2 : One of the 69 test documents , containing 10 narrative events . The protagonist is President Bush . Table 1 : Two characteristic topics for the Y slot of ‘ acquire ’ , along with their topic-biased Lin sim- ilarities scores Lint , compared with the original Lin similarity , for two rules . The relevance of each topic to different arguments of ‘ acquire ’ is illus- trated by showing the top 5 words in the argument y vector vacquire for which the illustrated topic is the most likely one . Table 1 : Mention Detection Results Figure 4 . TSVM optimization function for non-separable case ( Joachims , 1999 ) Table 2 . Results of 1000 sentences Figure 3 : Example of original hypotheses and 3- grams collected from them . Table 6 : Example patterns for parallelism Figure 1 : A tree showing head information Table 10 Translation results on the Hansards task . Figure 1 : Translation of PCC sample commentary Table 7 Experiments on the word length–precision relationship of the large corpus with threshold three . Table 1 . Example segmentations ( „ |‟ indicates the separator between adjacent snippets ) Table 4 : Contribution of each feture . ALL : all features , PER : perceptron model , WLM : word language model , PLM : POS language model , GPR : generating model , LPR : labelling model , LEN : word count penalty . Figure 2 : EuroParl topics ( T=400 ) Figure 3 : Improvement in ( gold mention ) RE . Table 4 : Evaluation of the GUITAR improvement - summarization ratio : 30 % . Figure 5 : Multiple Analyses for suis Figure 2 : Inuktitut : Parimunngaujumaniralauqsimanngittunga = “ I never said I wanted to go to Paris ” Figure 2 : ( a ) An undirected graph G representing the similarity matrix ; ( b ) The bipartite graph showing three clusters on G ; ( c ) The induced clusters U ; ( d ) The new graph G1 over clusters U ; ( e ) The new bipartite graph over G1 Table 6 : Syntactic features for featurama ( Czech ) . * mark statistically significantly better models compared to feat- urama ( sentence-based t-test with α = 0.05 ) . Fig . 1 . System architecture overview Figure 4 A comparison between ILP-Global and ILP-Local for two fragments of the test-set concept seizure . Figure 1 : BiTAM models for Bilingual document- and sentence-pairs . A node in the graph represents a random variable , and a hexagon denotes a parameter . Un-shaded nodes are hidden variables . All the plates represent replicates . The outmost plate ( M -plate ) represents M bilingual document-pairs , while the inner N -plate represents the N repeated choice of topics for each sentence-pairs in the document ; the inner J-plate represents J word-pairs within each sentence-pair . ( a ) BiTAM-1 samples one topic ( denoted by z ) per sentence-pair ; ( b ) BiTAM-2 utilizes the sentence-level topics for both the translation model ( i.e. , p ( f |e , z ) ) and the monolingual word distribution ( i.e. , p ( e|z ) ) ; ( c ) BiTAM-3 samples one topic per word-pair . Table 13 Descriptive statistics for WordNet hyp/syn relations on the coreference data set . Table 5 : Parser performance on Brown ; E , supervised adaptation Table 4 Standards and corpora . Figure 2 : OT grammar for devoicing compiled into an FST . Table 1 : Results of segmentation of entry titles ( F-score ( precision/recall ) ) . Figure 3 Back-off lattice with more specific distributions towards the top . Figure 7 : A Path in a Transducer for English Table 3 : Comparison results with TAC 2008 Three Top Ranked Systems ( system 1-3 demonstrate top 3 systems in TAC ) Table 1 : Key notation . Feature factorings are elaborated in Tab . 2 . Figure 1 . Correlation between cohesion-driven functions . Table 5 Effect of pruning parameter tp and heuristic function on error rate for direct-translation model ( Np = 50,000 ) . Table 1 : Trivial and single-feature baselines ( using SVM- acc unless noted otherwise ) Table 1 : Possible relations appearing on the edges of a DCS tree . Here , j , j 0 ∈ { 1 , 2 , . . . } and i ∈ { 1 , 2 , . . . } ∗ . Table 4 : Training phase : systems outperforming the baseline in terms of TRDR score . Fig . 1 . Fuzzy hierarchical clustering for Paraphrase Extraction . Table 1 . Comparison of Number of Bigrams at F-Measure 96.33 % Figure 3 . Calculation of `` Importance '' of Bigrams Table 2 : Example of first and second order features using a predefined n-gram size of 2 . Figure 1 . Language Model Pruning Algorithm Figure 9 Illustration of the IBM-style reordering constraint . Table 9 : Performance comparison with the literature for compound bracketing Table 3 : Experiment 2 : Results by training set size , θ = 1.0 Table 3 : Results of negated event/property detection on gold standard cue and scope annotation Table 11 Distribution of antecedent NP types for definite NP anaphora . Table 2 . Evaluation closed results on all data sets Figure 3 Positional sentence weight for varying Figure 6 : Average scores for different language pairs . Manual scoring is done by different judges , resulting in a not very meaningful comparison . Table 6 Comparison between three different decoders for word segmentation . Table 11 : Cross Framework Evaluation : Unlabeled TedEval on generalized gold trees in gold scenario , trained on 5k sentences and tested on 5k terminals . Figure 2 : Tagging accuracy on development data depending on context size Fig . 8 Average number of target phrase distribution sizes for source phrases for TRIBL and IGTree com- pared to the Moses baseline Table 2 : Results for the submitted runs Figure 1 : Plate diagram depicting the morphology model ( adapted from Goldwater et al . ( 2006 ) ) . Hyperparameters have been omitted for clarity . The left-hand plate depicts the base distribution P0 ; note that the morphological anal- yses lk are generated deterministically as ( tk , sk , fk ) . The observed words wi are also deterministic given zi = k and lk , since wi = sk ⊕ fk . Figure 2 : Cross-source Comparable Data Example ( each morph and target pair is shown in the same color ) Figure 1 A framework for jointly identifying and aligning bilingual NEs . Table 5 : Comparison of results for MUC6 Figure 2 : Duration ( in seconds ) of each lexical type Figure 1 : Connected components nearest neighbour ( NN ) clustering . D is the Kullback-Leibler distance . Fig . 1 . Integration of confidence measures – recall/precision curves ( figures in the legend correspond to resp . δ1 and δ2 ) . Table 7 The official vocabularies in Verbmobil . Figure 3 : Dev set learning curves for sentence lengths ≤ 70 . All three curves remain steep at the maximum training set size of 18818 trees . Table 14 Overview of the results for all WordNet algorithms for coreference . Table 2 : Comparing disagreements between the best local and global algorithms against the gold standard Table 1 : Syntactic Seeding Heuristics Table 7 % BLEU on tune and test sets for ZH→EN translation , showing the contribution of feature sets in our QPD model . Both QPD models are significantly better than the best Moses numbers on test sets 1 and 2 , but not on test set 3 . The full QPD model is significantly better than the version with only T GT T REE features on test set 1 but statistically indistinguishable on the other two test sets . Hiero is significantly better than the full QPD model on test set 2 but not on the other two . Table 7 : Unbalanced vs. balanced combining . All runs ignored the context . Evaluated on the Test data set . Figure 1 : Similarity graph after its sparsification Table 3 . Improvements of different tree setups Table 4 : Performance of various clustering-based seed sampling strategies on the held-out test data with the optimal cluster number for each clustering algorithm Table 4 . Accuracy of 5-fold cross-validation with self- extracted semantic features Table 4 : Example translations from the different methods . Boldface indicates correct translations . Table 3 : NIST BLEU scores on the German-English ( de- en ) and French-English ( fr-en ) Europarl test2008 set . Table 8 : Using an oracle Table 4 : Gross statistics for several different treebanks . Table 1 : Results of paraphrases evaluation for 100 sentences in French using English as the pivot lan- guage . Comparison between the baseline system MOSES and our algorithm MCPG . Figure 2 . Percentage of examples of major syntactic classes . Figure 10 : Analysis of NP coordination , in a distributive ( left ) and a collective interpretation ( right ) . Figure 4 : Improvement in ( predicted mention ) RE . Table 4 : Translation performance of baseline and bLSA-Adapted Chinese-English SMT systems on manual transcriptions and 1-best ASR hypotheses Table 6 Models with inflectional and lexical morphological features together ( predicted value-guided heuristic ) . Statistical significance tested only on predicted input , against the CORE 12 baseline . Table 4 : Extending training sets : an example Figure 1 : The NLM-WSD test set and some of its sub- sets . Note that the test set used by ( Joshi et al. , 2005 ) comprises the set union of the terms used by ( Liu et al. , 2004 ) and ( Leroy and Rindflesch , 2005 ) while the “ com- mon subset ” is formed from their intersection . Fig . 4 . Performance of the training algorithms , supervised against semi-supervised techniques . The semi-supervised precision is evaluated indirectly by using the predicted dataset as train-set against the human-annotated manual small dataset . Figure 3 : Example derivation for the query ‘ how many people visit the public library of new york annu- ally. ’ Underspecified constants are labelled with the words from the query that they are associated with for readability . Constants from O , written in typeset , are introduced in step ( c ) . Table 3 . Results on three query categories . Table 2 : Results on the STS video dataset . Table 1 Related research integrating context into word-based SMT ( WB-SMT ) models Table 13 Example translations for the translation direction English to German using three different reordering constraints : MON , EG , and S3 . Figure 1 . Consistently formatted term translation pairs Table 29 : Different Arabic Transliterations of `` Los Angeles '' Figure 2 : Learning curve of the hierarchical strategy and its comparison with the flat strategy for some major relation subtypes ( Note : FS for the flat strategy and HS for the hierarchical strategy ) Table 4 : Performance of different FS machines in terms of the percentage of unclassified entries . All the classified entries were correctly classified , yielding , as a result , a precision of 100 % . Figure 1 : Dependency graph for Czech sentence from the Prague Dependency Treebank1 Table 3 Scenarios in which we added hard constraints to the ILP . Table 2 : S/O classifier with and without SWSD . Table 1 : Symmetry of window size Table 1 : A sentence decomposed into its depen- dency edges , and the caseframes derived from those edges that we consider ( in black ) . Table 2 : Performance of WSD system using various combinations of learning algorithms and features . Table 6 : Optimized Edit Costs Table 2 Words ( excluding multiwords ) in WordNet 1.7.1 and the BNC without any data in SemCor . Table 3 : Feature templates used for the chunk s : = ws ws+1 ... we where ws and we represent the words at the beginning and ending of the target chunk respectively . pi is the part of speech tag of wi and sci is the shallow parse result of wi . Figure 1 : Graphical representation of our model . Hyper- parameters , the stickiness factor , and the frame and event initial and transition distributions are not shown for clar- ity . Table 17 Example translations for the translation direction French to English using the S3 reordering constraint . Table 3 Top 60 most frequent root phrases in DE→EN data with at least two words , shown with their counts . Shown in bold are the actual root words in the lexical dependency trees from which these phrases were extracted ; these are extracted along with the phrases and used for back-off features . Table 1 : Results for the acquisition of subcategori- sation frames . Table 13 Example translations for Chinese–English MT . Table 6 Baseline + Word Clustering by Relation + Re-ranking by Coreference + Re-ranking by Relation Table 3 Polysemous word types in the Senseval-2 and -3 English all-words tasks test documents with no data in SemCor ( 0 columns ) , or with very little data ( ≤ 1 and ≤ 5 occurrences ) . Note that there are no annotations for adverbs in the Senseval-3 documents . Figure 1 . Example of trellis of the modified Viterbi search Figure 2 : Word-to-word alignment . Table 6 : Evaluation of the GUITAR system without DN detection over a hand-annotated treebank Table 1 : Examples of templates suggested by DIRT and TEASE as having an entailment relation , in some direction , with the input template ‘ X change Y ’ . The entailment direction arrows were judged manually and added for readability . Table 2 The results of setting 1 ( Punctuation and other encoding information are not used ; the maximum length is 30 ) . Table 7 Effect of pruning parameter Np and heuristic function on error rate for direct-translation model ( tp = 10 12 ) . Figure 5 : The Wiktionary vs. tree bank tag sets . Around 90 % of the Wiktionary tag sets are identical or subsume tree bank tag sets . See text for details . Table 2 : Pearson ’ s ( r ) correlation results over the WMT all-en dataset , and the subset of the dataset that contains noun compounds Table 1 Performance of Paraphrase Recognition system on MSRPC [ 25 ] . Figure 2 : Sequence of POS-tagged units used to estimate the bilingual n-gram LM . Figure 1 : A Dictionary based Word Graph Table 3 : Best LO and LL configurations scores Figure 2 : A noun-phrase with sub-structure Table 3 : Accuracy for Different Part-Of-Speech Figure 1 : The first set of features in our model . All of them are binary . The final feature set includes two sets : the set here , and a set obtained by its conjunction with the verb ’ s lemma . Table 1 : Examples of SMT errors due to MWEs . Table 1 : Key notation . Table 2 : Disambiguation results . Figure 4 : A toy instance of lattice construction Table 8 FT detection results on the MSR gold test set . The ‘ All ’ column shows the results of detecting all 10 types of factoids , as described in Table 1 , which amount to 6630 factoids , as shown in Table 3 . Table 1 – Precision/recall for Le Monde corpus Table 6 : Sample targets for meta alternations with high AP and mid-coherence values . Figure 4 : Examples of signature caseframes found in Study 2 . Table 1 : Notation and signatures for our framework . Table 2 : Examples of unigram and bigram features extracted from Figure 1 . Figure 1 CATiB Annotation example . & ( - ) 23 1+ , 4 ! $ % ./0 tς ml HfydAt AlkA AlðkyAt fy AlmdArs AlHkwmy ( ‘ The writer ’ s smart granddaughters work for public schools ’ ) . The words in the tree are presented in the Arabic reading direction ( from right to left ) . Figure 2 : Speed in lookups per microsecond by data structure and number of 64-bit entries . Performance dips as each data structure outgrows the processor ’ s 12 MB L2 cache . Among hash tables , indicated by shapes , probing is initially slower but converges to 43 % faster than un- ordered or hash set . Interpolation search has a more ex- pensive pivot function but does less reads and iterations , so it is initially slower than binary search and set , but be- comes faster above 4096 entries . Table 2 Time comparison between FSAs and FSRAs . Table 6 : Results of the rule-based model : best guess Figure 1 : File formats . Trees ( a ) and ( b ) are aligned constituency and dependency trees for a mockup English example . Boxed labels are shared across the treebanks . Figure ( c ) shows an ambiguous lattice . The red part represents the yield of the gold tree . For brevity , we use empty feature columns , but of course lattice arcs may carry any morphological features , in the FEATS CoNLL format . Figure 11 : Two Paths in the Initial Malay Transducer Defined via Concatenation Figure 2 : English parse tree with empty elements marked . ( a ) As annotated in the Penn Treebank . ( b ) With empty elements reconﬁgured and slash categories added . Table 1 Baseline : Out-of-the-box BerkeleyParser performance on the dev-set . Table 6 : Listing of all seeds used for KEdis and KEpat , as well as the top-10 entities discovered by ES-all on one of our test folds . Table 2 : CoreLex ’ s basic types with their corresponding WordNet anchors . CAM adopts these as meta senses . Table 1 Taxonomy of Chinese words used in developing MSRSeg . Table 4 : Segmentation , POS tagging , and ( unlabeled attachment ) dependency F1 scores averaged over five trials on CTB-5c . Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01 ) . Figure 13 The accuracy/speed tradeoff graph for the phrase-structure parser . Table 5 The accuracies of various word segmentors over the second SIGHAN bakeoff data . Table 10 NW 21 identification results on PK test set . Figure 2 : Framework overview . Figure 1 : Latent Dependency coupling for the RE task . The D-C ONNECT factor expresses ternary connection re- lations because the shared head word of the proposed re- lation is unknown . As is convention , variables are repre- sented by circles , factors by rectangles . Table 3 : Impact of scaling techinques ( ILP− /ILPscale ) . Figure 2 A character sequence and its subsequence pairs . Table 9 : Dev set results for sentences of length ≤ 70 . Table 3 . Accuracy of the proposed system on a set which contains misspelled queries that the users had reformulated Table 13 Sample of human-interpretable Arabic TSG rules . Recursive rules like MWA→A MWA result from memoryless binarization of n-ary rules . This pre-processing step not only increases parsing accuracy , but also allows the generation of previously unseen MWEs of a given type . Figure 1 Examples of word alignment patterns in German–English that require the increased expressive power of synchronous tree adjoining grammar . Table 2 Statistics for Verbmobil task : training corpus ( Train ) , conventional dictionary ( Lex ) , development corpus ( Dev ) , test corpus ( Test ) ( Words* : words without punctuation marks ) . Figure 1 . Linking FrameNet frames and VerbNet classes Figure 1 : System Architecture . Table 9 : German-to-English Final System Results . Table 3 . Comparison our closed results with the top three in all test sets Table 15 Experimental results on the WMT 2009 test set Table 1 : Results for all experiments Table 3 : Scores for UPUC corpus Table 4 : The 10 best languages for the particle compo- nent of BANNARD using LCS . Table 11 : Effects of Popularity of Morphs Table 1 : Experiment 1 : Results for label unknown sense , WSD confidence level approach . θ : confi- dence threshold . σ : std . dev . Table 5 : In-domain ( first column ) and out-of-domain performance ( columns two to four ) on ACE 2005 . PET and BOW are abbreviated by P and B , respectively . If not specified BOW is marked . Table 8 Model accuracy using equal distribution of verb frequencies for the estimation of P ( c ) . Table 3 : Unsupervised and Supervised scores on the SemEval-2010 WSI Task for each feature and clustering models , with reference scores for the top performing systems for each evaluation shown below . Table 3 : Recall on different types of empty categories . YX = ( Yang and Xue , 2010 ) , Ours = split 6× . Table 6 Performance of proposed system on MSRVDC Dataset 1 . Table 1 : Distribution of activity types : Both databases contain a lot of discussing , informing and story-telling activities however the meeting data contains a lot more planning and advising . Figure 1 : Precision-recall curve for rescoring Table 1 : Dependency accuracy on 13 languages . Unlabeled ( UA ) and Labeled Accuracy ( LA ) . Table 1 : Devtest Set Statistics by Language Table 5 : Precision for 200 candidates ( Ev.Rec ) . Figure 4 : F-measure for the objective and subjective classes for cross-lingual bootstrapping Table 4 : Processing time for POS tagging of known words using contextual features ( In CPU seconds ) . Train : training time over + sentences . Brill ’ s learner was interrupted after 12 days of train- ing ( default threshold was used ) . Test : average number of seconds to evaluate a single sentence . All runs were done on the same machine . Figure 8 : The actual output of our parser trained with a fully annotated treebank . Table 1 : BLEU-4 scores of different systems Figure 3 : A complex Turkish-English word alignment ( alignment points in gray : EM/PY-U ( V ) ; black : PY- U ( S ) ) . Figure 4 : Results of IE Experiment Figure 3 . High TF/ITF words in “ Com-Com ” ( Numbers are TF/ITF score , frequency in the collec- tion ( TF ) , frequency in the corpus ( TF ) and word ) Table 1 : Details of the corpora . W.T . represents word types ; C.T . represents character types ; S.C. represents simpliﬁed Chinese ; T.C . represents traditional Chinese . Table 2 : Comparison of three statistical translation approaches ( test on text input : 251 sentences = 2197 words + 430 punctuation marks ) . Table 2 : Word segmentation on IWSLT data sets Table 2 : An example for the “ BMES ” representa- tion . The sentence is “ 我爱北京天安门 ” ( I love Bei- jing Tian-an-men square ) , which consists of 4 Chi- nese words : “ 我 ” ( I ) , “ 爱 ” ( love ) , “ 北京 ” ( Beijing ) , and “ 天安门 ” ( Tian-an-men square ) . Table 5 : ROUGE-W measures in EM learning Table 2 : Tagging accuracies on development data in percent . hLEX ( eI , f J , K , zK ) = # CO-OCCURRENCES ( LEX , eI , f J ) ( 20 ) 1 1 1 1 1 1 Figure 4 : Using different amounts of annotated training data for the article meta-classifier . Figure 1 . NPs in a sample from the Catalan training data ( left ) and the English translation ( right ) . Figure 2 : Precision-recall curve for the algorithms . Figure 2 : Example of the Character Tagging Method : Word boundaries are indicated by vertical lines ( ‘ | ’ ) . Table 3 : Average polysemy on SE2 and SE3 Table 2 : Dataset characteristics including the number of documents , annotated CEs , coreference chains , annotated CEs per chain ( average ) , and number of documents in the train/test split . We use st to indicate a standard train/test split . Table 8 : Example patterns of proteins and their do- mains Table 14 Comparison scores for HK open and AS open . Table 4 Ironic tweets that received every party and their election results . The ﬂuctuation describes the difference between the May 2012 election results and the previous . Table 7 : Performance of our system on the evalu- ation set Table 2 : Incremental evaluations , by incrementally adding new features ( word features and high dimensional edge features ) , new word detection , and ADF training ( replacing SGD training with ADF training ) . Number of passes is decided by empirical convergence of the training methods . Table 3 : Results for two kinds of headlines Figure 1 A general architecture for paraphrasing approaches leveraging the distributional similarity hypothesis . Table 1 : Examples of constructing Universal POS tag sets from the Wiktionary . Table 1 : Statistics on the Italian EVALITA 2009 and English CoNLL 2003 corpora . Figure 1 : Baseline results for human word lists . Data : 700 positive and 700 negative reviews . Figure 2 : V-Measure and paired FScore results for different partitionings of the dendrogram . The dashed vertical line indicates SP D Figure 4 : Breadth-first beam search algorithm of Och and Ney ( 2004 ) . Typically , a hypothesis stack H is maintained for each unique source coverage set . Table 5 : Evaluating Word Alignment Accuracies and Machine Translation Qualities for BiTAM Models , IBM Models , HMMs , and boosted BiTAMs using all the training data listed in Table . 1 . Other experimental conditions are similar to Table . 4 . Table 5 : Equation 1 settings Table 3 : F-score of two segmenters , with ( − ) and without ( + ) word token/type features . Figure 1 : Structure of a typical two-pass ma- chine translation system . N-best translations are generated by the decoder and the 1-best transla- tion is returned after rescored with additional feature functions . Table 4 DP-TSG notation . For consistency , we largely follow the notation of Liang , Jordan , and Klein ( 2010 ) . I exp [ m=1 m hm ( e 1 , f1 ) ] Figure 3 : Propagation : Core items Table 2 : Entity type constraints . Table 1 : Manual analysis of suggested corrections on CLC data . Table 2 : The performance of different resolution systems Table 3 : Op . Target - Op . Word Pair Extraction Figure 1 : The Stanford parser ( Klein and Manning , 2002 ) is unable to recover the verbal reading of the unvocalized surface form 0 an ( Table 1 ) Table 4 : The BLEU score of self-trained cascaded trans- lation model under five initial training sets . Table 6 . Feature impact experiments Table 2 : General Knowledge Sources Figure 2 : Perplexity measured on nt08 with the baseline LM ( std ) , with the LM estimated on the sampled texts ( generated texts ) , and with the inter- polation of both . Figure 2 . Self-Training for Name Tagging Table 6 : The System Performance of Integrating Cross Source and Cross Genre Information . Table 2 : Training phase : effect of similarity thresh- old ( a ) on Ave. MRR and TRDR . Table 1 : good # a # 15 SentiWN scores . Figure 3 : Changing a decision in the derivation lattice . All paths generate the observed data . The bold path rep- resents the current sample , and the dotted path represents a sidetrack in which one decision is changed . Figure 10 FSRA for a given CNF formula . Table 3 : Lexical features for relation extraction . Table 3 : STTS accuracies of the TnT tagger trained on the STTS tagset , the TnT tagger trained on the Tiger tagset , and our tagger trained on the Tiger tagset . Figure 7 The adjusted frequencies of character sequences . Table 4 Lexical entailment precision values for top-n similar words by the Bootstrapped LIN and the original LIN method . Table 2 . Performance of RDC trained on fp1/fp2/adj , and tested on adj . Table 1 : Mixed-case TER and BLEU , and lower-case METEOR scores on Arabic NIST MT03+MT04 . Table 11 Precision and partial recall of word lengths two to four of the first experiment on IT and AV . Table 4 The results of setting 3 ( Punctuation is used ; the maximum length is 30 ) . Table 3 . Performance comparison , the numbers in parentheses report the performance over the 24 ACE subtypes while the numbers outside paren- theses is for the 5 ACE major types Figure 5 : M-1 accuracy vs. number of samples . Table 2 : Composition Gold Standards Table 1 : The top-ranking feature for each group of features and the classifier of a slot Table 5 : New training and testing procedures Table 1 : The semantic roles of cases beside C-3 verb cluster Table 6 Most frequent semantic roles for each syntactic position . Figure 3 : Dev set learning curves for sentence lengths ≤ 70 . All three curves remain steep at the maximum training set size of 18818 trees . Table 3 : F-measure after successive addition of each global feature group Table 2 : Average accuracy for EM baseline and model variants across 503 languages . First panel : results on all languages . Second panel : results for 30 isolate and singleton languages . Third panel : results for 27 non-Latin alphabet languages ( Cyril- lic and Greek ) . Standard Deviations across lan- guages are about 2 % . Figure 1 : Example consensus network with votes on word arcs . Figure 5 : A filter RTG corresponding to Ex . 2 Table 2 : Classification results with 5-gram and fre- quency threshold 4 ( Method 2 ) Table 20 Feature templates for the phrase-structure parser . Figure 15 Reduplication – general case . Table 8 : Syncretism Example 2 Table 17 Results on large-scale Dutch-to-English translation Table 10 Model accuracy using equal distribution of verb frequencies for the estimation of P ( c ) . Table 1 : IBM Model 3 Table 11 The comparison between IWSLRR ( I ) , SS ( S ) , TONGO ( O ) , TH ( T ) , and ESA . ings of the 13th International Conference , pages 182–190 . Table 6 : Accuracy on S ENSEVAL-1 and S ENSEVAL-2 En- A. R. Golding and D. Roth . 1999 . A winnow-based appro glish test data ( only the supervised systems with a coverage of to context-sensitive spelling correction . Machine Learni at least 97 % were used to compute the mean and variance ) 34 ( 1-3 ) :107–130 . Table 6 . Size of the test data Figure 11 An example Chinese lexicalized phrase-structure parse tree . Table 10 : Scalability of BS on NIST task Table 2 : Features for SVM Learning of Prediction Model Figure 4 : Glue Semantics proof for ( 86 ) , English Way Construction ( means interpretation ) Table 7 Disambiguation results for G2 and X2 . Table 6 NER type-insensitive ( type-sensitive ) performance of different Chinese NE recognizers . Table 7 Web results for other-anaphora . Figure 2 : Example of a MUC-4 template Table 4 : Ordered List of Increased/Decreased Number of Correctly Tagged Words Table 2 . Thread length distribution . Figure 13 : A Template Network and Two Filler Networks Table 4 . Results from our SMT evaluation . The BLEU scores are the maximum over the Moses parameters explored for the given word alignment conﬁguration . Table 13 Large-scale clustering on D3 with n/na/nd/nad/ns-dass . Table 10 Occurrences of error types for the best other-anaphora algorithm algoWebv4 . Figure 3 : F-Score of the RF and SVM , GIZA++ and Levenshtein distance-based classifier on the second order dataset Table 1 : Examples of non-phonetic translations . Table 3 : Comparison of the three decoders by the ratio each decoder produced search errors . Figure 7 Algorithm for breadth-first search with pruning Table 1 : Table showing the number of pairs of different occurrences of the same token sequence , where one occurrence is given a certain label and the other occurrence is given a certain label . We show these counts both within documents , as well as over the whole corpus . As we would expect , most pairs of the same entity sequence are labeled the same ( i.e . the diagonal has most of the density ) at both the document and corpus levels . These statistics are from the CoNLL 2003 English training set . Table 4 : The fraction of verb pairs clustered together , as a function of the number of different senses between pair mem- bers ( results of the NN algorithm ) Table 7 Some of the possible Spanish translations of the English phrase make with their memory-based con- text-dependent translation probabilities ( rightmost column ) compared against context-independent transla- tion probabilities of the baseline system Table 14 Results of feature analysis . The second column denotes the proportion of manually annotated examples for which the feature value is non-zero . A detailed explanation of the other columns is provided in the body of the article . Table 6 % BLEU on tune and test sets for DE→EN translation , comparing the baselines to our QPD model with target syntactic features ( T GT T REE ) and then also with source syntax ( + T REE T O T REE ) . Here , merely using the additional round of tuning with the SSVM reranker improves the BLEU score to 19.9 , which is statistically indistinguishable from the two QPD feature sets . Differences between Hiero and the three 19.9 numbers are at the border of statistical significance ; the first two are statistically indistinguishable from Hiero but the third is different at p = 0.04 . Figure 2 : Graph of words for the target word paper . Numbers inside vertices correspond to their degree . Figure 1 : Precision and recall for prepositions . Figure 10 Lattice dependency parsing using an arc-factored dependency model . Lone indices like p and i denote nodes in the lattice , and an ordered pair like ( i , j ) denotes the lattice edge from node i to node j . S TART is the single start node in the lattice and F INAL is a set of final nodes . We use edgeScore ( i , j ) to denote the model score of crossing lattice edge ( i , j ) , which only includes the phrase-based features h 0 . We use arcScore ( ( i , j ) , ( l , m ) ) to denote the score of building the dependency arc from lattice edge ( i , j ) to its parent ( l , m ) ; arcScore only includes the QPD features h 00 . Table 1 : The F1-Measure value is shown for every kernel on each ACE-2005 main relation type . For every relation type the best result is shown in bold font . Figure 2 : Architecture of the Structured Output Layer Neural Network language model . Figure 1 : Rank trajectories of 4 LDA inferred topics , with incremental topic inference . The x-axis indicates the utterance number . The y-axis indicates a topic ’ s rank at each utterance . Figure 1 : General architecture of LINGUA Table 21 Experimental results for large-scale English-to-Chinese translation Table 5 : Comparison of the structured feature and the flat features extracted from parse trees Table 9 Model accuracy using unequal distribution of verb frequencies for the estimation of P ( c ) . Table 3 . Performance of Chinese system with perfect mentions and perfect relations Figure 1 : Proposed discourse structures for Ex . 4 : ( a ) In terms of informational relations ; ( b ) in terms of inten- tional relations Table 1 : Gibbs sampling for Bayesian inference outperforms both EM and Variational Bayesian EM . ∗ The output of EM alignment was used as the gold standard . Table 3 . Data used for training SMT models ( all counts in millions ) . Parallel data sets refer to the bitexts aligned to English and their token counts include both languages . Table 4 : Voting under hand-invented schemes . Figure 2 : Mixed Membership MEDLDA Table 4 : Number of features used according to different cut-off threshold . In the second column of the table are shown the number of features used when only the English context is considered . The third column correspond to English , German and Word-Classes contexts . Table 2 : MRRs of the frequency correlation meth- ods . Figure 1 : Example of word alignment Table 1 : Comparison of word alignment accuracy . The best results are indicated in bold type . The additional data set sizes are ( a ) 10k , ( b ) 50k , ( c ) 100k . Table 4 : Distribution of Pronoun Mentions and Fre- quency of c-command Features Table 4 : Quantitative Evaluation of Common Topic Finding ( “ cross-collection ” log-likelihood ) Figure 4 : MUC-7 : Level Distribution of Each of the Facts Table 1 : Tags used in LMR Tagging scheme . Figure 8 Extracting consistent bilingual phrasal correspondences from the shown sentence pairs . ( i1 , j1 ) × ( i2 , j2 ) denotes the correspondence fi1 . . . fj1 , ei2 . . . ej2 . Not all extracted correspondences are shown . Table 1 The scales of corpora . Table 1 : Kleene Regular-Expression Assignment Examples . Figure 2 : Instructions for judging of unsharpened factoids . Table 2 : BLEU scores on the test08 and news08 test data obtained by models trained by MERT and SVM . Table 14 Summary of results for unknown-boundary condition . Table 3 : MT06 Dev . Optimization & Test Set Spearman Correlation Results Table 2 Initial type-sensitive Chinese/English NER performance . Table 3 : Features for ‘ Astronomer Edwin Hubble was born in Marshfield , Missouri ’ . Table 5 : LO sentence configuration scores Figure 2 : A wRTG modelling the interdependency constraint for Fig . 1 Figure 1 : Results for three procedures over 4 datases . The horizontal axis corresponds to the context window size . Solid line represents the result of F SGM M + binary , dashed line denotes the result of CGDSV D + idf , and dotted line is the result of CGDterm + idf . Square marker denotes χ2 based feature ranking , while cross marker denotes f req based feature ranking . Table 7 NEA type-insensitive ( type-sensitive ) performance with the same English NE recognizer ( Mallet system ) and different Chinese NE recognizers . Fig . 5 The semantic graph of an English sentence and the semantic features extracted from it for an SMT phrase Table 2 : Values obtained for Precision , Recall and F- scores with method 1 by changing the minimum fre- quency of the correspondences to construct rules for foma . The rest of the options are the same in all three experiments : only one rule is applied within a word . Table 4 : Results for nouns Table 1 : Ten relation instances extracted by our system that did not appear in Freebase . Figure 1 : ( a ) RM and large margin solution comparison and ( b ) the spread of the projections given by each . RM and large margin solutions are shown with a darker dotted line and a darker solid line , respectively . Table 4 : Example input and best output found Table 4 . An example of some discovered paraphrases . Table 3 : First ten words with weigths and number of senses in WN of the Topic Signature for airport # n # 1 obtained from BNC using InfoMap Figure 2 : Dirichlet-Tree prior of depth two . Figure 1 : An example sequence representation . The subgraph on the left represents a bigram feature . The subgraph on the right represents a unigram feature that states the entity type of arg 2 . Table 2 : The NP chunking results for six sys- tems associated with the project . The baseline results have been obtained by selecting the most frequent chunk tag associated with each part-of- speech tag . The best results for this task have been obtained with a combination of seven learn- ers , five of which were operated by project mem- bers . The combination of these five performances is not far off these best results . Figure 8 : Surface composition of embedded structures . Table 4 : Translation performances ( BLEU % and NIST scores ) of NIST task : decoder ( 1-best ) , rescoring on original 2,400 N-best ( RESC1 ) and 4,000 N-best hypotheses ( RESC2 ) , re-decoding ( RD ) , n-gram expansion ( NE ) , confusion network ( CN ) and combination of all hypotheses ( COMB ) . Table 7 : Synonyms for home Table 3 : Feature counts for Ling and Seed feature sets . Table 2 : Examples of zero anaphora sentence length Figure 6 : Time consumption of the various change types in Table 1 : The pool of features for all languages . Figure 2 : Learning curves using different sam- pling strategies . Table 3 : Jumping POS in WordNet . Table 4 : Effect of different sets of reference translations used during tuning . Table 2 : Experiment 2 : Results for label unknown sense , NN-based outlier detection , θ = 1.0. σ : stan- dard deviation Figure 2 : F1 scores ( in % ) of SegTagDep on CTB- 5c-1 w.r.t . the training epoch ( x-axis ) and parsing feature weights ( in legend ) . Table 4 : Filtering results using the naive Bayes classifier . The number of entity candidates for the training set was 4179662 , and that of the develop- ment set was 418628 . Figure 1 : The semantic representations of a word W , its inverse W inv and its negation ¬W . The domain part of the representation remains un- changed , while the value part will partially be in- verted ( inverse ) , or inverted and scaled ( negation ) with 0 < µ < 1 . The ( separate ) functional repre- sentation also remains unchanged . Figure 5 : Example of the denotation for a DCS tree with a compare relation C. This denotation has two columns , one for each active node—the root node state and the marked node size . Figure 5 : Opinion PageRank Performance with varying parameter λ ( µ = 0.5 ) Figure 2 : A Portion of the Syntactic Tree . Table 2 : Segmentation results by a pure subword-based IOB tagging . The upper numbers are of the character- based and the lower ones , the subword-based . Table 10 The superiority of our joint model on three different domains indicated by type-insensitive ( type-sensitive ) performance ( those signiﬁcant entries are marked in comparison with baseline ) . Table 1 : Training set characteristics Table 9 : Scale-up to 160K on IWSLT data sets Table 5 : Correlations of resolution class scores with respect to the average . Table 5 : Type-level English POS Tag Ranking : We list the top 5 and bottom 5 POS tags in the lexicon and the predictions of our models under the best hyperparameter setting . Table 1 : Comparison of average per-document ter- comTER with invWER on the EVAL07 GALE Newswire ( “ NW ” ) and Weblogs ( “ WB ” ) data sets . Table 1 : POS tagging of unknown words using contextual features ( accuracy in percent ) .  is a classifier that uses only contextual features ,  + baseline is the same classifier with the addition of the baseline feature ( “ NNP ” or “ NN ” ) . Figure 6 : Average number of Pareto points Table 6 : Size and percentage of overlapping relations between KnowNet versions and WN+XWN Table 4 : MRRs on the augmented candidate list . Table 1 : Accuracies ( % ) for Word-Extraction Us- Litkowski and Hargraves ( 2007 ) selected exam- ing MALT Parser or Heuristics . ples based on a search for governors8 , most anno- Table 5 Error analysis of confidence measure with and without EIV tag Table 6 : LO cosine sentence configuration scores Table 11 : Performance comparison with the literature for compound interpretation Table 2 : Analysis of the number of relevant documents out of the top 10 and the total number of retrieved documents ( up to 100 ) for a sample of queries . Figure 3 : Learning curve of SuperSense on SE2 Table 2 : The Division of LDC annotated data into training and development test sets . Table 3 : Comparison of different methods on ACE 2004 data set . P , R and F stand for precision , recall and F1 , respectively . Figure 9 The difference between the results of four settings . Figure 2 : Logical form graphs aligned with sur- face forms in two languages . Table 1 Example distributions of German verbs . Table 3 . The F-measure improvement between the BMM-based CWS and it with WSM in the MSR_C track ( OOV is 0.034 ) using a , b , and c system dictionary . Table 3 : Performance on T3 using a pre-defined tree structure . Table 1 . Overall steps of proposed method Figure 5 : Contribution of combining the three caches Table 6 : Part-of-speech annotations of the three- character strings 細柳營 xi liu ying ‘ Little Willow military camp ’ and 新豐市 xin feng shi ‘ Xinfeng city ’ . Both are ‘ strings with internal structures ’ , with nested structures that perfectly match at all three levels . They are the noun phrases that end both verses in the couplet 忽過 新豐市 , 還歸細柳營 . Figure 4 : Creation of a Lexical Transducer . The .o . operator represents the composition operation . Table 15 The training , development , and test data for English dependency parsing . aJ = argmax p ( f J , aJ | eI ) ( 8 ) 1 1 1 1 J 1 Figure 5 The merging algorithm . ( a ) How the merging algorithm works for two simple parse trees to produce a shared forest . Note that for clarity , not all constituents are expanded fully . Leaf nodes with two entries represent paraphrases . ( b ) The word lattice generated by linearizing the forest in ( a ) . Table 4 : Mixed-case TER and BLEU , and lower- case METEOR scores on Chinese NIST MT05 . Figure 4 . Active Learning with Large Corpora Table 6 : Incremental dev set results for the manually annotated grammar ( sentences of length ≤ 70 ) . Table 8 : Simple parser vs full parser – morphological quality . The parsing models were trained on the first 5,000 sentences of the training data , the morphological tagger was trained on the full training set . Table 1 Words with the highest association scores , in decreasing order , for the word “ cigarette ” , as extracted automatically . Table 1 : Linking FrameNet frames and VerbNet classes . Figure 2 A latent layered POS tag representation . Table 3 : 2 billion word corpus statistics Table 3 : Final system results ( as F1 scores ) where IM is identification of mentions and S - Setting . For more details cf . ( Recasens et al. , 2010 ) . Table 1 : Gibbs sampling algorithm for IBM Model 1 ( im- plemented in the accompanying software ) . Figure 7 : Percentage of obtaining two clusters when applying CW on n-bipartite cliques Figure 8 The effect of varying the number of extracted related words on accuracy . Figure 7 : Computing the partition function of the conditional probability P r ( S|T ) . Sema ( s1 , s2 , t ) denotes all the seman- tic role features generated by combining s1 and s2 using t . Figure 1 : All the parameters of WSMs described in Section 2 used in all our experiments . Semicolon denotes OR . All the examined combinations of parameters are implied from reading the diagram from left to right . Figure 11 : Correlation between manual and automatic scores for French-English Figure 17 : After Applying Compile-Replace to the Lower Side Figure 5 : Merging left-to-right and right-to-left hypotheses ( ef and eb ) in bidirectional decoding method . Figure 5 ( a ) merge two open hypotheses , while Figure 5 ( b ) merge them with inserted zero fer- tility words . Figure 1 : A fragment of an entailment graph ( a ) , its SCC graph ( b ) and its reduced graph ( c ) . Nodes are predicates with typed variables ( see Section 5 ) , which are omitted in ( b ) and ( c ) for compactness . Figure 5 : MTO is not sensitive to the number of random substitutes sampled per word token . Table 4 : Mutual information between feature subset and class label with f req based feature ranking . Table 2 : Statistics of datasets . Figure 3 : Time consumption of transduction . Table 1 : Evaluation of context-sensitive convolution tree kernels using SPT on the major relation types of the ACE RDC 2003 ( inside the parentheses ) and 2004 ( outside the parentheses ) corpora . Figure 5 : Our approach for detecting parallel fragments . The lower part of the figure shows the source and target sentence together with their alignment . Above are displayed the initial signal and the filtered signal . The circles indicate which fragments of the target sentence are selected by the procedure . Table 3 : Signature caseframe densities for differ- ent sets of summarizers , for the initial and update guided summarization tasks ( Study 2 ) . ∗ : p < 0.005 . Figure 2 : With the English tree and alignment provided by a parser and aligner at test time , the Chinese parser finds the correct dependencies ( see §6 ) . A monolingual parser ’ s incor- rect edges are shown with dashed lines . Figure 2 : Graphical model of synonym pair gen- erative process Figure 3 The lattice for the Hebrew sequence ! ‫ ( בצ‌לם הנ‌עים‬see footnote 19 ) . Table 4 : Dataset statistics : development ( dev ) and test . Figure 1 : Architecture of Name-aware Machine Translation System . Figure 3 : Boxer output for Shared Task Text 2 Table 2 : Accuracy for Unsupervised , Bilingual , Wiktionary and Supervised models . Avg . is the average of all lan- guages except English . Unsupervised models are trained without dictionary and use an oracle to map tags to clusters . Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments . The Projection model uses a dictionary build directly from the part-of-speech projection . The D & P model extends the Projection model dictionary by using Label Propagation . Supervised models are trained using tree bank information with SHMM-ME : Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary and All TBD uses tree bank tag sets for all words . 50 , 100 and All Sent . models are trained in a supervised manner using increasing numbers of training sentences . Figure 5 : Comparison of MERT , PRO , and MIRA on tuning Urdu-English SBMT systems , and test results at every iteration . PRO performs comparably to MERT and MIRA . Table 1 : Evaluation of the manual annotation improvement - summarization ratio : 15 % . Figure 9 : Bi-partite neighboring co-occurrence graph ( a ) and second-order graph on neighboring co-occurrences ( b ) clustered with CW . Table 1 : Training , tuning , and test conditions Figure 2 : Fraction of the sentences that were transduced . Table 1 : Diacritized particles and pseudo-verbs that , after orthographic normalization , have the equivalent surface form 0 an . Table 1 : Size of Seed Lexicons Table 3 : Evaluation of the Turkish n-gram model . Table 5 : Evaluation of 100 randomly sampled variation nuclei types . Table 2 : Results from the empirical evaluation , including the Bayesian model without PoS tags ( Base- line ) , the alternating alignment-annotation algorithm ( AAA ) , the corresponding method but with super- vised PoS taggers for both languages ( Supervised ) , and comparable previous results on the same data . The number of alignment links |A| , of which |A ∩ S| are considered ( S ) ure , and |A ∩ P | ( P ) ossible , are reported . For convenience , precision ( P ) , recall ( R ) , F1 score ( F ) and Alignment Error Rate ( AER ) are also given . Table 2 : The 10 best languages for R EDDY using LCS . Figure 1 : Effect of model parameters on performance . The algorithm takes into account possibly unaligned words at the boundaries of the source or target language phrases . Table 8 shows the effect of the length of the language model history on translation quality . Table 4 : Number of recall errors according to mention type ( rows anaphor , columns antecedent ) . E.g. , the city Fez , Mo rocco ( figure 1 ) was tagged as a single LOCATION by one annotator and as two by the other . Figure 2 shows the labeled dependency graph of example ( 2 ) , taken from Talbanken05 . Table 2 shows the distribution of character type sequences that constitute the infrequent words in the EDR corpus . W can be encoded by an undirected graph G ( Figure 2 ( a ) ) , where the nouns are mapped to vertices and Wij is the edge weight between vertices i and j The results for French to English and for English to French are shown in Table 10 Figure 4 shows a constituent headed by a process nominal with an embedded adjective phrase . The TnT tagger achieves 86.3 % accuracy on the default tagset . Table 1 reports experimental results using lexical features only ; we observe that the stemming n-gram features boost the performance by one point ( 64.7 vs. 65.8 ) . We obtain the following decision rule : eI = argmax Pr ( eI | f J ) 1 1 1 I 1 M ) = argmax m hm ( eI , f J ) 1 1 I m=1 Probabilities We find that Equation ( 7 ) assigns too little proba bilities to long words ( 5 or more characters ) . Table 2 : Distributions of Morph Examples Figure 6 shows example sentences annotated by HGFC . Table 6 : Sample targets for meta alternations with high AP and mid-coherence values . If it starts with a lower case letter , and contains both upper and lower case letters , then ( mixedCaps , zone ) is set to 1 . he largest effect seems to come from taking into account the bigram dependence , which achieves an mWER of 32.9 % The renormalization needed in equation ( 3 ) requires a sum over manypossible sentences , for which we do not know of an efficient algorithm Figure 1 : Organisation of the hierarchical graph of concepts Following previous semantic noun classification experiments ( Pantel and Lin , 2002 ; Bergsma et al. , 2008 ) , we use the grammatical relations ( GRs ) as features for clustering . alignment models Pr ( f J , aJ | eI ) , So we estimate that English translations are present in the English part of the corpus for Table 2 . and 8 show word accuracy for Chasen , Juman , and our algorithm for parameter settings optimizing word precision , recall , and F-measure rates . An extract of the results is listed in table 1 Figure 6 : Metaphors tagged by the system ( in bold ) whereby the main source of disagreement was the presence of lexicalized metaphors , e.g . verbs such as impose , decline etc . and ap plied to an Arabic sentence in figure 1 . Table 1 : Approximate times in seconds to generate predictions of maximum word sequence length M , on a 1.2GHz processor , for the MEMD model . Table 2 shows a comparison with some existing toolkits that build deterministic , minimized automata/transducers . Tree setups P ( % ) R ( % ) F CS-SPT over SPT3 1.5 1.1 1.3 DSPT over SPT 1.1 5.6 3.8 UPST ( FPT ) over SPT 3.8 10.9 8.0 Table 3 . Table 1 : Examples of word bigrams including un known word tags example The bipartite graph K also induces a similarityagain ( Figure 1 ( e ) ) . W can be encoded by an undirected graph G ( Figure 2 ( a ) ) , where the nouns are mapped to vertices and Wij is the edge weight between vertices i and j In MUC6 , the best result is achieved by SRA ( Krupka , 1995 ) . For MUC6 , the reduction in error due to global features is 27 % , and for MUC7,14 % . ( 2004 ) makes use of a coding manual designed for a project studying genitive modification ( Garretson et al. , 2004 ) and presents an explicit annotation scheme for an _ Samma _ PO _ KP erfarenhet NN _ gjorde VV PT engelsmannen NN DD|HH imacy , illustrated by figure 1 . Table 5 : Meta alternations and their average precision values for the task . # e is the total number of English translation candidates in the period . An abridged version of the grammatical representation produced by the implemented grammar for this sentence is presented in Figure 1 , where the feature structures below the tree correspond to partial grammatical representations of the constituents 16 See Kamp and Reyle ( 1993 ) for a comprehensive rendering of DRT , and Branco ( 2000 , Chapter 5 ) for an . The unknown parameters are determined by maximizing the likelihood on the parallel training corpus : Equation ( 12 ) , which is a set of word models trained for each part of speech ( POS + Poisson + bigram ) . More re cently , the task of automatic supersense tagging has emerged for English ( Ciaramita and Johnson , 2003 ; Curran , 2005 ; Ciaramita and Altun , 2006 ; Paa and Reichartz , 2009 ) , as well as for Italian ( Picca et al. , 2008 ; Picca et al. , 2009 ; Attardi et al. , 2010 ) and Chinese ( Qiu et al. , 2011 ) , languages with WordNetsmapped to English WordNet.3 In principle , we be lieve supersenses ought to apply to nouns and verbsin any language , and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs , summarized in figure 2 Thus , we have crafted more specific explanations , sum marized for nouns in figure 2 . Reduction of conjuncts for NP coordination Figure 1 . An abridged version of the grammatical representation produced by the implemented grammar for this sentence is presented in Figure 1 , where the feature structures below the tree correspond to partial grammatical representations of the constituents 16 See Kamp and Reyle ( 1993 ) for a comprehensive rendering of DRT , and Branco ( 2000 , Chapter 5 ) for an . The Stanford parser ( Klein and Manning , 2002 ) is unable to recover the verbal reading of the unvocalized surface form 0 an ( Table 1 ) . An extract of the results is listed in table 1 On the contrary , in the above training stage , although the samples are not accurate enough to represent the distribution defined by Equation 7 for each alignment aj , it is accurate enough for computing the expected counts , which are defined at the corpus level . Table 1 : Scores for CityU corpus Table 5 : Meta alternations and their average precision values for the task . As shown in Figure 8 , relative word performance was not degraded and sometimes even slightly better . First , the source sentence words f J are grouped into phrases f K . For each phrase f an 1 1 alignment template z is chosen and the sequence of chosen alignment templates is reordered ( according to K ) . Among all possible target sentences , we will choose the sentence with the highest probability However , if we remove the mouse-node from its local graph illustrated in figure 1 , the graph decomposes into two parts , one representing the electronic device meaning of mouse and the other one representing its animal sense . Table 2 shows the experimental results with and without the stem n-grams features . For example , look ing at Figure 2 ( b ) , V on G can be grouped into three clusters u1 , u2 and u3 . Figure 3 shows the distribution of SSTs in the corpus . In fact , from the last column of Figure 8 we see that even if our algorithm has access to only five anno tated sequences when Juman has access to ten times as many , we still achieve better precision and better F measure . Figure 3 : Accuracy of part of speech estimation each part of speech and word type ( POS + WT + Poisson + bigram ) . If we compare the error rates in Table 7 , which correspond to about 55 search errors in Table 6 , we obtain an mWER of 36.7 % ( 53 search errors ) using no heuristic function and an mWER of 32.6 % ( 57 search errors ) using the combined heuristic function . The unknown parameters are determined by maximizing the likelihood on the parallel training corpus : # Cor is the number of correct English translations output . For example , look ing at Figure 2 ( b ) , V on G can be grouped into three clusters u1 , u2 and u3 . This sum can be computed efficiently using the algorithm shown in Figure 8 14http : //www.cis.upenn.edu/ dbikel/software.html Gold standard Automatic UAS LAS UAS LAS Baseline 89.87 84.92 89.87 84.92 Anim 89.81 84.94 89.87 84.99 Table 5 : Overall results in experiments with automatic features compared to gold standard features , expressed as unlabeled and labeled attachment scores . Table 1 evaluates the contributions of different kinds of constituent dependencies to extraction performance on the 7 relation types of the ACE RDC 2004 corpus Table 4 shows the performance on the test data . We compare in Table 2 the performance of Unified Parse and Semantic Trees with different kinds of Entity Semantic Tree setups using standard convolution tree kernel , while the SPT and DSPT with only entity-type information are listed for reference . Table 3 : Performance of the mention detection system including all ACE 04 subtasks Figure 4 shows an example of calculating the target side SRS based on a complicated TTS template . Except our own and MENE + reference resolution , the results in Table 6 are all official MUC7 results . We also investigated the effect of varying M . The results are shown in Table 2 . Table 1 , Figure 1 , and Figure 2 shows the AER results for different models . Table 2 shows the correctness evaluation results . The statistics of 96 these splits are shown in Table 2 . We include a list of per-category results for selected phrasal labels , POS tags , and dependencies in Table 8 . Table 3 : Precision statistics for pronouns . Table 3 : Examples of common character bigrams for each part of speech in the infrequent words pa rt of sp ee ch ch ar ac ter bi gr a m fre qu en cy no un nu m be r a dj e ct iv al v er b v er b ad je cti ve ad ve rb < e o w > < b o w > 1 S `` J < e o w > I t < e o w > L < e o w > < e o w > 13 43 4 8 4 3 2 7 2 1 3 69 63 resented all unknown words by one length model . Table 4 : Number of recall errors according to mention type ( rows anaphor , columns antecedent ) . Based on this experiment , we set the beam size of SegTagDep to 64 throughout the exper 64 96.28 92.37 74.96 0.48 Table 3 : F1 scores and speed ( in sentences per sec . ) The dataset ( table 1 ) consists of the main text of 28 articles selected from the topical domains of history , sports , science , and technology Table 5 shows that this reimplementation almost reproduces the accuracy of their implementation . The statistics of 96 these splits are shown in Table 2 . As an example , the probability of accepting the prediction in figure 1 is about .25 . L set of lemmas IL set of ( lemma-wise ) instances SL set of ( lemma-wise ) senses inst : L ( IL ) mapping lemma instances sns : L ( SL ) mapping lemma senses M set of meta senses meta : SL M mapping senses meta senses A M M set of meta alternations ( MAs ) A set of MA representations score : A S2 R scoring function for MAs repA : A A MA representation function comp : A S2 R compatibility function Table 1 : Notation and signatures for our framework . See Figure 3 for examples . Figure 3 shows the performance and processing time comparison of various models and their combinations . The results when we set M = 10 are shown in Table 1 . The first model is Equation ( 5 ) , which is the combina . tion of Poisson distribution and character zerogram Table 4 shows a confusion matrix for the classification of the nouns . Table 2 summarizes the results obtained with different taggers and tagsets on the development data . Tree setups P ( % ) R ( % ) F CS-SPT over SPT3 1.5 1.1 1.3 DSPT over SPT 1.1 5.6 3.8 UPST ( FPT ) over SPT 3.8 10.9 8.0 Table 3 . Table 8a shows that the best model recovers SBAR at only 71.0 % F1 . Our experimental data was drawn from 150 megabytes of 1993 Nikkei newswire ( see Figure I ) . By introducing the distinction of word type to the model of Equation ( 12 ) , we can derive a more sophis ticated unknown word model that reflects both word 3 When a Chinese character is used to represent a seman tically equivalent Japanese verb , its root is written in the Chinese character and its inflectional suffix is written in hi ragana . As illus trated in Figure 1 ( e ) , the NP coordination in the Qian et al . Table 1 reports experimental results using lexical features only ; we observe that the stemming n-gram features boost the performance by one point ( 64.7 vs. 65.8 ) . On the other hand , using our method of combining both sources of information and setting M = ∞ , 19 Chinese words ( i.e. , the first 22 Chinese words in Table 3 except 巴佐亚 , 坩埚 , 普利法 ) have their correct English translations at rank one position . MENE has only been tested on MUC7 . Table 2 : Results for different predictor configurations . > 10 nouns ( a ) ( b ) classified as 222 125 ( a ) class animate 49 3390 ( b ) class inanimate Table 4 : Confusion matrix for the MBLclassifier with a general feature space on the > 10 data set on Talbanken05 nouns . Figure 2 : Collecting evidence for a word boundary - are the non-straddling n-grams 8 1 and 82 more frequent than the straddling n-grams t 1 , t2 , and t3 ? e suggest the use of a log-linear model to incorporate the various knowledge sources into an overall translation system and to perform discriminative training of the free model parameters segmentation ( Table 2 ) . Table 3 presents the performance in terms of precision , recall , and F- measure of the whole system . Table 1 underscores the virtues of Sentence Recency : In the most recent sentence with antecedents satisfying the filters , there are on aver ble . For fair comparison , we have tabulated all results with the size of training data used ( Table 5 ( Abbreviations are listed in Table 2 . ) As shown in figure 3 , read times are much higher for predictions that get accepted , re B ( x , k , a ) = R1 ( x ) + T ( x , k ) E ( x , k ) , a = 1 R0 ( x ) , a = 0 flecting both a more careful perusal by the translator and the fact the rejected predictions are often simplywhere Ra ( x ) is the cost of reading x when it ulignored.2 In both cases there is a weak linear rela timately gets accepted ( a = 1 ) or rejected ( a = 0 ) , T ( x , k ) is the cost of manually typing xk , and E ( x , k ) is the edit cost of accepting x and erasing to the end of its first k characters . Figure 1 shows a probability estimation tree for the prediction of the probability of the nominative attribute of nouns . The breakdown of the different types of words found by ST in the test corpus is given in Table 3 . Figure 6 gives an overview of the decisions made in the alignment template model . Tables 4 and 5 show the effect of the pruning parameter tp with the histogram pruning parameter Np = 50,000 For MUC6 , the reduction in error due to global features is 27 % , and for MUC7,14 % . Table 1 : Feature templates for the full joint model . Then , every phrase f produces its translation e ( using the corresponding alignment template z ) . Table 1 The cost as a novel given name ( second position ) for hanzi from various radical classes . The annotation manual ( Teleman , 1974 ) states that a markable should be tagged as human ( H H ) if it may be replaced by the interrogative pronoun vem who and be referred to by the personal pronouns han he or hon she .There are clear similarities between the anno tation for human reference found in Talbanken05 and the annotation scheme for animacy discussed HUM Other animate Inanimate ORG ANIM CONC NCONC TIME PLACE Figure 1 : Animacy classification scheme ( Zaenen et al. , 2004 ) Figure 2 shows examples of the discovered patterns for the merger and acquisition topic . As can be seen in Table 4 , our training data is a lot less than those used by MENE and IdentiFinder3 . U = { up } m represent the hidden m struct a new graph G1 ( Figure 1 ( d ) ) with the clusters U as vertices . Figure 2 : Bootstrapping new heuristics . The list of the features used in our joint model is presented in Table 1 , where S01 S05 , W01 W21 , and T01 05 are taken from Zhang and Clark ( 2010 ) , and P01 P28 are taken from Huang and Sagae ( 2010 ) . As Table 1 shows , word bigrams whose infrequent word bigram To illustrate how SRF impacts the translation results , Figure 8 gives 3 examples of the MT outputs with and without the SRFs Figure 9 shows that in fact both contribute to producing good segmentations . Table ( 1 ) and Eq . W can be encoded by a undi rected graph G ( Figure 1 ( a ) ) , where the verbs are mapped to vertices and the Wij is the edge weight between vertices i and j . The second factor of Equation ( 13 ) is estimated from the Poisson distribution whose parameter For the Verbmobil task , we train the model parameters M according to the maximum class posterior probability criterion ( equation ( 4 ) ) . Table 1 evaluates the contributions of different kinds of constituent dependencies to extraction performance on the 7 relation types of the ACE RDC 2004 corpus In the table , Nc indicates the number of clusters in the inferred tree , while Nl indicates the closest match to the number of classes in the gold standard . For example , looking at Figure 1 ( b ) , V on G can be grouped into three clusters u1 , u2 and u3 . Figure 2 : Decoding algorithm for the standard Tree-to-String transducer . In Table 1 , period 1 is Jul 01 – Jul 15 , period 2 is Jul 16 – Jul 31 , … , period 12 is Dec 16 – Dec 31 . Table 2 : Distributions of Morph Examples # c is the total number of new Chinese source words in the period However , if we remove the mouse-node from its local graph illustrated in figure 1 , the graph decomposes into two parts , one representing the electronic device meaning of mouse and the other one representing its animal sense . The results when we set M = 10 are shown in Table 1 . In these experiments , the input lacks segmentation markers , hence the slightly different dev set baseline than in Table 6 . The corresponding figures for the test data are . 89.53 % for our tagger and 88.88 % for the TnT tag- ger . Only tokens with initCaps not found in commonWords are tested against each list in Table 2 . composite kernel 83.0 72.0 77.1 Zhou et al. , ( 2007 ) : composite kernel 82.2 70.2 75.8 Zhang et al. , ( 2006 ) : composite kernel 76.1 68.4 72.1 Zhao and Grishman , ( 2005 ) :4 composite kernel 69.2 70.5 70.4 Ours : CTK with UPST 80.1 70.7 75.1Zhou et al. , ( 2007 ) : context sensitive CTK with CS-SPT 81.1 66.7 73.2 Zhang et al. , ( 2006 ) : CTK with SPT 74.1 62.4 67.7 Table 4 . Table 1 shows the effect of the role-based preference on our data . The performance of these systems is shown in Table 1 . This approach has been suggested by Papineni , Roukos , and Ward ( 1997 , 1998 ) for a natural language understanding task . Table 8 shows the tagging accuracy of unknown words . Figure 1 shows the record for the headword orange followed by its collocates we directly model the posterior probability Pr ( eI| f J ) Tables 6 and 7 show the effect of the pruning pa rameter Np with the pruning parameter tp = 10 12 Tables 6 and 7 show the effect of the pruning pa rameter Np with the pruning parameter tp = 10 12 Figure 1 shows the absolute frequencies of sen tence recency values when only the most recent antecedent ( in the order just stated ) is considered . Figure 1 : Illustration of dictionary based segmenta tion finite state transducer 3.1 Bootstrapping . Figure 2 : Above : The complete supersense tagset for nouns ; each tag is briefly described by its symbol , NAME , short description , and examples . Cik Figure 1 : Local graph of the word mouse As a result , Arabic sentences are usually long relative to English , especially after Figure 4 shows a constituent headed by a process nominal with an embedded adjective phrase . The accuracy of a baseline tagger which chooses the most probable tag9 ignoring the context is 67.3 % without and 69.4 % with the supple 92.3 92.2 92.1 92 91.9 91.8 91.7 91.6 91.5 91.4 2 3 4 5 6 7 8 9 10 mentary lexicon . Table 4 : Effect of Arabic stemming features on coreference resolution . The dataset ( table 1 ) consists of the main text of 28 articles selected from the topical domains of history , sports , science , and technology As shown in Table 3 , using just context information alone , 10 Chinese words ( the first 10 ) have their correct English translations at rank one position . The tagset refinement increases the accuracy by about 0.6 % , and the external lexicon by another 3.5 % . Table 3 shows the results of both unconstrained and constrained versions of HGFC and those of AGG on the test set T3 ( where singular classes are removed to enable proper evaluation of the constrained method ) . Recall is somewhat difficult to estimate because we do not know whether the English translation of a Chinese word appears in the English part of the corpus . Table 4 Differences in performance between our system and Wang , Li , and Chang ( 1992 ) . able 1 shows the bilingual phrases containing between two and seven words that result from the application of this algorithm to the alignment of Figure 2 . The breakdown of the different types of words found by ST in the test corpus is given in Table 3 . Table 1 : Comparison against Stevenson and Joanis ( 2003 ) s result on T1 ( using similar features ) . We also investigated the effect of varying M . The results are shown in Table 2 . This sum can be computed efficiently using the algorithm shown in Figure 8 Table 1 : Performance of the mention detection system using lexical features only . the time-consuming renormalization in equation ( 3 ) is not needed in search Table 2 : CoreLex s basic types with their corresponding WordNet anchors . Table 2 : Distributions of Morph Examples In Figure 1 ( c ) we show a sentence one of about 500 people nominated for , where there exists a DISC relationship between the entities one and people Considering that the way the semantic where all ( T ) denotes all the possible target strings which can be generated from the source tree T . Given a set of TTS templates , the new partition function can be efficiently computed using the dynamic programming algorithm shown in Figure 7 . The dataset ( table 1 ) consists of the main text of 28 articles selected from the topical domains of history , sports , science , and technology Table 1 , Figure 1 , and Figure 2 shows the AER results for different models . Table 3 : Performance of the mention detection system including all ACE 04 subtasks Figure 6 gives an overview of the decisions made in the alignment template model . Table 7 shows the results . The tagset refinement increases the accuracy by about 0.6 % , and the external lexicon by another 3.5 % . Table 2 gives the mean values for the comparison of troughs placed by the segmentation algorithm to the segmentation points identified by the test subjects for all the texts We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4 , 5 , and 6 . Figure 3 : Performance of baseline and joint models w.r.t . the average processing time ( in sec . ) To differentiate between the coordinating and discourse separator functions of conjunctions ( Table 3 ) , we mark each CC with the label of its right sister ( splitCC ) . The list of the features used in our joint model is presented in Table 1 , where S01 S05 , W01 W21 , and T01 05 are taken from Zhang and Clark ( 2010 ) , and P01 P28 are taken from Huang and Sagae ( 2010 ) . See Table 1 for details . It should be emphasized that this constraint to consecutive phrases limits the expressive power . Table 2 shows the correctness evaluation results . # e is the total number of English translation candidates in the period . The bipartite graph K also induces a similarityagain ( Figure 1 ( e ) ) . W can be encoded by a undi rected graph G ( Figure 1 ( a ) ) , where the verbs are mapped to vertices and the Wij is the edge weight between vertices i and j . We compared the ATB5 to tree- banks for Chinese ( CTB6 ) , German ( Negra ) , and English ( WSJ ) ( Table 4 ) The baseline system in Table 3 refers to the maximum entropy system that uses only local features . As the search space increases expo nentially , it is not possible to explicitly represent it . MENE has only been tested on MUC7 . The results for French to English and for English to French are shown in Table 10 Cik Figure 1 : Local graph of the word mouse Figure 2 shows that the tagging accuracy tends to increase with the context size . Figure 4 : An example showing how to compute the target side position of a semantic role by using the median of its aligning points . Another interesting example is shown in Figure 1 ( b ) , where the base-NP of the second entity town is a possessive NP and there is no relationship between the entities one and town defined in the ACE corpus . This corresponds to maximizing the equivocation or maximizing the likelihood of the direct-translation model Probabilities We find that Equation ( 7 ) assigns too little proba bilities to long words ( 5 or more characters ) . Table 1 , Figure 1 , and Figure 2 shows the AER results for different models . able 2 shows the corpus statistics for this task . In the last two lines of Equation 3 , φǫ and each P ( f |e ) = `` £s c ( f |e ; f ( s ) , e ( s ) ) ( 4 ) φi are not free variables , but are determined by f s c ( f |e ; f ( s ) , e ( s ) ) the alignments . Table 2 shows the manual evaluation results based on the entire test set , and the improvement from SRF is significant at p < 0.005 based on a t-test . Figure 2 shows examples of the discovered patterns for the merger and acquisition topic . Fi gure 7 depic ts the comp atible brack ets and all comp atible brack ets rates . Table 2 : Results for different predictor configurations . If the token starts with a capital letter ( initCaps ) , then an additional feature ( init- Caps , zone ) is set to 1 The overall architecture of the statistical translation approach is summarized in Figure 1 . A standard criterion on a parallel training corpus consisting of S sentence pairs { ( fs , es ) : s = 1 , . PRO ( c ) Entity-Paired Tree ( EPT ) Figure 2 . The results are displayed in Table 3 . Table 2 Similarity matrix for segmentation judgments . As can be seen in Table 4 , our training data is a lot less than those used by MENE and IdentiFinder3 . U = { up } m represent the hidden m struct a new graph G1 ( Figure 1 ( d ) ) with the clusters U as vertices . Table 3 Classes of words found by ST for the test corpus . This list of 43 words is shown in Table 3 . The overall architecture of the log-linear modeling approach is summarized in Figure 1 Table 3 Classes of words found by ST for the test corpus . Although kanji sequences are difficult to seg ment , they can comprise a significant portion of Japanese text , as shown in Figure 1 . With an absolute frequency threshold of 10 , we obtain an accuracy of 95.4 % , which constitutes a 50 % reduction of error rate.Table 3 presents the experimental results rela tive to class . Table 5 shows the cross entropy per word and char acter perplexity of three unknown word model . Figure 1 : Illustration of dictionary based segmenta tion finite state transducer 3.1 Bootstrapping . Figure 3 : An example showing the combination of the semantic role sequences of the states . # Cor is the number of correct English translations output . The baseline system in Table 3 refers to the maximum entropy system that uses only local features . Table 5 Performance on morphological analysis . Table 3 shows the results of an evaluation based on the plain STTS tagset . As can be seen in figure 2 , wing `` part of a bird '' is closely related to tail , as is wing `` part of a plane '' Results are shown in Table 2 ; we see that better word alignment results do not lead to better translations . For example , no synset covers any combinations of the main words in Figure 2 , namely buy , acquire and merger For example , in the sentence bought one of town s two meat- packing plants as illustrated in Figure 1 ( a ) , the constituents before the headword plants can be removed from the parse tree . Figure 4 shows examples of alignment templates using the convolution parse tree kernel as depicted in Figure 1 . even after removal of the wing-node , the two areas of meaning are still linked via tail See Table 1 for details . Except our own and MENE + reference resolution , the results in Table 6 are all official MUC7 results . Figure 1 shows the absolute frequencies of sen tence recency values when only the most recent antecedent ( in the order just stated ) is considered . Table 3 : Precision statistics for pronouns . Thetheoretical upper bound of the decoding complex Figure 5 : Decoding algorithm using semantic role features . In Figure 4 we show an example of variation between the parsing models . Finally , Table 4 shows the results for the unconstrained HGFC on T2 and and T3 when the tree structure is not predefined but inferred automatically as described in section 3.2.3 . The overall architecture of the statistical translation approach is summarized in Figure 1 . Figure 7 : Computing the partition function of the conditional probability P r ( S|T ) . Table 1 : Feature templates for the full joint model . This leaves us with 60 meta alternations , shown in Table 5 . Table 1 shows the bilingual phrases containing between two and seven words that result from the application of this algorithm to the alignment of Figure 2 see table 3 ) as the impact of soft constraints is the weakest for the constrained method at this level . We also propose to use the features U01 U03 , which we found are effective to adjust the character Figure 1 : Illustration of the alignment of steps . Table 11 gives an overview on the training and test data . The alignment aJ that has the highest probability ( under a certain model ) is also called the Viterbi alignment ( of that model ) 14http : //www.cis.upenn.edu/ dbikel/software.html Gold standard Automatic UAS LAS UAS LAS Baseline 89.87 84.92 89.87 84.92 Anim 89.81 84.94 89.87 84.99 Table 5 : Overall results in experiments with automatic features compared to gold standard features , expressed as unlabeled and labeled attachment scores . Table 1 : Examples of word bigrams including un known word tags example Table 6 : Final results on CTB6 and CTB7 accuracies of POS tagging and dependency parsing were remarkably improved by 0.6 % and 2.4 % , respectively corresponding to 8.3 % and 10.2 % error reduction . At the morp heme level , stems are divid ed from their affixe s. For exam ple , altho ugh both naga no ( Naga no ) and shi ( city ) can appea r as indivi dual words , nagano shi ( Nag ano city ) is brack eted as [ [ naga no ] [ s hi ] ] , since here shi Figure 3 : Determining word boundaries . ( prec2 in Table 8 ) For the graph depicted in Figure 1 this algorithm computes the clusters { They , Leaders } , { Paris } and { recent developments } . Improvements of different tree setups over SPT on the ACE RDC 2004 corpus Finally , Table 4 compares our system with other state-of-the-art kernel-based systems on the 7 relation types of the ACE RDC 2004 corpus . Figure 5 shows the decoding algorithm incorporating the SRR features . Table 5 shows type- and token-level error rates for each corpus . E.g . rat and printer are very different in meaning , but they are both closely related to different meanings of mouse Comparison of different systems on the ACE RDC 2004 corpus In Table 3 we summarize the improvements of different tree setups over SPT . the time-consuming renormalization in equation ( 3 ) is not needed in search We choose t = 1 , 5 , and 30 for the fertility HMM For instance , the gain for the prediction in figure 1 would be 2 7 8 = 6 . The use of the language model feature in equation ( 18 ) helps take long-range dependencies better into account Table 1 underscores the virtues of Sentence Recency : In the most recent sentence with antecedents satisfying the filters , there are on aver ble . A natural unit for B ( x , k , a ) is the number of keystrokes saved , so all elements of the above equation are converted to this measure . The entity features can be attached under the top node , the entity nodes , or directly combined with the entity nodes as in Figure 1 . Figure 2 plots AP by for all meta alternations . Tables 6 and 7 show the effect of the pruning pa rameter Np with the pruning parameter tp = 10 12 A natural unit for B ( x , k , a ) is the number of keystrokes saved , so all elements of the above equation are converted to this measure . In these experiments , the input lacks segmentation markers , hence the slightly different dev set baseline than in Table 6 . Table 11 gives an overview on the training and test data . Figure 3 show the training time for different models . # c is the total number of new Chinese source words in the period The third model is Equation ( 11 ) , which is a set of word models trained for each word type ( WT +Poisson+ bigram ) . For example , if a token starts with a capital letter and ends with a period ( such as Mr. ) , then the feature InitCapPeriod is set to 1 , etc Table 3 lists new conceptsthat CAM introduces to manipulate vector represen tations . Figure 4 shows an example of calculating the target side SRS based on a complicated TTS template . The TnT tagger achieves 86.3 % accuracy on the default tagset . Tables 4 and 5 show the effect of the pruning parameter tp with the histogram pruning parameter Np = 50,000 . Table 4 shows the performance on the test data . By introducing the distinction of word type to the model of Equation ( 12 ) , we can derive a more sophis ticated unknown word model that reflects both word 3 When a Chinese character is used to represent a seman tically equivalent Japanese verb , its root is written in the Chinese character and its inflectional suffix is written in hi ragana . If we compare the error rates in Table 7 , which correspond to about 55 search errors in Table 6 , we obtain an mWER of 36.7 % ( 53 search errors ) using no heuristic function and an mWER of 32.6 % ( 57 search errors ) using the combined heuristic function ( Equation ( 7 ) ) ( Poisson + hi gram ) . Figure 4 shows examples of alignment templates Recall is somewhat difficult to estimate because we do not know whether the English translation of a Chinese word appears in the English part of the corpus . Table 3 illustrates the effects of different components of the user model by showing results for simulated users who read infinitely fast and accept only predictions having positive benefit ( superman ) ; who read normally but accept like superman ( rational ) ; and who match the standard user model ( real ) . Figure 5 shows our morpheme accuracy results . Table 1 : Effect of Factors antecedent is found in the previous context , subsequent sentences are inspected ( cataphora ) , also ordered by proximity to the pronoun . Figure 1 : Illustration of dictionary based segmenta tion finite state transducer 3.1 Bootstrapping . Table 3 shows the performance and speed of the full joint model ( with no dictionaries ) on CTB5c1 with respect to the beam size . see table 3 ) as the impact of soft constraints is the weakest for the constrained method at this level . Table 1 : Snapshot of the supersense-annotated data . In the table , Nc indicates the number of clusters in the inferred tree , while Nl indicates the closest match to the number of classes in the gold standard . As shown in figure 1 , a similarity matrix W models one-hop transitions that follow the links from vertices to neighbors . Figure 1 shows examples of the feature SRR . n all four tables , we provide theresults for using no heuristic functions and three variants of an increasingly infor mative heuristic function . Table 2 presents the distributions of some examples of morphs and their targets in English Twitter and Chinese Sina Weibo . The renormalization needed in equation ( 3 ) requires a sum over manypossible sentences , for which we do not know of an efficient algorithm The second condition is necessary to allow for single-character words ( see Figure 3 ) . We see that the language model perplexity improves from 4,781 for a unigram model to 29.9 for a trigram model . Table 8 shows that by using word type and part of speech information , recall is improved from 28.1 % to 40.6 % and precision is improved from 57.3 % to 64.1 % . The second model is Equa tion ( 13 ) , which is a set of word models trained for Figure 2 shows the labeled dependency graph of example ( 2 ) , taken from Talbanken05 . E.g . rat and printer are very different in meaning , but they are both closely related to different meanings of mouse We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4 , 5 , and 6 . Table 1 shows the usefulness evaluation result . Table 9 shows the training and test corpus statistics . see table 3 ) as the impact of soft constraints is the weakest for the constrained method at this level . But it conflates the coordinating and discourse separator functions of wa ( < ..4.b � � ) into one analysis : conjunction ( Table 3 ) . For the graph depicted in Figure 1 this algorithm computes the clusters { They , Leaders } , { Paris } and { recent developments } . Table 1 , Figure 1 , and Figure 2 shows the AER results for different models . Table 3 presents the performance in terms of precision , recall , and F- measure of the whole system . In our segmentation system , a hybrid strategy is applied ( Figure 1 ) : First , forward maximum matching ( Chen and Liu , 1992 ) , which is a dictionary-based method , is used to generate a segmentation result . An extract of the results is listed in table 1 Contribution of constituent dependencies in respective mode ( inside parentheses ) and accumulative mode ( outside parentheses ) The table shows that the final DSPT achieves the best performance of 77.4 % /65.4 % /70.9 in precision/recall/F-measure respectively after applying all the dependencies , with the increase of F-measure by 8.2 units compared to the baseline MCT . As shown in Figure 8 , relative word performance was not degraded and sometimes even slightly better . For the Verbmobil task , we train the model parameters M according to the maximum class posterior probability criterion ( equation ( 4 ) ) . The translations of 6 of the 43 words are words in the dictionary ( denoted as “ comm. ” in Table 3 ) and 4 of the 43 words appear less than 10 times in the English part of the corpus ( denoted as “ insuff ” ) . Figure 1 : Example of a prediction for English to French translation . However , this information is hard to extract reliably from the available data ; and even if were obtainable , many of the 0.3 0.2 0.1 0 60 50 40 30 20 10 0 10 20 30 40 50 60 gain ( length of correct prefix length of incorrect suffix ) Figure 2 : Probability that a prediction will be accepted versus its gain . ( 2011 ) , we confirmed that omission of the look-ahead features results in a 0.26 % decrease in the parsing accuracy on CTB5d ( dev ) .Figure 2 : F1 scores ( in % ) of SegTagDep on CTB 5c1 w.r.t . the training epoch ( x-axis ) and parsing feature weights ( in legend ) . Table 8 shows the tagging accuracy of unknown words . The TnT tagger achieves 86.3 % accuracy on the default tagset . Figure 2 shows empirical estimates of p ( a = 1|2k l ) from the TransType data . Also , the CRF model using maximum subword-based tagging ( Zhang et al. , 2006 ) and the CRF model using minimum subword-based tagging , both of which are statistical methods , are used individually to solve the Figure 1 : Outline of the segmentation process 2.1 Forward Maximum Matching . Figure 1 Architecture of the statistical translation approach based on Bayes decision rule . As each global feature group is added to the list of features , we see improvements to both MUC6 and Another interesting example is shown in Figure 1 ( b ) , where the base-NP of the second entity town is a possessive NP and there is no relationship between the entities one and town defined in the ACE corpus . Table 6 : Sample targets for meta alternations with high AP and mid-coherence values . Table 2 : Scores for MSRA corpus Throughout in this paper , we used Equation ( 9 ) to compute the word spelling probabilities . The points labelled smoothed in figure 2 were obtained using a sliding-average smoother , and the model curve was obtained using two-component Gaussian mixtures to fit the smoothed empirical likelihoods p ( gain|a = 0 ) and p ( gain|a = 1 ) . igure 1 gives an example . Table 1 The cost as a novel given name ( second position ) for hanzi from various radical classes . Figure 5 shows the decoding algorithm incorporating the SRR features . where ECD|S , T ( fi ) , the expected count of a feature over all derivations given a pair of tree and string , can be computed using the modified inside- outside algorithm described in Section 3.2 , and ECS |T ( fi ) , the expected count of a feature over all possible target strings given the source tree , can be computed in a similar way to the partition function described in Figure 7 . The performance of these systems is shown in Table 1 . W can be encoded by an undirected graph G ( Figure 2 ( a ) ) , where the nouns are mapped to vertices and Wij is the edge weight between vertices i and j Table 1 : Output of word sense clustering . E.g . rat and printer are very different in meaning , but they are both closely related to different meanings of mouse As shown in figure 3 , read times are much higher for predictions that get accepted , re B ( x , k , a ) = R1 ( x ) + T ( x , k ) E ( x , k ) , a = 1 R0 ( x ) , a = 0 flecting both a more careful perusal by the translator and the fact the rejected predictions are often simplywhere Ra ( x ) is the cost of reading x when it ulignored.2 In both cases there is a weak linear rela timately gets accepted ( a = 1 ) or rejected ( a = 0 ) , T ( x , k ) is the cost of manually typing xk , and E ( x , k ) is the edit cost of accepting x and erasing to the end of its first k characters . Figure 7 shows a structogram of the algorithm Figure 1 describes the components and how this system works . Figure 3 : Accuracy of part of speech estimation each part of speech and word type ( POS + WT + Poisson + bigram ) . If the token starts with a capital letter ( initCaps ) , then an additional feature ( init- Caps , zone ) is set to 1 Table 1 , Figure 1 , and Figure 2 shows the AER results for different models . Table 2 shows the NA M N O M PR O NA M 34 13 ( 21 % ) 67 ( 6 6 % ) 11 ( 4 6 % ) N O M 43 ( 67 % ) 21 48 ( 4 9 % ) 9 ( 8 9 % ) PR O 86 8 ( 32 % ) 17 71 ( 5 5 % ) 53 08 ( 2 4 % ) Table 2 : Number of clustering decisions made according to mention type ( rows anaphor , columns antecedent ) and percentage of wrong decisions . Figure 2 shows an example of a symmetrized alignment this error is not counted , the tagging accuracy on the development data rises from 92.17 % to 94.27 % . A standard criterion on a parallel training corpus consisting of S sentence pairs { ( fs , es ) : s = 1 , . To simplify the description , we assume in Figure 2 that a bigram language model is used and all the TTS templates are binarized . Figure 7 : Computing the partition function of the conditional probability P r ( S|T ) . For the graph depicted in Figure 1 this algorithm computes the clusters { They , Leaders } , { Paris } and { recent developments } . In addition to the basic regular expression operators shown in table 1 , the formalism is extended in various ways . Table 2 : Accuracy for MBL and SVM classifiers on Talbanken05 nouns in accumulated frequency bins by Parole frequency . Figure 3 gives the algorithm phrase-extract that computes the phrases In Table 7 we give results for several evaluation metrics . This approach can be seen as a generalization of the originally suggested source channel modeling framework for statistical machine translation In all four tables , we provide theresults for using no heuristic functions and three variants of an increasingly infor mative heuristic function . Improvements of different tree setups over SPT on the ACE RDC 2004 corpus Finally , Table 4 compares our system with other state-of-the-art kernel-based systems on the 7 relation types of the ACE RDC 2004 corpus . The results for French to English and for English to French are shown in Table 10 Recall is somewhat difficult to estimate because we do not know whether the English translation of a Chinese word appears in the English part of the corpus . The argmax operation denotes the search problem , that is , the generation of the output sentence in the target language Our experimental data was drawn from 150 megabytes of 1993 Nikkei newswire ( see Figure I ) . Figure 1 ( d ) shows a sentence maintain rental property he owns in the state , where the ART.User-or-Owner relation holds between the entities property and he igure 1 gives an example . As we will see from Table 3 , not much improvement is derived from this feature . Figure 2 shows the F1 scores of the proposed model ( SegTagDep ) on CTB5c1 with respect to the training epoch and different parsing feature weights , where Seg , Tag , and Dep respectively denote the F1 scores of word segmentation , POS tagging , and dependency parsing . We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4 , 5 , and 6 . bothmentions are in a parallel construction in adja Figure 1 : An example graph modeling relations between mentions . The coupling between B and is removed by setting H = B 1 : n min ( W , H H T ) , s.t . hip = 1 ( 1 ) H , i=1 BT Dl Bl according to equation 4 l end for return BL , BL 1 ... B1 Additional steps need to be performed in order to extract a tree from the hierarchical graph . The third model is Equation ( 11 ) , which is a set of word models trained for each word type ( WT +Poisson+ bigram ) . Table 2 : Performance of the mention detection system using lexical , syntactic , gazetteer features as well as features obtained by running other named-entity classifiers named-entity classifiers ( with different semantic tag sets ) . Table 1 : Snapshot of the supersense-annotated data . Among all possible target sentences , we will choose the sentence with the highest probability Table 1 shows the bilingual phrases containing between two and seven words that result from the application of this algorithm to the alignment of Figure 2 tion ( 13 ) is estimated from the relative frequency of the corresponding events in the training corpus . In Table 7 we give results for several evaluation metrics . Table 1 , Figure 1 , and Figure 2 shows the AER results for different models . Figure 2 shows empirical estimates of p ( a = 1|2k l ) from the TransType data . We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors ( Table 8c ) . To illustrate the complete user model , in the figure 1 example the benefit of accepting would be7 2 4.2 = .8 keystrokes and the benefit of reject ing would be .2 keystrokes . Table 2 : Performance of the mention detection system using lexical , syntactic , gazetteer features as well as features obtained by running other named-entity classifiers named-entity classifiers ( with different semantic tag sets ) . Table 4 shows the number of sentences , words , and characters of the training and test sets . Table 7 : Word segmentation accuracy of unknown words r e c pr ec F Po iss on + bi gr a m W T + P oi ss o n + b i g r a m P O S + P o is s o n + b i g r a m P O S + W T + P o is s o n + bi g ra m 31 .8 45 .5 39 .7 42 . This class-based model gives reasonable results : for six radical classes , Table 1 gives the estimated cost for an unseen hanzi in the class occurring as the second hanzi in a double GIVEN name . In these experiments , the input lacks segmentation markers , hence the slightly different dev set baseline than in Table 6 . In Table 7 we give results for several evaluation metrics . Table 2 : Performance of Algorithms switch under different input data . The graph displayed in Figure 1 is the graph constructed for the mentions Leaders , Paris , recent developments and They from the example sentence at the beginning of this Section , where R = { P AnaPron , P Subject , N Number } . This group consists of 10 features based on the string , as listed in Table 1 . Our experimental data was drawn from 150 megabytes of 1993 Nikkei newswire ( see Figure I ) . Table 9 shows the training and test corpus statistics . vecI : IL Rk instance vector computation C : Rk m Rk centroid computation vecL : L Rk lemma ( type ) vector computation repM : M Rk meta sense representation Table 3 : Additional notation and signatures for CAM explicit sense disambiguation , CAM represents lemmas by their type vectors , i.e. , the centroid of their instances , and compares their vectors ( attributes ) to those of the meta alternation hence the name . The distribution of errors is displayed in Table 4 . The tagset refinement increases the accuracy by about 0.6 % , and the external lexicon by another 3.5 % . # o is the total number of output English translations . The first example in Table 3 shows that words ending in ' - ' are likely to be nouns . In Table 5 we present results from small test cor pora for the productive affixes handled by the current version of the system ; as with names , the segmentation of morphologically derived words is generally either right or wrong . Figure 9 shows that in fact both contribute to producing good segmentations . The result is shown in Table 4 : the baseline numbers without stem features are listed under Base , and the results of the coreference system with stem features are listed under Base+Stem . To compute the third factor of Equation ( 13 ) , we have to estimate the character bigram probabilities that are classified by word type and part of speech . This group consists of 10 features based on the string , as listed in Table 1 . Evaluation results are listed in Table 2 . Figure 8 : Relative word accuracy as a function of training set size . Figure 6 gives an overview of the decisions made in the alignment template model . Table 5 Performance on morphological analysis . To simplify the description , we assume in Figure 2 that a bigram language model is used and all the TTS templates are binarized . ( prec2 in Table 8 ) Prec . is the precision . Preliminary observations show that the different neighbours in Table 1 can be used to indicate with great accuracy which of the senses is being used But it conflates the coordinating and discourse separator functions of wa ( < ..4.b � � ) into one analysis : conjunction ( Table 3 ) . Table 7 shows the results . If M = 10 , 15 Chinese words ( i.e. , the first 19 Chinese words in Table 3 except 叶玛斯 , 巴佐亚 , 坩埚 , 普利法 ) have their correct English translations at rank one position . The graph displayed in Figure 1 is the graph constructed for the mentions Leaders , Paris , recent developments and They from the example sentence at the beginning of this Section , where R = { P AnaPron , P Subject , N Number } . This sentence should be tagged as shown in table 1 . Table 8 : Part of speech tagging accuracy of unknown words ( the last column represents the percentage of correctly tagged unknown words in the correctly segmented unknown words The results are shown in the corr row of table 2 , for exact character-probability estimates . In our segmentation system , a hybrid strategy is applied ( Figure 1 ) : First , forward maximum matching ( Chen and Liu , 1992 ) , which is a dictionary-based method , is used to generate a segmentation result . The third model is Equation ( 11 ) , which is a set of word models trained for each word type ( WT +Poisson+ bigram ) . Figure 7 shows a structogram of the algorithm This is usually straightforward , with the exception of the case where the words that are aligned to a particular role s span in the source side are not continuous in the target side , as shown in Figure 4 . Results are shown in Table 2 ; we see that better word alignment results do not lead to better translations . Table 2 compares the results of the unconstrained version of HGFC against those of AGG on our largest test set T2 . We obtain the following decision rule : eI = argmax Pr ( eI | f J ) 1 1 1 I 1 M ) = argmax m hm ( eI , f J ) 1 1 I m=1 Figure 2 shows the word length distribution of words consists of only kanji characters and words consists of only katakana characters . using the convolution parse tree kernel as depicted in Figure 1 . The corresponding figures for the test data are . 89.53 % for our tagger and 88.88 % for the TnT tag- ger . Table 3 illustrates the effects of different components of the user model by showing results for simulated users who read infinitely fast and accept only predictions having positive benefit ( superman ) ; who read normally but accept like superman ( rational ) ; and who match the standard user model ( real ) . Figure 2 : Dependency representation of example ( 2 ) from Talbanken05 . and 8 show word accuracy for Chasen , Juman , and our algorithm for parameter settings optimizing word precision , recall , and F-measure rates . We illustrate its use with an example ( see Then , we average the contributions of each n-gram order : Figure 2 ) . A natural unit for B ( x , k , a ) is the number of keystrokes saved , so all elements of the above equation are converted to this measure . Figure 1 : Examples of the semantic role features assuming that the semantic roles have been tagged for the source sentences . We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors ( Table 8c ) . Figure 1 : Examples of the semantic role features assuming that the semantic roles have been tagged for the source sentences . E.g. , the city Fez , Mo rocco ( figure 1 ) was tagged as a single LOCATION by one annotator and as two by the other . Table 1 : Effect of Factors antecedent is found in the previous context , subsequent sentences are inspected ( cataphora ) , also ordered by proximity to the pronoun . L set of lemmas IL set of ( lemma-wise ) instances SL set of ( lemma-wise ) senses inst : L ( IL ) mapping lemma instances sns : L ( SL ) mapping lemma senses M set of meta senses meta : SL M mapping senses meta senses A M M set of meta alternations ( MAs ) A set of MA representations score : A S2 R scoring function for MAs repA : A A MA representation function comp : A S2 R compatibility function Table 1 : Notation and signatures for our framework . Figure 4 : An example showing how to compute the target side position of a semantic role by using the median of its aligning points . Figure 1 : Effect of model parameters on performance . Table 3 lists new conceptsthat CAM introduces to manipulate vector represen tations . As we will see from Table 3 , not much improvement is derived from this feature . This approach can be seen as a generalization of the originally suggested source channel modeling framework for statistical machine translation Figure 2 : Local graph of the word wing of a graph We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4 , 5 , and 6 . In MUC6 , the best result is achieved by SRA ( Krupka , 1995 ) . Figure 6 : EM Algorithm For Estimating TTS Templates and Semantic Features framework ( May and Knight , 2007 ) . As an example , the probability of accepting the prediction in figure 1 is about .25 . The last is exhibited for the first mention in figure 1 , where one annotator chose ARTIFACT ( referring tothe physical book ) while the other chose COMMUNICATION ( the content ) Table 5 shows the cross entropy per word and char acter perplexity of three unknown word model . We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4 , 5 , and 6 . # o is the total number of output English translations . The results are shown in the corr row of table 2 , for exact character-probability estimates . Table 8 shows that tagging precision is im proved from 88.2 % to 96.6 % . Table 7 : Word segmentation accuracy of unknown words r e c pr ec F Po iss on + bi gr a m W T + P oi ss o n + b i g r a m P O S + P o is s o n + b i g r a m P O S + W T + P o is s o n + bi g ra m 31 .8 45 .5 39 .7 42 . For example , if x in figure 1 were evenir aux choses , then x14 would map to v1 = evenir , w2 = aux , and u3 = cho . Table 2 shows the NA M N O M PR O NA M 34 13 ( 21 % ) 67 ( 6 6 % ) 11 ( 4 6 % ) N O M 43 ( 67 % ) 21 48 ( 4 9 % ) 9 ( 8 9 % ) PR O 86 8 ( 32 % ) 17 71 ( 5 5 % ) 53 08 ( 2 4 % ) Table 2 : Number of clustering decisions made according to mention type ( rows anaphor , columns antecedent ) and percentage of wrong decisions . Table 5 shows that by changing the word spelling model from zerogram to bigram , character perplex ity is greatly reduced . For MUC7 , there are also no published results on systems trained on only the official training data of 200 aviation disaster articles . This list of 43 words is shown in Table 3 . E.g. , the city Fez , Mo rocco ( figure 1 ) was tagged as a single LOCATION by one annotator and as two by the other . This table also shows that : ( 1 ) Both modification within base-NPs and modification to NPs contribute much to performance improvement , acquiring the increase of F- measure by 4.4/2.4 units in mode M1 and 4.4/2.3 units in mode M2 respectively . Table 3 : Performance on T3 using a predefined tree structure . Table 4 Differences in performance between our system and Wang , Li , and Chang ( 1992 ) . Shortest-enclosed Path Tree from P/R/F ( 81.1/6.7/73.2 ) of Dynamic Context-Sensitive Shortest- enclosed Path Tree according to Table 2 ( Zhou et al. , 2007 ) Table 3 presents the performance in terms of precision , recall , and F- measure of the whole system . Table 4 shows a confusion matrix for the classification of the nouns . Figure 7 : Computing the partition function of the conditional probability P r ( S|T ) . Figure 2 shows an example of a symmetrized alignment Table 8 shows that tagging precision is im proved from 88.2 % to 96.6 % . Table 3 shows the results of an evaluation based on the plain STTS tagset . If we compare the error rates in Table 7 , which correspond to about 55 search errors in Table 6 , we obtain an mWER of 36.7 % ( 53 search errors ) using no heuristic function and an mWER of 32.6 % ( 57 search errors ) using the combined heuristic function Table 1 : BLEU4 scores of different systems Source Launching1 New2 Diplomatic3 Offensive4 SRF On 1 2 3 4 SRF Off 2 3 4 It1 is2 therefore3 necessary4 to5 speed6 up7 the8 equal better worse With SRF vs. W/O SRF 72 % 20.2 % 7.8 % Source transformation9 of10 traditional11 industries12 with13 high14 technologies15 Figure 3 shows the distribution of SSTs in the corpus . Figure 1 portrays how the states are aligned using the proposed scheme , where a subtree is denoted as a rectangle with its partial index shown inside it . his optimization can be performed using the expectation maximization ( EM ) algorithm ( Dempster , Laird , and Rubin 1977 ) . In all four tables , we provide theresults for using no heuristic functions and three variants of an increasingly infor mative heuristic function Figure 1 : A sentence from the article Islamic GoldenAge , with the supersense tagging from one of two anno tators . As an example , the probability of accepting the prediction in figure 1 is about .25 . The alignment aJ that has the highest probability ( under a certain model ) is also called the Viterbi alignment ( of that model ) Table 1 displays the performance of our model and of the systems that obtained the best ( Fernandes et al. , 2012 ) and the median performance in the MUC B3 CEAFe average R P F1 R P F1 R P F1 CoNLL 12 English development data be st 64 . The corresponding translation quality improves from an mWER of 45.9 % to an mWER of 31.8 % . The argmax operation denotes the search problem , that is , the generation of the output sentence in the target language As Figure 3 shows , word type information improves the prediction accuracy significantly . We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4 , 5 , and 6 . To reduce the memory requirement of the alignment templates , we compute these probabilities only for phrases up to a certain maximal length in the source language . For instance , the gain for the prediction in figure 1 would be 2 7 8 = 6 . Table 1 : BLEU4 scores of different systems Source Launching1 New2 Diplomatic3 Offensive4 SRF On 1 2 3 4 SRF Off 2 3 4 It1 is2 therefore3 necessary4 to5 speed6 up7 the8 equal better worse With SRF vs. W/O SRF 72 % 20.2 % 7.8 % Source transformation9 of10 traditional11 industries12 with13 high14 technologies15 Table 8 shows that by using word type and part of speech information , recall is improved from 28.1 % to 40.6 % and precision is improved from 57.3 % to 64.1 % . The corresponding figures for the test data are . 89.53 % for our tagger and 88.88 % for the TnT tag- ger . Figure 2 shows the labeled dependency graph of example ( 2 ) , taken from Talbanken05 . This table also shows that : ( 1 ) Both modification within base-NPs and modification to NPs contribute much to performance improvement , acquiring the increase of F- measure by 4.4/2.4 units in mode M1 and 4.4/2.3 units in mode M2 respectively . As shown in figure 3 , read times are much higher for predictions that get accepted , re B ( x , k , a ) = R1 ( x ) + T ( x , k ) E ( x , k ) , a = 1 R0 ( x ) , a = 0 flecting both a more careful perusal by the translator and the fact the rejected predictions are often simplywhere Ra ( x ) is the cost of reading x when it ulignored.2 In both cases there is a weak linear rela timately gets accepted ( a = 1 ) or rejected ( a = 0 ) , T ( x , k ) is the cost of manually typing xk , and E ( x , k ) is the edit cost of accepting x and erasing to the end of its first k characters . We illustrate its use with an example ( see Then , we average the contributions of each n-gram order : Figure 2 ) . Figure 1 : Effect of model parameters on performance . The reordering of the semantic roles from source to target is computed for each TTS template as part of the template extraction process , using the word-level alignments between the LHS/RHS of the TTS template ( e.g. , Figure 3 ) . Table 4 : Segmentation , POS tagging , and ( unlabeled attachment ) dependency F1 scores averaged over five trials on CTB5c . Shortest-enclosed Path Tree from P/R/F ( 81.1/6.7/73.2 ) of Dynamic Context-Sensitive Shortest- enclosed Path Tree according to Table 2 ( Zhou et al. , 2007 ) Figure 2 : Word length distribution of kanji words and katakana words length model does not reflect the variation of the word length distribution resulting from the Japanese orthography . Another interesting example is shown in Figure 1 ( b ) , where the base-NP of the second entity town is a possessive NP and there is no relationship between the entities one and town defined in the ACE corpus . IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM At the morp heme level , stems are divid ed from their affixe s. For exam ple , altho ugh both naga no ( Naga no ) and shi ( city ) can appea r as indivi dual words , nagano shi ( Nag ano city ) is brack eted as [ [ naga no ] [ s hi ] ] , since here shi Figure 3 : Determining word boundaries . vecI : IL Rk instance vector computation C : Rk m Rk centroid computation vecL : L Rk lemma ( type ) vector computation repM : M Rk meta sense representation Table 3 : Additional notation and signatures for CAM explicit sense disambiguation , CAM represents lemmas by their type vectors , i.e. , the centroid of their instances , and compares their vectors ( attributes ) to those of the meta alternation hence the name . If we directly translate the EM algorithm into the log- linear model , the problem becomes maximizing 0 X P r ( S , T , D ) = X @ Y P r ( t ) Y 1 P r ( f ) A the data likelihood represented by feature weights instead of feature probabilities : D D t D f F ( S , T .role , D ) Though the above formulation , which makes the P r ( S , T ) = D exp i i fi ( S , T , D ) total probability of all the pairs of trees and strings P P exp P f ( S , T , D ) St , T t Dt i i i less than 1 , is not a strict generative model , we can still use the EM algorithm ( Dempster et al. , 1977 ) to estimate the probability of the TTS templates and the semantic features , as shown in Figure 6 . Thus , we have crafted more specific explanations , sum marized for nouns in figure 2 . able 1 shows the bilingual phrases containing between two and seven words that result from the application of this algorithm to the alignment of Figure 2 . Table 2 contains results for two different translation models . Contribution of constituent dependencies in respective mode ( inside parentheses ) and accumulative mode ( outside parentheses ) The table shows that the final DSPT achieves the best performance of 77.4 % /65.4 % /70.9 in precision/recall/F-measure respectively after applying all the dependencies , with the increase of F-measure by 8.2 units compared to the baseline MCT . Figure 2 : Above : The complete supersense tagset for nouns ; each tag is briefly described by its symbol , NAME , short description , and examples . Figure 4 shows an example of calculating the target side SRS based on a complicated TTS template . In all four tables , we provide theresults for using no heuristic functions and three variants of an increasingly infor mative heuristic function . Table 3 shows the effect of constraining the maximum length of the alignment templates in the source language . As illus trated in Figure 1 ( e ) , the NP coordination in the Qian et al . The result is shown in Table 4 : the baseline numbers without stem features are listed under Base , and the results of the coreference system with stem features are listed under Base+Stem . The model 1 The total 74,597 sentence pairs used in experiments are those in the FBIS corpus whose English part can be parsed using Charniak ( 2000 ) The Stanford parser ( Klein and Manning , 2002 ) is unable to recover the verbal reading of the unvocalized surface form 0 an ( Table 1 ) . To differentiate between the coordinating and discourse separator functions of conjunctions ( Table 3 ) , we mark each CC with the label of its right sister ( splitCC ) . we directly model the posterior probability Pr ( eI| f J ) A comparison of the lines for grammatical roles and for surface order in Table 1 shows that the same is true in German . Figure 3 shows the performance and processing time comparison of various models and their combinations . The breakdown of the different types of words found by ST in the test corpus is given in Table 3 . Table 4 : NMI of unconstrained HGFC when trees for T2 and T3 are inferred automatically . We compared the ATB5 to tree- banks for Chinese ( CTB6 ) , German ( Negra ) , and English ( WSJ ) ( Table 4 ) In this section , we try to compare our results with those obtained by IdentiFinder ' 97 ( Bikel et al. , 1997 ) , IdentiFinder ' 99 ( Bikel et al. , 1999 ) , and MENE ( Borthwick , 1999 ) . In the table , Nc indicates the number of clusters in the inferred tree , while Nl indicates the closest match to the number of classes in the gold standard . Typically , the translation probability Pr ( eI | f J ) is decomposed via additional hid 1 1 den variables However , the spelling model , especially the character bigrams in Equation ( 17 ) are hard to es timate because of the data sparseness . To illustrate the complete user model , in the figure 1 example the benefit of accepting would be7 2 4.2 = .8 keystrokes and the benefit of reject ing would be .2 keystrokes . Thus , we have crafted more specific explanations , sum marized for nouns in figure 2 . The direct translation probability is given by The last is exhibited for the first mention in figure 1 , where one annotator chose ARTIFACT ( referring tothe physical book ) while the other chose COMMUNICATION ( the content ) Figure 2 : Average Precision and Coherence ( ) for each meta alternation . Table 3 : Precision statistics for pronouns . Figure 1 suggests that the best setup is dependent on the specific meta sense or meta alternation being MACRO MICRO repM gram gramlex lex space type MACRO MICRO repA Figure 1 : Word length distribution of unknown words and its estimate by Poisson distribution The last is exhibited for the first mention in figure 1 , where one annotator chose ARTIFACT ( referring tothe physical book ) while the other chose COMMUNICATION ( the content ) Equation ( 12 ) , which is a set of word models trained for each part of speech ( POS + Poisson + bigram ) . Nc Nl HGFC uncons trained A G G N MI F N M I F 13 0 13 3 57 .3 1 36 .6 5 54 .2 2 32 .6 2 11 4 11 7 54 .6 7 37 .9 6 51 .3 5 32 .4 4 50 51 37 .7 5 40 .0 0 32 .6 1 32 .7 8 Table 2 : Performance on T2 using a predefined tree structure . Ta SegTag 97.66 93.61 SegTagDep 97.73 94.46 SegTag ( d ) 98.18 94.08 SegTagDep ( d ) 98.26 94.64 Table 5 : Final results on CTB5j 76 75 74 ble 4 shows the segmentation , POS tagging , and dependency parsing F1 scores of these models on CTB5c . Table 2 : Performance of Algorithms switch under different input data . Figure 1 is , in fact , a weighted sum of these two distributions . Based on this experiment , we set the beam size of SegTagDep to 64 throughout the exper 64 96.28 92.37 74.96 0.48 Table 3 : F1 scores and speed ( in sentences per sec . ) Letting u1 be the prefix of the word that ends in v1 ( eg , r in figure 1 ) , w1 = u1v1 , and h = htu1 : is less accurate because it ignores the alignment rela tion between s and h , which is captured by even the simplest noisy-channel models . The distribution of errors is displayed in Table 4 . Table 9 shows that MADA produces a high quality segmentation , and that the effect of cascading segmentation errors on parsing is only 1.92 % F1 . In Figure 1 ( c ) we show a sentence one of about 500 people nominated for , where there exists a DISC relationship between the entities one and people The probabilities P ( < U-t > lwi_I ) can be esti mated from the relative frequencies in the training corpus whose infrequent words are replaced with their corresponding unknown word tags based on their part of speeches 2 Table 1 shows examples of word bigrams including unknown word tags . Collocations were automatically located in a text by looking up pairwise words in this lexicon In fact , from the last column of Figure 8 we see that even if our algorithm has access to only five anno tated sequences when Juman has access to ten times as many , we still achieve better precision and better F measure . Figure 3 : Time to read and accept or reject proposals versus their length tion , because the empirical probability of acceptance is very low when it is less than zero and rises rapidly as it increases . Figure 1 shows a probability estimation tree for the prediction of the probability of the nominative attribute of nouns . The bottom-up decoding algorithm for the TTS transducer is sketched in Figure 2 . More re cently , the task of automatic supersense tagging has emerged for English ( Ciaramita and Johnson , 2003 ; Curran , 2005 ; Ciaramita and Altun , 2006 ; Paa and Reichartz , 2009 ) , as well as for Italian ( Picca et al. , 2008 ; Picca et al. , 2009 ; Attardi et al. , 2010 ) and Chinese ( Qiu et al. , 2011 ) , languages with WordNetsmapped to English WordNet.3 In principle , we be lieve supersenses ought to apply to nouns and verbsin any language , and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs , summarized in figure 2 Figure 1 : Organisation of the hierarchical graph of concepts Following previous semantic noun classification experiments ( Pantel and Lin , 2002 ; Bergsma et al. , 2008 ) , we use the grammatical relations ( GRs ) as features for clustering . Table 1 presents an overview of the animacy data Clas s Ani mat e Type s Tok 6 4 4 ens cover ed 6 0 1 0 Inan imat e 691 0 3 4 8 2 2 Tota l 755 4 4 0 8 3 2 Table 1 : The animacy data set from Talbanken05 ; number of noun lemmas ( Types ) and tokens in each class . As for the unknown word model , word-based char acter bigrams are computed from the words with Table 5 : Cross entropy ( CE ) per word and character perplexity ( PP ) of each unknown word model Part of Speech Estimation Accuracy 0.95 0.9 frequency one ( 49,653 words ) . The second factor of Equation ( 13 ) is estimated from the Poisson distribution whose parameter In all four tables , we provide theresults for using no heuristic functions and three variants of an increasingly infor mative heuristic function This list of 43 words is shown in Table 3 . Figure 3 : Accuracy of part of speech estimation each part of speech and word type ( POS + WT + Poisson + bigram ) . Table 4 shows the number of sentences , words , and characters of the training and test sets . The entity features can be attached under the top node , the entity nodes , or directly combined with the entity nodes as in Figure 1 . To reduce the memory requirement of the alignment templates , we compute these probabilities only for phrases up to a certain maximal length in the source language . Figure 2 : Average Precision and Coherence ( ) for each meta alternation . segmentation ( Table 2 ) . The first model is Equation ( 5 ) , which is the combina . tion of Poisson distribution and character zerogram Considering that the way the semantic where all ( T ) denotes all the possible target strings which can be generated from the source tree T . Given a set of TTS templates , the new partition function can be efficiently computed using the dynamic programming algorithm shown in Figure 7 . Figure 1 suggests that the best setup is dependent on the specific meta sense or meta alternation being MACRO MICRO repM gram gramlex lex space type MACRO MICRO repA In Figure 1 ( c ) we show a sentence one of about 500 people nominated for , where there exists a DISC relationship between the entities one and people Typically , the translation probability Pr ( eI | f J ) is decomposed via additional hid 1 1 den variables Table 6 lists a sample of targets for the five meta alternations involved . Table 1 : Feature templates for the full joint model . For example , looking at Figure 1 ( b ) , V on G can be grouped into three clusters u1 , u2 and u3 . Strube ( 1998 ) s centeri ng appro ach ( whos e senten ce orderi ng is designate d as SR2 in Table 2 ) also deals with and even prefer s intrase ntenti al anaph ora , which raises the upper limit to a more accept able 80.2 % . Figure 1 : ( a ) An undirected graph G representing the similarity matrix ; ( b ) The bipartite graph showing three clusters on G ; ( c ) The induced clusters U ; ( d ) The new graph G1 over clusters U ; ( e ) The new bipartite graph over G1 p |vi ) = Vl 1 ... bothmentions are in a parallel construction in adja Figure 1 : An example graph modeling relations between mentions . The first machine , illustrated in Figure 1 encodes the prefix and suffix expansion rules , producing a lattice of possible segmentations . The results are shown in the corr row of table 2 , for exact character-probability estimates . The second model is the combination of Poisson distribution ( Equation ( 6 ) ) and character bigram As column 5 ( SVM ) in table 2 shows , the classification results are very similar to the results obtained with MBL.12 We furthermore find a very similar set of errors , and in particular , we find that 51.0 % of the errors for the inanimate class are nouns with the gradient animacy properties presented in ( 9 ) - ( 13 ) above . Figure 2 shows the F1 scores of the proposed model ( SegTagDep ) on CTB5c1 with respect to the training epoch and different parsing feature weights , where Seg , Tag , and Dep respectively denote the F1 scores of word segmentation , POS tagging , and dependency parsing . In Table 5 we present results from small test cor pora for the productive affixes handled by the current version of the system ; as with names , the segmentation of morphologically derived words is generally either right or wrong . For the Verbmobil task , we train the model parameters M according to the maximum class posterior probability criterion ( equation ( 4 ) ) . Table 2 : CoreLex s basic types with their corresponding WordNet anchors . Table 3 shows examples of common char acter bigrams for each part of speech in the infre quent words of the EDR corpus . Figure 6 : Examples of word , morpheme , and compatible-bracket errors . Table 1 ) , i.e. , the degree to which a sense pair ( s1 , s2 ) matches a meta alternation a . Table 3 : Results for different user simulations . For MUC6 , the reduction in error due to global features is 27 % , and for MUC7,14 % . This is because the lefthand side of Equation ( 7 ) represents the probability of the string c1 Table 5 shows that this reimplementation almost reproduces the accuracy of their implementation . In order to effectively capture entity-related semantic features , and their combined features as well , especially bi-gram or tri-gram features , we build an Entity-related Semantic Tree ( EST ) in three ways as illustrated in Figure 2 . Table 1 shows our results and the results of Stevenson and Joanis ( 2003 ) on T1 when employing AGG using Ward as the linkage criterion . PRO ( c ) Entity-Paired Tree ( EPT ) Figure 2 . Figure 2 shows an example of a symmetrized alignment Table 6 lists a sample of targets for the five meta alternations involved . Figure 3 shows the part of speech prediction accu racy of two unknown word model without context . Figure 1 shows the influence of the four parameters . and ap plied to an Arabic sentence in figure 1 . Probabilities We find that Equation ( 7 ) assigns too little proba bilities to long words ( 5 or more characters ) . # c is the total number of new Chinese source words in the period Figure 6 shows example sentences annotated by HGFC . As Figure 3 shows , word type information improves the prediction accuracy significantly . Table 2 : Scores for MSRA corpus As we will see from Table 3 , not much improvement is derived from this feature . Figure 1 : ( a ) An undirected graph G representing the similarity matrix ; ( b ) The bipartite graph showing three clusters on G ; ( c ) The induced clusters U ; ( d ) The new graph G1 over clusters U ; ( e ) The new bipartite graph over G1 p |vi ) = Vl 1 ... Table 1 ; capital letters designate sets and small letters elements of sets ) .2 For a lemma l like lamb , we want to knowhow well a meta alternation ( such as ANIMAL FOOD ) explains a pair of its senses ( such as the animal and food senses of lamb ) .3 This is formalized through the function score , which maps a meta alternation and two senses onto a score . Figure 1 shows a probability estimation tree for the prediction of the probability of the nominative attribute of nouns . Table 6 shows the word segmentation accuracy of four unknown word models over test set-2 . As shown in figure 1 , a similarity matrix W models one-hop transitions that follow the links from vertices to neighbors . Figure 2 shows that the tagging accuracy tends to increase with the context size . Examples of the deletion features can be found in Figure 1 . The bottom-up decoding algorithm for the TTS transducer is sketched in Figure 2 . Table 1 underscores the virtues of Sentence Recency : In the most recent sentence with antecedents satisfying the filters , there are on aver ble . the target Chen Guangcheng only appears once in Weibo Table 2 : Distribution of the sentences where the semantic role features give no/positive/negative impact to the sentence fluency in terms of the completeness and ordering of the semantic roles.classes in VerbNet ( Dang et al. , 1998 ) . So we estimate that English translations are present in the English part of the corpus for Table 2 . Letting u1 be the prefix of the word that ends in v1 ( eg , r in figure 1 ) , w1 = u1v1 , and h = htu1 : is less accurate because it ignores the alignment rela tion between s and h , which is captured by even the simplest noisy-channel models . Table 2 presents the distributions of some examples of morphs and their targets in English Twitter and Chinese Sina Weibo . Tables 4 and 5 show the effect of the pruning parameter tp with the histogram pruning parameter Np = 50,000 . Table 2 summarizes the results obtained with different taggers and tagsets on the development data . For example , the pairwise words orange and peel form a collocation . vecI : IL Rk instance vector computation C : Rk m Rk centroid computation vecL : L Rk lemma ( type ) vector computation repM : M Rk meta sense representation Table 3 : Additional notation and signatures for CAM explicit sense disambiguation , CAM represents lemmas by their type vectors , i.e. , the centroid of their instances , and compares their vectors ( attributes ) to those of the meta alternation hence the name . However , the learning curves in Figure 3 show that the Berkeley parser does not exceed our manual grammar by as wide a margin as has been shown for other languages ( Petrov , 2009 ) . The graph displayed in Figure 1 is the graph constructed for the mentions Leaders , Paris , recent developments and They from the example sentence at the beginning of this Section , where R = { P AnaPron , P Subject , N Number } . For example , look ing at Figure 2 ( b ) , V on G can be grouped into three clusters u1 , u2 and u3 . Figure 9 : Entries indicate whether best performance is achieved using the local maximum condition ( M ) , the threshold condition ( T ) , or both . Table 1 presents the wide range of cases that are used to create the morphs . Table 3 shows the results of an evaluation based on the plain STTS tagset . Figure 3 : An example showing the combination of the semantic role sequences of the states . We include a list of per-category results for selected phrasal labels , POS tags , and dependencies in Table 8 . Table 5 breaks down the performance of the best CAM model by meta alternation . Table 3 shows the effect of constraining the maximum length of the alignment templates in the source language . We choose t = 1 , 5 , and 30 for the fertility HMM Figure 7 : Compatible brackets and all-compatible bracket rates when word accuracy is optimized . Length Distribution In word segmentation , one of the major problems of the word length model of Equation ( 6 ) is the decom position of unknown words . Column four ( MBL ) in table 2 shows the accuracy obtained with all features in the general feature space . Figure 9 : Entries indicate whether best performance is achieved using the local maximum condition ( M ) , the threshold condition ( T ) , or both . As Table 1 shows , word bigrams whose infrequent word bigram The overall architecture of the statistical translation approach is summarized in Figure 1 . Figure 2 : Decoding algorithm for the standard Tree-to-String transducer . As shown in figure 1 , a similarity matrix W models one-hop transitions that follow the links from vertices to neighbors . Table 1 shows the bilingual phrases containing between two and seven words that result from the application of this algorithm to the alignment of Figure 2 Figure 3 shows the part of speech prediction accu racy of two unknown word model without context . For example , no synset covers any combinations of the main words in Figure 2 , namely buy , acquire and merger e suggest the use of a log-linear model to incorporate the various knowledge sources into an overall translation system and to perform discriminative training of the free model parameters he largest effect seems to come from taking into account the bigram dependence , which achieves an mWER of 32.9 % Table 4 : Effect of Arabic stemming features on coreference resolution . We see that the language model perplexity improves from 4,781 for a unigram model to 29.9 for a trigram model . ( 2011 ) , we confirmed that omission of the look-ahead features results in a 0.26 % decrease in the parsing accuracy on CTB5d ( dev ) .Figure 2 : F1 scores ( in % ) of SegTagDep on CTB 5c1 w.r.t . the training epoch ( x-axis ) and parsing feature weights ( in legend ) . We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4 , 5 , and 6 . tion ( 13 ) is estimated from the relative frequency of the corresponding events in the training corpus . Figure 8 : Relative word accuracy as a function of training set size . So we estimate that English translations are present in the English part of the corpus for Table 2 . Comparison of segmentation algorithm using different linguistic features . We compare in Table 2 the performance of Unified Parse and Semantic Trees with different kinds of Entity Semantic Tree setups using standard convolution tree kernel , while the SPT and DSPT with only entity-type information are listed for reference . Table 9 shows that MADA produces a high quality segmentation , and that the effect of cascading segmentation errors on parsing is only 1.92 % F1 . For example , if a token starts with a capital letter and ends with a period ( such as Mr. ) , then the feature InitCapPeriod is set to 1 , etc This class-based model gives reasonable results : for six radical classes , Table 1 gives the estimated cost for an unseen hanzi in the class occurring as the second hanzi in a double GIVEN name . Prec . is the precision . Table 3 : Examples of common character bigrams for each part of speech in the infrequent words pa rt of sp ee ch ch ar ac ter bi gr a m fre qu en cy no un nu m be r a dj e ct iv al v er b v er b ad je cti ve ad ve rb < e o w > < b o w > 1 S `` J < e o w > I t < e o w > L < e o w > < e o w > 13 43 4 8 4 3 2 7 2 1 3 69 63 resented all unknown words by one length model . Consider the example in Figure 1 Ta SegTag 97.66 93.61 SegTagDep 97.73 94.46 SegTag ( d ) 98.18 94.08 SegTagDep ( d ) 98.26 94.64 Table 5 : Final results on CTB5j 76 75 74 ble 4 shows the segmentation , POS tagging , and dependency parsing F1 scores of these models on CTB5c . However , the learning curves in Figure 3 show that the Berkeley parser does not exceed our manual grammar by as wide a margin as has been shown for other languages ( Petrov , 2009 ) . A simple lexicalized PCFG with second order Markovization gives relatively poor performance : 75.95 % F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline ( Table 7 ) . Table 4 shows the number of sentences , words , and characters of the training and test sets . and Table 6 show a comparison of the segmentation and POS tagging accuracies with other state-of-the-art models . In Table 1 , period 1 is Jul 01 – Jul 15 , period 2 is Jul 16 – Jul 31 , … , period 12 is Dec 16 – Dec 31 . Figure 3 gives the algorithm phrase-extract that computes the phrases Table 4 shows a confusion matrix for the classification of the nouns . Table 2 : Distribution of the sentences where the semantic role features give no/positive/negative impact to the sentence fluency in terms of the completeness and ordering of the semantic roles.classes in VerbNet ( Dang et al. , 1998 ) . tion ( 13 ) is estimated from the relative frequency of the corresponding events in the training corpus . where ECD|S , T ( fi ) , the expected count of a feature over all derivations given a pair of tree and string , can be computed using the modified inside- outside algorithm described in Section 3.2 , and ECS |T ( fi ) , the expected count of a feature over all possible target strings given the source tree , can be computed in a similar way to the partition function described in Figure 7 . Tables 4 and 5 show the effect of the pruning parameter tp with the histogram pruning parameter Np = 50,000 Although kanji sequences are difficult to seg ment , they can comprise a significant portion of Japanese text , as shown in Figure 1 . Table 7 : Word segmentation accuracy of unknown words r e c pr ec F Po iss on + bi gr a m W T + P oi ss o n + b i g r a m P O S + P o is s o n + b i g r a m P O S + W T + P o is s o n + bi g ra m 31 .8 45 .5 39 .7 42 . Figure 2 : Word length distribution of kanji words and katakana words length model does not reflect the variation of the word length distribution resulting from the Japanese orthography . The accuracy of a baseline tagger which chooses the most probable tag9 ignoring the context is 67.3 % without and 69.4 % with the supple 92.3 92.2 92.1 92 91.9 91.8 91.7 91.6 91.5 91.4 2 3 4 5 6 7 8 9 10 mentary lexicon . As can be seen in Table 4 , our training data is a lot less than those used by MENE and IdentiFinder3 . As shown in Table 3 , using just context information alone , 10 Chinese words ( the first 10 ) have their correct English translations at rank one position . The annotation manual ( Teleman , 1974 ) states that a markable should be tagged as human ( H H ) if it may be replaced by the interrogative pronoun vem who and be referred to by the personal pronouns han he or hon she .There are clear similarities between the anno tation for human reference found in Talbanken05 and the annotation scheme for animacy discussed HUM Other animate Inanimate ORG ANIM CONC NCONC TIME PLACE Figure 1 : Animacy classification scheme ( Zaenen et al. , 2004 ) Figure 9 : Entries indicate whether best performance is achieved using the local maximum condition ( M ) , the threshold condition ( T ) , or both . If the token starts with a capital letter ( initCaps ) , then an additional feature ( init- Caps , zone ) is set to 1 Table 1 presents the wide range of cases that are used to create the morphs . A standard criterion on a parallel training corpus consisting of S sentence pairs { ( fs , es ) : s = 1 , . Table 1 , Figure 1 , and Figure 2 shows the AER results for different models . A simple lexicalized PCFG with second order Markovization gives relatively poor performance : 75.95 % F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline ( Table 7 ) . The performance of these systems is shown in Table 1 . In this section , we try to compare our results with those obtained by IdentiFinder ' 97 ( Bikel et al. , 1997 ) , IdentiFinder ' 99 ( Bikel et al. , 1999 ) , and MENE ( Borthwick , 1999 ) . Figure 5 shows our morpheme accuracy results . Table ( 1 ) and Eq . As for the unknown word model , word-based char acter bigrams are computed from the words with Table 5 : Cross entropy ( CE ) per word and character perplexity ( PP ) of each unknown word model Part of Speech Estimation Accuracy 0.95 0.9 frequency one ( 49,653 words ) . The accuracy of a baseline tagger which chooses the most probable tag9 ignoring the context is 67.3 % without and 69.4 % with the supple 92.3 92.2 92.1 92 91.9 91.8 91.7 91.6 91.5 91.4 2 3 4 5 6 7 8 9 10 mentary lexicon . As a result , Arabic sentences are usually long relative to English , especially after Table 2 shows a comparison with some existing toolkits that build deterministic , minimized automata/transducers . Table 3 shows the results of both unconstrained and constrained versions of HGFC and those of AGG on the test set T3 ( where singular classes are removed to enable proper evaluation of the constrained method ) . Table 2 : Distribution of the sentences where the semantic role features give no/positive/negative impact to the sentence fluency in terms of the completeness and ordering of the semantic roles.classes in VerbNet ( Dang et al. , 1998 ) . Consider the example in Figure 1 This approach has been suggested by Papineni , Roukos , and Ward ( 1997 , 1998 ) for a natural language understanding task . Figure 1 shows the absolute frequencies of sen tence recency values when only the most recent antecedent ( in the order just stated ) is considered . Table 1 ; capital letters designate sets and small letters elements of sets ) .2 For a lemma l like lamb , we want to knowhow well a meta alternation ( such as ANIMAL FOOD ) explains a pair of its senses ( such as the animal and food senses of lamb ) .3 This is formalized through the function score , which maps a meta alternation and two senses onto a score . ( Equation ( 7 ) ) ( Poisson + hi gram ) . The reordering of the semantic roles from source to target is computed for each TTS template as part of the template extraction process , using the word-level alignments between the LHS/RHS of the TTS template ( e.g. , Figure 3 ) . To compute the third factor of Equation ( 13 ) , we have to estimate the character bigram probabilities that are classified by word type and part of speech . of the 43 words are translated to English multi-word phrases ( denoted as “ phrase ” in Table 3 ) . ( 2004 ) in figure 1 . bothmentions are in a parallel construction in adja Figure 1 : An example graph modeling relations between mentions . # Cor is the number of correct English translations output . Table 1 shows empirical search timings for various values of M , for the MEMD model described in the next section . The model 1 The total 74,597 sentence pairs used in experiments are those in the FBIS corpus whose English part can be parsed using Charniak ( 2000 ) Table 5 shows that by changing the word spelling model from zerogram to bigram , character perplex ity is greatly reduced . Tables 6 and 7 show the effect of the pruning pa rameter Np with the pruning parameter tp = 10 12 > 10 nouns ( a ) ( b ) classified as 222 125 ( a ) class animate 49 3390 ( b ) class inanimate Table 4 : Confusion matrix for the MBLclassifier with a general feature space on the > 10 data set on Talbanken05 nouns . range free green lemon peel red state yellow By far the most frequent tagging error was the confusion of nominative and accusative case . To reduce the memory requirement of the alignment templates , we compute these probabilities only for phrases up to a certain maximal length in the source language . The coupling between B and is removed by setting H = B 1 : n min ( W , H H T ) , s.t . hip = 1 ( 1 ) H , i=1 BT Dl Bl according to equation 4 l end for return BL , BL 1 ... B1 Additional steps need to be performed in order to extract a tree from the hierarchical graph . See Fig ure 6 for some examples . Figure 1 portrays how the states are aligned using the proposed scheme , where a subtree is denoted as a rectangle with its partial index shown inside it . Table 1 The cost as a novel given name ( second position ) for hanzi from various radical classes . In addition to the basic regular expression operators shown in table 1 , the formalism is extended in various ways . Table 1 : Results of different systems on the CoNLL 12 English data sets . Figure 1 shows examples of the feature SRR . We also propose to use the features U01 U03 , which we found are effective to adjust the character Figure 1 : Illustration of the alignment of steps . Table 8 : Part of speech tagging accuracy of unknown words ( the last column represents the percentage of correctly tagged unknown words in the correctly segmented unknown words The points labelled smoothed in figure 2 were obtained using a sliding-average smoother , and the model curve was obtained using two-component Gaussian mixtures to fit the smoothed empirical likelihoods p ( gain|a = 0 ) and p ( gain|a = 1 ) . able 2 shows the corpus statistics for this task . Table 4 : Sample of experimental items for the meta alternation anmfod . Table 1 presents an overview of the animacy data Clas s Ani mat e Type s Tok 6 4 4 ens cover ed 6 0 1 0 Inan imat e 691 0 3 4 8 2 2 Tota l 755 4 4 0 8 3 2 Table 1 : The animacy data set from Talbanken05 ; number of noun lemmas ( Types ) and tokens in each class . Figure 5 shows the decoding algorithm incorporating the SRR features . Tree setups P ( % ) R ( % ) F SPT 76.3 59.8 67.1 DSPT 77.4 65.4 70.9 UPST ( BOF ) 80.4 69.7 74.7 UPST ( FPT ) 80.1 70.7 75.1 UPST ( EPT ) 79.9 70.2 74.8 Table 2 . Tables 4 and 5 show the effect of the pruning parameter tp with the histogram pruning parameter Np = 50,000 . Table 4 : NMI of unconstrained HGFC when trees for T2 and T3 are inferred automatically . Table 8 shows the effect of the length of the language model history on translation quality . This is usually straightforward , with the exception of the case where the words that are aligned to a particular role s span in the source side are not continuous in the target side , as shown in Figure 4 . Table 1 : Snapshot of the supersense-annotated data . For example , the pairwise words orange and peel form a collocation . In general , as shown in this figure , there may be additional transformations to make the translation task simpler for the algorithm . To simplify the description , we assume in Figure 2 that a bigram language model is used and all the TTS templates are binarized . In MUC6 , the best result is achieved by SRA ( Krupka , 1995 ) . Figure 2 : Local graph of the word wing of a graph Table 5 shows that by changing the word spelling model from zerogram to bigram , character perplex ity is greatly reduced . Figure 2 : Dependency representation of example ( 2 ) from Talbanken05 . # e is the total number of English translation candidates in the period . We establish a corresponding feature function by multiplying the probability of all used alignment templates and taking the logarithm ( prec2 in Table 8 ) Table 1 lists both the success rate maximally achievable ( broken down according to different types of pronouns ) and the average number of antecedents remaining after applying each factor . even after removal of the wing-node , the two areas of meaning are still linked via tail Column four ( MBL ) in table 2 shows the accuracy obtained with all features in the general feature space . Figure 2 shows empirical estimates of p ( a = 1|2k l ) from the TransType data . Figure 3 show the training time for different models . Table 2 shows the manual evaluation results based on the entire test set , and the improvement from SRF is significant at p < 0.005 based on a t-test . If we directly translate the EM algorithm into the log- linear model , the problem becomes maximizing 0 X P r ( S , T , D ) = X @ Y P r ( t ) Y 1 P r ( f ) A the data likelihood represented by feature weights instead of feature probabilities : D D t D f F ( S , T .role , D ) Though the above formulation , which makes the P r ( S , T ) = D exp i i fi ( S , T , D ) total probability of all the pairs of trees and strings P P exp P f ( S , T , D ) St , T t Dt i i i less than 1 , is not a strict generative model , we can still use the EM algorithm ( Dempster et al. , 1977 ) to estimate the probability of the TTS templates and the semantic features , as shown in Figure 6 . Length Distribution In word segmentation , one of the major problems of the word length model of Equation ( 6 ) is the decom position of unknown words . equation 2 ) says that the prob ability of starting a new entity , given the current mention m and the previous entities e1 , e2 , , et , is simply 1 minus the maximum link probability between the current mention and one of the previous entities . Table 3 shows the effect of constraining the maximum length of the alignment templates in the source language . range free green lemon peel red state yellow Considering that the way the semantic where all ( T ) denotes all the possible target strings which can be generated from the source tree T . Given a set of TTS templates , the new partition function can be efficiently computed using the dynamic programming algorithm shown in Figure 7 . Table 9 shows that MADA produces a high quality segmentation , and that the effect of cascading segmentation errors on parsing is only 1.92 % F1 . The second factor of Equation ( 13 ) is estimated from the Poisson distribution whose parameter Figure 2 : Local graph of the word wing of a graph We establish a corresponding feature function by multiplying the probability of all used alignment templates and taking the logarithm In fact , from the last column of Figure 8 we see that even if our algorithm has access to only five anno tated sequences when Juman has access to ten times as many , we still achieve better precision and better F measure . The use of the language model feature in equation ( 18 ) helps take long-range dependencies better into account Figure 3 : Performance of baseline and joint models w.r.t . the average processing time ( in sec . ) As column 5 ( SVM ) in table 2 shows , the classification results are very similar to the results obtained with MBL.12 We furthermore find a very similar set of errors , and in particular , we find that 51.0 % of the errors for the inanimate class are nouns with the gradient animacy properties presented in ( 9 ) - ( 13 ) above . We choose t = 1 , 5 , and 30 for the fertility HMM The overall architecture of the log-linear modeling approach is summarized in Figure 1 Figure 1 : Example of a prediction for English to French translation . Tree setups P ( % ) R ( % ) F SPT 76.3 59.8 67.1 DSPT 77.4 65.4 70.9 UPST ( BOF ) 80.4 69.7 74.7 UPST ( FPT ) 80.1 70.7 75.1 UPST ( EPT ) 79.9 70.2 74.8 Table 2 . Table 8 shows that tagging precision is im proved from 88.2 % to 96.6 % . n our experimentations , SVMlight ( Joachims , 1998 ) with the tree kernel function ( 75.0 ) ( 53.7 ) ( 62.6 ) Table 1 Table 3 : Performance on T3 using a predefined tree structure . Table 8b shows that verbal nouns are the hardest pre-terminal categories to identify . We compare in Table 2 the performance of Unified Parse and Semantic Trees with different kinds of Entity Semantic Tree setups using standard convolution tree kernel , while the SPT and DSPT with only entity-type information are listed for reference . Figure 1shows the word length distribution of in frequent words in the EDR corpus , and the estimate of word length distribution by Equation ( 6 ) whose parameter ( .A = 4.8 ) is the average word length of infrequent words . Ta SegTag 97.66 93.61 SegTagDep 97.73 94.46 SegTag ( d ) 98.18 94.08 SegTagDep ( d ) 98.26 94.64 Table 5 : Final results on CTB5j 76 75 74 ble 4 shows the segmentation , POS tagging , and dependency parsing F1 scores of these models on CTB5c . Table 8a shows that the best model recovers SBAR at only 71.0 % F1 . CoreLex defines a layer of abstraction above WordNet consisting of 39 basic types , coarse- grained ontological classes ( Table 2 ) . For example , the expressions in Figure 2 are identified as paraphrases by this method ; so these three patterns will be placed in the same pattern set . n all four tables , we provide theresults for using no heuristic functions and three variants of an increasingly infor mative heuristic function . Figure 5 shows our morpheme accuracy results . Table 2 : Character type configuration of infrequent words in the EDR corpus The direct translation probability is given by Results : Table I gives the results for the comparison of the troughs placed by the segmentation algorithm to the known subject change points . See Table 1 for details . Figure 1 : Examples of the semantic role features assuming that the semantic roles have been tagged for the source sentences . Table 4 : The amount of training and test sets The first factor in the righthand side of Equa tion ( 13 ) is estimated from the relative frequency of the corresponding events in the training corpus . and Table 6 show a comparison of the segmentation and POS tagging accuracies with other state-of-the-art models . Table 3 shows examples of common char acter bigrams for each part of speech in the infre quent words of the EDR corpus . The first machine , illustrated in Figure 1 encodes the prefix and suffix expansion rules , producing a lattice of possible segmentations . This is especially true in the case of quotations—which are common in the ATB—where ( 1 ) will follow a verb like ( 2 ) ( Figure 1 ) . Tables 6 and 7 show the effect of the pruning pa rameter Np with the pruning parameter tp = 10 12 Table 6 : Final results on CTB6 and CTB7 accuracies of POS tagging and dependency parsing were remarkably improved by 0.6 % and 2.4 % , respectively corresponding to 8.3 % and 10.2 % error reduction . Table 2 shows a comparison with some existing toolkits that build deterministic , minimized automata/transducers . ( 2004 ) makes use of a coding manual designed for a project studying genitive modification ( Garretson et al. , 2004 ) and presents an explicit annotation scheme for an _ Samma _ PO _ KP erfarenhet NN _ gjorde VV PT engelsmannen NN DD|HH imacy , illustrated by figure 1 . The bipartite graph K also induces a similarityagain ( Figure 1 ( e ) ) . Figure 6 : Examples of word , morpheme , and compatible-bracket errors . Table 2 shows these similarity measures . In Figure 4 we show an example of variation between the parsing models . Figure 1 Architecture of the statistical translation approach based on Bayes decision rule . Table 4 Differences in performance between our system and Wang , Li , and Chang ( 1992 ) . With an absolute frequency threshold of 10 , we obtain an accuracy of 95.4 % , which constitutes a 50 % reduction of error rate.Table 3 presents the experimental results rela tive to class . Figure 6 shows example sentences annotated by HGFC . Fi gure 7 depic ts the comp atible brack ets and all comp atible brack ets rates . Table 1 : Output of word sense clustering . The results are displayed in Table 3 . If it starts with a lower case letter , and contains both upper and lower case letters , then ( mixedCaps , zone ) is set to 1 . The figure schematically shows a small portion of the graph describing the concepts of mechanism ( concrete ) , political system and relationship ( abstract ) at two levels of generality . Length Distribution In word segmentation , one of the major problems of the word length model of Equation ( 6 ) is the decom position of unknown words . Table 3 : Scores for UPUC corpusFrom those tables , we can see that a simple ma jority voting algorithm produces accuracy that is higher than each individual system and reasonably high F-scores overall . Table 2 shows the correctness evaluation results . By introducing the distinction of word type to the model of Equation ( 12 ) , we can derive a more sophis ticated unknown word model that reflects both word 3 When a Chinese character is used to represent a seman tically equivalent Japanese verb , its root is written in the Chinese character and its inflectional suffix is written in hi ragana . Table 1 : Scores for CityU corpus Table 4 : Segmentation , POS tagging , and ( unlabeled attachment ) dependency F1 scores averaged over five trials on CTB5c . In the following , we describe the criterion that defines the set of phrases that is consistent with the word alignment matrix In the following , we describe the criterion that defines the set of phrases that is consistent with the word alignment matrix The baseline system in Table 3 refers to the maximum entropy system that uses only local features . Examples of the deletion features can be found in Figure 1 . Evaluation results are listed in Table 2 . In addition to the basic regular expression operators shown in table 1 , the formalism is extended in various ways . Collocation : Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio ( Church and Hanks , 1990 ) and outputted to a lexicon . Figure 1 : Statistics from 1993 Japanese newswire ( NIKKEI ) , 79,326,406 characters total . Table 8b shows that verbal nouns are the hardest pre-terminal categories to identify . To differentiate between the coordinating and discourse separator functions of conjunctions ( Table 3 ) , we mark each CC with the label of its right sister ( splitCC ) . Table 5 Performance on morphological analysis . his optimization can be performed using the expectation maximization ( EM ) algorithm ( Dempster , Laird , and Rubin 1977 ) . Table 2 : Statistics of datasets . Also , the CRF model using maximum subword-based tagging ( Zhang et al. , 2006 ) and the CRF model using minimum subword-based tagging , both of which are statistical methods , are used individually to solve the Figure 1 : Outline of the segmentation process 2.1 Forward Maximum Matching . These semantic features Figure 8 : Examples of the MT outputs with and without SRFs The probability of using an alignment template to translate a specific source language phrase f is estimated by means of relative frequency MENE has only been tested on MUC7 . In order to effectively capture entity-related semantic features , and their combined features as well , especially bi-gram or tri-gram features , we build an Entity-related Semantic Tree ( EST ) in three ways as illustrated in Figure 2 . Examples are given in Table 4 . We obtain the following decision rule : eI = argmax Pr ( eI | f J ) 1 1 1 I 1 M ) = argmax m hm ( eI , f J ) 1 1 I m=1 Table 2 : Performance of the mention detection system using lexical , syntactic , gazetteer features as well as features obtained by running other named-entity classifiers named-entity classifiers ( with different semantic tag sets ) . This class-based model gives reasonable results : for six radical classes , Table 1 gives the estimated cost for an unseen hanzi in the class occurring as the second hanzi in a double GIVEN name . Table 2 summarizes the results obtained with different taggers and tagsets on the development data . Table 5 shows that this reimplementation almost reproduces the accuracy of their implementation . Table 1 , Figure 1 , and Figure 2 shows the AER results for different models . Tree setups P ( % ) R ( % ) F CS-SPT over SPT3 1.5 1.1 1.3 DSPT over SPT 1.1 5.6 3.8 UPST ( FPT ) over SPT 3.8 10.9 8.0 Table 3 . ( 2004 ) in figure 1 . Figure 3 : Performance of baseline and joint models w.r.t . the average processing time ( in sec . ) The figure schematically shows a small portion of the graph describing the concepts of mechanism ( concrete ) , political system and relationship ( abstract ) at two levels of generality . If we directly translate the EM algorithm into the log- linear model , the problem becomes maximizing 0 X P r ( S , T , D ) = X @ Y P r ( t ) Y 1 P r ( f ) A the data likelihood represented by feature weights instead of feature probabilities : D D t D f F ( S , T .role , D ) Though the above formulation , which makes the P r ( S , T ) = D exp i i fi ( S , T , D ) total probability of all the pairs of trees and strings P P exp P f ( S , T , D ) St , T t Dt i i i less than 1 , is not a strict generative model , we can still use the EM algorithm ( Dempster et al. , 1977 ) to estimate the probability of the TTS templates and the semantic features , as shown in Figure 6 . As each global feature group is added to the list of features , we see improvements to both MUC6 and This is because the lefthand side of Equation ( 7 ) represents the probability of the string c1 Table 2 shows the experimental results with and without the stem n-grams features . The coupling between B and is removed by setting H = B 1 : n min ( W , H H T ) , s.t . hip = 1 ( 1 ) H , i=1 BT Dl Bl according to equation 4 l end for return BL , BL 1 ... B1 Additional steps need to be performed in order to extract a tree from the hierarchical graph . Table 2 : Statistics of datasets . Comparison of different systems on the ACE RDC 2004 corpus In Table 3 we summarize the improvements of different tree setups over SPT . See Figure 3 for examples . where ECD|S , T ( fi ) , the expected count of a feature over all derivations given a pair of tree and string , can be computed using the modified inside- outside algorithm described in Section 3.2 , and ECS |T ( fi ) , the expected count of a feature over all possible target strings given the source tree , can be computed in a similar way to the partition function described in Figure 7 . The unknown parameters are determined by maximizing the likelihood on the parallel training corpus : Figure 5 gives an example of the word alignment and phrase alignment of a German English sentence pair.We describe our model using a log-linear modeling approach Figure 9 shows that in fact both contribute to producing good segmentations . As Table 1 shows , word bigrams whose infrequent word bigram Table 4 shows the performance on the test data . Figure 1 : Statistics from 1993 Japanese newswire ( NIKKEI ) , 79,326,406 characters total . alignment models Pr ( f J , aJ | eI ) , Preliminary observations show that the different neighbours in Table 1 can be used to indicate with great accuracy which of the senses is being used Figure 2 shows examples of the discovered patterns for the merger and acquisition topic . If it is made up of all capital letters , then ( allCaps , zone ) is set to 1 Figure 4 shows a constituent headed by a process nominal with an embedded adjective phrase . Results : Table I gives the results for the comparison of the troughs placed by the segmentation algorithm to the known subject change points . Figure 3 show the training time for different models . This sentence should be tagged as shown in table 1 . Letting u1 be the prefix of the word that ends in v1 ( eg , r in figure 1 ) , w1 = u1v1 , and h = htu1 : is less accurate because it ignores the alignment rela tion between s and h , which is captured by even the simplest noisy-channel models . Figure 2 plots AP by for all meta alternations . Table 1 shows empirical search timings for various values of M , for the MEMD model described in the next section . Figure 1 ( d ) shows a sentence maintain rental property he owns in the state , where the ART.User-or-Owner relation holds between the entities property and he Table 1 : Comparison against Stevenson and Joanis ( 2003 ) s result on T1 ( using similar features ) . Table 5 shows the cross entropy per word and char acter perplexity of three unknown word model . We establish a corresponding feature function by multiplying the probability of all used alignment templates and taking the logarithm If we compare the error rates in Table 7 , which correspond to about 55 search errors in Table 6 , we obtain an mWER of 36.7 % ( 53 search errors ) using no heuristic function and an mWER of 32.6 % ( 57 search errors ) using the combined heuristic function . In general , as shown in this figure , there may be additional transformations to make the translation task simpler for the algorithm . In the last two lines of Equation 3 , φǫ and each P ( f |e ) = `` £s c ( f |e ; f ( s ) , e ( s ) ) ( 4 ) φi are not free variables , but are determined by f s c ( f |e ; f ( s ) , e ( s ) ) the alignments . Table 1 ) , i.e. , the degree to which a sense pair ( s1 , s2 ) matches a meta alternation a . We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors ( Table 8c ) . Improvements of different tree setups over SPT on the ACE RDC 2004 corpus Finally , Table 4 compares our system with other state-of-the-art kernel-based systems on the 7 relation types of the ACE RDC 2004 corpus . Table 3 shows the results of both unconstrained and constrained versions of HGFC and those of AGG on the test set T3 ( where singular classes are removed to enable proper evaluation of the constrained method ) . Table 1 shows our results and the results of Stevenson and Joanis ( 2003 ) on T1 when employing AGG using Ward as the linkage criterion . In our segmentation system , a hybrid strategy is applied ( Figure 1 ) : First , forward maximum matching ( Chen and Liu , 1992 ) , which is a dictionary-based method , is used to generate a segmentation result . Figure 1 : A sentence from the article Islamic GoldenAge , with the supersense tagging from one of two anno tators . this error is not counted , the tagging accuracy on the development data rises from 92.17 % to 94.27 % . We set Table 6 : Word segmentation accuracy of all words r e c pr ec F Po iss on +b igr a m W T +P oi ss on +b igr a m P O S + Po iss on +b igr a m P O S + W T + Po iss on + bi gr a m 94 .5 94 .4 94 .4 94 .6 93 .1 93 .8 93 .6 93 .7 93 .8 94 .1 94 .0 94 .1 In order to effectively capture entity-related semantic features , and their combined features as well , especially bi-gram or tri-gram features , we build an Entity-related Semantic Tree ( EST ) in three ways as illustrated in Figure 2 . The probability of using an alignment template to translate a specific source language phrase f is estimated by means of relative frequency We compared the ATB5 to tree- banks for Chinese ( CTB6 ) , German ( Negra ) , and English ( WSJ ) ( Table 4 ) If we compare the error rates in Table 7 , which correspond to about 55 search errors in Table 6 , we obtain an mWER of 36.7 % ( 53 search errors ) using no heuristic function and an mWER of 32.6 % ( 57 search errors ) using the combined heuristic function Table 2 compares the results of the unconstrained version of HGFC against those of AGG on our largest test set T2 . Figure 1 is , in fact , a weighted sum of these two distributions . Table 1 displays the performance of our model and of the systems that obtained the best ( Fernandes et al. , 2012 ) and the median performance in the MUC B3 CEAFe average R P F1 R P F1 R P F1 CoNLL 12 English development data be st 64 . However , this information is hard to extract reliably from the available data ; and even if were obtainable , many of the 0.3 0.2 0.1 0 60 50 40 30 20 10 0 10 20 30 40 50 60 gain ( length of correct prefix length of incorrect suffix ) Figure 2 : Probability that a prediction will be accepted versus its gain . Figure 2 : Average Precision and Coherence ( ) for each meta alternation . For example , in the sentence bought one of town s two meat- packing plants as illustrated in Figure 1 ( a ) , the constituents before the headword plants can be removed from the parse tree . For fair comparison , we have tabulated all results with the size of training data used ( Table 5 A token that is allCaps will also be initCaps . Table 2 shows these similarity measures . The first model is Equation ( 5 ) , which is the combina . tion of Poisson distribution and character zerogram Fi gure 7 depic ts the comp atible brack ets and all comp atible brack ets rates . Figure 6 : Examples of word , morpheme , and compatible-bracket errors . of the 43 words are translated to English multi-word phrases ( denoted as “ phrase ” in Table 3 ) . even after removal of the wing-node , the two areas of meaning are still linked via tail For MUC7 , there are also no published results on systems trained on only the official training data of 200 aviation disaster articles . The second condition is necessary to allow for single-character words ( see Figure 3 ) . The algorithm takes into account possibly unaligned words at the boundaries of the source or target language phrases . More re cently , the task of automatic supersense tagging has emerged for English ( Ciaramita and Johnson , 2003 ; Curran , 2005 ; Ciaramita and Altun , 2006 ; Paa and Reichartz , 2009 ) , as well as for Italian ( Picca et al. , 2008 ; Picca et al. , 2009 ; Attardi et al. , 2010 ) and Chinese ( Qiu et al. , 2011 ) , languages with WordNetsmapped to English WordNet.3 In principle , we be lieve supersenses ought to apply to nouns and verbsin any language , and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs , summarized in figure 2 To illustrate how SRF impacts the translation results , Figure 8 gives 3 examples of the MT outputs with and without the SRFs The Stanford parser ( Klein and Manning , 2002 ) is unable to recover the verbal reading of the unvocalized surface form 0 an ( Table 1 ) . Figure 6 : Metaphors tagged by the system ( in bold ) whereby the main source of disagreement was the presence of lexicalized metaphors , e.g . verbs such as impose , decline etc . In this section , we try to compare our results with those obtained by IdentiFinder ' 97 ( Bikel et al. , 1997 ) , IdentiFinder ' 99 ( Bikel et al. , 1999 ) , and MENE ( Borthwick , 1999 ) . Then , every phrase f produces its translation e ( using the corresponding alignment template z ) . A token that is allCaps will also be initCaps . This corresponds to maximizing the equivocation or maximizing the likelihood of the direct-translation model See Fig ure 6 for some examples . 5http : //cactus.aistnara.ac.jp/lab/nltlchasen.html 6http : //pine.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html Word accuracy 90 CHASEN JUMAN opllnizt oplnuo recall opiJTozt F Figure 4 : Word accuracy . First , the source sentence words f J are grouped into phrases f K . For each phrase f an 1 1 alignment template z is chosen and the sequence of chosen alignment templates is reordered ( according to K ) . Table 2 contains results for two different translation models . Figure 1 is , in fact , a weighted sum of these two distributions . The probabilities P ( < U-t > lwi_I ) can be esti mated from the relative frequencies in the training corpus whose infrequent words are replaced with their corresponding unknown word tags based on their part of speeches 2 Table 1 shows examples of word bigrams including unknown word tags . Figure 2 shows the word length distribution of words consists of only kanji characters and words consists of only katakana characters . Figure 3 : Distribution of supersense mentions by domain ( left ) , and counts for tags occurring over 800 times ( below ) . For example , if x in figure 1 were evenir aux choses , then x14 would map to v1 = evenir , w2 = aux , and u3 = cho . As the search space increases expo nentially , it is not possible to explicitly represent it . If M = 10 , 15 Chinese words ( i.e. , the first 19 Chinese words in Table 3 except 叶玛斯 , 巴佐亚 , 坩埚 , 普利法 ) have their correct English translations at rank one position . segmentation ( Table 2 ) . Table 2 shows the NA M N O M PR O NA M 34 13 ( 21 % ) 67 ( 6 6 % ) 11 ( 4 6 % ) N O M 43 ( 67 % ) 21 48 ( 4 9 % ) 9 ( 8 9 % ) PR O 86 8 ( 32 % ) 17 71 ( 5 5 % ) 53 08 ( 2 4 % ) Table 2 : Number of clustering decisions made according to mention type ( rows anaphor , columns antecedent ) and percentage of wrong decisions . Table 1 : Performance of the mention detection system using lexical features only . Comparison of different systems on the ACE RDC 2004 corpus In Table 3 we summarize the improvements of different tree setups over SPT . This is especially true in the case of quotations—which are common in the ATB—where ( 1 ) will follow a verb like ( 2 ) ( Figure 1 ) . The model is described using a log-linear modeling approach , which is a generalization of the often used source channel approach This sentence should be tagged as shown in table 1 . In Table 5 we present results from small test cor pora for the productive affixes handled by the current version of the system ; as with names , the segmentation of morphologically derived words is generally either right or wrong . To illustrate the complete user model , in the figure 1 example the benefit of accepting would be7 2 4.2 = .8 keystrokes and the benefit of reject ing would be .2 keystrokes . This table also shows that : ( 1 ) Both modification within base-NPs and modification to NPs contribute much to performance improvement , acquiring the increase of F- measure by 4.4/2.4 units in mode M1 and 4.4/2.3 units in mode M2 respectively . As shown in Table 3 , using just context information alone , 10 Chinese words ( the first 10 ) have their correct English translations at rank one position . Figure 7 shows a structogram of the algorithm Thetheoretical upper bound of the decoding complex Figure 5 : Decoding algorithm using semantic role features . Figure 2 : Bootstrapping new heuristics . Although kanji sequences are difficult to seg ment , they can comprise a significant portion of Japanese text , as shown in Figure 1 . igure 1 gives an example . The bottom-up decoding algorithm for the TTS transducer is sketched in Figure 2 . Figure 1 : ( a ) An undirected graph G representing the similarity matrix ; ( b ) The bipartite graph showing three clusters on G ; ( c ) The induced clusters U ; ( d ) The new graph G1 over clusters U ; ( e ) The new bipartite graph over G1 p |vi ) = Vl 1 ... Table 7 shows the results . On the contrary , in the above training stage , although the samples are not accurate enough to represent the distribution defined by Equation 7 for each alignment aj , it is accurate enough for computing the expected counts , which are defined at the corpus level . Table 6 : Sample targets for meta alternations with high AP and mid-coherence values . The sources of our dictionaries are listed in Table 2 . CoreLex defines a layer of abstraction above WordNet consisting of 39 basic types , coarse- grained ontological classes ( Table 2 ) . For MUC7 , there are also no published results on systems trained on only the official training data of 200 aviation disaster articles . Comparison of segmentation algorithm using different linguistic features . The overall architecture of the log-linear modeling approach is summarized in Figure 1 Table 1 : BLEU4 scores of different systems Source Launching1 New2 Diplomatic3 Offensive4 SRF On 1 2 3 4 SRF Off 2 3 4 It1 is2 therefore3 necessary4 to5 speed6 up7 the8 equal better worse With SRF vs. W/O SRF 72 % 20.2 % 7.8 % Source transformation9 of10 traditional11 industries12 with13 high14 technologies15 Equation ( 12 ) , which is a set of word models trained for each part of speech ( POS + Poisson + bigram ) . Table 3 Classes of words found by ST for the test corpus . Table 1 : Output of word sense clustering . Figure 6 : Metaphors tagged by the system ( in bold ) whereby the main source of disagreement was the presence of lexicalized metaphors , e.g . verbs such as impose , decline etc . Figure 3 shows the performance and processing time comparison of various models and their combinations . Tables 4 and 5 show the effect of the pruning parameter tp with the histogram pruning parameter Np = 50,000 For instance , the gain for the prediction in figure 1 would be 2 7 8 = 6 . Examples are given in Table 4 . Table 5 shows type- and token-level error rates for each corpus . Table 2 : Accuracy for MBL and SVM classifiers on Talbanken05 nouns in accumulated frequency bins by Parole frequency . The first machine , illustrated in Figure 1 encodes the prefix and suffix expansion rules , producing a lattice of possible segmentations . Table 1 reports experimental results using lexical features only ; we observe that the stemming n-gram features boost the performance by one point ( 64.7 vs. 65.8 ) . # o is the total number of output English translations . As a result , Arabic sentences are usually long relative to English , especially after We also investigated the effect of varying M . The results are shown in Table 2 . Strube ( 1998 ) s centeri ng appro ach ( whos e senten ce orderi ng is designate d as SR2 in Table 2 ) also deals with and even prefer s intrase ntenti al anaph ora , which raises the upper limit to a more accept able 80.2 % . > 10 nouns ( a ) ( b ) classified as 222 125 ( a ) class animate 49 3390 ( b ) class inanimate Table 4 : Confusion matrix for the MBLclassifier with a general feature space on the > 10 data set on Talbanken05 nouns . Contribution of constituent dependencies in respective mode ( inside parentheses ) and accumulative mode ( outside parentheses ) The table shows that the final DSPT achieves the best performance of 77.4 % /65.4 % /70.9 in precision/recall/F-measure respectively after applying all the dependencies , with the increase of F-measure by 8.2 units compared to the baseline MCT . Table 8a shows that the best model recovers SBAR at only 71.0 % F1 . The second model is Equa tion ( 13 ) , which is a set of word models trained for Table 2 : Results for different predictor configurations . Table 1 shows the usefulness evaluation result . Figure 1 ( d ) shows a sentence maintain rental property he owns in the state , where the ART.User-or-Owner relation holds between the entities property and he Figure 1 Architecture of the statistical translation approach based on Bayes decision rule . Tables 6 and 7 show the effect of the pruning pa rameter Np with the pruning parameter tp = 10 12 In all four tables , we provide theresults for using no heuristic functions and three variants of an increasingly infor mative heuristic function . The reordering of the semantic roles from source to target is computed for each TTS template as part of the template extraction process , using the word-level alignments between the LHS/RHS of the TTS template ( e.g. , Figure 3 ) . See Fig ure 6 for some examples . The use of the language model feature in equation ( 18 ) helps take long-range dependencies better into account We see from Table 5 , that the improvement in overall parse results is mainly in terms of dependency labeling , reflected in the LAS score . Figure 1 shows the influence of the four parameters . If it starts with a lower case letter , and contains both upper and lower case letters , then ( mixedCaps , zone ) is set to 1 . If M = 10 , 15 Chinese words ( i.e. , the first 19 Chinese words in Table 3 except 叶玛斯 , 巴佐亚 , 坩埚 , 普利法 ) have their correct English translations at rank one position . Table 6 : Final results on CTB6 and CTB7 accuracies of POS tagging and dependency parsing were remarkably improved by 0.6 % and 2.4 % , respectively corresponding to 8.3 % and 10.2 % error reduction . In Table 1 , period 1 is Jul 01 – Jul 15 , period 2 is Jul 16 – Jul 31 , … , period 12 is Dec 16 – Dec 31 . composite kernel 83.0 72.0 77.1 Zhou et al. , ( 2007 ) : composite kernel 82.2 70.2 75.8 Zhang et al. , ( 2006 ) : composite kernel 76.1 68.4 72.1 Zhao and Grishman , ( 2005 ) :4 composite kernel 69.2 70.5 70.4 Ours : CTK with UPST 80.1 70.7 75.1Zhou et al. , ( 2007 ) : context sensitive CTK with CS-SPT 81.1 66.7 73.2 Zhang et al. , ( 2006 ) : CTK with SPT 74.1 62.4 67.7 Table 4 . Figure 6 : EM Algorithm For Estimating TTS Templates and Semantic Features framework ( May and Knight , 2007 ) . Reduction of conjuncts for NP coordination Figure 1 . Table 8 : Part of speech tagging accuracy of unknown words ( the last column represents the percentage of correctly tagged unknown words in the correctly segmented unknown words Table 1 : Approximate times in seconds to generate predictions of maximum word sequence length M , on a 1.2GHz processor , for the MEMD model . n our experimentations , SVMlight ( Joachims , 1998 ) with the tree kernel function ( 75.0 ) ( 53.7 ) ( 62.6 ) Table 1 However , the learning curves in Figure 3 show that the Berkeley parser does not exceed our manual grammar by as wide a margin as has been shown for other languages ( Petrov , 2009 ) . Throughout in this paper , we used Equation ( 9 ) to compute the word spelling probabilities . In general , as shown in this figure , there may be additional transformations to make the translation task simpler for the algorithm . Table 1 : Results of different systems on the CoNLL 12 English data sets . the time-consuming renormalization in equation ( 3 ) is not needed in search The figure schematically shows a small portion of the graph describing the concepts of mechanism ( concrete ) , political system and relationship ( abstract ) at two levels of generality . We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4 , 5 , and 6 . The probability of using an alignment template to translate a specific source language phrase f is estimated by means of relative frequency However , if we remove the mouse-node from its local graph illustrated in figure 1 , the graph decomposes into two parts , one representing the electronic device meaning of mouse and the other one representing its animal sense . Figure 2 : Word length distribution of kanji words and katakana words length model does not reflect the variation of the word length distribution resulting from the Japanese orthography . The sources of our dictionaries are listed in Table 2 . Table 4 : Segmentation , POS tagging , and ( unlabeled attachment ) dependency F1 scores averaged over five trials on CTB5c . Table 2 gives the mean values for the comparison of troughs placed by the segmentation algorithm to the segmentation points identified by the test subjects for all the texts Figure 2 : Collecting evidence for a word boundary - are the non-straddling n-grams 8 1 and 82 more frequent than the straddling n-grams t 1 , t2 , and t3 ? Throughout in this paper , we used Equation ( 9 ) to compute the word spelling probabilities . For example , if a token starts with a capital letter and ends with a period ( such as Mr. ) , then the feature InitCapPeriod is set to 1 , etc The list of the features used in our joint model is presented in Table 1 , where S01 S05 , W01 W21 , and T01 05 are taken from Zhang and Clark ( 2010 ) , and P01 P28 are taken from Huang and Sagae ( 2010 ) . Consider the example in Figure 1 As shown in Figure 8 , relative word performance was not degraded and sometimes even slightly better . Table 1 shows the usefulness evaluation result . Table 1 : Approximate times in seconds to generate predictions of maximum word sequence length M , on a 1.2GHz processor , for the MEMD model . we directly model the posterior probability Pr ( eI| f J ) Table 2 : Character type configuration of infrequent words in the EDR corpus Based on this experiment , we set the beam size of SegTagDep to 64 throughout the exper 64 96.28 92.37 74.96 0.48 Table 3 : F1 scores and speed ( in sentences per sec . ) Table 2 : Performance of Algorithms switch under different input data . A comparison of the lines for grammatical roles and for surface order in Table 1 shows that the same is true in German . Figure 6 : EM Algorithm For Estimating TTS Templates and Semantic Features framework ( May and Knight , 2007 ) . Examples are given in Table 4 . Table 3 : Results for different user simulations . Cik Figure 1 : Local graph of the word mouse As for the unknown word model , word-based char acter bigrams are computed from the words with Table 5 : Cross entropy ( CE ) per word and character perplexity ( PP ) of each unknown word model Part of Speech Estimation Accuracy 0.95 0.9 frequency one ( 49,653 words ) . Table 3 shows the performance and speed of the full joint model ( with no dictionaries ) on CTB5c1 with respect to the beam size . alignment models Pr ( f J , aJ | eI ) , where N is role features when combining two children states , and ex amples can be found in Figure 3 . If it is made up of all capital letters , then ( allCaps , zone ) is set to 1 the target Chen Guangcheng only appears once in Weibo The entity features can be attached under the top node , the entity nodes , or directly combined with the entity nodes as in Figure 1 . Reduction of conjuncts for NP coordination Figure 1 . Figure 1 suggests that the best setup is dependent on the specific meta sense or meta alternation being MACRO MICRO repM gram gramlex lex space type MACRO MICRO repA The results when we set M = 10 are shown in Table 1 . ( 2004 ) in figure 1 . See Figure 3 for examples . The probabilities P ( < U-t > lwi_I ) can be esti mated from the relative frequencies in the training corpus whose infrequent words are replaced with their corresponding unknown word tags based on their part of speeches 2 Table 1 shows examples of word bigrams including unknown word tags . Table 2 : Statistics of datasets . The annotation manual ( Teleman , 1974 ) states that a markable should be tagged as human ( H H ) if it may be replaced by the interrogative pronoun vem who and be referred to by the personal pronouns han he or hon she .There are clear similarities between the anno tation for human reference found in Talbanken05 and the annotation scheme for animacy discussed HUM Other animate Inanimate ORG ANIM CONC NCONC TIME PLACE Figure 1 : Animacy classification scheme ( Zaenen et al. , 2004 ) Figure 4 shows examples of alignment templates e suggest the use of a log-linear model to incorporate the various knowledge sources into an overall translation system and to perform discriminative training of the free model parameters These semantic features Figure 8 : Examples of the MT outputs with and without SRFs n our experimentations , SVMlight ( Joachims , 1998 ) with the tree kernel function ( 75.0 ) ( 53.7 ) ( 62.6 ) Table 1 The first example in Table 3 shows that words ending in ' - ' are likely to be nouns . Figure 3 gives the algorithm phrase-extract that computes the phrases Table 5 shows type- and token-level error rates for each corpus . he largest effect seems to come from taking into account the bigram dependence , which achieves an mWER of 32.9 % Evaluation results are listed in Table 2 . Typically , the translation probability Pr ( eI | f J ) is decomposed via additional hid 1 1 den variables As illus trated in Figure 1 ( e ) , the NP coordination in the Qian et al . Table 3 shows examples of common char acter bigrams for each part of speech in the infre quent words of the EDR corpus . This approach has been suggested by Papineni , Roukos , and Ward ( 1997 , 1998 ) for a natural language understanding task . The graphical structure depicted in Figure 1 models these relations between the four mentions Leaders , Paris , recent developments and They . As can be seen in figure 2 , wing `` part of a bird '' is closely related to tail , as is wing `` part of a plane '' Figure 2 shows that the tagging accuracy tends to increase with the context size . At the morp heme level , stems are divid ed from their affixe s. For exam ple , altho ugh both naga no ( Naga no ) and shi ( city ) can appea r as indivi dual words , nagano shi ( Nag ano city ) is brack eted as [ [ naga no ] [ s hi ] ] , since here shi Figure 3 : Determining word boundaries . Table 1 evaluates the contributions of different kinds of constituent dependencies to extraction performance on the 7 relation types of the ACE RDC 2004 corpus This group consists of 10 features based on the string , as listed in Table 1 . Table 1 presents the wide range of cases that are used to create the morphs . Table 3 : Examples of common character bigrams for each part of speech in the infrequent words pa rt of sp ee ch ch ar ac ter bi gr a m fre qu en cy no un nu m be r a dj e ct iv al v er b v er b ad je cti ve ad ve rb < e o w > < b o w > 1 S `` J < e o w > I t < e o w > L < e o w > < e o w > 13 43 4 8 4 3 2 7 2 1 3 69 63 resented all unknown words by one length model . On the other hand , using our method of combining both sources of information and setting M = ∞ , 19 Chinese words ( i.e. , the first 22 Chinese words in Table 3 except 巴佐亚 , 坩埚 , 普利法 ) have their correct English translations at rank one position . Table 1 : Scores for CityU corpus Table 4 : The amount of training and test sets The first factor in the righthand side of Equa tion ( 13 ) is estimated from the relative frequency of the corresponding events in the training corpus . The model is described using a log-linear modeling approach , which is a generalization of the often used source channel approach Examples of the deletion features can be found in Figure 1 . Table 3 : Performance on T3 using a predefined tree structure . ( 2011 ) , we confirmed that omission of the look-ahead features results in a 0.26 % decrease in the parsing accuracy on CTB5d ( dev ) .Figure 2 : F1 scores ( in % ) of SegTagDep on CTB 5c1 w.r.t . the training epoch ( x-axis ) and parsing feature weights ( in legend ) . Figure 2 : Above : The complete supersense tagset for nouns ; each tag is briefly described by its symbol , NAME , short description , and examples . Table 3 shows the performance and speed of the full joint model ( with no dictionaries ) on CTB5c1 with respect to the beam size . Figure 1shows the word length distribution of in frequent words in the EDR corpus , and the estimate of word length distribution by Equation ( 6 ) whose parameter ( .A = 4.8 ) is the average word length of infrequent words . Table 9 shows the training and test corpus statistics . The statistics of 96 these splits are shown in Table 2 . Figure 1 : Word length distribution of unknown words and its estimate by Poisson distribution A token that is allCaps will also be initCaps . For example , the expressions in Figure 2 are identified as paraphrases by this method ; so these three patterns will be placed in the same pattern set . Prec . is the precision . Thetheoretical upper bound of the decoding complex Figure 5 : Decoding algorithm using semantic role features . Table 2 : CoreLex s basic types with their corresponding WordNet anchors . For example , in the sentence bought one of town s two meat- packing plants as illustrated in Figure 1 ( a ) , the constituents before the headword plants can be removed from the parse tree . Table 1 ; capital letters designate sets and small letters elements of sets ) .2 For a lemma l like lamb , we want to knowhow well a meta alternation ( such as ANIMAL FOOD ) explains a pair of its senses ( such as the animal and food senses of lamb ) .3 This is formalized through the function score , which maps a meta alternation and two senses onto a score . Shortest-enclosed Path Tree from P/R/F ( 81.1/6.7/73.2 ) of Dynamic Context-Sensitive Shortest- enclosed Path Tree according to Table 2 ( Zhou et al. , 2007 ) The second model is the combination of Poisson distribution ( Equation ( 6 ) ) and character bigram Table 4 : Number of recall errors according to mention type ( rows anaphor , columns antecedent ) . Table 2 shows these similarity measures . The graphical structure depicted in Figure 1 models these relations between the four mentions Leaders , Paris , recent developments and They . Table 1 lists both the success rate maximally achievable ( broken down according to different types of pronouns ) and the average number of antecedents remaining after applying each factor . n all four tables , we provide theresults for using no heuristic functions and three variants of an increasingly infor mative heuristic function . If it is made up of all capital letters , then ( allCaps , zone ) is set to 1 The corresponding translation quality improves from an mWER of 45.9 % to an mWER of 31.8 % . Figure 1 shows the record for the headword orange followed by its collocates Table 2 shows the distribution of character type sequences that constitute the infrequent words in the EDR corpus . Table 2 shows the experimental results with and without the stem n-grams features . Table 1 : Performance of the mention detection system using lexical features only . Table 2 Similarity matrix for segmentation judgments . Table 8 shows that by using word type and part of speech information , recall is improved from 28.1 % to 40.6 % and precision is improved from 57.3 % to 64.1 % . Results : Table I gives the results for the comparison of the troughs placed by the segmentation algorithm to the known subject change points . A simple lexicalized PCFG with second order Markovization gives relatively poor performance : 75.95 % F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline ( Table 7 ) . Comparison of segmentation algorithm using different linguistic features . Figure 1 : Statistics from 1993 Japanese newswire ( NIKKEI ) , 79,326,406 characters total . of the 43 words are translated to English multi-word phrases ( denoted as “ phrase ” in Table 3 ) . Table 6 shows the word segmentation accuracy of four unknown word models over test set-2 . However , this information is hard to extract reliably from the available data ; and even if were obtainable , many of the 0.3 0.2 0.1 0 60 50 40 30 20 10 0 10 20 30 40 50 60 gain ( length of correct prefix length of incorrect suffix ) Figure 2 : Probability that a prediction will be accepted versus its gain . and ap plied to an Arabic sentence in figure 1 . The points labelled smoothed in figure 2 were obtained using a sliding-average smoother , and the model curve was obtained using two-component Gaussian mixtures to fit the smoothed empirical likelihoods p ( gain|a = 0 ) and p ( gain|a = 1 ) . It should be emphasized that this constraint to consecutive phrases limits the expressive power . We see from Table 5 , that the improvement in overall parse results is mainly in terms of dependency labeling , reflected in the LAS score . Table 5 breaks down the performance of the best CAM model by meta alternation . We illustrate its use with an example ( see Then , we average the contributions of each n-gram order : Figure 2 ) . Table 6 lists a sample of targets for the five meta alternations involved . his optimization can be performed using the expectation maximization ( EM ) algorithm ( Dempster , Laird , and Rubin 1977 ) . Table 2 : Character type configuration of infrequent words in the EDR corpus To illustrate how SRF impacts the translation results , Figure 8 gives 3 examples of the MT outputs with and without the SRFs Table 1 displays the performance of our model and of the systems that obtained the best ( Fernandes et al. , 2012 ) and the median performance in the MUC B3 CEAFe average R P F1 R P F1 R P F1 CoNLL 12 English development data be st 64 . CoreLex defines a layer of abstraction above WordNet consisting of 39 basic types , coarse- grained ontological classes ( Table 2 ) . Collocation : Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio ( Church and Hanks , 1990 ) and outputted to a lexicon . The direct translation probability is given by where N is role features when combining two children states , and ex amples can be found in Figure 3 . In the following , we describe the criterion that defines the set of phrases that is consistent with the word alignment matrix Again , Table 2 shows that using stem n-grams features gave a small boost to the whole main-type classification system4 . Table 2 shows the manual evaluation results based on the entire test set , and the improvement from SRF is significant at p < 0.005 based on a t-test . For example , if x in figure 1 were evenir aux choses , then x14 would map to v1 = evenir , w2 = aux , and u3 = cho . Table 2 compares the results of the unconstrained version of HGFC against those of AGG on our largest test set T2 . Only tokens with initCaps not found in commonWords are tested against each list in Table 2 . Figure 4 : An example showing how to compute the target side position of a semantic role by using the median of its aligning points . The model 1 The total 74,597 sentence pairs used in experiments are those in the FBIS corpus whose English part can be parsed using Charniak ( 2000 ) Figure 3 : Time to read and accept or reject proposals versus their length tion , because the empirical probability of acceptance is very low when it is less than zero and rises rapidly as it increases . We see that the language model perplexity improves from 4,781 for a unigram model to 29.9 for a trigram model . Table 1 : Effect of Factors antecedent is found in the previous context , subsequent sentences are inspected ( cataphora ) , also ordered by proximity to the pronoun . where N is role features when combining two children states , and ex amples can be found in Figure 3 . Table 2 Similarity matrix for segmentation judgments . As column 5 ( SVM ) in table 2 shows , the classification results are very similar to the results obtained with MBL.12 We furthermore find a very similar set of errors , and in particular , we find that 51.0 % of the errors for the inanimate class are nouns with the gradient animacy properties presented in ( 9 ) - ( 13 ) above . Figure 3 : Distribution of supersense mentions by domain ( left ) , and counts for tags occurring over 800 times ( below ) . We set Table 6 : Word segmentation accuracy of all words r e c pr ec F Po iss on +b igr a m W T +P oi ss on +b igr a m P O S + Po iss on +b igr a m P O S + W T + Po iss on + bi gr a m 94 .5 94 .4 94 .4 94 .6 93 .1 93 .8 93 .6 93 .7 93 .8 94 .1 94 .0 94 .1 Table 1 presents an overview of the animacy data Clas s Ani mat e Type s Tok 6 4 4 ens cover ed 6 0 1 0 Inan imat e 691 0 3 4 8 2 2 Tota l 755 4 4 0 8 3 2 Table 1 : The animacy data set from Talbanken05 ; number of noun lemmas ( Types ) and tokens in each class . The result is shown in Table 4 : the baseline numbers without stem features are listed under Base , and the results of the coreference system with stem features are listed under Base+Stem . The translations of 6 of the 43 words are words in the dictionary ( denoted as “ comm. ” in Table 3 ) and 4 of the 43 words appear less than 10 times in the English part of the corpus ( denoted as “ insuff ” ) . Table 3 : Scores for UPUC corpusFrom those tables , we can see that a simple ma jority voting algorithm produces accuracy that is higher than each individual system and reasonably high F-scores overall . We include a list of per-category results for selected phrasal labels , POS tags , and dependencies in Table 8 . The second model is the combination of Poisson distribution ( Equation ( 6 ) ) and character bigram composite kernel 83.0 72.0 77.1 Zhou et al. , ( 2007 ) : composite kernel 82.2 70.2 75.8 Zhang et al. , ( 2006 ) : composite kernel 76.1 68.4 72.1 Zhao and Grishman , ( 2005 ) :4 composite kernel 69.2 70.5 70.4 Ours : CTK with UPST 80.1 70.7 75.1Zhou et al. , ( 2007 ) : context sensitive CTK with CS-SPT 81.1 66.7 73.2 Zhang et al. , ( 2006 ) : CTK with SPT 74.1 62.4 67.7 Table 4 . For example , no synset covers any combinations of the main words in Figure 2 , namely buy , acquire and merger Except our own and MENE + reference resolution , the results in Table 6 are all official MUC7 results . Figure 8 : Relative word accuracy as a function of training set size . Preliminary observations show that the different neighbours in Table 1 can be used to indicate with great accuracy which of the senses is being used Tree setups P ( % ) R ( % ) F SPT 76.3 59.8 67.1 DSPT 77.4 65.4 70.9 UPST ( BOF ) 80.4 69.7 74.7 UPST ( FPT ) 80.1 70.7 75.1 UPST ( EPT ) 79.9 70.2 74.8 Table 2 . Table 8 shows the tagging accuracy of unknown words . Table 3 illustrates the effects of different components of the user model by showing results for simulated users who read infinitely fast and accept only predictions having positive benefit ( superman ) ; who read normally but accept like superman ( rational ) ; and who match the standard user model ( real ) . This is especially true in the case of quotations—which are common in the ATB—where ( 1 ) will follow a verb like ( 2 ) ( Figure 1 ) . Table 1 shows the effect of the role-based preference on our data . Finally , Table 4 shows the results for the unconstrained HGFC on T2 and and T3 when the tree structure is not predefined but inferred automatically as described in section 3.2.3 . To compute the third factor of Equation ( 13 ) , we have to estimate the character bigram probabilities that are classified by word type and part of speech . Figure 3 : Distribution of supersense mentions by domain ( left ) , and counts for tags occurring over 800 times ( below ) . Table 2 : Accuracy for MBL and SVM classifiers on Talbanken05 nouns in accumulated frequency bins by Parole frequency . Table 5 : Meta alternations and their average precision values for the task . PRO ( c ) Entity-Paired Tree ( EPT ) Figure 2 . Figure 2 : Bootstrapping new heuristics . Figure 1 shows the influence of the four parameters . The second condition is necessary to allow for single-character words ( see Figure 3 ) . Again , Table 2 shows that using stem n-grams features gave a small boost to the whole main-type classification system4 . Also , the CRF model using maximum subword-based tagging ( Zhang et al. , 2006 ) and the CRF model using minimum subword-based tagging , both of which are statistical methods , are used individually to solve the Figure 1 : Outline of the segmentation process 2.1 Forward Maximum Matching . We see from Table 5 , that the improvement in overall parse results is mainly in terms of dependency labeling , reflected in the LAS score . Figure 1shows the word length distribution of in frequent words in the EDR corpus , and the estimate of word length distribution by Equation ( 6 ) whose parameter ( .A = 4.8 ) is the average word length of infrequent words . W can be encoded by a undi rected graph G ( Figure 1 ( a ) ) , where the verbs are mapped to vertices and the Wij is the edge weight between vertices i and j . Table 3 : Scores for UPUC corpusFrom those tables , we can see that a simple ma jority voting algorithm produces accuracy that is higher than each individual system and reasonably high F-scores overall . Figure 2 : Collecting evidence for a word boundary - are the non-straddling n-grams 8 1 and 82 more frequent than the straddling n-grams t 1 , t2 , and t3 ? A comparison of the lines for grammatical roles and for surface order in Table 1 shows that the same is true in German . ( Abbreviations are listed in Table 2 . ) Table 1 shows the effect of the role-based preference on our data . and 8 show word accuracy for Chasen , Juman , and our algorithm for parameter settings optimizing word precision , recall , and F-measure rates . Table 4 : Sample of experimental items for the meta alternation anmfod . the target Chen Guangcheng only appears once in Weibo whose unvocalized surface forms 0 an are indistinguishable . This is because the lefthand side of Equation ( 7 ) represents the probability of the string c1 Table 4 : Effect of Arabic stemming features on coreference resolution . This is usually straightforward , with the exception of the case where the words that are aligned to a particular role s span in the source side are not continuous in the target side , as shown in Figure 4 . ( Equation ( 7 ) ) ( Poisson + hi gram ) . Table 2 contains results for two different translation models . We also propose to use the features U01 U03 , which we found are effective to adjust the character Figure 1 : Illustration of the alignment of steps . Figure 1 shows examples of the feature SRR . ( 2004 ) makes use of a coding manual designed for a project studying genitive modification ( Garretson et al. , 2004 ) and presents an explicit annotation scheme for an _ Samma _ PO _ KP erfarenhet NN _ gjorde VV PT engelsmannen NN DD|HH imacy , illustrated by figure 1 . For fair comparison , we have tabulated all results with the size of training data used ( Table 5 Figure 1 shows the record for the headword orange followed by its collocates whose unvocalized surface forms 0 an are indistinguishable . Figure 3 : Time to read and accept or reject proposals versus their length tion , because the empirical probability of acceptance is very low when it is less than zero and rises rapidly as it increases . On the other hand , using our method of combining both sources of information and setting M = ∞ , 19 Chinese words ( i.e. , the first 22 Chinese words in Table 3 except 巴佐亚 , 坩埚 , 普利法 ) have their correct English translations at rank one position . Table 1 ) , i.e. , the degree to which a sense pair ( s1 , s2 ) matches a meta alternation a . If we compare the error rates in Table 7 , which correspond to about 55 search errors in Table 6 , we obtain an mWER of 36.7 % ( 53 search errors ) using no heuristic function and an mWER of 32.6 % ( 57 search errors ) using the combined heuristic function . By far the most frequent tagging error was the confusion of nominative and accusative case . 14http : //www.cis.upenn.edu/ dbikel/software.html Gold standard Automatic UAS LAS UAS LAS Baseline 89.87 84.92 89.87 84.92 Anim 89.81 84.94 89.87 84.99 Table 5 : Overall results in experiments with automatic features compared to gold standard features , expressed as unlabeled and labeled attachment scores . ( Abbreviations are listed in Table 2 . ) Table 2 gives the mean values for the comparison of troughs placed by the segmentation algorithm to the segmentation points identified by the test subjects for all the texts The graphical structure depicted in Figure 1 models these relations between the four mentions Leaders , Paris , recent developments and They . Table 1 : Results of different systems on the CoNLL 12 English data sets . Table 4 : The amount of training and test sets The first factor in the righthand side of Equa tion ( 13 ) is estimated from the relative frequency of the corresponding events in the training corpus . For example , looking at Figure 1 ( b ) , V on G can be grouped into three clusters u1 , u2 and u3 . The model is described using a log-linear modeling approach , which is a generalization of the often used source channel approach In all four tables , we provide theresults for using no heuristic functions and three variants of an increasingly infor mative heuristic function Finally , Table 4 shows the results for the unconstrained HGFC on T2 and and T3 when the tree structure is not predefined but inferred automatically as described in section 3.2.3 . The distribution of errors is displayed in Table 4 . Table 4 : Sample of experimental items for the meta alternation anmfod . Nc Nl HGFC uncons trained A G G N MI F N M I F 13 0 13 3 57 .3 1 36 .6 5 54 .2 2 32 .6 2 11 4 11 7 54 .6 7 37 .9 6 51 .3 5 32 .4 4 50 51 37 .7 5 40 .0 0 32 .6 1 32 .7 8 Table 2 : Performance on T2 using a predefined tree structure . Figure 2 : Decoding algorithm for the standard Tree-to-String transducer . Figure 2 shows the F1 scores of the proposed model ( SegTagDep ) on CTB5c1 with respect to the training epoch and different parsing feature weights , where Seg , Tag , and Dep respectively denote the F1 scores of word segmentation , POS tagging , and dependency parsing . The translations of 6 of the 43 words are words in the dictionary ( denoted as “ comm. ” in Table 3 ) and 4 of the 43 words appear less than 10 times in the English part of the corpus ( denoted as “ insuff ” ) . Strube ( 1998 ) s centeri ng appro ach ( whos e senten ce orderi ng is designate d as SR2 in Table 2 ) also deals with and even prefer s intrase ntenti al anaph ora , which raises the upper limit to a more accept able 80.2 % . able 2 shows the corpus statistics for this task . range free green lemon peel red state yellow As Figure 3 shows , word type information improves the prediction accuracy significantly . Figure 1 describes the components and how this system works . It should be emphasized that this constraint to consecutive phrases limits the expressive power . In Figure 4 we show an example of variation between the parsing models . Nc Nl HGFC uncons trained A G G N MI F N M I F 13 0 13 3 57 .3 1 36 .6 5 54 .2 2 32 .6 2 11 4 11 7 54 .6 7 37 .9 6 51 .3 5 32 .4 4 50 51 37 .7 5 40 .0 0 32 .6 1 32 .7 8 Table 2 : Performance on T2 using a predefined tree structure . Figure 3 : An example showing the combination of the semantic role sequences of the states . Then , every phrase f produces its translation e ( using the corresponding alignment template z ) . This leaves us with 60 meta alternations , shown in Table 5 . This corresponds to maximizing the equivocation or maximizing the likelihood of the direct-translation model Figure 2 plots AP by for all meta alternations . Table 8 shows the effect of the length of the language model history on translation quality . IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM Again , Table 2 shows that using stem n-grams features gave a small boost to the whole main-type classification system4 . This leaves us with 60 meta alternations , shown in Table 5 . this error is not counted , the tagging accuracy on the development data rises from 92.17 % to 94.27 % . Figure 1 : Word length distribution of unknown words and its estimate by Poisson distribution Figure 1 portrays how the states are aligned using the proposed scheme , where a subtree is denoted as a rectangle with its partial index shown inside it . We set Table 6 : Word segmentation accuracy of all words r e c pr ec F Po iss on +b igr a m W T +P oi ss on +b igr a m P O S + Po iss on +b igr a m P O S + W T + Po iss on + bi gr a m 94 .5 94 .4 94 .4 94 .6 93 .1 93 .8 93 .6 93 .7 93 .8 94 .1 94 .0 94 .1 equation 2 ) says that the prob ability of starting a new entity , given the current mention m and the previous entities e1 , e2 , , et , is simply 1 minus the maximum link probability between the current mention and one of the previous entities . Table 1 shows empirical search timings for various values of M , for the MEMD model described in the next section . Only tokens with initCaps not found in commonWords are tested against each list in Table 2 . Figure 1 : Example of a prediction for English to French translation . In all four tables , we provide theresults for using no heuristic functions and three variants of an increasingly infor mative heuristic function . U = { up } m represent the hidden m struct a new graph G1 ( Figure 1 ( d ) ) with the clusters U as vertices . As each global feature group is added to the list of features , we see improvements to both MUC6 and Table 5 breaks down the performance of the best CAM model by meta alternation . In all four tables , we provide theresults for using no heuristic functions and three variants of an increasingly infor mative heuristic function . This approach can be seen as a generalization of the originally suggested source channel modeling framework for statistical machine translation Figure 5 gives an example of the word alignment and phrase alignment of a German English sentence pair.We describe our model using a log-linear modeling approach Table 1 lists both the success rate maximally achievable ( broken down according to different types of pronouns ) and the average number of antecedents remaining after applying each factor . Results are shown in Table 2 ; we see that better word alignment results do not lead to better translations . For example , the pairwise words orange and peel form a collocation . This sum can be computed efficiently using the algorithm shown in Figure 8 As the search space increases expo nentially , it is not possible to explicitly represent it . However , the spelling model , especially the character bigrams in Equation ( 17 ) are hard to es timate because of the data sparseness . Among all possible target sentences , we will choose the sentence with the highest probability Figure 2 shows the word length distribution of words consists of only kanji characters and words consists of only katakana characters . 5http : //cactus.aistnara.ac.jp/lab/nltlchasen.html 6http : //pine.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html Word accuracy 90 CHASEN JUMAN opllnizt oplnuo recall opiJTozt F Figure 4 : Word accuracy . 5http : //cactus.aistnara.ac.jp/lab/nltlchasen.html 6http : //pine.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html Word accuracy 90 CHASEN JUMAN opllnizt oplnuo recall opiJTozt F Figure 4 : Word accuracy . The argmax operation denotes the search problem , that is , the generation of the output sentence in the target language Figure 1 : A sentence from the article Islamic GoldenAge , with the supersense tagging from one of two anno tators . The sources of our dictionaries are listed in Table 2 . whose unvocalized surface forms 0 an are indistinguishable . Table 1 : Examples of word bigrams including un known word tags example Column four ( MBL ) in table 2 shows the accuracy obtained with all features in the general feature space . The alignment aJ that has the highest probability ( under a certain model ) is also called the Viterbi alignment ( of that model ) The renormalization needed in equation ( 3 ) requires a sum over manypossible sentences , for which we do not know of an efficient algorithm and Table 6 show a comparison of the segmentation and POS tagging accuracies with other state-of-the-art models . The first example in Table 3 shows that words ending in ' - ' are likely to be nouns . Table 3 : Results for different user simulations . In the last two lines of Equation 3 , φǫ and each P ( f |e ) = `` £s c ( f |e ; f ( s ) , e ( s ) ) ( 4 ) φi are not free variables , but are determined by f s c ( f |e ; f ( s ) , e ( s ) ) the alignments . In all four tables , we provide theresults for using no heuristic functions and three variants of an increasingly infor mative heuristic function . As can be seen in figure 2 , wing `` part of a bird '' is closely related to tail , as is wing `` part of a plane '' Figure 1 : Organisation of the hierarchical graph of concepts Following previous semantic noun classification experiments ( Pantel and Lin , 2002 ; Bergsma et al. , 2008 ) , we use the grammatical relations ( GRs ) as features for clustering . Table 2 presents the distributions of some examples of morphs and their targets in English Twitter and Chinese Sina Weibo . But it conflates the coordinating and discourse separator functions of wa ( < ..4.b � � ) into one analysis : conjunction ( Table 3 ) . Figure 3 shows the distribution of SSTs in the corpus . The algorithm takes into account possibly unaligned words at the boundaries of the source or target language phrases . Table 3 lists new conceptsthat CAM introduces to manipulate vector represen tations . These semantic features Figure 8 : Examples of the MT outputs with and without SRFs The corresponding translation quality improves from an mWER of 45.9 % to an mWER of 31.8 % . Table 11 gives an overview on the training and test data . L set of lemmas IL set of ( lemma-wise ) instances SL set of ( lemma-wise ) senses inst : L ( IL ) mapping lemma instances sns : L ( SL ) mapping lemma senses M set of meta senses meta : SL M mapping senses meta senses A M M set of meta alternations ( MAs ) A set of MA representations score : A S2 R scoring function for MAs repA : A A MA representation function comp : A S2 R compatibility function Table 1 : Notation and signatures for our framework . IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM With an absolute frequency threshold of 10 , we obtain an accuracy of 95.4 % , which constitutes a 50 % reduction of error rate.Table 3 presents the experimental results rela tive to class . However , the spelling model , especially the character bigrams in Equation ( 17 ) are hard to es timate because of the data sparseness . Table 1 shows our results and the results of Stevenson and Joanis ( 2003 ) on T1 when employing AGG using Ward as the linkage criterion . On the contrary , in the above training stage , although the samples are not accurate enough to represent the distribution defined by Equation 7 for each alignment aj , it is accurate enough for computing the expected counts , which are defined at the corpus level . We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4 , 5 , and 6 . Table 8b shows that verbal nouns are the hardest pre-terminal categories to identify . Collocation : Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio ( Church and Hanks , 1990 ) and outputted to a lexicon . Table 1 : Comparison against Stevenson and Joanis ( 2003 ) s result on T1 ( using similar features ) . The second model is Equa tion ( 13 ) , which is a set of word models trained for Collocations were automatically located in a text by looking up pairwise words in this lexicon Table 3 : Performance of the mention detection system including all ACE 04 subtasks Figure 2 : Dependency representation of example ( 2 ) from Talbanken05 . Figure 1 describes the components and how this system works . Table 2 : Scores for MSRA corpus Table 4 : NMI of unconstrained HGFC when trees for T2 and T3 are inferred automatically . using the convolution parse tree kernel as depicted in Figure 1 . Table 1 , Figure 1 , and Figure 2 shows the AER results for different models . Collocations were automatically located in a text by looking up pairwise words in this lexicon Figure 7 : Compatible brackets and all-compatible bracket rates when word accuracy is optimized . Table 6 shows the word segmentation accuracy of four unknown word models over test set-2 . Figure 3 shows the part of speech prediction accu racy of two unknown word model without context . able 1 shows the bilingual phrases containing between two and seven words that result from the application of this algorithm to the alignment of Figure 2 . An abridged version of the grammatical representation produced by the implemented grammar for this sentence is presented in Figure 1 , where the feature structures below the tree correspond to partial grammatical representations of the constituents 16 See Kamp and Reyle ( 1993 ) for a comprehensive rendering of DRT , and Branco ( 2000 , Chapter 5 ) for an . For example , the expressions in Figure 2 are identified as paraphrases by this method ; so these three patterns will be placed in the same pattern set . Table 2 shows the distribution of character type sequences that constitute the infrequent words in the EDR corpus . Figure 7 : Compatible brackets and all-compatible bracket rates when word accuracy is optimized . The results are displayed in Table 3 . First , the source sentence words f J are grouped into phrases f K . For each phrase f an 1 1 alignment template z is chosen and the sequence of chosen alignment templates is reordered ( according to K ) . Table ( 1 ) and Eq . equation 2 ) says that the prob ability of starting a new entity , given the current mention m and the previous entities e1 , e2 , , et , is simply 1 minus the maximum link probability between the current mention and one of the previous entities . Figure 5 gives an example of the word alignment and phrase alignment of a German English sentence pair.We describe our model using a log-linear modeling approach By far the most frequent tagging error was the confusion of nominative and accusative case . A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC A word in one language can be translated to zero , one , or several words in other languages . Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation . We built a fertility hidden Markov model by adding fertility to the hidden Markov model . This model not only achieves lower alignment error rate than the hidden Markov model , but also runs faster . It is similar in some ways to IBM Model 4 , but is much easier to understand . We use Gibbs sampling for parameter estimation , which is more principled than the neighborhood method used in IBM Model 4 . IBM models and the hidden Markov model ( HMM ) for word alignment are the most influential statistical word alignment models ( Brown et al. , 1993 ; Vogel et al. , 1996 ; Och and Ney , 2003 ) . There are three kinds of important information for word alignment models : lexicality , locality and fertility . IBM Model 1 uses only lexical information ; IBM Model 2 and the hidden Markov model take advantage of both lexical and locality information ; IBM Models 4 and 5 use all three kinds of information , and they remain the state of the art despite the fact that they were developed almost two decades ago . Recent experiments on large datasets have shown that the performance of the hidden Markov model is very close to IBM Model 4 . Nevertheless , we believe that IBM Model 4 is essentially a better model because it exploits the fertility of words in the tar get language . However , IBM Model 4 is so complex that most researches use the GIZA++ software package ( Och and Ney , 2003 ) , and IBM Model 4 itself is treated as a black box . The complexity in IBM Model 4 makes it hard to understand and to improve . Our goal is to build a model that includes lexicality , locality , and fertility ; and , at the same time , to make it easy to understand . We also want it to be accurate and computationally efficient . There have been many years of research on word alignment . Our work is different from others in essential ways . Most other researchers take either the HMM alignments ( Liang et al. , 2006 ) or IBM Model 4 alignments ( Cherry and Lin , 2003 ) as input and perform post-processing , whereas our model is a potential replacement for the HMM and IBM Model 4 . Directly modeling fertility makes our model fundamentally different from others . Most models have limited ability to model fertility . ( 2006 ) learn the alignment in both translation directions jointly , essentially pushing the fertility towards 1 . ITG models ( Wu , 1997 ) assume the fertility to be either zero or one . It can model phrases , but the phrase has to be contiguous . There have been works that try to simulate fertility using the hidden Markov model ( Toutanova et al. , 2002 ; Deng and Byrne , 2005 ) , but we prefer to model fertility directly . Our model is a coherent generative model that combines the HMM and IBM Model 4 . It is easier to understand than IBM Model 4 ( see Section 3 ) . Our model also removes several undesired properties in IBM Model 4 . We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing , pages 596–605 , MIT , Massachusetts , USA , 911 October 2010 . Qc 2010 Association for Computational Linguistics estimation . Our distortion parameters are similar to IBM Model 2 and the HMM , while IBM Model 4 uses inverse distortion ( Brown et al. , 1993 ) . Our model assumes that fertility follows a Poisson distribution , while IBM Model 4 assumes a multinomial distribution , and has to learn a much larger number of parameters , which makes it slower and less reliable . Our model is much faster than IBM Model 4 . In fact , we will show that it is also faster than the HMM , and has lower alignment error rate than the HMM . Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility . ( 1993 ) and Och and Ney ( 2003 ) first compute the Viterbi alignments for simpler models , then consider only some neighbors of the Viterbi alignments for modeling fertility . If the optimal alignment is not in those neighbors , this method will not be able find the opti total of I + 1 empty words for the HMM model1 . Moore ( 2004 ) also suggested adding multiple empty words to the target sentence for IBM Model 1 . After we add I + 1 empty words to the target sentence , the alignment is a mapping from source to target word positions : a : j → i , i = aj where j = 1 , 2 , . , J and i = 1 , 2 , . Words from position I + 1 to 2I + 1 in the target sentence are all empty words . We allow each source word to align with exactly one target word , but each target word may align with multiple source words . The fertility φi of a word ei at position i is defined as the number of aligned source words : J mal alignment . We use the Markov Chain Monte Carlo ( MCMC ) method for training and decoding , φi = j=1 δ ( aj , i ) which has nice probabilistic guarantees . ( 2008 ) applied the Markov Chain Monte Carlo method to word alignment for machine translation ; they do not model word fertility . where δ is the Kronecker delta function : ( 1 if x = y δ ( x , y ) = 0 otherwise In particular , the fertility of all empty words in 2.1 Alignment and Fertility . the target sentence is `` £2I +1 `` £2I +1 φi . We define φǫ ≡ 2I +1 i=I +1 φi . For a bilingual sentence pair e1 and Given a source sentence f J = f1 , f2 , . , fJ and a f J , we have `` £I φi + φǫ = J . target sentence eI 1 = e1 , e2 , . , eI , we define the 1 i=1 The inverted alignments for position i in the tar alignments between the two sentences as a subset of the Cartesian product of the word positions . ( 1993 ) , we assume that each source word is aligned to exactly one target word . get sentence are a set Bi , such that each element in Bi is aligned with i , and all alignments of i are in Bi . Inverted alignments are explicitly used in IBM Models 3 , 4 and 5 , but not in our model , which is We denote as aJ = a1 , a2 , . , aJ the alignments one reason that our model is easier to understand . between f J and eI . When a word fj is not aligned 1 1 with any word e , aj is 0 . For convenience , we add an empty word ǫ to the target sentence at position 0 ( i.e. , e0 = ǫ ) . However , as we will see , we have to add more than one empty word for the HMM . 2.2 IBM Model 1 and HMM . IBM Model 1 and the HMM are both generative models , and both start by defining the probability of alignments and source sentence given the In order to compute the “ jump probability ” in the target sentence : P ( aJJ 1 ) ; the data likeli HMM model , we need to know the position of the 1 , f1 |e2I +1 hood can be computed by summing over alignments : aligned target word for the previous source word . If the previous source word aligns to an empty word , 1 If fj . −1 does not align with an empty word and fj alignswe could use the position of the empty word to indi with an empty word , we want to record the position of the target word that fj−1 aligns with . There are I + 1 possibilities : fj is cate the nearest previous source word that does not align to an empty word . For this reason , we use a the first word in the source sentence , or fj the target word . −1 aligns with one ofP ( f J |e2I +1 ) = `` £ J P ( aJ , f J |e2I +1 ) . The alignwhere the first two equations imply that the proba 1 1 a1 1 1 1 ments aJ are the hidden variables . The expectation maximization algorithm is used to learn the parameters such that the data likelihood is maximized . Without loss of generality , P ( aJ , f J |e2I +1 ) can bility of jumping to an empty word is either 0 or p0 , and the third equation implies that the probability of jumping from a nonempty word is the same as the probability of jumping from the corespondent empty 1 1 1 be decomposed into length probabilities , distortion probabilities ( also called alignment probabilities ) , and lexical probabilities ( also called translation probabilities ) : P ( aJ , f J |e2I +1 ) 1 1 1 J word . The absolute position in the HMM is not important , because we re-parametrize the distortion probability in terms of the distance between adjacent alignment points ( Vogel et al. , 1996 ; Och and Ney , 2003 ) : = P ( J |e2I +1 ) n P ( aj , fj |f j−1 , aj−1 , e2I +1 ) c ( i − i′ ) 1 j=1 1 1 1 P ( i|i′ , I ) = `` £ i′′ c ( i′′ − i′ ) J = P ( J |e2I +1 ) n P ( aj |f j−1 , aj−1 , e2I +1 ) × where c ( ) is the count of jumps of a given distance . 1 j=1 1 1 1 In IBM Model 1 , the word order does not mat ter . The HMM is more likely to align a source P ( fj |f j−1 , aj , e2I +1 ) l 1 1 1 where P ( J |e2I +1 ) is a length probability , word to a target word that is adjacent to the previous aligned target word , which is more suitable than IBM Model 1 because adjacent words tend to form ( aj |f j−1 , aj−1 2I +1P 1 1 , e1 ) is a distortion prob phrases . ability and P ( fj |f j probability . −1 , aj , e 2I +1 1 ) is a lexical For these two models , in theory , the fertility for a target word can be as large as the length of the IBM Model 1 assumes a uniform distortion probability , a length probability that depends only on the length of the target sentence , and a lexical probability that depends only on the aligned target word : J source sentence . In practice , the fertility for a target word in IBM Model 1 is not very big except for rare target words , which can become a garbage collector , and align to many source words ( Brown et al. , 1993 ; Och and Ney , 2003 ; Moore , 2004 ) . The HMM is P ( aJ , f J |e2I +1 ) = P ( J |I ) n P ( f |e ) less likely to have this garbage collector problem be 1 1 1 ( 2I + 1 ) J j=1 j aj cause of the alignment probability constraint . However , fertility is an inherent cross language propertyThe hidden Markov model assumes a length prob ability that depends only on the length of the target sentence , a distortion probability that depends only on the previous alignment and the length of the target sentence , and a lexical probability that depends only on the aligned target word : P ( aJ , f J |e2I +1 ) = 1 1 1 J P ( J |I ) n P ( aj |aj−1 , I ) P ( fj |ea ) j=1 In order to make the HMM work correctly , we enforce the following constraints ( Och and Ney , 2003 ) : and these two models can not assign consistent fertility to words . This is our motivation for adding fertility to these two models , and we expect that the resulting models will perform better than the baseline models . Because the HMM performs much better than IBM Model 1 , we expect that the fertility hidden Markov model will perform much better than the fertility IBM Model 1 . Throughout the paper , “ our model ” refers to the fertility hidden Markov model . Due to space constraints , we are unable to provide details for IBM Models 3 , 4 and 5 ; see Brown et al . ( 1993 ) and Och and Ney ( 2003 ) . But we want to point out that the locality property modeled in the HMM is missing in IBM Model 3 , and is modeled invertedly in IBM Model 4 . IBM Model 5 removes deficiency ( Brown et al. , 1993 ; Och and Ney , 2003 ) from IBM Model 4 , but it is computationally very expensive due to the larger number of parameters than IBM Model 4 , and IBM Model 5 often provides no improvement on alignment accuracy . Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities ( for each nonempty target word and all empty words ) , alignments , and the source sentence given the target sentence : P ( φI , φǫ , aJ , f J |e2I +1 ) ; 1 1 1 1 are further away from the mean have low probability . IBM Models 3 , 4 , and 5 use a multinomial distribution for fertility , which has a much larger number of parameters to learn . Our model has only one parameter for each target word , which can be learned more reliably . In the fertility IBM Model 1 , we assume that the distortion probability is uniform , and the lexical probability depends only on the aligned target word : P ( φI , φǫ , aJ , f J |e2I +1 ) the data likelihood can be computed by 1 1 1 I φi 1 λ ( ei ) summing over fertilities and alignments : n λ ( ei ) e− × P ( f J |e2I +1 ) = `` £ I J P ( φI , φǫ , aJ , f J |e2I +1 ) . 1 1 φ1 , φǫ , a1 1 1 1 1 The fertility for a nonempty word ei is a random variable φi , and we assume φi follows a Poisson distribution Poisson ( φi ; λ ( ei ) ) . The sum of the fer ( I λ ( ǫ ) ) φǫ e− ( I λ ( ǫ ) ) φǫ ! × J tilities of all the empty words ( φǫ ) grows with the length of the target sentence . Therefore , we assume that φǫ follows a Poisson distribution with parameter I λ ( ǫ ) . Now P ( φI , φǫ , aJ , f J |e2I +1 ) can be decomposed 1 ( 2I + 1 ) J n P ( fj | j=1 eaj ) ( 1 ) 1 1 1 1 in the following way : P ( φI , φǫ , aJ , f J |e2I +1 ) In the fertility HMM , we assume that the distor tion probability depends only on the previous alignment and the length of the target sentence , and that 1 1 1 1 = P ( φI |e2I +1 ) P ( φǫ|φI , e2I +1 ) × 1 1 1 1 J the lexical probability depends only on the aligned target word : n P ( aj , fj |f j−1 , aj−1 , e2I +1 , φI , φǫ ) j=1 1 1 1 1 P ( φI , φǫ , aJ , f J |e2I +1 ) = n λ ( ei ) e−λ ( ei ) 1 1 1 I φ 1 λ ( e ) φi ! × = n λ ( ei ) i e− i i=1 ( I λ ( ǫ ) ) φǫ e−I λ ( ǫ ) φǫ ! × φ i=1 ( I λ ( ǫ ) ) φǫ ! × e− ( I λ ( ǫ ) ) J n P ( aj |f j−1 , aj−1 , e2I +1 I φǫ ! × J j=1 1 1 1 , φ1 , φǫ ) × n P ( aj | j=1 aj−1 , I ) P ( fj | eaj ) ( 2 ) P ( fj |f j−1 , aj , e2I +1 , φI , φǫ ) l 1 1 1 1 Superficially , we only try to model the length 1 |e2I +1probability more accurately . However , we also en When we compute P ( f J 1 ) , we only sum force the fertility for the same target word across the corpus to be consistent . The expected fertility for a nonempty word ei is λ ( ei ) , and the expected fertil over fertilities that agree with the alignments : ity for all empty words is I λ ( ǫ ) . We can remove the deficiency for fertility IBM Model 1 by assuming a different distortion λ ( e ) = s c ( φ| e ; f , e ) s c ( k|e ; f ( s ) , e ( s ) ) ( 6 ) probability : the distortion probability is 0 if fertility where s is the number of bilingual sentences , andis not consistent with alignments , and uniform oth c ( f |e ; f J 2I +1 ˜ J J 2I +1 erwise . The total number of consistent fertility and 1 , e1 ) = P ( a1 |f1 , e1 ) × J alignments is J ! Replacing 1 with a1 φǫ ! ( 2I +1 ) J δ ( fj , f ) δ ( ei , e ) J ! , we have : c ( a|a′ ; f J , e2I +1 ) = j P˜ ( aJ |f J , e2I +1 ) × P ( φI , φǫ , aJ , f J |e2I +1 ) 1 1 1 1 1 J 1 1 1 1 a1 I = n λ ( ei ) φi e−λ ( ei ) × i=1 ( I λ ( ǫ ) ) φǫ e− ( I λ ( ǫ ) ) × c ( φ|e ; f1 , e1 ) = δ ( aj , a ) δ ( aj−1 , a′ ) j P˜ ( a1 |f1 , e1 ) × J 2I +1 J J 2I +1 J n P ( fj |ea ) J 1 φ δ ( e , e ) J ! j i i j=1 i c ( k|e ; f J , e2I +1 ) = k ( ei ) δ ( ei , e ) In our experiments , we did not find a noticeable 1 1 change in terms of alignment accuracy by removing the deficiency . We estimate the parameters by maximizing P ( f J |e2I +1 ) using the expectation maximization These equations are for the fertility hidden Markov model . For the fertility IBM Model 1 , we do not need to estimate the distortion probability . Although we can estimate the parameters by using 1 1 ( EM ) algorithm ( Dempster et al. , 1977 ) . The the EM algorithm , in order to compute the expected counts , we have to sum over all possible alignments1 , which is , unfortunately , exponential . We devel Algorithm 1 : One iteration of E-step : draw t samples for each aj for each sentence pairoped a Gibbs sampling algorithm ( Geman and Ge ( f J 1 ) in the corpus man , 1984 ) to compute the expected counts . 1 , e2I +1 J 2I +1 For each target sentence e2I +1 and source sentence f J , we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1 . During the training stage , we try all 2I + 1 possible alignments for aj but fix all other alignments.2 We choose alignment aj with probabil J 2I +1 for ( f1 , e1 ) in the corpus do Initialize aJ with IBM Model 1 ; for t do for j do for i do aj = i ; Compute P ( aJ , f J |e2I +1 ) ; ity P ( aj |a1 , · · · aj−1 , aj+1 · · · aJ , f1 , e1 ) , which can be computed in the following way : end 1 1 1 P ( aj |a1 , · · · , aj 1 , a , · · · , a , f J , e2I +1 ) − j+1 J 1 1 J J 2I +1 Draw a sample for aj using Equation 7 ; Update counts ; = P ( a1 , f1 |e1 ) ( 7 ) end `` £ J J 2I +1 aj P ( a1 , f1 |e1 ) For each alignment variable aj , we choose t samples . This Gibbs sampling method updates parameters constantly , so it is an “ online learning ” algorithm . However , this sampling method needs a large amount of communication between machines in order to keep the parameters up to date if we compute the expected counts in parallel . Instead , we do “ batch learning ” : we fix the parameters , scan through the entire corpus and compute expected counts in parallel ( E-step ) ; then combine all the counts together and update the parameters ( M- step ) . This is analogous to what IBM models and end end We also consider initializing the alignments using the HMM Viterbi algorithm in the E-step . In this case , the fertility hidden Markov model is not faster than the HMM . Fortunately , initializing using IBM Model 1 Viterbi does not decrease the accuracy in any noticeable way , and reduces the complexity of the Gibbs sampling algorithm . In the testing stage , the sampling algorithm is the same as above except that we keep the alignments 1 that maximize P ( a1 , f1 |e2I +1 ) . We need more the HMM do in the EM algorithms . The algorithm aJ J J 1 for the E-step on one machine ( all machines are independent ) is in Algorithm 1 . For the fertility hidden Markov model , updating P ( aJ , f J |e2I +1 ) whenever we change the alignment 1 1 1 aj can be done in constant time , so the complexity of choosing t samples for all aj ( j = 1 , 2 , . , J ) is O ( tI J ) . This is the same complexity as the HMM if t is O ( I ) , and it has lower complexity if t is a constant . Surprisingly , we can achieve better results than the HMM by computing as few as 1 sample for each alignment , so the fertility hidden Markov model is much faster than the HMM . Even when choosing t such that our model is 5 times faster than the HMM , we achieve better results . 2 For fertility IBM Model 1 , we only need to compute I + 1. values because e2I +1 are identical empty words . samples in the testing stage because it is unlikely to get to the optimal alignments by sampling a few times for each alignment . Interestingly , we found that throwing away the fertility and using the HMM Viterbi decoding achieves same results as the sampling approach ( we can ignore the difference because it is tiny ) , but is faster . Therefore , we use Gibbs sampling for learning and the HMM Viterbi decoder for testing . Gibbs sampling for the fertility IBM Model 1 is similar but simpler . We omit the details here . Al ig n m en t M o d e l P R A E R e n → c n I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M MF 3 0 I B M 4 49 .6 55 .4 62 .6 65 .4 66 .8 67 .8 66 .8 55 .3 57 .1 59 .5 59 .1 60 .8 62 .3 64 .1 4 7 . 5 c n → e n I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M MF 3 0 I B M 4 52 .6 55 .9 66 .1 68 .6 71 .1 71 .1 69 .3 53 .7 56 .4 62 .1 60 .2 62 .2 62 .7 68 .5 4 6 . 0.48 0.46 0.44 I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M M F 3 0 I B M 4 0.42 0.4 0.38 0.36 0.34 0.32 0.48 0.46 0.44 I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M M F 3 0 I B M 4 0.42 0.4 0.38 0.36 0.34 0.32 5000 4000 I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M M F 3 0 I B M 4 3000 2000 1000 0 Figure 3 : The training time for each model is calculated from scratch . For example , the training time of IBM Model 4 includes the training time of IBM Model 1 , the HMM , and IBM Model 3 . We evaluated our model by computing the word alignment and machine translation quality . We use the alignment error rate ( AER ) as the word alignment evaluation criterion . Let A be the alignments output by word alignment system , P be a set of possible alignments , and S be a set of sure alignments both labeled by human beings . S is a subset of P . Precision , recall , and AER are defined as follows : recall = |A ∩ S| |S| precision = |A ∩ P | |A| AER ( S , P , A ) = 1 |A ∩ S| + |A ∩ P | |A| + |S| AER is an extension to F-score . We evaluate our fertility models on a ChineseEnglish corpus . The ChineseEnglish data taken from FBIS newswire data , and has 380K sentence pairs , and we use the first 100K sentence pairs as our training data . We used hand-aligned data as reference . The ChineseEnglish data has 491 sentence pairs . We initialize IBM Model 1 and the fertility IBM Model 1 with a uniform distribution . We smooth all parameters ( λ ( e ) and P ( f |e ) ) by adding a small value ( 10−8 ) , so they never become too small . We run both models for 5 iterations . AER results are computed using the IBM Model 1 Viterbi alignments , and the Viterbi alignments obtained from the Gibbs sampling algorithm . We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1 . We smooth all parameters ( λ ( e ) , P ( a|a′ ) and P ( f |e ) ) by adding a small value ( 10−8 ) . We run both models for 5 iterations . AER results are computed using traditional HMM Viterbi decoding for both models . It is always difficult to determine how many samples are enough for sampling algorithms . However , both fertility models achieve better results than their baseline models using a small amount of samples . For the fertility IBM Model 1 , we sample 10 times for each aj , and restart 3 times in the training stage ; we sample 100 times and restart 12 times in the testing stage . For the fertility HMM , we sample 30 times for each aj with no restarting in the training stage ; no sampling in the testing stage because we use traditional HMM Viterbi decoding for testing . More samples give no further improvement . Initially , the fertility IBM Model 1 and fertility HMM did not perform well . If a target word e only appeared a few times in the training corpus , our model can not reliably estimate the parameter λ ( e ) . Hence , smoothing is needed . One may try to solve it by forcing all these words to share a same parameter λ ( einfrequent ) . Unfortunately , this does not solve the problem because all infrequent words tend to have larger fertility than they should . We solve the problem in the following way : estimate the parameter λ ( enon empty ) for all nonempty words , all infrequent words share this parameter . We consider words that appear less than 10 times as infrequent words . We can see that the fertility IBM Model 1 consistently outperforms IBM Model 1 , and the fertility HMM consistently outperforms the HMM . The fertility HMM not only has lower AER than the HMM , it also runs faster than the HMM . In fact , with just 1 sample for each alignment , our model archives lower AER than the HMM , and runs more than 5 times faster than the HMM . It is possible to use sampling instead of dynamic programming in the HMM to reduce the training time with no decrease in AER ( often an increase ) . We conclude that the fertility HMM not only has better AER results , but also runs faster than the hidden Markov model . We also evaluate our model by computing the machine translation BLEU score ( Papineni et al. , 2002 ) using the Moses system ( Koehn et al. , 2007 ) . The training data is the same as the above word alignment evaluation bitexts , with alignments for each model symmetrized using the grow-diag-final heuristic . Our test is 633 sentences of up to length 50 , with four references . Model BLEU HMM 19.55 HMMF30 19.26 IBM4 18.77 We developed a fertility hidden Markov model that runs faster and has lower AER than the HMM . Our model is thus much faster than IBM Model 4 . Our model is also easier to understand than IBM Model 4 . The Markov Chain Monte Carlo method used in our model is more principled than the heuristic-based neighborhood method in IBM Model 4 . While better word alignment results do not necessarily correspond to better translation quality , our translation results are comparable in translation quality to both the HMM and IBM Model 4 . Acknowledgments We would like to thank Tagyoung Chung , Matt Post , and the anonymous reviewers for helpful comments . This work was supported by NSF grants IIS0546554 and IIS0910611 . In this article , we describe an efficient beam search algorithm for statistical machine translation based on dynamic programming ( DP ) . The search algorithm uses the translation model presented in Brown et al . Starting from a DP-based solution to the traveling-salesman problem , we present a novel technique to restrict the possible word reorderings between source and target language in order to achieve an efficient search algorithm . Word reordering restrictions especially useful for the translation direction German to English are presented . The restrictions are generalized , and a set of four parameters to control the word reordering is introduced , which then can easily be adopted to new translation directions . The beam search procedure has been successfully tested on the Verbmobil task ( German to English , 8,000-word vocabulary ) and on the Canadian Hansards task ( French to English , 100,000-word vocabulary ) . For the medium-sized Verbmobil task , a sentence can be translated in a few seconds , only a small number of search errors occur , and there is no performance degradation as measured by the word error criterion used in this article . This article is about a search procedure for statistical machine translation ( MT ) . The task of the search procedure is to find the most likely translation given a source sentence and a set of model parameters . Here , we will use a trigram language model and the translation model presented in Brown et al . Since the number of possible translations of a given source sentence is enormous , we must find the best output without actually generating the set of all possible translations ; instead we would like to focus on the most likely translation hypotheses during the search process . For this purpose , we present a data-driven beam search algorithm similar to the one used in speech recognition search algorithms ( Ney et al . The major difference between the search problem in speech recognition and statistical MT is that MT must take into account the different word order for the source and the target language , which does not enter into speech recognition . Tillmann , Vogel , Ney , and Zubiaga ( 1997 ) proposes a dynamic programming ( DP ) based search algorithm for statistical MT that monotonically translates the input sentence from left to right . The word order difference is dealt with using a suitable preprocessing step . Although the resulting search procedure is very fast , the preprocessing is language specific and requires a lot of manual IBM T. J. Watson Research Center , Yorktown Heights , NY 10598 . Email : ctill @ us.ibm.com . The research reported here was carried out while the author was with Lehrstuhl fu r Informatik VI , Computer Science Department , RWTH Aachen . Lehrstuhl fu r Informatik VI , Computer Science Department , RWTH Aachen , D-52056 Aachen , Germany . Email : ney @ informatik.rwthaachen.de . Oc 2003 Association for Computational Linguistics work . Currently , most search algorithms for statistical MT proposed in the literature are based on the A concept ( Nilsson 1971 ) . Here , the word reordering can be easily included in the search procedure , since the input sentence positions can be processed in any order . The work presented in Berger et al . ( 1996 ) that is based on the A concept , however , introduces word reordering restrictions in order to reduce the overall search space . The search procedure presented in this article is based on a DP algorithm to solve the traveling-salesman problem ( TSP ) . A data-driven beam search approach is presented on the basis of this DP-based algorithm . The cities in the TSP correspond to source positions of the input sentence . By imposing constraints on the possible word reorderings similar to that described in Berger et al . ( 1996 ) , the DP-based approach becomes more effective : when the constraints are applied , the number of word re- orderings is greatly reduced . The original reordering constraint in Berger et al . ( 1996 ) is shown to be a special case of a more general restriction scheme in which the word reordering constraints are expressed in terms of simple combinatorical restrictions on the processed sets of source sentence positions.1 A set of four parameters is given to control the word reordering . Additionally , a set of four states is introduced to deal with grammatical reordering restrictions ( e.g. , for the translation direction German to English , the word order difference between the two languages is mainly due to the German verb group . In combination with the reordering restrictions , a data-driven beam search organization for the search procedure is proposed . A beam search pruning technique is conceived that jointly processes partial hypotheses according to two criteria : ( 1 ) The partial hypotheses cover the same set of source sentence positions , and ( 2 ) the partial hypotheses cover sets C of source sentence positions of equal car dinality . A partial hypothesis is said to cover a set of source sentence positions when exactly the positions in the set have already been processed in the search process . To verify the effectiveness of the proposed techniques , we report and analyze results for two translation tasks : the German to English Verbmobil task and French to English Canadian Hansards task.The article is structured as follows . Section 2 gives a short introduction to the trans lation model used and reports on other approaches to the search problem in statistical MT . In Section 3 , a DP-based search approach is presented , along with appropriate pruning techniques that yield an efficient beam search algorithm . Section 4 reports and analyzes translation results for the different translation directions . In Section 5 , we conclude with a discussion of the achieved results . In this article , we use the translation model presented in Brown et al . ( 1993 ) , and the mathematical notation we use here is taken from that paper as well : a source string 1 = f1 fj fJ is to be translated into a target string eI = e1 ei eI . Here , I is the length of the target string , and J is the length of the source string . Among all possible target strings , we will choose the string with the highest probability as given by Bayes 1 The word reordering restriction used in the search procedure described in Berger et al . ( 1996 ) is not . mentioned in Brown et al . ( 1993 ) , although exactly the translation model described there is used . Equivalently , we use exactly the translation model described in Brown et al . ( 1993 ) but try different reordering restrictions for the DP-based search procedure . 98 decision rule : eI = arg max { Pr ( eI | f J ) } 1 I 1 1 1 = arg max { Pr ( eI ) Pr ( f J | eI ) } ( 1 ) I 1 1 1 1 Pr ( eI ) is the language model of the target language , whereas Pr ( f J | eI ) is the string 1 1 1translation model . The language model probability is computed using a trigram lan guage model . The string translation probability Pr ( f J | eI ) is modeled using a series of 1 1five models of increasing complexity in training . Here , the model used for the trans lation experiments is the IBM4 model . This model uses the same parameter set as the IBM5 model , which in preliminary experiments did not yield better translationresults . The actual implementation used during the experiments is described in Al Onaizan et al . ( 1999 ) and in Och and Ney ( 2000 ) . The argmax operation denotes the search problem ( i.e. , the generation of the output sentence in the target language ) . The transformations may range from simple word categorization to more complex preprocessing steps that require someparsing of the source string . In this article , however , we will use only word catego 99 rization as an explicit transformation step . In the search procedure both the language and the translation model are applied after the text transformation steps . The following types of parameters are used for the IBM4 translation model : Lexicon probabilities : We use the lexicon probability p ( f | e ) for translating the single target word e as the single source word f . A source word f may be translated by the null word e0 ( i.e. , it does not produce any target word e ) . A translation probability p ( f | e0 ) is trained along with the regular translation probabilities . Fertilities : A single target word e may be aligned to n = 0 , 1 or more source words . This is explicitly modeled by the fertility parameter ( n | e ) : the probability that the target word e is translated by n source words is ( n | e ) . The fertility for the null word is treated specially ( for details see Brown et al . ( 1996 ) describes the extension of a partial hypothesis by a pair of target words ( e1 , e ) , where e1 is not connected to any source word f . In this case , the so-called spontaneous target word e1 is accounted for with the fertility . Here , the translation probability ( 0 | e1 ) and no- translation probability p ( f | e1 ) . Class-based distortion probabilities : When covering a source sentence position j , we use distortion probabilities that depend on the previously covered source sentence positions ( we say that a source sentence position j is covered for a partial hypothesis when it is taken account of in the translation process by generating a target word or the null word e0 ) . ( 1993 ) , two types of distortion probabilities are distinguished : ( 1 ) the leftmost word of a set of source words f aligned to the same target word e ( which is called the head ) is placed , and ( 2 ) the remaining source words are placed . Two separate distributions are used for these two cases . For placing the head the center function center ( i ) ( Brown et al . [ 1993 ] uses the notation 8i ) is used : the average position of the source words with which the target word ei 1 is aligned . The distortion probabilities are class-based : They depend on the word class F ( f ) of a covered source word f as well as on the word class E ( e ) of the previously generated target word e. The classes are automatically trained ( Brown et al . When the IBM4 model parameters are used during search , an input sentence can be processed one source position at a time in a certain order primarily determined by the distortion probabilities . We will use the following simplified set of translation model parameters : lexicon probabilities p ( f | e ) and distortion probabilities p ( j | j1 , J ) . Here , j is the currently covered input sentence position and j1 is the previously covered input sentence position . The input sentence length J is included , since we would like to think of the distortion probability as normalized according to J . No fertility probabilities or null word probabilities are used ; thus each source word f is translated as exactly one target word e and each target word e is translated as exactly one source word f . The simplified notation will help us to focus on the most relevant details of the DP-based search procedure . The simplified set of parameters leads to an unrealistic assumption about the length of the source and target sentence , namely , I = J . During the translation experiments we will , of course , not make this assumption . The implementation details for using the full set of IBM4 model parameters are given in Section 3.9.2 . 100 2.2 Search Algorithms for Statistical Machine Translation . In this section , we give a short overview of search procedures used in statistical MT : Brown et al . ( 1990 ) and Brown et al . ( 1993 ) describe a statistical MT system that is based on the same statistical principles as those used in most speech recognition systems ( Jelinek 1976 ) . ( 1994 ) describes the French-to-English Candide translation system , which uses the translation model proposed in Brown et al . A detailed description of the decoder used in that system is given in Berger et al . ( 1996 ) but has never been published in a paper : Throughout the search process , partial hypotheses are maintained in a set of priority queues . There is a single priority queue for each subset of covered positions in the source string . In practice , the priority queues areinitialized only on demand ; far fewer than the full number of queues possible are actu ally used . The priority queues are limited in size , and only the 1,000 hypotheses with the highest probability are maintained . Each priority queue is assigned a threshold to select the hypotheses that are going to be extended , and the process of assigning these thresholds is rather complicated . A restriction on the possible word reorderings , which is described in Section 3.6 , is applied . Wang and Waibel ( 1997 ) presents a search algorithm for the IBM2 translation model based on the A concept and multiple stacks . An extension of this algorithm is demonstrated in Wang and Waibel ( 1998 ) . Here , a reshuffling step on top of the original decoder is used to handle more complex translation models ( e.g. , the IBM3 model is added ) . Translation approaches that use the IBM2 model parameters but are based on DP are presented in Garc a-Varea , Casacuberta , and Ney ( 1998 ) and Niessen et al . An approach based on the hidden Markov model alignments as used in speech recognition is presented in Tillmann , Vogel , Ney , and Zubiaga ( 1997 ) and Tillmann , Vogel , Ney , Zubiaga , and Sawaf ( 1997 ) . This approach assumes that source and target language have the same word order , and word order differences are dealt with in a preprocessing stage . The work by Wu ( 1996 ) also uses the original IBM model parameters and obtains an efficient search algorithm by restricting the possible word reorderings using the so-called stochastic bracketing transduction grammar . Three different decoders for the IBM4 translation model are compared in Germann et al . The first is a reimplementation of the stack-based decoder described in Berger et al . The second is a greedy decoder that starts with an approximate solution and then iteratively improves this first rough solution . The third converts the decoding problem into an integer program ( IP ) , and a standard software package for solving IP is used . Although the last approach is guaranteed to find the optimal solution , it is tested only for input sentences of length eight or shorter . This article will present a DP-based beam search decoder for the IBM4 translation model . The decoder is designed to carry out an almost full search with a small number of search errors and with little performance degradation as measured by the word error criterion . A preliminary version of the work presented here was published in Tillmann and Ney ( 2000 ) . To explicitly describe the word order difference between source and target language , Brown et al . ( 1993 ) introduced an alignment concept , in which a source position j is mapped to exactly one target position i : regular alignment : j i = aj 101 . May of fourth the on you visit not can colleague my case this In I d F k m K S a v M n b . n i a a e o i m i a i e e l n s l n e m i l e e i c s n l r h u e t t c g e h e n e n Regular alignment example for the translation direction German to English . For each German source word there is exactly one English target word on the alignment path . An example for this kind of alignment is given in Figure 2 , in which each German source position j is mapped to an English target position i . ( 1993 ) , this alignment concept is used for model IBM1 through model IBM5 . For search purposes , we use the inverted alignment concept as introduced in Niessen et al . ( 1998 ) and Ney et al . An inverted alignment is defined as follows : inverted alignment : i j = bi Here , a target position i is mapped to a source position j . The coverage constraint for an inverted alignment is not expressed by the notation : Each source position j should be hit exactly once by the path of the inverted alignment bI = b1 bi bI . The advantage of the inverted alignment concept is that we can construct target sentence hypotheses from bottom to top along the positions of the target sentence . Using the inverted alignments in the maximum approximation , we rewrite equation ( 1 ) to obtain the following search criterion , in which we are looking for the most likely target 102 Figure 3 Illustration of the transitions in the regular and in the inverted alignment model . The regular alignment model ( left figure ) is used to generate the sentence from left to right ; the inverted alignment model ( right figure ) is used to generate the sentence from bottom to top . Note that in equation ( 2 ) two products over i are merged into a single product over i . The translation probability p ( f J | eI ) is computed in 1 1 the maximum approximation using the distortion and the lexicon probabilities . Finally , p ( J | I ) is the sentence length model , which will be dropped in the following ( it is not used in the IBM4 translation model ) . For each source sentence f J to be translated , we are searching for the unknown mapping that optimizes equation ( 2 ) : i ( bi , ei ) In Section 3.3 , we will introduce an auxiliary quantity that can be evaluated recursively using DP to find this unknown mapping . We will explicitly take care of the coverage constraint by introducing a coverage set C of source sentence positions that have already been processed . Figure 3 illustrates the concept of the search algorithm using inverted alignments : Partial hypotheses are constructed from bottom to top along the positions of the target sentence . Partial hypotheses of length i 1 are extended to obtain partial hypotheses of the length i . Extending a partial hypothesis means covering a source sentence position j that has not yet been covered . For a given grid point in the 103 Table 1 DP-based algorithm for solving traveling-salesman problems due to Held and Karp . The outermost loop is over the cardinality of subsets of already visited cities . input : cities j = 1 , ... , J with distance matrix djj 1 initialization : D ( { k } , k ) : = d1k for each path length c = 2 , ... , J do for each pair ( C , j ) , where C { 2 , ... , J } and j C and |C| = c do D ( C , j ) = min { djj 1 + D ( C\ { j } , j ) } traceback : j1 C\ { j } find shortest tour : D = min [ D ( { 2 , ... , J } , k ) + dk1 ] k { 2 , ... , J } recover optimal sequence of cities translation lattice , the unknown target word sequence can be obtained by tracing back the translation decisions to the partial hypothesis at stage i = 1 . The grid points are defined in Section 3.3 . In the left part of the figure the regular alignment concept is shown for comparison purposes . 3.2 Held and Karp Algorithm for Traveling-Salesman Problem . Held and Karp ( 1962 ) presents a DP approach to solve the TSP , an optimization problem that is defined as follows : Given are a set of cities { 1 , ... , J } and for each pair of cities j , j1 the cost djj 1 > 0 for traveling from city j to city j1 . We are looking for the shortest tour , starting and ending in city 1 , that visits all cities in the set of cities exactly once . We are using the notation C for the set of cities , since it corresponds to a coverage set of processed source positions in MT . A straightforward way to find the shortest tour is by trying all possible permutations of the J cities . The resulting algorithm has a complexity of O ( J ! ) . DP can be used , however , to find the shortest tour in O ( J2 2J ) , which is a much smaller complexity for larger values of J . The approach recursively evaluates the quantity D ( C , j ) : D ( C , j ) : = costs of the partial tour starting in city 1 , ending in city j , and visiting all cities in C Subsets of cities C of increasing cardinality c are processed . The algorithm , shown in Table 1 , works because not all permutations of cities have to be considered explicitly . During the computation , for a pair ( C , j ) , the order in which the cities in C have been visited can be ignored ( except j ) ; only the costs for the best path reaching j has to be stored . For the initialization the costs for starting from city 1 are set : D ( { k } , k ) = d1k for each k { 2 , ... , |C| } . Then , subsets C of increasing cardinality are processed . Finally , the cost for the optimal tour is obtained in the second-to-last line of the algorithm . The optimal tour itself can be found using a back-pointer array in which the optimal decision for each grid point ( C , j ) is stored . Figure 4 illustrates the use of the algorithm by showing the supergraph that is searched in the Held and Karp algorithm for a TSP with J = 5 cities . When traversing the lattice from left to right following the different possibilities , a partial path to a node j corresponds to the subset C of all cities on that path together with the last visited 104 Figure 4 Illustration of the algorithm by Held and Karp for a traveling salesman problem with J = 5 cities . Not all permutations of cities have to be evaluated explicitly . For a given subset of cities the order in which the cities have been visited can be ignored . Of all the different paths merging into the node j , only the partial path with the smallest cost has to be retained for further computation . 3.3 DP-Based Algorithm for Statistical Machine Translation . In this section , the Held and Karp algorithm is applied to statistical MT . Using the concept of inverted alignments as introduced in Section 3.1 , we explicitly take care of the coverage constraint by introducing a coverage set C of source sentence positions that have already been processed . Here , the correspondence is according to the fact that each source sentence position has to be covered exactly once , fulfilling the coverage constraint . The cities of the more complex translation TSP correspond roughly to triples ( e1 , e , j ) , the notation for which is given below . The final path output by the translation algorithm will contain exactly one triple ( e1 , e , j ) for each source position j . The algorithm processes subsets of partial hypotheses with coverage sets C of increasing cardinality c. For a trigram language model , the partial hypotheses are of the form ( e1 , e , C , j ) , where e1 , e are the last two target words , C is a coverage set for the already covered source positions , and j is the last covered position . The target word sequence that ends in e1 , e is stored as a back pointer to the predecessor partial hypothesis ( and recursively to its predecessor hypotheses ) and is not shown in the notation . Each distance in the TSP now corresponds to the negative logarithm of the product of the translation , distortion , and language model probabilities . The following 105 Table 2 DP-based algorithm for statistical MT that consecutively processes subsets C of source sentence positions of increasing cardinality . The DP equation is evaluated recursively for each hypothesis ( e1 , e , C , j ) . The resulting algorithm is depicted in Table 2 . Some details concerning the initialization and the finding of the best target language string are presented in Section 3.4. p ( $ | e , e1 ) is the trigram language probability for predicting the sentence boundary symbol $ . The complexity of the algorithm is O ( E3 J2 2J ) , where E is the size of the target language vocabulary . 3.4 Verb Group Reordering : German to English . The above search space is still too large to translate even a medium-length inputsentence . On the other hand , only very restricted reorderings are necessary ; for ex ample , for the translation direction German to English , the word order difference is mostly restricted to the German verb group . The approach presented here assumes a mostly monotonic traversal of the source sentence positions from left to right.2 A small number of positions may be processed sooner than they would be in that monotonic traversal . Each source position then generates a certain number of target words . The restrictions are fully formalized in Section 3.5.A typical situation is shown in Figure 5 . When translating the sentence monotoni cally from left to right , the translation of the German finite verb kann , which is the left verbal brace in this case , is skipped until the German noun phrase mein Kollege , which is the subject of the sentence , is translated . Then , the right verbal brace is translated : 2 Also , this assumption is necessary for the beam search pruning techniques to work efficiently.. 106 . May of fourth the on you visit not can colleague my case this In I d F k m K S a v M n b . Figure 5 n i a e l s l e m a e o n i l n n l e g e i m i a i e e e i c s r h u t t c e h n e n Word reordering for the translation direction German to English : The reordering is restricted to the German verb group . The infinitive besuchen and the negation particle nicht . The following restrictions are used : One position in the source sentence may be skipped for a distance of up to L = 4 source positions , and up to two source positions may be moved for a distance of at most R = 10 source positions ( the notation L and R shows the relation to the handling of the left and right verbal brace ) . To formalize the approach , we introduce four verb group states S : Initial : A contiguous initial block of source positions is covered . Skip : One word may be skipped , leaving a hole in the monotonic traversal . Move : Up to two words may be moved from later in the sentence . Cover : The sentence is traversed monotonically until the state Initial is reached . Skip Initial Move Cover 1 . 6. kann 7. nicht 9 . Mai 8. besuchen 10. am 3 . 11. vierten Order in which the German source positions are covered for the German-to-English reordering example given in Figure 5 . The states Move and Skip both allow a set of upcoming words to be processed sooner than would be the case in the monotonic traversal . The state Initial is entered whenever there are no uncovered positions to the left of the rightmost covered position . The sequence of states needed to carry out the word reordering example in Figure 5 is given in Figure 6 . The 13 source sentence words are processed in the order shown . A formal specification of the state transitions is given in Section 3.5 . Any number of consecutive German verb phrases in a sentence can be processed by the algorithm . The finite-state control presented here is obtained from a simple analysis of the German- to-English word reordering problem and is not estimated from the training data . It can be viewed as an extension of the IBM4 model distortion probabilities . Using the above states , we define partial hypothesis extensions of the following type : ( S1 , C\ { j } , j1 ) ( S , C , j ) Not only the coverage set C and the positions j , j1 , but also the verb group states S , S1 , are taken into account . For the sake of brevity , we have omitted the target language words e , e1 in the notation of the partial hypothesis extension . For each extension an uncovered position is added to the coverage set C of the partial hypothesis , and the verb group state S may change . A more detailed description of the partial hypothesis extension for a certain state S is given in the next section in a more generalcontext . Covering the first uncovered position in the source sentence , we use the lan 108 guage model probability p ( e | $ , $ ) . Here , $ is the sentence boundary symbol , which is thought to be at position 0 in the target sentence . The search starts in the hypothesis ( Initial , { } , 0 ) . { } denotes the empty set , where no source sentence position is covered . The following recursive equation is evaluated : Qe1 ( e , S , C , j ) ( 3 ) = p ( fj | e ) max e11 , S1 , j1 ( S1 , C\ { j } , j1 ) ( S , C , j ) j1 C\ { j } { p ( j | j1 , J ) p ( e | e1 , e11 ) Qe11 ( e1 , S1 , C\ { j } , j1 ) } The search ends in the hypotheses ( Initial , { 1 , ... , J } , j ) ; the last covered position may be in the range j { J L , ... , J } , because some source positions may have been skipped at the end of the input sentence . { 1 , ... , J } denotes a coverage set including all positions from position 1 to position J . The final translation probability QF is QF = max e , e1 j { J L , ... , J } p ( $ | e , e1 ) Qe1 ( e , Initial , { 1 , ... , J } , j ) ( 4 ) where p ( $ | e , e1 ) denotes the trigram language model , which predicts the sentence boundary $ at the end of the target sentence . QF can be obtained using an algorithm very similar to the one given in Table 2 . The complexity of the verb group reordering for the translation direction German to English is O ( E3 J ( R2 L R ) ) , as shown in Tillmann ( 2001 ) . 3.5 Word Reordering : Generalization . For the translation direction English to German , the word reordering can be restricted in a similar way as for the translation direction German to English . Again , the word order difference between the two languages is mainly due to the German verb group . During the translation process , the English verb group is decomposed as shown inFigure 7 . When the sentence is translated monotonically from left to right , the trans lation of the English finite verb can is moved , and it is translated as the German left verbal brace before the English noun phrase my colleague , which is the subject of the sentence . The translations of the infinitive visit and of the negation particle not areskipped until later in the translation process . For this translation direction , the trans lation of one source sentence position may be moved for a distance of up to L = 4 source positions , and the translation of up to two source positions may be skipped for a distance of up to R = 10 source positions ( we take over the L and R notation from the previous section ) . Thus , the role of the skipping and the moving are simply reversed with respect to their roles in German-to-English translation . For the example translation in Figure 7 , the order in which the source sentence positions are covered is given in Figure 8.We generalize the two approaches for the different translation directions as fol lows : In both approaches , we assume that the source sentence is mainly processedmonotonically . A small number of upcoming source sentence positions may be pro cessed earlier than they would be in the monotonic traversal : The states Skip and Move are used as explained in the preceding section . The positions to be processed outside the monotonic traversal are restricted as follows : The number of positions dealt with in the states Move and Skip is restricted . There are distance restrictions on the source positions processed in those states . besuchen nicht Mai vierten am Sie Kollege mein kann Fall diesem In I t c m c c n v y o t f o M . n h a y o a o i o n h o f a Figure 7 i s l n t s u s e l i e t a g u e e u y r t h Word reordering for the translation direction English to German : The reordering is restricted to the English verb group . These restrictions will be fully formalized later in this section . In the state Move , some source sentence positions are moved from later in the sentence to earlier . After source sentence positions are moved , they are marked , and the translation of the sentence is continued monotonically , keeping track of the positions already covered . To formalize the approach , we introduce four reordering states S : Initial : A contiguous initial block of source positions is covered . Skip : A restricted number of source positions may be skipped , leaving holes in the monotonic traversal . Move : A restricted number of words may be moved from later in the sentence . Cover : The sentence is traversed monotonically until the state Initial is reached . To formalize the approach , the following notation is introduced : rmax ( C ) = max c c C 110 7. you 8. on 9. the 10. fourth 11. of 12 . 13. not 5. my Cover Skip Initial 1 . 2. this 3. case 6. colleague 14. visit Move Figure 8 15 . 4. can Order in which the English source positions are covered for the English-to-German reordering example given in Figure 7. lmin ( C ) = mi n c c / C u ( C ) m ( C ) w ( C ) = = = car d ( { c | c / C and c < rmax ( C ) } ) car d ( { c | c C an d c > lmi n ( C ) } ) rma x ( C ) lmi n ( C ) rmax ( C ) is the rightmost covered and lmin ( C ) is the leftmost uncovered source position . u ( C ) is the number of skipped positions , and m ( C ) is the number of moved positions . The function card ( ) returns the cardinality of a set of source positions . The function w ( C ) describes the window size in which the word reordering takes place . A procedural description for the computation of the set of successor hypotheses for a given partial hypothesis ( S , C , j ) is given in Table 3 . There are restrictions on the possible successor states : A partial hypothesis in state Skip can not be expanded into a partial hypothesis in state Move and vice versa . If the coverage set for the newly generated hypothesis covers a contiguous initial block of source positions , the state Initial is entered . No other state S is considered as a successor state in this case ( hence the use of the continue statement in the procedural description ) . The set of successor hypotheses Succ by which to extend the partial hypothesis ( S , C , j ) is computed using the constraints defined by the values for numskip , widthskip , nummove , and widthmove , as explained in the Appendix . In particular , a source position k is discarded for extension if the window restrictions are violated . Within the restrictions all possible successors are computed . It can be observed that the set of successors , as computed in Table 3 , is never empty . 111 Table 3 Procedural description to compute the set Succ of successor hypotheses by which to extend a partial hypothesis ( S , C , j ) . input : partial hypothesis ( S , C , j ) Succ : = { } for each k / C do Set C = C { k } if u ( C ) = 0 Succ : = Succ ( Initial , C , k ) continue if ( S = Initial ) or ( S = Skip ) if w ( C ) widthskip and u ( C ) numskip Succ : = Succ ( Skip , C , k ) if ( S = Initial ) or ( S = Move ) if k I= lmin ( C ) and w ( C ) widthmove and m ( C ) nummove Succ : = Succ ( Move , C , k ) if ( S = Move ) or ( S = Cover ) if ( lmin ( C ) = k ) Succ : = Succ ( Cover , C , k ) output : set Succ of successor hypotheses There is an asymmetry between the two reordering states Move and Skip : While in state Move , the algorithm is not allowed to cover the position lmin ( C ) . It must first enter the state Cover to do so . In contrast , for the state Skip , the newly generated hypothesis always remains in the state Skip ( until the state Initial is entered . ) This is motivated by the word reordering for the German verb group . After the right verbal brace has been processed , no source words may be moved into the verbal brace from later in the sentence . There is a redundancy in the reorderings : The same reordering might be carried out using either the state Skip or Move , especially if widthskip and widthmove are about the same . The additional computational burden is alleviated somewhat by the fact that the pruning , as introduced in Section 3.8 , does not distinguish hypotheses according to the states . A complexity analysis for different reordering constraints is given in Tillmann ( 2001 ) . 3.6 Word Reordering : IBM-Style Restrictions . We now compare the new word reordering approach with the approach used in Berger et al . In the approach presented in this article , source sentence words are aligned with hypothesized target sentence words.3 When a source sentence word is aligned , we say its position is covered . During the search process , a partial hypothesis is extended by choosing an uncovered source sentence position , and this choice is restricted . Only one of the first n uncovered positions in a coverage set may be chosen , where n is set to 4 . This choice is illustrated in Figure 9 . In the figure , covered positions are marked by a filled circle , and uncovered positions are marked by an unfilled circle . Positions that may be covered next are marked by an unfilled square . The restrictions for a coverage set C can be expressed in terms of the expression u ( C ) defined in the previous section : The number of uncovered source sentence positions to the left of the rightmost covered position . Demanding u ( C ) 3 , we obtain the S3 restriction 3 In Berger et al . ( 1996 ) , a morphological analysis is carried out and word morphemes are processed . Here , we process only full-form words . 112 uncovered position covered position uncovered position for extension 1 j J Figure 9 Illustration of the IBM-style reordering constraint . An upper bound of O ( E3 J4 ) for the word reordering complexity is given in Tillmann ( 2001 ) . In order to demonstrate the complexity of the proposed reordering constraints , wehave modified our translation algorithm to show , for the different reordering con straints , the overall number of successor states generated by the algorithm given inTable 3 . The number of successors shown in Figure 10 is counted for a pseudotransla tion task in which a pseudo source word x is translated into the identically pseudo target word x . No actual optimization is carried out ; the total number of successors is simply counted as the algorithm proceeds through subsets of increasing cardinality . The complexity differences for the different reordering constraints result from the different number of coverage subsets C and corresponding reordering states S allowed . For the different reordering constraints we obtain the following results ( the abbreviations MON , GE , EG , and S3 are taken from the Appendix ) : MON : For this reordering restriction , a partial hypothesis is always extended by the position lmin ( C ) , hence the number of processed arcs is J. GE , EG : These two reordering constraints are very similar in terms of complexity : The number of word reorderings is heavily restricted in each . Actually , since the distance restrictions ( expressed by the variables widthskip and widthmove ) apply , the complexity is linear in the length of the input sentence J. S3 : The S3 reordering constraint has a complexity close to J4 . Since no distance restrictions for the skipped positions apply , the overall search space is significantly larger than for the GE or EG restriction . 113 1e+07 1e+06 `` J 4 `` `` S 3 `` `` E G `` `` G E `` `` M O N `` 100000 10000 1000 100 10 1 0 5 10 15 20 25 30 35 40 45 50 Figure 10 Number of processed arcs for the pseudotranslation task as a function of the input sentence length J ( y-axis is given in log scale ) . The complexity for the four different reordering constraints MON , GE , EG , and S3 is given . The complexity of the S3 constraint is close to J4 . 3.8 Beam Search Pruning Techniques . To speed up the search , a beam search strategy is used . There is a direct analogy to the data-driven search organization used in continuous-speech recognition ( Ney et al . The full DP search algorithm proceeds cardinality-synchronously over subsets of source sentence positions of increasing cardinality . Using the beam search concept , the search can be focused on the most likely hypotheses . The hypotheses Qe1 ( e , C , j ) are distinguished according to the coverage set C , with two kinds of pruning based on this coverage set : 1 . The coverage pruning is carried out separately for each coverage set C.. 2 . The cardinality pruning is carried out jointly for all coverage sets C with . the same cardinality c = c ( C ) . After the pruning is carried out , we retain for further consideration only hypotheses with a probability close to the maximum probability . The number of surviving hypotheses is controlled by four kinds of thresholds : the coverage pruning threshold tC the coverage histogram threshold nC the cardinality pruning threshold tc the cardinality histogram threshold nc For the coverage and the cardinality pruning , the probability Qe1 ( e , C , j ) is adjusted to take into account the uncovered source sentence positions = { 1 , ... , J } \C . To make 114 this adjustment , for a source word f at an uncovered source position , we precompute an upper bound p ( f ) for the product of language model and lexicon probability : p ( f ) = max p ( e e1 , e11 ) p ( f e ) e11 , e1 , e The above optimization is carried out only over the word trigrams ( e , e1 , e11 ) that have actually been seen in the training data . Additionally , the observation pruning described below is applied to the possible translations e of a source word f . The upper bound is used in the beam search concept to increase the comparability between hypotheses covering different coverage sets . Even more benefit from the upper bound p ( f ) can be expected if the distortion and the fertility probabilities are taken into account ( Tillmann 2001 ) . Using the definition of p ( f ) , the following modified probability Q e1 ( e , C , j ) is used to replace the original probability Qe1 ( e , C , j ) , and all pruning is applied to the new probability : Q e1 ( e , C , j ) = Qe1 ( e , C , j ) n p ( fj ) j For the translation experiments , equation ( 3 ) is recursively evaluated over subsets of source positions of equal cardinality . For reasons of brevity , we omit the state descrip tion S in equation ( 3 ) , since no separate pruning according to the states S is carried out . The set of surviving hypotheses for each cardinality c is referred to as the beam . The size of the beam for cardinality c depends on the ambiguity of the translation task for that cardinality . To fully exploit the speedup of the DP beam search , the search space is dynamically constructed as described in Tillmann , Vogel , Ney , Zubiaga , and Sawaf ( 1997 ) , rather than using a static search space . To carry out the pruning , the maximum probabilities with respect to each coverage set C and cardinality c are computed : Coverage pruning : Hypotheses are distinguished according to the subset of covered positions C. The probability Q ( C ) is defined : Q ( ) = max Q e1 ( e , , j ) e , e1 , j Cardinality pruning : Hypotheses are distinguished according to the cardinality c ( C ) of subsets C of covered positions . The probability Q ( c ) is defined for all hypotheses with c ( C ) = c : Q ( c ) = max Q ( ) C c ( C ) =c The coverage pruning threshold tC and the cardinality pruning threshold tc are used to prune active hypotheses . We call this pruning translation pruning . Hypotheses are pruned according to their translation probability : Q e1 ( e , C , j ) < tC Q ( C ) Q e1 ( e , C , j ) < tc Q ( c ) For the translation experiments presented in Section 4 , the negative logarithms of the actual pruning thresholds tc and tC are reported . A hypothesis ( e1 , e , C , j ) is discarded if its probability is below the corresponding threshold . For the current experiments , the 115 coverage and the cardinality threshold are constant for different coverage sets C and cardinalities c. Together with the translation pruning , histogram pruning is carried out : The overall number N ( C ) of active hypotheses for the coverage set C and the overall number N ( c ) of active hypotheses for all subsets of a given cardinality may not exceed a given number ; again , different numbers are used for coverage and cardinality pruning . The coverage histogram pruning is denoted by nC , and the cardinality histogram pruning is denoted by nc : N ( C ) > nC N ( c ) > nc If the numbers of active hypotheses for each coverage set C and cardinality c , N ( C ) and N ( c ) , exceed the above thresholds , only the partial hypotheses with the highest translation probabilities are retained ( e.g. , we may use nC = 1,000 for the coverage histogram pruning ) . The third type of pruning conducted observation pruning : The number of words that may be produced by a source word f is limited . For each source language word f the list of its possible translations e is sorted according to p ( f | e ) puni ( e ) where puni ( e ) is the unigram probability of the target language word e. Only the best no target words e are hypothesized during the search process ( e.g. , during the experiments to hypothesize , the best no = 50 words was sufficient . In this section , we describe the implementation of the beam search algorithm presented in the previous sections and show how it is applied to the full set of IBM4 model parameters . The implementation described here is similar to that used in beam search speech recognition systems , as presented in Ney et al . The similarities are given mainly in the following : The implementation is data driven . Both its time and memory requirements are strictly linear in the number of path hypotheses ( disregarding the sorting steps explained in this section ) . The search procedure is developed to work most efficiently when the input sentences are processed mainly monotonically from left to right . The algorithm works cardinality-synchronously , meaning that all the hypotheses that are processed cover subsets of source sentence positions of equal cardinality c. Since full search is prohibitive , we use a beam search concept , as in speech recognition . We use appropriate pruning techniques in connection with our cardinality-synchronous search procedure . Table 4 shows a two-list implementation of the search algorithm given in Table 2 in which the beam pruning is included . The two lists are referred to as S and Snew : S is the list of hypotheses that are currently expanded , and Snew is the list of newly 116 Table 4 Two-list implementation of a DP-based search algorithm for statistical MT . input : source string f1 fj fJ initial hypothesis lists : S = { ( $ , $ , { } , 0 ) } for each cardinality c = 1 , 2 , ... , J do Snew = { } for each hypothesis ( e , e , C , j ) S , where j C and |C| = c do Expand ( e , e , C , j ) using probabilities p ( fj | e ) p ( j | j , J ) p ( e | e , e ) Look up and add or update expanded hypothesis in Snew Sort hypotheses in Snew according to translation score Carry out cardinality pruning Sort hypotheses in Snew according to coverage set C and translation score Carry out coverage pruning Bookkeeping of surviving hypotheses in Snew S : = Snew output : get best target word sequence eI from bookkeeping array generated hypotheses . The search procedure processes subsets of covered source sentence positions of increasing cardinality . The search starts with S = { ( $ , $ , { } , 0 ) } , where $ denotes the sentence start symbol for the immediate two predecessor words and { } denotes the empty coverage set , in which no source position is covered yet . For the initial search state , the position last covered is set to 0 . A set S of active hypotheses is expanded for each cardinality c using lexicon model , language model , and distortion model probabilities . The newly generated hypotheses are added to the hypothesis set Snew ; for hypotheses that are not distinguished according to our DP approach , only the best partial hypothesis is retained for further consideration . This so-called recombination is implemented as a set of simple lookup and update operations on the set Snew of partial hypotheses . During the partial hypothesis extensions , an anticipated pruning is carried out : Hypotheses are discarded before they are considered for recombination and are never added to Snew . ( The anticipated pruning is not shown in Table 4 . It is based on the pruning thresholds described in Section 3.8 . ) After the extension of all partial hypotheses in S , a pruning step is carried out for the hypotheses in the newly generated set Snew . The pruning is based on two simple sorting steps on the list of partial hypotheses Snew . ( Instead of sorting the partial hypothe ses , we might have used hashing . ) First , the partial hypotheses are sorted according to their translation scores ( within the implementation , all probabilities are converted into translation scores by taking the negative logarithm log ( ) ) . Cardinality prun ing can then be carried out simply by running down the list of hypotheses , starting with the maximum-probability hypothesis , and applying the cardinality thresholds . Then , the partial hypotheses are sorted a second time according to their coverage set C and their translation score . After this sorting step , all partial hypotheses that cover the same subset of source sentence positions are located in consecutive fragments in the overall list of partial hypotheses . Coverage pruning is carried out in a single run over the list of partial hypotheses : For each fragment corresponding to the same coverage set C , the coverage pruning threshold is applied . The partial hypotheses that survive the two pruning stages are then written into the so-called bookkeeping array ( Ney et al . For the next expansion step , the set S is set to the newly generated list of hypotheses . Finally , the target translation is constructed from the bookkeeping array . 117 3.9.2 Details for IBM4 Model . In this section , we outline how the DP-based beam search approach can be carried out using the full set of IBM4 parameters . ( More details can be found in Tillmann [ 2001 ] or in the cited papers . ) First , the full set of IBM4 parameters does not make the simplifying assumption given in Section 3.1 , namely , that source and target sentences are of equal length : Either a target word e may be aligned with several source words ( its fertility is greater than one ) or a single source word may produce zero , one , or two target words , as described in Berger et al . ( 1996 ) , or both . Zero target words are generated if f is aligned to the null word e0 . Generating a single target word e is the regular case . Two target words ( e1 , e11 ) may be generated . The costs for generating the target word e1 are given by its fertility ( 0 | e1 ) and the language model probability ; no lexicon probability is used . During the experiments , we restrict ourselves to triples of target words ( e , e1 , e11 ) actually seen in the training data . This approach is used for the French-to-English translation experiments presented in this article . Another approach for mapping a single source language word to several target language words involves preprocessing by the word-joining algorithm given in Till- mann ( 2001 ) , which is similar to the approach presented in Och , Tillmann , and Ney ( 1999 ) . Target words are joined during a training phase , and several joined target language words are dealt with as a new lexicon entry . This approach is used for the German-to-English translation experiments presented in this article . In order to deal with the IBM4 fertility parameters within the DP-based concept , we adopt the distinction between open and closed hypotheses given in Berger et al . A hypothesis is said to be open if it is to be aligned with more source positions than it currently is ( i.e. , at least two ) . Otherwise it is called closed . The difference between open and closed is used to process the input sentence one position a time ( for details see Tillmann 2001 ) . The word reordering restrictions and the beam search pruning techniques are directly carried over to the full set of IBM4 parameters , since they are based on restrictions on the coverage vectors C only . To ensure its correctness , the implementation was tested by carrying out forced alignments on 500 German-to-English training sentence pairs . In a forced alignment , the source sentence f J and the target sentence eI are kept fixed , and a full search with 1 1 out reordering restrictions is carried out only over the unknown alignment aJ . The language model probability is divided out , and the resulting probability is compared to the Viterbi probability as obtained by the training procedure . For 499 training sentencesthe Viterbi alignment probability as obtained by the forced-alignment search was exactly the same as the one produced by the training procedure . In one case the forced alignment search did obtain a better Viterbi probability than the training procedure . Translation experiments are carried out for the translation directions German to English and English to German ( Verbmobil task ) and for the translation directions French to English and English to French ( Canadian Hansards task ) . Section 4.1 reports on the performance measures used . Section 4.2 shows translation results for the Verbmobil task . Sections 4.2.1 and 4.2.2 describe that task and the preprocessing steps applied . In Sections 4.2.3 through 4.2.5 , the efficiency of the beam search pruning techniques is shown for German-to-English translation , as the most detailed experiments are conducted for that direction . Section 4.2.6 gives translation results for the translation direction English to German . In Section 4.3 , translation results for the Canadian Hansards task are reported . 118 4.1 Performance Measures for Translation Experiments . To measure the performance of the translation methods , we use three types of au tomatic and easy-to-use measures of the translation errors . Additionally , a subjective evaluation involving human judges is carried out ( Niessen et al . The following evaluation criteria are employed : WER ( word error rate ) : The WER is computed as the minimum number of substitution , insertion , and deletion operations that have to be performed to convert the generated string into the reference target string . This performance criterion is widely used in speech recognition . The minimum is computed using a DP algorithm and is typically referred to as edit or Levenshtein distance . mWER ( multireference WER ) : We use the Levenshtein distance between the automatic translation and several reference translations as a measure of the translation errors . For example , on the Verbmobil TEST-331 test set , an average of six reference translations per automatic translation are available . The Levenshtein distance between the automatic translation and each of the reference translations is computed , and the minimum Levenshtein distance is taken . The resulting measure , the mWER , is more robust than the WER , which takes into account only a single reference translation . PER ( position-independent word error rate ) : In the case in which only a single reference translation per sentence is available , we introduce as an additional measure the position-independent word error rate ( PER ) . This measure compares the words in the two sentences without taking the word order into account . Words in the reference translation that have no counterpart in the translated sentence are counted as substitution errors . Depending on whether the translated sentence is longer or shorter than the reference translation , the remaining words result in either insertion ( if the translated sentence is longer ) or deletion ( if the translated sentence is shorter ) errors . The PER is guaranteed to be less than or equal to the WER . The PER is more robust than the WER since it ignores translation errors due to different word order in the translated and reference sentences . SSER ( subjective sentence error rate ) : For a more fine-grained evaluation of the translation results and to check the validity of the automatic evaluation measures subjective judgments by test persons are carried out ( Niessen et al . The following scale for the error count per sentence is used in these subjective evaluations : 0.0 : semantically correct and syntactically correct : 0.5 : semantically correct and syntactically wrong : 1.0 : semantically wrong ( independent of syntax ) Each translated sentence is judged by a human examiner according to the above error scale ; several human judges may be involved in judging the same translated sentence . Subjective evaluation is carried out only for the Verbmobil TEST-147 test set . 119 Table 5 Training and test conditions for the German-to-English Verbmobil corpus ( *number of words without punctuation ) . German English Trai nin g : Se nte nc es W or ds 58,0 5 1 9 , 5 2 3 73 54 9 , 92 1 W or ds* 4 1 8 , 9 7 9 45 3 , 63 2 Voc abu lary : Siz e 7 , 9 1 1 4 , 6 4 8 Sin gle to ns 3 , 4 5 3 1 , 6 9 9 TEST 331 : Se nte nc es W or ds 33 5 , 5 9 1 1 6 , 2 7 9 Bigram/Trigram Perplexity 84.0/68.2 49.3/38.3 TEST-147 : Sentences 147 Words 1,968 2,173 Bigram/Trigram Perplexity 34.6/28.1 4.2 Verbmobil Translation Experiments . 4.2.1 The Task and the Corpus . The translation system is tested on the Verbmobil task ( Wahlster 2000 ) . In that task , the goal is the translation of spontaneous speech in face to-face situations for an appointment scheduling domain . We carry out experiments for both translation directions : German to English and English to German . Although the Verbmobil task is still a limited-domain task , it is rather difficult in terms of vocabulary size , namely , about 5,000 words or more for each of the two languages ; second , the syntactic structures of the sentences are rather unrestricted . Although the ultimate goal of the Verbmobil project is the translation of spoken language , the input used for the translation experiments reported on in this article is mainly the ( more or less ) correct orthographic transcription of the spoken sentences . Thus , the effects of spontaneous speech are present in the corpus ; the effect of speech recognition errors , however , is not covered . The corpus consists of 58,073 training pairs ; its characteristics are given in Table 5 . For the translation experiments , a trigram language model with a perplexity of 28.1 is used . The following two test corpora are used for the translation experiments : TEST-331 : This test set consists of 331 test sentences . Only automatic evaluation is carried out on this test corpus : The WER and the mWER are computed . For each test sentence in the source language there is a range of acceptable reference translations ( six on average ) provided by a human translator , who is asked to produce word-to-word translations wherever it is possible . Part of the reference sentences are obtained by correcting automatic translations of the test sentences that are produced using the approach presented in this article with different reordering constraints . The other part is produced from the source sentences without looking at any of their translations . The TEST-331 test set is used as held-out data for parameter optimization ( for the language mode scaling factor and for the distortion model scaling factor ) . Furthermore , the beam search experiments in which the effect of the different pruning thresholds is demonstrated are carried out on the TEST-331 test set . TEST-147 : The second , separate test set consists of 147 test sentences . Translation results are given in terms of mWER and SSER . No parameter optimization 120 is carried out on the TEST-147 test set ; the parameter values as obtained from the experiments on the TEST-331 test set are used . To improve the translation performance the following preprocessing steps are carried out : Categorization : We use some categorization , which consists of replacing a single word by a category . The only words that are replaced by a category label are proper nouns denoting German cities . Using the new labeled corpus , all probability models are trained anew . To produce translations in the normal language , the categories are translated by rule and are inserted into the target sentence . Word joining : Target language words are joined using a method similar to the one described in Och , Tillmann , and Ney ( 1999 ) . Words are joined to handle cases like the German compound noun Zahnarzttermin for the English dentist s appointment , because a single word has to be mapped to two or more target words . The word joining is applied only to the target language words ; the source language sentences remain unchanged . During the search process several joined target language words may be generated by a single source language word . Manual lexicon : To account for unseen words in the test sentences and to obtain a greater number of focused translation probabilities p ( f | e ) , we use a bilin gual GermanEnglish dictionary . For each word e in the target vocabulary , we create a list of source translations f according to this dictionary . The translation probability pdic ( f | e ) for the dictionary entry ( f , e ) is defined as 1 pdic ( f | e ) = Ne if ( f , e ) is in dictionary 0 otherwise where Ne is the number of source words listed as translations of the target word e. The dictionary probability pdic ( f | e ) is linearly combined with the automatically trained translation probabilities paut ( f | e ) to obtain smoothed probabilities p ( f | e ) : p ( f | e ) = ( 1 ) pdic ( f | e ) + paut ( f | e ) For the translation experiments , the value of the interpolation parameter is fixed at = 0.5 . 4.2.3 Effect of the Scaling Factors . In speech recognition , in which Bayes decision rule is applied , a language model scaling factor LM is used ; a typical value is LM 15 . This scaling factor is employed because the language model probabilities are more reliably estimated than the acoustic probabilities . Following this use of a language model scaling factor in speech recognition , such a factor is introduced into statistical MT , too . The optimization criterion in equation ( 1 ) is modified as follows : eI = arg max { p ( eI ) LM p ( f J | eI ) } 1 I 1 1 1 1 where p ( eI ) is the language model probability of the target language sentence . In the experiments presented here , a trigram language model is used to compute p ( eI ) . The 121 Table 6 Computing time , mWER , and SSER for three different reordering constraints on the TEST-147 test set . During the translation experiments , reordered words are not allowed to cross punctuation marks . Re or de rin g co nst rai nt C P U ti m e [ s e c ] m W E R [ % ] S S E R [ % ] MO N 0 . 6 28 .6 GE 5 . 3 21 .0 S3 1 3 . 4 19 .9 effect of the language model scaling factor LM is studied on the TEST-331 test set . A minimum mWER is obtained for LM = 0.8 , as reported in Tillmann ( 2001 ) . Unlike in speech recognition , the translation model probabilities seem to be estimated as reliably as the language model probabilities in statistical MT . A second scaling factor D is introduced for the distortion model probabilities p ( j | j1 , J ) . A minimum mWER is obtained for D = 0.4 , as reported in Tillmann ( 2001 ) . The WER and mWER on the TEST-331 test set increase significantly , if no distortion probability is used , for the case D = 0.0 . The benefit of a distortion probability scaling factor of D = 0.4 comes from the fact that otherwise , a low distortion probability might suppress long-distant word reordering that is important for German-to-English verb group reordering . The setting LM = 0.8 and D = 0.4 is used for all subsequent translation results ( including the translation direction English to German ) . 4.2.4 Effect of the Word Reordering Constraints . Table 6 shows the computing time , mWER , and SSER on the TEST-147 test set as a function of three reordering constraints : MON , GE , and S3 ( as discussed in the Appendix ) . The computing time is given in terms of central processing unit ( CPU ) time per sentence ( on a 450 MHz Pentium III personal computer ) . For the SSER , it turns out that restricting the word reordering such that it may not cross punctuation marks improves translation performance significantly . The average length of the sentence fragments that are separated by punctuation marks is rather small : 4.5 words per fragment . A coverage pruning threshold 4 of tC = 5.0 and an observation pruning of no = 50 are applied during the experiments . No other type of pruning is used.5 The MON constraint performs worst in terms of both mWER and SSER . The computing time is small , since no reordering is carried out . Constraints GE and S3 perform nearly identically in terms of both mWER and SSER . The GE constraint , however , works about three times as fast as the S3 constraint . Table 7 shows example translations obtained under the three different reordering constraints . Again , the MON reordering constraint performs worst . In the second and third translation examples , the S3 word reordering constraint performs worse than the GE reordering constraint , since it can not take the word reordering due to the German verb group properly into account . The German finite verbs bin ( second example ) and ko nnten ( third example ) are too far away from the personal pronouns ich and Sie ( six 4 For the translation experiments , the negative logarithm of the actual pruning thresholds tc and tC is . reported ; for simplicity reasons we do not change the notation . 5 In a speech-to-speech demo system , we would use the GE reordering restriction and a slightly sharper . pruning in order to achieve translation times of about one second per sentence . 122 Table 7 Example translations for the translation direction German to English using three different reordering constraints : MON , GE , and S3 . Input : Ja , wunderbar . MON : Yes , wonderful . GE : Yes , wonderful . S3 : Yes , wonderful . Input : Das ist zu knapp , weil ich ab dem dritten in Kaiserslautern bin . Wie wa re es denn am a hm Samstag , dem zehnten Februar ? MON : That is too tight , because I from the third in Kaiserslautern . In fact only on the third . How about a hm Saturday , the tenth of February ? GE : That is too tight , because I am from the third in Kaiserslautern . In fact only on the third . A hm how about Saturday , February the tenth ? S3 : That is too tight , from the third because I will be in Kaiserslautern . In fact only on the third . A hm how about Saturday , February the tenth ? Input : Wenn Sie dann noch den siebzehnten ko nnten , wa re das toll , ja . MON : If you then also the seventeenth could , would be the great , yes . GE : If you could then also the seventeenth , that would be great , yes . S3 : Then if you could even take seventeenth , that would be great , yes . Input : Ja , das kommt mir sehr gelegen . Machen wir es dann am besten so . MON : Yes , that suits me perfectly . Do we should best like that . GE : Yes , that suits me fine . We do it like that then best . S3 : Yes , that suits me fine . We should best do it like that . and five source sentence positions , respectively ) to be reordered properly . In the last example , the less restrictive S3 reordering constraint leads to a better translation ; the GE translation is still acceptable , though . 4.2.5 Effect of the Beam Search Pruning Thresholds . In this section , the effect of the beam search pruning is demonstrated . Translation results on the TEST-331 test set are presented to evaluate the effectiveness of the pruning techniques.6 The quality of the search algorithm with respect to the GE and S3 reordering constraints is evaluated using two criteria : 1 . The number of search errors for a certain combination of pruning . A search error occurs for a test sentence if the final translation probability QF for a candidate translation eI as given in equation ( 4 ) is smaller than a reference probability for that test sentence . We will compute reference probabilities two ways , as explained below . The mWER performance measure is computed as a function of the . Generally , decreasing the pruning threshold 6 The CPU times on the TEST-331 set are higher , since the average fragment length is greater than for the . 123 Effect of the coverage pruning threshold tC on the number of search errors and mWER on the TEST-331 test set ( no cardinality pruning carried out : tc = ) . A cardinality histogram pruning of 200,000 is applied to restrict the maximum overall size of the search space . The negative logarithm of tC is reported . Re or de rin g co nst rai nt t C C P U ti m e [ s e c ] S e a r c h e r r o r s Qref > QF QF > QF m W E R [ % ] GE 0 . 2 1 3 1 8 3 2 3 7 3 . 4 3 2 3 1 3 0 1 5 3 . 4 3 1 0 2 2 6 3 0 . 7 5 5 1 4 2 2 5 . 6 3 5 2 4 . 5 1 5 6 2 2 4 . 0 6 3 0 2 4 . 5 1 3 0 0 2 4 . 4 8 3 1 4 3 2 4 7 0 . 2 1 2 2 5 3 0 3 5 0 . 2 4 2 2 3 3 1 . 5 1 9 0 1 2 9 2 8 . 0 8 3 0 2 8 . 3 leads to a higher word error rate , since the optimal path through the translation lattice is missed , resulting in translation errors . Two automatically generated reference probabilities are used . These probabilities are computed separately for the reordering constraints GE and S3 ( the difference is not shown in the notation , but will be clear from the context ) : Qref : A forced alignment is carried out between each of the test sentences and its corresponding reference translation ; only a single reference translation for each test sentence is used . The probability obtained for the reference translation is denoted by Qref . QF : A translation is carried out with conservatively large pruning thresholds , yielding a translation close to the one with the maximum translation probability . The translation probability for that translation is denoted by QF . First , in a series of experiments we study the effect of the coverage and cardinality pruning for the reordering constraints GE and S3 . ( When we report on the different pruning thresholds , we will show the negative logarithm of those pruning thresholds . ) The experiments are carried out on two different pruning dimensions : 1 . In Table 8 , only coverage pruning using threshold tC is carried out ; no . cardinality pruning is applied : tc = . In Table 9 , only cardinality pruning using threshold tc is carried out ; no . coverage pruning is applied : tC = . Both tables use an observation pruning of no = 50 . The effect of the coverage pruning threshold tC is demonstrated in Table 8 . For the translation experiments reportedin this table , the cardinality pruning threshold is set to tc = ; thus , no compari son between partial hypotheses that do not cover the same set C of source sentence 124 Effect of the cardinality pruning threshold tc on the number of search errors and mWER on the TEST-331 test set ( no coverage pruning is carried out : tC = ) . A coverage histogram pruning of 1,000 is applied to restrict the overall size of the search space . The negative logarithm of tc is shown . Re or de rin g co nst rai nt t c C P U ti m e [ s e c ] S e a r c Qre f > QF h error s QF > QF m W E R [ % ] GE 1 . 0 3 4 5 2 8 7 4 8 . 0 6 2 0 2 7 7 4 1 . 1 3 1 6 2 6 6 3 7 . 3 0 6 2 3 9 3 4 . 5 5 2 2 1 2 3 0 . 2 1 0 6 2 6 . 2 3 2 2 5 . 0 2 1 0 3 3 1 5 1 . 0 5 1 2 8 3 4 6 . 1 0 1 2 7 4 4 3 . 2 2 2 5 1 4 0 . 5 0 2 2 7 3 7 . 3 1 7 1 3 2 . 8 9 9 3 0 . 3 4 9 2 8 . 0 4 3 0 2 8 . 2 positions is carried out . To restrict the overall size of the search space in terms of CPU time and memory requirements , a cardinality pruning of nc = 200,000 is applied . As can be seen from Table 8 , mWER and the number of search errors decrease significantly as the coverage pruning threshold tC increases . For the GE reordering constraint , mWER decreases from 73.5 % to 24.9 % . For a coverage pruning threshold tC 5.0 , mWER remains nearly constant at 25.0 % , although search errors still occur . For the S3 reordering constraint , mWER decreases from 70.0 % to 28.3 % . The largest coverage threshold tested for the S3 constraint is tC = 5.0 , since for larger threshold values tC , the search procedure can not be carried out because of memory and time restrictions . The number of search errors is reduced as the coverage pruning threshold is increased . It turns out to be difficult to verify search errors by looking at the reference translation probabilities Qref alone . The translation with the maximum translation probability seems to be quite narrowly defined . The coverage pruning is more effective for the GE constraint than for the S3 constraint , since the overall search space for the GE reordering is smaller . Table 9 shows the effect of the cardinality pruning threshold tc on mWER when no coverage pruning is carried out ( a histogram coverage pruning of 1,000 is applied to restrict the overall size of the search space ) . The cardinality threshold tc has a strong effect on mWER , which decreases significantly as the cardinality threshold tc increases . For the GE reordering constraint , mWER decreases from 48.5 % to 24.9 % ; for the S3 reordering constraint , mWER decreases from 51.4 % to 28.2 % . For the coverage threshold t = 15.0 , the GE constraint works about four times as fast as the S3 constraint , since the overall search space for the S3 constraint is much larger . Although the overall search space is much larger for the S3 constraint , for smaller values of the coverage 125 Effect of observation pruning on the number of search errors and mWER on the TEST-331 test set ( parameter setting : tc = , tC = 10.0 ) . No histogram pruning is applied . The results are reported for the GE constraint . Ob ser vat ion pr un in g no C P U ti m e [ s e c ] S e a r c h e r r o r s Qref > QF QF > QF m W E R [ % ] 1 2 . 0 1 3 2 8 4 2 9 . 9 6 2 3 9 2 6 . 8 2 1 9 6 2 5 . 6 2 1 4 0 2 5 . 9 9 9 2 4 . 8 25 2 3 8 4 4 2 4 . 5 50 6 3 0 2 4 . 9 threshold tC 5.0 , the S3 constraint works as fast as the GE constraint or even faster , because only a very small portion of the overall search space is searched for small values of the cardinality pruning threshold tc . There is some computational overhead in expanding a partial hypothesis for the GE constraint because the finite-state control has to be handled . No results are obtained for the S3 constraint and the coverage threshold tc = 17.5 because of memory restrictions . The number of search errors is reduced as the cardinality pruning threshold is increased . Again , it is difficult to verify search errors by looking at the reference translation probabilities alone . Both coverage and cardinality pruning are more efficient for the GE reordering constraint than for the S3 reordering constraint . For the S3 constraint , no translation results are obtained for a coverage threshold tc > 5.0 without cardinality pruning applied because of memory and computing time restrictions . For the GE constraint virtually a full search can be carried out where only observation pruning is applied : Identical target translations and translation probabilities are produced for the hypoth esis files for the two cases ( 1 ) tC = 10.0 , tc = , and ( 2 ) tC = , tc = 15.0 . ( Actually , for one test sentence in the TEST-331 test set , the translations are different , although the translation probabilities are exactly the same . ) Since the pruning is carried out independently on two different pruning dimensions , no search errors will occur if the thresholds are further increased . Table 10 shows the effect of the observation pruning parameter no on mWER for the reordering constraint GE . mWER is significantly reduced by hypothesizing up to the best 50 target words e for a source language word f . mWER increases from 24.9 % to 29.3 % when the number of hypothesized words is decreased to only a single word.Table 11 demonstrates the effect of the combination of the coverage pruning thresh old tC = 5.0 and the cardinality pruning threshold tc = 12.5 , where the actual values are found in informal experiments : In a typical setting of the two parameters tc should be at least twice as big as tC . For the GE reordering constraint , the average computing time is about seven seconds per sentence without any loss in translation performance as measured in terms of mWER . For the S3 reordering constraint , the average computing time per sentence is 27 seconds . Again , the combination of coverage and cardinality pruning works more efficiently for the GE constraint . The memory requirement for the algorithm is about 100 MB . A series of translation experiments for the translation direction English to German are also carried out . The results , given 126 Demonstration of the combination of the two pruning thresholds tC = 5.0 and tc = 12.5 to speed up the search process for the two reordering constraints GE and S3 ( no = 50 ) . The translation performance is shown in terms of mWER on the TEST-331 test set . Reordering tC tc CPU time Search errors mWER con stra int [ s e c ] Q ref > Q F QF > QF [ % ] GE 5 . 9 0 3 8 24 .7 S3 5 . 9 0 6 5 29 .2 Table 12 Translation results for the translation direction English to German on the TEST-331 test set . The results are given in terms of computing time , WER , and PER for three different reordering constraints : MON , EG , and S3 . Re or de rin g co nst rai nt C P U ti m e [ s e c ] W E R [ % ] P E R [ % ] MO N 0 . 8 in terms of WER and PER , are shown in Table 12 . For the English-to-German translation direction , a single reference translation for each test sentence is used to carry out the automatic evaluation . The translation task for the translation direction English to German is more difficult than for the translation direction German to English ; the trigram language model perplexity increases from 38.3 to 68.2 on the TEST-331 test set , as can be seen in Table 5 . No parameter optimization is carried out for this translation direction ; the parameter settings are carried over from the results obtained in Table 11 . The word error rates for the translation direction English to German are significantly higher than those for the translation direction German to English . There are several reasons for this : German vocabulary and perplexity are significantly larger than those for English , and only a single reference translation per test sentence is available for English-to-German translation . There is only a very small difference in terms of word error rates for the reordering constraints EG and S3 ; in particular , WER is 70.1 % for both . The reordering constraint MON performs slightly worse : WER increases to 70.6 % , and PER increases to 57.0 % . Table 13 shows translation examples for the translation direction English to German . The MON constraint performs worst ; there is no significant difference in quality of translations produced under the EG and the S3 constraints . 4.3 Canadian Hansards Translation Experiments . 4.3.1 The Task and the Corpus . The second corpus on which we perform translationexperiments is the Hansard corpus . By law , the proceedings of the Canadian parliament are recorded in both French and English . ( For historical reasons , these proceed ings are called Hansards . ) The remarks of the parliament members are written down in whichever of the two languages they use . They are then translated into the other language to produce complete sets of the proceedings , one in French and the other in English . The resulting bilingual data have been sentence-aligned using statistical methods ( Brown et al . Originally , about three million sentences were selected . Here , we use a subset of the original training data ; the details regarding this subset 127 Example translations for the translation direction English to German using three different reordering constraints : MON , EG , and S3 . Input : Yeah , that wouldn t be bad . Do you have any ideas where I could stay ? MON : Ja , das wa re schade . Haben Sie irgendwelche Ideen wo ich ko nnten u bernachten ? EG : Ja , das wa re nicht schlecht . Haben Sie irgendwelche Ideen wo wir wohnen ko nnten ? S3 : Ja , das wa re nicht schlecht . Haben Sie irgendwelche Ideen wo wir wohnen ko nnten ? Input : Oh , that sounds great . Could you arrange a suite for me ? MON : Oh , das klingt gut . Ko nnten Sie unbedingt ein Suite bei mir ? EG : Oh , das klingt gut . Ko nnten Sie einen Suite ausmachen fu r mich ? S3 : Oh , das klingt gut . Ko nnten Sie mir einen Suite ausmachen ? Input : Well , I still need your signature here and then I will check with your company . MON : Also , ich konnte Arbeitskraft Unterschrift hier und ich werde nachsehen mit Ihrer Firma . EG : Also , ich bra uchte noch Ihre Unterschrift und dann gucke ich hier mit Ihrer Firma . S3 : Also , ich brauche hier noch Ihre Unterschrift und dann werde ich veranlassen mit Ihrer Firma . Table 14 Training and test conditions for the Hansards task ( *number of words without punctuation ) . French English Train : Sentences 1,470,473 W or ds 2 4 , 3 3 8 , 1 9 5 22 ,1 63 ,0 92 W or ds* 2 2 , 1 7 5 , 0 6 9 20 ,0 63 ,3 78 Voc abu lary : Siz e 1 0 0 , 2 6 9 7 8 , 3 3 2 Sin gle to ns 4 0 , 1 9 9 3 1 , 3 1 9 Test : Sentences 5,432 Words 97,646 80,559 Bigr./Tri . Perplexity 196.9/121.8 269.9/179.8 are given in Table 14 . The Hansards corpus presents by far a more difficult task than the Verbmobil corpus in terms of vocabulary size and number of training sentences . The training and test sentences are less restrictive than for the Verbmobil task . For the translation experiments on the Hansards corpus , no word joining is carried out . Two target words can be produced by a single source word , as described in Section 3.9.2 . As can be seen in Table 15 for the translation direction French to English and in Table 16 for the translation direction English to French , the word error rates are rather high compared to those for the Verbmobil task . The reason for the higher error rates is that , as noted in the previous section , the Hansards task is by far less restrictive than the Verbmobil task , and the vocabulary size is much 128 Computing time , WER , and PER for the translation direction French to English using the two reordering constraints MON and S3 . An almost full search is carried out . Reordering CPU time WER PER con stra int [ s e c ] [ % ] [ % ] MO N 2 . 0 S3 5 8 0 . 4 Table 16 Computing time , WER , and PER for the translation direction English to French using the two reordering constraints MON and S3 . An almost full search is carried out . Reordering CPU time WER PER con stra int [ s e c ] [ % ] [ % ] MO N 2 . 3 S3 1 8 9 . There is only a slight difference in performance between the MON and the S3 reordering constraints on the Hansards task . The computation time is also rather high compared to the Verbmobil task : For the S3 constraint , the average translation time is about 3 minutes per sentence for the translation direction English to French and about 10 minutes per sentence for the translation direction French to English . The following parameter setting is used for the experiment conducted here : tC = 5.0 , tc = 10.0 , nC = 250 , and to = 12 . ( The actual parameters are chosen in informal experiments to obtain reasonable CPU times while permitting only a small number of search errors . ) No cardinality histogram pruning is carried out . As for the German- to-English translation experiments , word reordering is restricted so that it may not cross punctuation boundaries . The resulting fragment lengths are much larger for the translation direction English to French , and still larger for the translation direction French to English , when compared to the fragment lengths for the translation direction German to English , hence the high CPU times . In an additional experiment for the translation direction French to English and the reordering constraint S3 , we find we can speed up the translation time to about 18 seconds per sentence by using the following parameter setting : tC = 3.0 , tc = 7.5 , nC = 20 , nc = 400 , and no = 5 . For the resulting hypotheses file , PER increases only slightly , from 51.4 % to 51.6 % . Translation examples for the translation direction French to English under the S3 reordering constraint are given in Table 17 . The French input sentences show some preprocessing that is carried out beforehand to simplify the translation task ( e.g. , des is transformed into de les and l est is transformed into le est ) . The translations produced are rather approximative in some cases , although the general meaning is often preserved . We have presented a DP-based beam search algorithm for the IBM4 translation model . The approach is based on a DP solution to the TSP , and it gains efficiency by imposing constraints on the allowed word reorderings between source and target language . A data-driven search organization in conjunction with appropriate pruning techniques 129 Example translations for the translation direction French to English using the S3 reordering constraint . Input Je crois que cela donne une bonne ide e de les principes a ` retenir et de ce que devraient e tre nos responsabilite s. S3 I think it is a good idea of the principles and to what should be our responsibility . Input Je pense que , inde pendamment de notre parti , nous trouvons tous cela inacceptable . S3 I think , regardless of our party , we find that unacceptable . Input Je ai le intention de parler surtout aujourd hui de les nombreuses ame liorations apporte es a ` les programmes de pensions de tous les Canadiens . S3 I have the intention of speaking today about the many improvements in pensions for all Canadians especially those programs . Input Chacun en lui - me me est tre ` s complexe et le lien entre les deux le est encore davantage de sorte que pour beaucoup la situation pre sente est confuse . S3 Each in itself is very complex and the relationship between the two is more so much for the present situation is confused . For the medium-sized Verbmobil task , a sentence can be translated in a few seconds on average , with a small number of search errors and no performance degradation as measured by the word error criterion used . Word reordering is parameterized using a set of four parameters , in such a way that it can easily be adopted to new translation directions . A finite-state control is added , and its usefulness is demonstrated for the translation direction German to English , in which the word order difference between the two languages is mainly due to the German verb group . Future work might aim at a tighter integration of the IBM4 model distortion probabilities and the finite-state control ; the finite-state control itself may be learned from training data . The applicability of the algorithm applied in the experiments in this article is not restricted to the IBM translation models or to the simplified translation model used in the description of the algorithm in Section 3 . Since the efficiency of the beam search approach is based on restrictions on the allowed coverage vectors C alone , the approach may be used for different types of translation models as well ( e.g. , for the multiword-based translation model proposed in Och , Tillmann , and Ney [ 1999 ] ) . On the other hand , since the decoding problem for the IBM4 translation model is provably NP-complete , as shown in Knight ( 1999 ) and Germann et al . ( 2001 ) , wordreordering restrictions as introduced in this article are essential for obtaining an effi cient search algorithm that guarantees that a solution close to the optimal one will be found . Appendix : Quantification of Reordering Restrictions To quantify the reordering restrictions in Section 3.5 , the four non-negative numbers numskip , widthskip , nummove , and widthmove are used ( width skip corresponds to L , widthmove corresponds to R in Section 3.4 ; here , we use a more intuitive notation ) . Within the implementation of the DP search , the restrictions are provided to the 130 algorithm as an input parameter of the following type : S numskip widthskip M nummove widthmove The meaning of the reordering string is as follows : The two numbers following S that are separated by an underscore describe the way words may be skipped ; the two numbers following M that are separated by an underscore describe the way words may be moved during word reordering . The first number after S and M denotes the number of positions that may be skipped or moved , respectively ( e.g. , for the translation direction German to English [ GE in the chart below ] , one position may be skipped and two positions may be moved ) . The second number after S and M restricts the distance a word may be skipped or moved , respectively . These width parameters restrict the word reordering to take place within a window of a certain size , established by the distance between the positions lmin ( C ) and rmax ( C ) as defined in Section 3.5 . In the notation , either the substring headed by S or that headed by M ( or both ) may be omitted altogether to indicate that the corresponding reordering is not allowed . Any numerical value in the string may be set to INF , denoting that an arbitrary number of positions may be skipped/moved or that the moving or skipping distance may be arbitrarily large . The following reordering strings are used in this article : Word reordering Description string E The empty string denotes the reordering restriction in which ( short : MON ) no reordering is allowed . S 01 04 M 02 10 This string describes the German-to-English word reordering . ( short : GE ) Up to one word may be skipped for at most 4 positions , and up to two words may be moved up to 10 positions . S 02 10 M 01 04 This string describes the English-to-German word reordering . ( short : EG ) Up to two words may be skipped for at most 10 positions and up to one word may be moved for up to 4 positions . S 03 INF This string describes the IBM-style word reordering ( short : S3 ) given in Section 3.6 . Up to three words may be skipped for an unrestricted number of positions . No words may be moved . S INF INF or These strings denote the word reordering without M INF INF restrictions . ( short : NO ) The word reordering strings can be directly used as input parameters to the DP-based search procedure to test different reordering restrictions within a single implementation . This work has been supported as part of the Verbmobil project ( contract number 01 IV 601 A ) by the German Federal . Ministry of Education , Science , Research and Technology and as part of the Eutrans 131 project ( ESPRIT project number 30268 ) by the European Community . Some of the experiments on the Canadian Hansards task have been carried out by Nicola Ueffing using the existing implementation of the search algorithm ( Och , Ueffing , and Ney [ 2001 ] ) . We would like to thank the anonymous reviewers for their detailed comments on an earlier version of this article . Also , we would like to thank Niyu Ge , Scott McCarley , Salim Roukos , Nicola Ueffing , and Todd Ward for their valuable remarks . A Stochastic Finite-State Word-Segmentation Algorithm for Chinese The initial stage of text analysis for any NLP task usually involves the tokenization of the input into words . For languages like English one can assume , to a first approximation , that word boundaries are given by whitespace or punctuation . In various Asian languages , including Chinese , on the other hand , whitespace is never used to delimit words , so one must resort to lexical information to `` reconstruct '' the word-boundary information . In this paper we present a stochastic finite-state model wherein the basic workhorse is the weighted finite-state transducer . The model segments Chinese text into dictionary entries and words derived by various productive lexical processes , and -- since the primary intended application of this model is to text-to-speech synthesis -- provides pronunciations for these words . We evaluate the system 's performance by comparing its segmentation 'Tudgments '' with the judgments of a pool of human segmenters , and the system is shown to perform quite well . Any NLP application that presumes as input unrestricted text requires an initial phase of text analysis ; such applications involve problems as diverse as machine translation , information retrieval , and text-to-speech synthesis ( TIS ) . An initial step of any text analysis task is the tokenization of the input into words . For a language like English , this problem is generally regarded as trivial since words are delimited in English text by whitespace or marks of punctuation . Thus in an English sentence such as I 'm going to show up at the ACL one would reasonably conjecture that there are eight words separated by seven spaces . A moment 's reflection will reveal that things are not quite that simple . There are clearly eight orthographic words in the example given , but if one were doing syntactic analysis one would probably want to consider I 'm to consist of two syntactic words , namely I and am . If one is interested in translation , one would probably want to consider show up as a single dictionary word since its semantic interpretation is not trivially derivable from the meanings of show and up . And if one is interested in TIS , one would probably consider the single orthographic word ACL to consist of three phonological words-lei s ' i d/-corresponding to the pronunciation of each of the letters in the acronym . Space- or punctuation-delimited * 700 Mountain Avenue , 2d451 , Murray Hill , NJ 07974 , USA . Email : rlls @ bell-labs . com t 700 Mountain Avenue , 2d451 , Murray Hill , NJ 07974 , USA . Email : cls @ bell-labs . com t 600 Mountain Avenue , 2c278 , Murray Hill , NJ 07974 , USA . Email : gale @ research . com Cambridge , UK Email : nc201 @ eng.cam.ac.uk 1996 Association for Computational Linguistics ( a ) B ) ( , : & ; ? ' H o w d o y o u s a y o c t o p u s i n J a p a n e s e ? ' ( b ) P l a u s i b l e S e g m e n t a t i o n I B X I I 1 : & I 0 0 r i 4 w e n 2 z h a n g l y u 2 z e n 3 m e 0 s h u o l ' J a p a n e s e ' ' o c t o p u s ' ' h o w ' ' s a y ' ( c ) Figure 1 I m p l a u s i b l e S e g m e n t a t i o n [ ] lxI 1 : & I ri4 wen2 zhangl yu2zen3 me0 shuol 'Japan ' 'essay ' 'fish ' 'how ' 'say ' A Chinese sentence in ( a ) illustrating the lack of word boundaries . In ( b ) is a plausible segmentation for this sentence ; in ( c ) is an implausible segmentation . orthographic words are thus only a starting point for further analysis and can only be regarded as a useful hint at the desired division of the sentence into words . Whether a language even has orthographic words is largely dependent on the writing system used to represent the language ( rather than the language itself ) ; the notion `` orthographic word '' is not universal . Most languages that use Roman , Greek , Cyrillic , Armenian , or Semitic scripts , and many that use Indian-derived scripts , mark orthographic word boundaries ; however , languages written in a Chinese-derived writ ing system , including Chinese and Japanese , as well as Indian-derived writing systems of languages like Thai , do not delimit orthographic words.1 Put another way , written Chinese simply lacks orthographic words . In Chinese text , individual characters of the script , to which we shall refer by their traditional name of hanzi , Z are written one after another with no intervening spaces ; a Chinese sentence is shown in Figure 1.3 Partly as a result of this , the notion `` word '' has never played a role in Chinese philological tradition , and the idea that Chinese lacks any thing analogous to words in European languages has been prevalent among Western sinologists ; see DeFrancis ( 1984 ) . Twentieth-century linguistic work on Chinese ( Chao 1968 ; Li and Thompson 1981 ; Tang 1988,1989 , inter alia ) has revealed the incorrectness of this traditional view . All notions of word , with the exception of the orthographic word , are as relevant in Chinese as they are in English , and just as is the case in other languages , a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese , see Nagata ( 1994 ) , inter alia.. 2 Chinese ? l* han4zi4 'Chinese character ' ; this is the same word as Japanese kanji.. 3 Throughout this paper we shall give Chinese examples in traditional orthography , followed . immediately by a Romanization into the pinyin transliteration scheme ; numerals following each pinyin syllable represent tones . Examples will usually be accompanied by a translation , plus a morpheme-by-morpheme gloss given in parentheses whenever the translation does not adequately serve this purpose . In the pinyin transliterations a dash ( - ) separates syllables that may be considered part of the same phonological word ; spaces are used to separate plausible phonological words ; and a plus sign ( + ) is used , where relevant , to indicate morpheme boundaries of interest . raphy : A ren2 'person ' is a fairly uncontroversial case of a monographemic word , and rplil zhong1guo2 ( middle country ) 'China ' a fairly uncontroversial case of a di graphernic word . The relevance of the distinction between , say , phonological words and , say , dictionary words is shown by an example like rpftl_A : ; ! : Hfllil zhong1hua2 ren2min2 gong4he2-guo2 ( China people republic ) 'People 's Republic of China . ' Arguably this consists of about three phonological words . On the other hand , in a translation system one probably wants to treat this string as a single dictionary word since it has a conventional and somewhat unpredictable translation into English . Thus , if one wants to segment words-for any purpose-from Chinese sentences , one faces a more difficult task than one does in English since one can not use spacing as a guide . For example , suppose one is building a ITS system for Mandarin Chinese . For that application , at a minimum , one would want to know the phonological word boundaries . Now , for this application one might be tempted to simply bypass the segmentation problem and pronounce the text character-by-character . However , there are several reasons why this approach will not in general work : 1 . Many hanzi have more than one pronunciation , where the correct . pronunciation depends upon word affiliation : tfJ is pronounced deO when it is a prenominal modification marker , but di4 in the word tfJ mu4di4 'goal ' ; fl ; is normally ganl 'dry , ' but qian2 in a person 's given name . including Third Tone Sandhi ( Shih 1986 ) , which changes a 3 ( low ) tone into a 2 ( rising ) tone before another 3 tone : ' j '' ; gil , xiao3 [ lao3 shu3 ] 'little rat , ' becomes xiao3 { lao2shu3 ] , rather than xiao2 { lao2shu3 ] , because the rule first applies within the word lao3shu3 'rat , ' blocking its phrasal application . In various dialects of Mandarin certain phonetic rules apply at the word . For example , in Northern dialects ( such as Beijing ) , a full tone ( 1 , 2 , 3 , or 4 ) is changed to a neutral tone ( 0 ) in the final syllable of many words : Jll donglgual 'winter melon ' is often pronounced donglguaO . The high 1 tone of J1l would not normally neutralize in this fashion if it were functioning as a word on its own . TIS systems in general need to do more than simply compute the . pronunciations of individual words ; they also need to compute intonational phrase boundaries in long utterances and assign relative prominence to words in those utterances . It has been shown for English ( Wang and Hirschberg 1992 ; Hirschberg 1993 ; Sproat 1994 , inter alia ) that grammatical part of speech provides useful information for these tasks . Given that part-of-speech labels are properties of words rather than morphemes , it follows that one can not do part-of-speech assignment without having access to word-boundary information . Making the reasonable assumption that similar information is relevant for solving these problems in Chinese , it follows that a prerequisite for intonation-boundary assignment and prominence assignment is word segmentation . The points enumerated above are particularly related to ITS , but analogous arguments can easily be given for other applications ; see for example Wu and Tseng 's ( 1993 ) discussion of the role of segmentation in information retrieval . There are thus some very good reasons why segmentation into words is an important task . A minimal requirement for building a Chinese word segmenter is obviously a dictionary ; furthermore , as has been argued persuasively by Fung and Wu ( 1994 ) , one will perform much better at segmenting text by using a dictionary constructed with text of the same genre as the text to be segmented . For novel texts , no lexicon that consists simply of a list of word entries will ever be entirely satisfactory , since the list will inevitably omit many constructions that should be considered words . Among these are words derived by various productive processes , including : 1 . Morphologically derived words such as , xue2shengl+men0 . ( student+plural ) 'students , ' which is derived by the affixation of the plural affix f , menD to the nounxue2shengl . Personal names such as 00 , 3R ; zhoulenl-lai2 'Zhou Enlai . ' can expect famous names like Zhou Enlai 's to be in many dictionaries , but names such as : fi lf ; f ; shi2jil-lin2 , the name of the second author of this paper , will not be found in any dictionary . Again , famous place names will most likely be found in the dictionary , but less well-known names , such as 1PM R ; bu4lang3-shi4wei2-ke4 'Brunswick ' ( as in the New Jersey town name 'New Brunswick ' ) will not generally be found . In this paper we present a stochastic finite-state model for segmenting Chinese text into words , both words found in a ( static ) lexicon as well as words derived via the above-mentioned productive processes . The segmenter handles the grouping of hanzi into words and outputs word pronunciations , with default pronunciations for hanzi it can not group ; we focus here primarily on the system 's ability to segment text appropriately ( rather than on its pronunciation abilities ) . The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers . It also incorporates the Good-Turing method ( Baayen 1989 ; Church and Gale 1991 ) in estimating the likelihoods of previously unseen con structions , including morphological derivatives and personal names . We will evaluate various specific aspects of the segmentation , as well as the overall segmentation per formance . This latter evaluation compares the performance of the system with that of several human judges since , as we shall show , even people do not agree on a single correct way to segment a text . Finally , this effort is part of a much larger program that we are undertaking to develop stochastic finite-state methods for text analysis with applications to TIS and other areas ; in the final section of this paper we will briefly discuss this larger program so as to situate the work discussed here in a broader context . A Brief Introduction to the Chinese Writing System Most readers will undoubtedly be at least somewhat familiar with the nature of the Chinese writing system , but there are enough common misunderstandings that it is as well to spend a few paragraphs on properties of the Chinese script that will be relevant to topics discussed in this paper . The first point we need to address is what type of linguistic object a hanzi repre sents . Much confusion has been sown about Chinese writing by the use of the term ideograph , suggesting that hanzi somehow directly represent ideas . The most accurate characterization of Chinese writing is that it is morphosyllabic ( DeFrancis 1984 ) : each hanzi represents one morpheme lexically and semantically , and one syllable phonologi cally . Thus in a two-hanzi word like lflli ? J zhong1guo2 ( middle country ) 'China ' there are two syllables , and at the same time two morphemes . Of course , since the number of attested ( phonemic ) Mandarin syllables ( roughly 1400 , including tonal distinctions ) is far smaller than the number of morphemes , it follows that a given syllable could in principle be written with any of several different hanzi , depending upon which morpheme is intended : the syllable zhongl could be lfl 'middle , ''clock , ''end , ' or , 'loyal . ' A morpheme , on the other hand , usually corresponds to a unique hanzi , though there are a few cases where variant forms are found . Finally , quite a few hanzi are homographs , meaning that they may be pronounced in several different ways , and in extreme cases apparently represent different morphemes : The prenominal modifi cation marker eg deO is presumably a different morpheme from the second morpheme of eg mu4di4 , even though they are written the same way.4 The second point , which will be relevant in the discussion of personal names in Section 4.4 , relates to the internal structure of hanzi . Following the system devised under the Qing emperor Kang Xi , hanzi have traditionally been classified according to a set of approximately 200 semantic radicals ; members of a radical class share a particular structural component , and often also share a common meaning ( hence the term 'semantic ' ) . For example , hanzi containing the INSECT radical ! R tend to denote insects and other crawling animals ; examples include tr wal 'frog , ' feng1 'wasp , ' and ! Itt she2 'snake . ' Similarly , hanzi sharing the GHOST radical _m tend to denote spirits and demons , such as _m gui3 'ghost ' itself , II : mo2 'demon , ' and yan3 'nightmare . ' While the semantic aspect of radicals is by no means completely predictive , the semantic homogeneity of many classes is quite striking : for example 254 out of the 263 examples ( 97 % ) of the INSECT class listed by Wieger ( 1965 , 77376 ) denote crawling or invertebrate animals ; similarly 21 out of the 22 examples ( 95 % ) of the GHOST class ( page 808 ) denote ghosts or spirits . As we shall argue , the semantic class affiliation of a hanzi constitutes useful information in predicting its properties . There is a sizable literature on Chinese word segmentation : recent reviews include Wang , Su , and Mo ( 1990 ) and Wu and Tseng ( 1993 ) . Roughly speaking , previous work can be divided into three categories , namely purely statistical approaches , purely lexi cal rule-based approaches , and approaches that combine lexical information with sta tistical information . The present proposal falls into the last group . Purely statistical approaches have not been very popular , and so far as we are aware earlier work by Sproat and Shih ( 1990 ) is the only published instance of such an approach . In that work , mutual information was used to decide whether to group adjacent hanzi into two-hanzi words . Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary . A related point is that mutual information is helpful in augmenting existing electronic dictionaries , ( cf . 4 To be sure , it is not always true that a hanzi represents a syllable or that it represents a morpheme . example , in Northern Mandarin dialects there is a morpheme -r that attaches mostly to nouns , and which is phonologically incorporated into the syllable to which it attaches : thus men2+r ( door+R ) 'door ' is realized as mer2 . This is orthographically represented as 7C . so that 'door ' would be and in this case the hanzi 7C , does not represent a syllable . Similarly , there is no compelling evidence that either of the syllables of f.ifflll binllang2 'betelnut ' represents a morpheme , since neither can occur in any context without the other : more likely fjfflll binllang2 is a disyllabic morpheme . ( See Sproat and Shih 1995 . ) However , the characterization given in the main body of the text is correct sufficiently often to be useful . Church and Hanks [ 1989 ] ) , and we have used lists of character pairs ranked by mutual information to expand our own dictionary . Nonstochastic lexical-knowledge-based approaches have been much more numer ous . Two issues distinguish the various proposals . The first concerns how to deal with ambiguities in segmentation . The second concerns the methods used ( if any ) to ex tend the lexicon beyond the static list of entries provided by the machine-readable dictionary upon which it is based . The most popular approach to dealing with seg mentation ambiguities is the maximum matching method , possibly augmented with further heuristics . This method , one instance of which we term the `` greedy algorithm '' in our evaluation of our own system in Section 5 , involves starting at the beginning ( or end ) of the sentence , finding the longest word starting ( ending ) at that point , and then repeating the process starting at the next ( previous ) hanzi until the end ( begin ning ) of the sentence is reached . Papers that use this method or minor variants thereof include Liang ( 1986 ) , Li et al . ( 1991 } , Gu and Mao ( 1994 ) , and Nie , Jin , and Hannan ( 1994 ) . The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it , since the method is guaranteed to produce only one segmentation . Methods that allow multiple segmentations must provide criteria for choosing the best segmentation . Some approaches depend upon some form of con straint satisfaction based on syntactic or semantic features ( e.g. , Yeh and Lee [ 1991 ] , which uses a unification-based approach ) . Others depend upon various lexical heuris tics : for example Chen and Liu ( 1992 ) attempt to balance the length of words in a three-word window , favoring segmentations that give approximately equal length for each word . Methods for expanding the dictionary include , of course , morphological rules , rules for segmenting personal names , as well as numeral sequences , expressions for dates , and so forth ( Chen and Liu 1992 ; Wang , Li , and Chang 1992 ; Chang and Chen 1993 ; Nie , Jin , and Hannan 1994 ) . Lexical-knowledge-based approaches that include statistical information generally presume that one starts with all possible segmentations of a sentence , and picks the best segmentation from the set of possible segmentations using a probabilistic or cost based scoring mechanism . Approaches differ in the algorithms used for scoring and selecting the best path , as well as in the amount of contextual information used in the scoring process . The simplest approach involves scoring the various analyses by costs based on word frequency , and picking the lowest cost path ; variants of this approach have been described in Chang , Chen , and Chen ( 1991 ) and Chang and Chen ( 1993 ) . More complex approaches such as the relaxation technique have been applied to this problem Fan and Tsai ( 1988 } . Note that Chang , Chen , and Chen ( 1991 ) , in addition to word-frequency information , include a constraint-satisfication model , so their method is really a hybrid approach . Several papers report the use of part-of-speech information to rank segmentations ( Lin , Chiang , and Su 1993 ; Peng and Chang 1993 ; Chang and Chen 1993 ) ; typically , the probability of a segmentation is multiplied by the probability of the tagging ( s ) for that segmentation to yield an estimate of the total probability for the analysis . Statistical methods seem particularly applicable to the problem of unknown-word identification , especially for constructions like names , where the linguistic constraints are minimal , and where one therefore wants to know not only that a particular se quence of hanzi might be a name , but that it is likely to be a name with some probabil ity . Several systems propose statistical methods for handling unknown words ( Chang et al . 1992 ; Lin , Chiang , and Su 1993 ; Peng and Chang 1993 ) . Some of these approaches ( e.g. , Lin , Chiang , and Su [ 1993 ] ) attempt to identify unknown words , but do not ac tually tag the words as belonging to one or another class of expression . This is not ideal for some applications , however . For instance , for TTS it is necessary to know that a particular sequence of hanzi is of a particular category because that knowl edge could affect the pronunciation ; consider , for example the issues surrounding the pronunciation of ganl I qian2 discussed in Section 1 . Following Sproat and Shih ( 1990 ) , performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recalP It is fairly standard to report precision and recall scores in the mid to high 90 % range . However , it is almost universally the case that no clear definition of what constitutes a `` correct '' segmentation is given , so these performance measures are hard to evaluate . Indeed , as we shall show in Section 5 , even human judges differ when presented with the task of segmenting a text into words , so a definition of the criteria used to determine that a given segmentation is correct is crucial before one can interpret such measures . In a few cases , the criteria for correctness are made more explicit . For example Chen and Liu ( 1992 ) report precision and recall rates of over 99 % , but this counts only the words that occur in the test corpus that also occur in their dictionary . Besides the lack of a clear definition of what constitutes a correct segmentation for a given Chinese sentence , there is the more general issue that the test corpora used in these evaluations differ from system to system , so meaningful comparison between systems is rendered even more difficult . The major problem for all segmentation systems remains the coverage afforded by the dictionary and the lexical rules used to augment the dictionary to deal with unseen words . The dictionary sizes reported in the literature range from 17,000 to 125,000 entries , and it seems reasonable to assume that the coverage of the base dictionary constitutes a major factor in the performance of the various approaches , possibly more important than the particular set of methods used in the segmentation . Furthermore , even the size of the dictionary per se is less important than the appropriateness of the lexicon to a particular test corpus : as Fung and Wu ( 1994 ) have shown , one can obtain substantially better segmentation by tailoring the lexicon to the corpus to be segmented . Chinese word segmentation can be viewed as a stochastic transduction problem . More formally , we start by representing the dictionary D as a Weighted Finite State Trans ducer ( WFST ) ( Pereira , Riley , and Sproat 1994 ) . Let H be the set of hanzi , p be the set of pinyin syllables with tone marks , and P be the set of grammatical part-of-speech labels . Then each arc of D maps either from an element of H to an element of p , or from E-i.e. , the empty string-to an element of P. More specifically , each word is represented in the dictionary as a sequence of arcs , starting from the initial state of D and labeled with an element 5 of Hxp , which is terminated with a weighted arc labeled with an element of Ex P. The weight represents the estimated cost ( negative log probability ) of the word . Next , we represent the input sentence as an unweighted finite-state acceptor ( FSA ) I over H. Let us assume the existence of a function Id , which takes as input an FSA A , and produces as output a transducer that maps all and only the strings of symbols accepted by A to themselves ( Kaplan and Kay 1994 ) . We can 5 Recall that precision is defined to be the number of correct hits divided by the total number of items . selected ; and that recall is defined to be the number of correct hits divided by the number of items that should have been selected . then define the best segmentation to be the cheapest or best path in Id ( I ) o D* ( i.e. , Id ( I ) composed with the transitive closure of 0 ) .6 Consider the abstract example illustrated in Figure 2 . In this example there are four `` input characters , '' A , B , C and D , and these map respectively to four `` pronunciations '' a , b , c and d. Furthermore , there are four `` words '' represented in the dictionary . These are shown , with their associated costs , as follows : ABj nc 4.0 AB C/jj 6.0 CD /vb 5 . 0 D/ nc 5.0 The minimal dictionary encoding this information is represented by the WFST in Figure 2 ( a ) . An input ABCD can be represented as an FSA as shown in Figure 2 ( b ) . This FSA I can be segmented into words by composing Id ( I ) with D* , to form the WFST shown in Figure 2 ( c ) , then selecting the best path through this WFST to produce the WFST in Figure 2 ( d ) . This WFST represents the segmentation of the text into the words AB and CD , word boundaries being marked by arcs mapping between f and part-of-speech labels . Since the segmentation corresponds to the sequence of words that has the lowest summed unigram cost , the segmenter under discussion here is a zeroth-order model . It is important to bear in mind , though , that this is not an inherent limitation of the model . For example , it is well-known that one can build a finite-state bigram ( word ) model by simply assigning a state Si to each word Wi in the vocabulary , and having ( word ) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si , the cost on aj is the bigram cost of WiWj- ( Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state . ) In Section 6 we dis cuss other issues relating to how higher-order language models could be incorporated into the model . As we have seen , the lexicon of basic words and stems is represented as a WFST ; most arcs in this WFST represent mappings between hanzi and pronunciations , and are costless . Each word is terminated by an arc that represents the transduction between f and the part of speech of that word , weighted with an estimated cost for that word . The cost is computed as follows , where N is the corpus size and f is the frequency : ( 1 ) Besides actual words from the base dictionary , the lexicon contains all hanzi in the Big 5 Chinese code/ with their pronunciation ( s ) , plus entries for other characters that can be found in Chinese text , such as Roman letters , numerals , and special symbols . Note that hanzi that are not grouped into dictionary words ( and are not identified as single hanzi words ) , or into one of the other categories of words discussed in this paper , are left unattached and tagged as unknown words . Other strategies could readily 6 As a reviewer has pointed out , it should be made clear that the function for computing the best path is . an instance of the Viterbi algorithm . 7 Big 5 is the most popular Chinese character coding standard in use in Taiwan and Hong Kong . based on the traditional character set rather than the simplified character set used in Singapore and Mainland China . ( a ) IDictionary D I D : d/0.000 B : b/0.000 B : b/0.000 ( b ) ( c ) ( d ) I B e s t P a t h ( I d ( I ) o D * ) I cps : nd4 . ! l ( l ( ) Figure 2 An abstract example illustrating the segmentation algorithm . The transitive closure of the dictionary in ( a ) is composed with Id ( input ) ( b ) to form the WFST ( c ) . The segmentation chosen is the best path through the WFST , shown in ( d ) . ( In this figure eps is c ) be implemented , though , such as a maximal-grouping strategy ( as suggested by one reviewer of this paper ) ; or a pairwise-grouping strategy , whereby long sequences of unattached hanzi are grouped into two-hanzi words ( which may have some prosodic motivation ) . We have not to date explored these various options . Word frequencies are estimated by a re-estimation procedure that involves apply ing the segmentation algorithm presented here to a corpus of 20 million words,8 using 8 Our training corpus was drawn from a larger corpus of mixed-genre text consisting mostly of . newspaper material , but also including kungfu fiction , Buddhist tracts , and scientific material . This larger corpus was kindly provided to us by United Informatics Inc. , R.O.C . a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used : in other words , derived words not in the base dictionary and personal and foreign names were not used . The best analysis of the corpus is taken to be the true analysis , the frequencies are re-estimated , and the algorithm is repeated until it converges . Clearly this is not the only way to estimate word-frequencies , however , and one could consider applying other methods : in partic ular since the problem is similar to the problem of assigning part-of-speech tags to an untagged corpus given a lexicon and some initial estimate of the a priori probabilities for the tags , one might consider a more sophisticated approach such as that described in Kupiec ( 1992 ) ; one could also use methods that depend on a small hand-tagged seed corpus , as suggested by one reviewer . In any event , to date , we have not compared different methods for deriving the set of initial frequency estimates . Note also that the costs currently used in the system are actually string costs , rather than word costs . This is because our corpus is not annotated , and hence does not distinguish between the various words represented by homographs , such as , which could be /adv jiangl 'be about to ' orInc jiang4 ' ( military ) general'-as in 1j\xiao3jiang4 'little general . ' In such cases we assign all of the estimated probability mass to the form with the most likely pronunciation ( determined by inspection ) , and assign a very small probability ( a very high cost , arbitrarily chosen to be 40 ) to all other variants . In the case of , the most common usage is as an adverb with the pronunciation jiangl , so that variant is assigned the estimated cost of 5.98 , and a high cost is assigned to nominal usage with the pronunciation jiang4 . The less favored reading may be selected in certain contexts , however ; in the case of , for example , the nominal reading jiang4 will be selected if there is morphological information , such as a following plural affix ir , menD that renders the nominal reading likely , as we shall see in Section 4.3 . Figure 3 shows a small fragment of the WFST encoding the dictionary , containing both entries forjust discussed , g : t zhonglhua2 min2guo2 ( China Republic ) 'Republic of China , ' and i inl . 4.2 A Sample Segmentation Using Only Dictionary Words Figure 4 shows two possible paths from the lattice of possible analyses of the input sentence B X : . : .S : P : l 'How do you say octopus in Japanese ? ' previously shown in Figure 1 . As noted , this sentence consists of four words , namely B X ri4wen2 'Japanese , ' : , zhanglyu2 'octopus/ : & P : l zen3me0 'how , ' and IDt shuol 'say . ' As indicated in Figure 1 ( c ) , apart from this correct analysis , there is also the analysis taking B ri4 as a word ( e.g. , a common abbreviation for Japan ) , along with X : wen2zhangl 'essay/ and f ! ! . Both of these analyses are shown in Figure 4 ; fortunately , the correct analysis is also the one with the lowest cost , so it is this analysis that is chosen . The method just described segments dictionary words , but as noted in Section 1 , there are several classes of words that should be handled that are not found in a standard dictionary . One class comprises words derived by productive morphologi cal processes , such as plural noun formation using the suffix ir , menD . ( Other classes handled by the current system are discussed in Section 5 . ) The morphological anal ysis itself can be handled using well-known techniques from finite-state morphol 9 The initial estimates are derived from the frequencies in the corpus of the strings of hanzi making up . each word in the lexicon whether or not each string is actually an instance of the word in question . : _ADV : 5.88 If : ! : zhong1 : 0.0 tjl : huo2 :0.0 ( R : spub : /ic of Ch : ina ) + . , _ , ... I : jlong4 :0.0 ( mUifaty genG181 ) 0 : _NC : 40.0 Figure 3 Partial Chinese Lexicon ( NC = noun ; NP = proper noun ) .c=- - I =- : il : . ; ss : ; zhangt '- : . `` \ ) `` o ' `` \ : J '\ ; . : : ... ... ... .0 6.51 9.51 : jj / JAPANESE OCTOPUS 10 28i : _nc HOW SAY f B : rl4 : il : : wen2 t '- : zhang ! : \ : yu2 e : _nc [ : : ! ! : zen3 l ! f : moO t : _adv il ! : shuot , :_vb i i i 1 10.03 13 ... 7.96 5.55 1 l ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... J.. Figure 4 Input lattice ( top ) and two segmentations ( bottom ) of the sentence 'How do you say octopus in Japanese ? ' . A non-optimal analysis is shown with dotted lines in the bottom frame . ogy ( Koskenniemi 1983 ; Antworth 1990 ; Tzoukermann and Liberman 1990 ; Karttunen , Kaplan , and Zaenen 1992 ; Sproat 1992 ) ; we represent the fact that ir , attaches to nouns by allowing t : -transitions from the final states of all noun entries , to the initial state of the sub-WFST representing f , . However , for our purposes it is not sufficient to repre sent the morphological decomposition of , say , plural nouns : we also need an estimate of the cost of the resulting word . For derived words that occur in our corpus we can estimate these costs as we would the costs for an underived dictionary entry . So , 1 : f , xue2shengl+men0 ( student+PL ) 'students ' occurs and we estimate its cost at 11.43 ; similarly we estimate the cost of f , jiang4+men0 ( general+PL ) 'generals ' ( as in ' J ' f , xiao3jiang4+men0 'little generals ' ) , at 15.02 . But we also need an estimate of the probability for a non-occurring though possible plural form like i JJ1l.f , nan2gua1-men0 'pumpkins . ' 10 Here we use the Good-Turing estimate ( Baayen 1989 ; Church and Gale 1991 ) , whereby the aggregate probability of previously unseen instances of a construction is estimated as ni/N , where N is the total number of observed tokens and n1 is the number of types observed only once . Let us notate the set of previously unseen , or novel , members of a category X as unseen ( X ) ; thus , novel members of the set of words derived in f , menO will be de noted unseen ( f , ) . For irt the Good-Turing estimate just discussed gives us an estimate of p ( unseen ( f , ) I f , ) -the probability of observing a previously unseen instance of a construction in ft given that we know that we have a construction in f , . This Good Turing estimate of p ( unseen ( f , ) If , ) can then be used in the normal way to define the probability of finding a novel instance of a construction in ir , in a text : p ( unseen ( f , ) ) = p ( unseen ( f , ) I f , ) p ( fn Here p ( ir , ) is just the probability of any construction in ft as estimated from the frequency of such constructions in the corpus . Finally , as suming a simple bigram backoff model , we can derive the probability estimate for the particular unseen word i 1J1l . irL as the product of the probability estimate for i JJ1l. , and the probability estimate just derived for unseen plurals in ir , : p ( i 1J1l.ir , ) p ( i 1J1l . ) p ( unseen ( f , ) ) . The cost estimate , cost ( i JJ1l.fn is computed in the obvious way by summing the negative log probabilities of i JJ1l . Figure 5 shows how this model is implemented as part of the dictionary WFST . There is a ( costless ) transition between the NC node and f , . The transition from f , to a final state transduces c to the grammatical tag PL with cost cost ( unseen ( f , ) ) : cost ( i JJ1l.ir , ) == cost ( i JJ1l . ) + cost ( unseen ( fm , as desired . For the seen word ir , 'gen erals , ' there is an c : NC transduction from to the node preceding ir , ; this arc has cost cost ( f , ) - cost ( unseen ( f , ) ) , so that the cost of the whole path is the desired cost ( f , ) . This representation gives ir , an appropriate morphological decomposition , pre serving information that would be lost by simply listing ir , as an unanalyzed form . Note that the backoff model assumes that there is a positive correlation between the frequency of a singular noun and its plural . An analysis of nouns that occur in both the singular and the plural in our database reveals that there is indeed a slight but significant positive correlation-R2 = 0.20 , p < 0.005 ; see Figure 6 . This suggests that the backoff model is as reasonable a model as we can use in the absence of further information about the expected cost of a plural form . 10 Chinese speakers may object to this form , since the suffix f , menD ( PL ) is usually restricted to . attaching to terms denoting human beings . However , it is possible to personify any noun , so in children 's stories or fables , i JJ1l . f , nan2gual+men0 'pumpkins ' is by no means impossible . J : j : l : zhongl :0.0 ; m , Jlong4 :0.0 ( mHHaryg9tltHBI ) : _ADV : 5.98 : hua2 : o.o E : _NC : 4.41 : mln2 : o.o mm : guo2 : 0.0 ( RopubllcofChlna ) ... .. , . 0 Figure 5 An example of affixation : the plural affix . Full Chinese personal names are in one respect simple : they are always of the form family+given . The family name set is restricted : there are a few hundred single-hanzi family names , and about ten double-hanzi ones . For a sequence of hanzi that is a possible name , we wish to assign a probability to that sequence qua name . We can model this probability straightforwardly enough with a probabilistic version of the grammar just given , which would assign probabilities to the individual rules . For example , given a sequence F1G1G2 , where F1 is a legal single-hanzi family name , and Plural Nouns X g 0 g `` ' X X 0 T ! i c '' ' . 0 X u } `` ' o ; .2 X X > < X X XX X X X X X X x X X X X X x X V X X X X . ; t'*- XXX : OX X X X X X X 9 x X X XX XX X X X X X X X XXX : < X X > O < XX > ! KXX XI < > < C X X XX : X : X X `` ' X X XX > OO < X > D < XIK X X X X X X -- XX : XXX X X C X X X ... C : XXX X Xll < X X > < XX > IIC : liiC : oiiiiCI -- 8 ! X : liiOC ! I ! S8K X X X 10 100 1000 10000 log ( F ) _base : R '' 2=0.20 ( p < 0.005 ) X 100000 Figure 6 Plot of log frequency of base noun , against log frequency of plural nouns . G1 and G2 are hanzi , we can estimate the probability of the sequence being a name as the product of : the probability that a word chosen randomly from a text will be a name-p ( rule 1 ) , and the probability that the name is of the form 1hanzi-family 2hanzi-given-p ( rule 2 ) , and the probability that the family name is the particular hanzi F1-p ( rule 6 ) , and the probability that the given name consists of the particular hanzi G1 and G2-p ( rule 9 ) This model is essentially the one proposed in Chang et al . The first probability is estimated from a name count in a text database , and the rest of the probabilities are estimated from a large list of personal names.n Note that in Chang et al . 's model the p ( rule 9 ) is estimated as the product of the probability of finding G 1 in the first position of a two-hanzi given name and the probability of finding G2 in the second position of a two-hanzi given name , and we use essentially the same estimate here , with some modifications as described later on . This model is easily incorporated into the segmenter by building a WFST restrict ing the names to the four licit types , with costs on the arcs for any particular name summing to an estimate of the cost of that name . This WFST is then summed with the WFST implementing the dictionary and morphological rules , and the transitive closure of the resulting transducer is computed ; see Pereira , Riley , and Sproat ( 1994 ) for an explanation of the notion of summing WFSTs.12 Conceptual Improvements over Chang et al . There are two weaknesses in Chang et al . 's model , which we improve upon . First , the model assumes independence between the first and second hanzi of a double given name . Yet , some hanzi are far more probable in women 's names than they are in men 's names , and there is a similar list of male-oriented hanzi : mixing hanzi from these two lists is generally less likely than would be predicted by the independence model . As a partial solution , for pairs of hanzi that co-occur sufficiently often in our namelists , we use the estimated bigram cost , rather than the independence-based cost . The second weakness is purely conceptual , and probably does not affect the per formance of the model . For previously unseen hanzi in given names , Chang et al . assign a uniform small cost ; but we know that some unseen hanzi are merely acci dentally missing , whereas others are missing for a reason-for example , because they have a bad connotation . As we have noted in Section 2 , the general semantic class to which a hanzi belongs is often predictable from its semantic radical . Not surprisingly some semantic classes are better for names than others : in our corpora , many names are picked from the GRASS class but very few from the SICKNESS class . Other good classes include JADE and GOLD ; other bad classes are DEATH and RAT . We can better predict the probability of an unseen hanzi occurring in a name by computing a within-class Good-Turing estimate for each radical class . Assuming unseen objects within each class are equiprobable , their probabilities are given by the Good-Turing theorem as : cis E ( n ' J.ls ) Po oc N * E ( N8ls ) ( 2 ) where p815 is the probability of one unseen hanzi in class cls , E ( n ' J.15 ) is the expected number of hanzi in cls seen once , N is the total number of hanzi , and E ( N ( / 5 ) is the expected number of unseen hanzi in class cls . The use of the Good-Turing equation presumes suitable estimates of the unknown expectations it requires . In the denomi 11 We have two such lists , one containing about 17,000 full names , and another containing frequencies of . hanzi in the various name positions , derived from a million names . 12 One class of full personal names that this characterization does not cover are married women 's names . where the husband 's family name is optionally prepended to the woman 's full name ; thus ; f : *lf # i xu3lin2-yan2hai3 would represent the name that Ms. Lin Yanhai would take if she married someone named Xu . This style of naming is never required and seems to be losing currency . It is formally straightforward to extend the grammar to include these names , though it does increase the likelihood of overgeneration and we are unaware of any working systems that incorporate this type of name . We of course also fail to identify , by the methods just described , given names used without their associated family name . This is in general very difficult , given the extremely free manner in which Chinese given names are formed , and given that in these cases we lack even a family name to give the model confidence that it is identifying a name . JA DE G O L D G R AS S SI C K NE SS DE AT H R A T 14 . 42 nator , the N31s can be measured well by counting , and we replace the expectation by the observation . In the numerator , however , the counts of ni1s are quite irregular , in cluding several zeros ( e.g. , RAT , none of whose members were seen ) . However , there is a strong relationship between ni1s and the number of hanzi in the class . For E ( ni1s ) , then , we substitute a smooth S against the number of class elements . This smooth guarantees that there are no zeroes estimated . The final estimating equation is then : ( 3 ) Since the total of all these class estimates was about 10 % off from the Turing estimate n1/N for the probability of all unseen hanzi , we renormalized the estimates so that they would sum to n 1jN . Note that the good classes JADE , GOLD and GRASS have lower costs than the bad classes SICKNESS , DEATH and RAT , as desired , so the trend observed for the results of this method is in the right direction . 4.5 Transliterations of Foreign Words . Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name . Since foreign names can be of any length , and since their original pronunciation is effectively unlimited , the identi fication of such names is tricky . Fortunately , there are only a few hundred hanzi that are particularly common in transliterations ; indeed , the commonest ones , such as E. bal , m er3 , and iij al are often clear indicators that a sequence of hanzi containing them is foreign : even a name like ! : i*m xia4mi3-er3 'Shamir , ' which is a legal Chi nese personal name , retains a foreign flavor because of liM . As a first step towards modeling transliterated names , we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary , and we estimate the probabil ity of occurrence of each hanzi in a transliteration ( pTN ( hanzi ; ) ) using the maximum likelihood estimate . As with personal names , we also derive an estimate from text of the probability of finding a transliterated name of any kind ( PTN ) . Finally , we model the probability of a new transliterated name as the product of PTN and PTN ( hanzi ; ) for each hanzi ; in the putative name.13 The foreign name model is implemented as an WFST , which is then summed with the WFST implementing the dictionary , morpho 13 The current model is too simplistic in several respects . For instance , the common `` suffixes , '' -nia ( e.g. , . Virginia ) and -sia are normally transliterated as fbSi ! ni2ya3 and @ 5:2 xilya3 , respectively . The interdependence between fb or 1/ ! i , and 5:2 is not captured by our model , but this could easily be remedied . logical rules , and personal names ; the transitive closure of the resulting machine is then computed . In this section we present a partial evaluation of the current system , in three parts . The first is an evaluation of the system 's ability to mimic humans at the task of segmenting text into word-sized units ; the second evaluates the proper-name identification ; the third measures the performance on morphological analysis . To date we have not done a separate evaluation of foreign-name recognition . Evaluation of the Segmentation as a Whole . Previous reports on Chinese segmentation have invariably cited performance either in terms of a single percent-correct score , or else a single precision-recall pair . The problem with these styles of evaluation is that , as we shall demonstrate , even human judges do not agree perfectly on how to segment a given text . Thus , rather than give a single evaluative score , we prefer to compare the performance of our method with the judgments of several human subjects . To this end , we picked 100 sentences at random containing 4,372 total hanzi from a test corpus.14 ( There were 487 marks of punctuation in the test sentences , including the sentence-final periods , meaning that the average inter-punctuation distance was about 9 hanzi . ) We asked six native speakers-three from Taiwan ( TlT3 ) , and three from the Mainland ( M1M3 ) -to segment the corpus . Since we could not bias the subjects towards a particular segmentation and did not presume linguistic sophistication on their part , the instructions were simple : subjects were to mark all places they might plausibly pause if they were reading the text aloud . An examination of the subjects ' bracketings confirmed that these instructions were satisfactory in yielding plausible word-sized units . ( See also Wu and Fung [ 1994 ] . ) Various segmentation approaches were then compared with human performance : 1 . A greedy algorithm ( or maximum-matching algorithm ) , GR : proceed through the sentence , taking the longest match with a dictionary entry at each point . An anti-greedy algorithm , AG : instead of the longest match , take the . shortest match at each point . The method being described-henceforth ST.. Two measures that can be used to compare judgments are : 1 . For each pair of judges consider one judge as the standard , . computing the precision of the other 's judgments relative to this standard . For each pair of judges , consider one judge as the standard , . computing the recall of the other 's judgments relative to this standard . Clearly , for judges h and h taking h as standard and computing the precision and recall for Jz yields the same results as taking h as the standard , and computing for h , 14 All evaluation materials , with the exception of those used for evaluating personal names were drawn . from the subset of the United Informatics corpus not used in the training of the models . Jud ges A G G R ST M 1 M 2 M 3 T1 T2 T3 AG 0.7 0 0.7 0 0 . 4 3 0.4 2 0.6 0 0.6 0 0.6 2 0.5 9 GR 0.9 9 0 . 6 2 0.6 4 0.7 9 0.8 2 0.8 1 0.7 2 ST 0 . 6 4 0.6 7 0.8 0 0.8 4 0.8 2 0.7 4 M1 0.7 7 0.6 9 0.7 1 0.6 9 0.7 0 M2 0.7 2 0.7 3 0.7 1 0.7 0 M3 0.8 9 0.8 7 0.8 0 T1 0.8 8 0.8 2 T2 0.7 8 respectively , the recall and precision . We therefore used the arithmetic mean of each interjudge precision-recall pair as a single measure of interjudge similarity . The average agreement among the human judges is .76 , and the average agreement between ST and the humans is .75 , or about 99 % of the interhuman agreement.15 One can better visualize the precision-recall similarity matrix by producing from that matrix a distance matrix , computing a classical metric multidimensional scaling ( Torgerson 1958 ; Becker , Chambers , Wilks 1988 ) on that dis tance matrix , and plotting the first two most significant dimensions . The result of this is shown in Figure 7 . The horizontal axis in this plot represents the most significant dimension , which explains 62 % of the variation . In addition to the automatic methods , AG , GR , and ST , just discussed , we also added to the plot the values for the current algorithm using only dictionary entries ( i.e. , no productively derived words or names ) . This is to allow for fair comparison between the statistical method and GR , which is also purely dictionary-based . As can be seen , GR and this `` pared-down '' statistical method perform quite similarly , though the statistical method is still slightly better.16 AG clearly performs much less like humans than these methods , whereas the full statistical algorithm , including morphological derivatives and names , performs most closely to humans among the automatic methods . It can also be seen clearly in this plot that two of the Taiwan speakers cluster very closely together , and the third Tai wan speaker is also close in the most significant dimension ( the x axis ) . Two of the Mainlanders also cluster close together but , interestingly , not particularly close to the Taiwan speakers ; the third Mainlander is much more similar to the Taiwan speakers . Clearly the percentage of productively formed words is quite small ( for this particular corpus ) , meaning that dictionary entries are covering most of the 15 GR is .73 or 96 % .. 16 As one reviewer points out , one problem with the unigram model chosen here is that there is still a. tendency to pick a segmentation containing fewer words . That is , given a choice between segmenting a sequence abc into abc and ab , c , the former will always be picked so long as its cost does not exceed the summed costs of ab and c : while ; it is possible for abc to be so costly as to preclude the larger grouping , this will certainly not usually be the case . In this way , the method reported on here will necessarily be similar to a greedy method , though of course not identical . As the reviewer also points out , this is a problem that is shared by , e.g. , probabilistic context-free parsers , which tend to pick trees with fewer nodes . The question is how to normalize the probabilities in such a way that smaller groupings have a better shot at winning . This is an issue that we have not addressed at the current stage of our research . i..f , .. '' c ' 0 + 0 `` 0 ' + a n t i g r e e d y x g r e e d y < > c u r r e n t m e t h o d o d i e t . o n l y Taiwan 0 ; ; 0 c CD E i5 0 '' ' 9 9 Mainland -0.30.20.1 0.0 0.1 0.2 Dimension 1 ( 62 % ) Figure 7 Classical metric multidimensional scaling of distance matrix , showing the two most significant dimensions . The percentage scores on the axis labels represent the amount of variation in the data explained by the dimension in question . Word type N % Dic tion ary entr ies 2 , 5 4 3 9 7 . 4 7 Mor pho logi call y deri ved wor ds 3 0 . 1 1 Fore ign tran slite rati ons 9 0 . 3 4 Per son al na mes 5 4 2 . Nonetheless , the results of the comparison with human judges demonstrates that there is mileage being gained by incorporating models of these types of words . It may seem surprising to some readers that the interhuman agreement scores reported here are so low . However , this result is consistent with the results of ex periments discussed in Wu and Fung ( 1994 ) . Wu and Fung introduce an evaluation method they call nk-blind . Under this scheme , n human judges are asked independently to segment a text . Their results are then compared with the results of an automatic segmenter . For a given `` word '' in the automatic segmentation , if at least k of the hu man judges agree that this is a word , then that word is considered to be correct . For eight judges , ranging k between 1 and 8 corresponded to a precision score range of 90 % to 30 % , meaning that there were relatively few words ( 30 % of those found by the automatic segmenter ) on which all judges agreed , whereas most of the words found by the segmenter were such that one human judge agreed . To evaluate proper-name identification , we randomly se lected 186 sentences containing 12,000 hanzi from our test corpus and segmented the text automatically , tagging personal names ; note that for names , there is always a sin gle unambiguous answer , unlike the more general question of which segmentation is correct . The performance was 80.99 % recall and 61.83 % precision . Interestingly , Chang et al . report 80.67 % recall and 91.87 % precision on an 11,000 word corpus : seemingly , our system finds as many names as their system , but with four times as many false hits . However , we have reason to doubt Chang et al . Without using the same test corpus , direct comparison is obviously difficult ; fortunately , Chang et al . include a list of about 60 sentence fragments that exemplify various categories of performance for their system . The performance of our system on those sentences ap peared rather better than theirs . On a set of 11 sentence fragments-the A set-where they reported 100 % recall and precision for name identification , we had 73 % recall and 80 % precision . However , they list two sets , one consisting of 28 fragments and the other of 22 fragments , in which they had 0 % recall and precision . On the first of these-the B set-our system had 64 % recall and 86 % precision ; on the second-the C set-it had 33 % recall and 19 % precision . Note that it is in precision that our over all performance would appear to be poorer than the reported performance of Chang et al. , yet based on their published examples , our system appears to be doing better precisionwise . Thus we have some confidence that our own performance is at least as good as that of Chang et al . In a more recent study than Chang et al. , Wang , Li , and Chang ( 1992 ) propose a surname-driven , non-stochastic , rule-based system for identifying personal names.17 Wang , Li , and Chang also compare their performance with Chang et al . Fortunately , we were able to obtain a copy of the full set of sentences from Chang et al . on which Wang , Li , and Chang tested their system , along with the output of their system.18 In what follows we will discuss all cases from this set where our performance on names differs from that of Wang , Li , and Chang . In these examples , the names identified by the two systems ( if any ) are underlined ; the sentence with the correct segmentation is boxed.19 The differences in performance between the two systems relate directly to three issues , which can be seen as differences in the tuning of the models , rather than repre senting differences in the capabilities of the model per se . The first issue relates to the completeness of the base lexicon . The Wang , Li , and Chang system fails on fragment ( b ) because their system lacks the word youlyoul 'soberly ' and misinterpreted the thus isolated first youl as being the final hanzi of the preceding name ; similarly our system failed in fragment ( h ) since it is missing the abbreviation i : lJI ! This is a rather important source of errors in name identifi cation , and it is not really possible to objectively evaluate a name recognition system without considering the main lexicon with which it is used . 17 They also provide a set of title-driven rules to identify names when they occur before titles such as $ t . 1 : xianlshengl 'Mr . ' or i : l : itr ! J tai2bei3 shi4zhang3 'Taipei Mayor . ' Obviously , the presence of a title after a potential name N increases the probability that N is in fact a name . Our system does not currently make use of titles , but it would be straightforward to do so within the finite-state framework that we propose . 18 We are grateful to ChaoHuang Chang for providing us with this set . Note that Wang , Li , and Chang 's . set was based on an earlier version of the Chang et a ! . paper , and is missing 6 examples from the A set . 19 We note that it is not always clear in Wang , Li , and Chang 's examples which segmented words . constitute names , since we have only their segmentation , not the actual classification of the segmented words . Therefore in cases where the segmentation is identical between the two systems we assume that tagging is also identical . Our System Wang , Li , and Chang a . For example , the Wang , Li , and Chang system fails on the sequence 1 : f : p : ] nian2 nei4 sa3 in ( k ) since 1F nian2 is a possible , but rare , family name , which also happens to be written the same as the very common word meaning 'year . ' Our system fails in ( a ) because of $ shenl , a rare family name ; the system identifies it as a family name , whereas it should be analyzed as part of the given name . Finally , the statistical method fails to correctly group hanzi in cases where the individual hanzi comprising the name are listed in the dictionary as being relatively high-frequency single-hanzi words . An example is in ( i ) , where the system fails to group t ; , f ; ? `` $ ? t ! : lin2yang2gang3 as a name , because all three hanzi can in principle be separate words ( t ; , f ; lin2 'wood ' ; ? `` $ yang2 'ocean ' ; ? t ! ; gang3 'harbor ' ) . In many cases these failures in recall would be fixed by having better estimates of the actual prob abilities of single-hanzi words , since our estimates are often inflated . A totally non stochastic rule-based system such as Wang , Li , and Chang 's will generally succeed in such cases , but of course runs the risk of overgeneration wherever the single-hanzi word is really intended . The first four affixes are so-called resultative affixes : they denote some prop erty of the resultant state of a verb , as in E7 wang4bu4-liao3 ( forget-not-attain ) ' can not forget . ' The last affix in the list is the nominal plural f , men0.20 In the are the ( typical ) classes of words to which the affix attaches , the number found in the test corpus by the method , the number correct ( with a precision measure ) , and the number missed ( with a recall measure ) . In this paper we have argued that Chinese word segmentation can be modeled ef fectively using weighted finite-state transducers . This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives , and models for personal names and foreign names in transliteration . Other kinds of productive word classes , such as company names , abbreviations ( termed fijsuolxie3 in Mandarin ) , and place names can easily be 20 Note that 7 in E 7 is normally pronounced as leO , but as part of a resultative it is liao3.. handled given appropriate models . ( For some recent corpus-based work on Chinese abbreviations , see Huang , Ahrens , and Chen [ 1993 ] . ) We have argued that the proposed method performs well . However , some caveats are in order in comparing this method ( or any method ) with other approaches to seg mentation reported in the literature . First of all , most previous articles report perfor mance in terms of a single percent-correct score , or else in terms of the paired measures of precision and recall . What both of these approaches presume is that there is a sin gle correct segmentation for a sentence , against which an automatic algorithm can be compared . We have shown that , at least given independent human judgments , this is not the case , and that therefore such simplistic measures should be mistrusted . This is not to say that a set of standards by which a particular segmentation would count as correct and another incorrect could not be devised ; indeed , such standards have been proposed and include the published PRCNSC ( 1994 ) and ROCLING ( 1993 ) , as well as the unpublished Linguistic Data Consortium standards ( ca . However , until such standards are universally adopted in evaluating Chinese segmenters , claims about performance in terms of simple measures like percent correct should be taken with a grain of salt ; see , again , Wu and Fung ( 1994 ) for further arguments supporting this conclusion . Second , comparisons of different methods are not meaningful unless one can eval uate them on the same corpus . Unfortunately , there is no standard corpus of Chinese texts , tagged with either single or multiple human judgments , with which one can compare performance of various methods . One hopes that such a corpus will be forth coming . Finally , we wish to reiterate an important point . The major problem for our seg menter , as for all segmenters , remains the problem of unknown words ( see Fung and Wu [ 1994 ] ) . We have provided methods for handling certain classes of unknown words , and models for other classes could be provided , as we have noted . However , there will remain a large number of words that are not readily adduced to any produc tive pattern and that would simply have to be added to the dictionary . This implies , therefore , that a major factor in the performance of a Chinese segmenter is the quality of the base dictionary , and this is probably a more important factor-from the point of view of performance alone-than the particular computational methods used . The method reported in this paper makes use solely of unigram probabilities , and is therefore a zeroeth-order model : the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation . However , as we have noted , nothing inherent in the approach precludes incorporating higher-order constraints , provided they can be effectively modeled within a finite-state framework . For example , as Gan ( 1994 ) has noted , one can construct examples where the segmen tation is locally ambiguous but can be determined on the basis of sentential or even discourse context . Two sets of examples from Gan are given in ( 1 ) and ( 2 ) ( : : : : : : Gan 's Appendix B , exx . lla/llb and 14a/14b respectively ) . In ( 1 ) the sequencema3lu4 can not be resolved locally , but depends instead upon broader context ; similarly in ( 2 ) , the sequence : : : tcai2neng2 can not be resolved locally : 1 . ; m t 7 leO z h e 4 pil m a 3 lu 4 sh an g4 bi ng 4 t h i s CL ( assi fier ) horse w ay on sic k A SP ( ec t ) 'This horse got sick on the way ' ( b ) 1 : . til y zhe4 tiao2 ma3lu4 hen3 shao3 this CL road very few 'Very few cars pass by this road ' : $ chel jinglguo4 car pass by 2 . ( a ) I f f fi * fi :1 } ' l ij 1 : { 1M m m s h e n 3 m e 0 shi2 ho u4 wo 3 cai2 ne ng 2 ke4 fu 2 zh e4 ge 4 ku n4 w h a t ti m e I just be abl e ov er co m e thi s C L dif fic 'When will I be able to overcome this difficulty ? ' ( b ) 89 :1 t & tal de cai2neng2 hen3 he DE talent very 'He has great talent ' f.b ga ol hig h While the current algorithm correctly handles the ( b ) sentences , it fails to handle the ( a ) sentences , since it does not have enough information to know not to group the sequences.ma3lu4 and ? Gan 's solution depends upon a fairly sophisticated language model that attempts to find valid syntactic , semantic , and lexical relations between objects of various linguistic types ( hanzi , words , phrases ) . An example of a fairly low-level relation is the affix relation , which holds between a stem morpheme and an affix morpheme , such as f1 -menD ( PL ) . A high-level relation is agent , which relates an animate nominal to a predicate . Particular instances of relations are associated with goodness scores . Particular relations are also consistent with particular hypotheses about the segmentation of a given sentence , and the scores for particular relations can be incremented or decremented depending upon whether the segmentations with which they are consistent are `` popular '' or not . While Gan 's system incorporates fairly sophisticated models of various linguistic information , it has the drawback that it has only been tested with a very small lexicon ( a few hundred words ) and on a very small test set ( thirty sentences ) ; there is therefore serious concern as to whether the methods that he discusses are scalable . Another question that remains unanswered is to what extent the linguistic information he considers can be handled-or at least approximated-by finite-state language models , and therefore could be directly interfaced with the segmentation model that we have presented in this paper . For the examples given in ( 1 ) and ( 2 ) this certainly seems possible . Consider first the examples in ( 2 ) . The segmenter will give both analyses :1 cai2 neng2 'just be able , ' and ? ] cai2neng2 'talent , ' but the latter analysis is preferred since splitting these two morphemes is generally more costly than grouping them . In ( 2a ) , we want to split the two morphemes since the correct analysis is that we have the adverb :1 cai2 'just , ' the modal verb neng2 'be able ' and the main verb R : Hke4fu2 'overcome ' ; the competing analysis is , of course , that we have the noun :1 cai2neng2 'talent , ' followed by } 'lijke4fu2 'overcome . ' Clearly it is possible to write a rule that states that if an analysis Modal+ Verb is available , then that is to be preferred over Noun+ Verb : such a rule could be stated in terms of ( finite-state ) local grammars in the sense of Mohri ( 1993 ) . Turning now to ( 1 ) , we have the similar problem that splitting.into.ma3 'horse ' andlu4 'way ' is more costly than retaining this as one word .ma3lu4 'road . ' However , there is again local grammatical information that should favor the split in the case of ( 1a ) : both .ma3 'horse ' and .ma3 lu4 are nouns , but only .ma3 is consistent with the classifier pil , the classifier for horses.21 By a similar argument , the preference for not splitting , lm could be strengthened in ( lb ) by the observation that the classifier 1 ' 1* tiao2 is consistent with long or winding objects like , lm ma3lu4 'road ' but not with , ma3 'horse . ' Note that the sets of possible classifiers for a given noun can easily be encoded on that noun by grammatical features , which can be referred to by finite-state grammatical rules . Thus , we feel fairly confident that for the examples we have considered from Gan 's study a solution can be incorporated , or at least approximated , within a finite-state framework . With regard to purely morphological phenomena , certain processes are not han dled elegantly within the current framework Any process involving reduplication , for instance , does not lend itself to modeling by finite-state techniques , since there is no way that finite-state networks can directly implement the copying operations required . Mandarin exhibits several such processes , including A-not-A question formation , il lustrated in ( 3a ) , and adverbial reduplication , illustrated in ( 3b ) : 3 . ( a ) ; IE shi4 'be ' = > ; IE ; IE shi4bu2-shi4 ( be-not-be ) 'is it ? ' gaolxing4 'happy ' = > F.i'JF.i ' J Jl ! gaolbu4-gaolxing4 ( hap-not-happy ) 'happy ? ' gaolxing4 'happy'= > F.i'JF.i'JJI ! JI ! gaolgaolxing4xing4 'happily ' In the particular form of A-not-A reduplication illustrated in ( 3a ) , the first syllable of the verb is copied , and the negative markerbu4 'not ' is inserted between the copy and the full verb . In the case of adverbial reduplication illustrated in ( 3b ) an adjective of the form AB is reduplicated as AABB . The only way to handle such phenomena within the framework described here is simply to expand out the reduplicated forms beforehand , and incorporate the expanded forms into the lexical transducer . Despite these limitations , a purely finite-state approach to Chinese word segmentation enjoys a number of strong advantages . The model we use provides a simple framework in which to incorporate a wide variety of lexical information in a uniform way . The use of weighted transducers in particular has the attractive property that the model , as it stands , can be straightforwardly interfaced to other modules of a larger speech or natural language system : presumably one does not want to segment Chinese text for its own sake but instead with a larger purpose in mind . As described in Sproat ( 1995 ) , the Chinese segmenter presented here fits directly into the context of a broader finite-state model of text analysis for speech synthesis . Furthermore , by inverting the transducer so that it maps from phonemic transcriptions to hanzi sequences , one can apply the segmenter to other problems , such as speech recognition ( Pereira , Riley , and Sproat 1994 ) . Since the transducers are built from human-readable descriptions using a lexical toolkit ( Sproat 1995 ) , the system is easily maintained and extended . While size of the resulting transducers may seem daunting-the segmenter described here , as it is used in the Bell Labs Mandarin TTS system has about 32,000 states and 209,000 arcs-recent work on minimization of weighted machines and transducers ( cf . 21 In Chinese , numerals and demonstratives can not modify nouns directly , and must be accompanied by . The particular classifier used depends upon the noun . Mohri [ 1995 ] ) shows promise for improving this situation . The model described here thus demonstrates great potential for use in widespread applications . This flexibility , along with the simplicity of implementation and expansion , makes this framework an attractive base for continued research . We thank United Informatics for providing us with our corpus of Chinese text , and BDC for the 'Behavior ChineseEnglish Electronic Dictionary . ' We further thank Dr. J.-S. Chang of Tsinghua University , Taiwan , R.O.C. , for kindly providing us with the name corpora . We also thank ChaoHuang Chang , reviewers for the 1994 ACL conference , and four anonymous reviewers for Computational Linguistics for useful comments . Foma : a finite-state compiler and library Foma is a compiler , programming language , and C library for constructing finite-state automata and transducers for various uses . It has specific support for many natural language processing applications such as producing morphological and phonological analyzers . Foma is largely compatible with the Xerox/PARC finite-state toolkit . It also embraces Unicode fully and supports various different formats for specifying regular expressions : the Xerox/PARC format , a Perl-like format , and a mathematical format that takes advantage of the ‘ Mathematical Operators ’ Unicode block . Foma is a finite-state compiler , programming language , and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research , constructing lexical analyzers for programming languages , and building morphological/phonological analyzers , as well as spellchecking applications . The compiler allows users to specify finite-state automata and transducers incrementally in a similar fashion to AT & T ’ s fsm ( Mohri et al. , 1997 ) and Lextools ( Sproat , 2003 ) , the Xerox/PARC finite- state toolkit ( Beesley and Karttunen , 2003 ) and the SFST toolkit ( Schmid , 2005 ) . One of Foma ’ s design goals has been compatibility with the Xerox/PARC toolkit . Another goal has been to allow for the ability to work with n-tape automata and a formalism for expressing first-order logical constraints over regular languages and n-tape- transductions . Foma is licensed under the GNU general public license : in keeping with traditions of free software , the distribution that includes the source code comes with a user manual and a library of examples . The compiler and library are implemented in C and an API is available . The API is in many ways similar to the standard C library < regex.h > , and has similar calling conventions . However , all the low-level functions that operate directly on automata/transducers are also available ( some 50+ functions ) , including regular expression primitives and extended functions as well as automata deter- minization and minimization algorithms . These may be useful for someone wanting to build a separate GUI or interface using just the existing low- level functions . The API also contains , mainly for spell-checking purposes , functionality for finding words that match most closely ( but not exactly ) a path in an automaton . This makes it straightforward to build spell-checkers from morphological transducers by simply extracting the range of the transduction and matching words approximately . Unicode ( UTF8 ) is fully supported and is in fact the only encoding accepted by Foma . It has been successfully compiled on Linux , Mac OS X , and Win32 operating systems , and is likely to be portable to other systems without much effort . Retaining backwards compatibility with Xerox/PARC and at the same time extending the formalism means that one is often able to construct finite-state networks in equivalent various ways , either through ASCII-based operators or through the Unicode-based extensions . For example , one can either say : ContainsX = Σ* X Σ* ; MyWords = { cat } | { dog } | { mouse } ; MyRule = n - > m || p ; ShortWords = [ MyLex1 ] 1 ∩ Σˆ < 6 ; or : Proceedings of the EACL 2009 Demonstrations Session , pages 29–32 , Athens , Greece , 3 April 2009 . Qc 2009 Association for Computational Linguistics Operators Compatibility variant Function [ ] ( ) [ ] ( ) grouping parentheses , optionality ∀ ∃ N/A quantifiers \ ‘ term negation , substitution/homomorphism : : cross-product + ∗ + ∗ Kleene closures ˆ < n ˆ > n ˆ { m , n } ˆ < n ˆ > n ˆ { m , n } iterations 1 2 .1 .2 .u .l domain & range .f N/A eliminate all unification flags $ $ . complement , containment operators / ./ . N/A N/A ‘ ignores ’ , left quotient , right quotient , ‘ inside ’ quotient ∈ ∈/ = /= N/A language membership , position equivalence ≺ < > precedes , follows ∨ ∪ ∧ ∩ - .P . union , intersection , set minus , priority unions = > - > ( - > ) @ - > = > - > ( - > ) @ - > context restriction , replacement rules < > shuffle ( asynchronous product ) × ◦ .x . cross-product , composition Table 1 : Horizontal lines separate precedence classes . * ; define MyWords { cat } | { dog } | { mouse } ; define MyRule n - > m || _ p ; define ShortWords Mylex.i.l & ? ˆ < 6 ; One such extension is the ability to use of a form of first-order logic to make existential statements over languages and transductions ( Hulden , 2008 ) . For instance , suppose we have defined an arbitrary regular language L , and want to further define a language that contains only one factor of L , we can do so by : OneL = ( ∃x ) ( x ∈ L ∧ ( ∃y ) ( y ∈ L ∧ ( x = y ) ) ) ; Here , quantifiers apply to substrings , and we attribute the usual meaning to ∈ and ∧ , and a kind of concatenative meaning to the predicate S ( t1 , t2 ) . Hence , in the above example , OneL defines the language where there exists a string x such that x is a member of the language L and there does not exist a string y , also in L , such that y would occur in a different position than x . This kind of logical specification of regular languages can be very useful for building some languages that would be quite cumbersome to express with other regular expression operators . In fact , many of the internally complex operations of Foma are built through a reduction to this type of logical expressions . As mentioned , Foma supports reading and writing of the LEXC file format , where morphological categories are divided into so-called continuation classes . This practice stems back from the earliest two-level compilers ( Karttunen et al. , 1987 ) . Below is a simple example of the format : Multichar_Symbols +Pl +Sing LEXICON Root Nouns ; LEXICON Nouns cat Plural ; church Plural ; LEXICON Plural +Pl : % ˆs # ; +Sing # ; The Foma API gives access to basic functions , such as constructing a finite-state machine from a regular expression provided as a string , performing a transduction , and exhaustively matching against a given string starting from every position . The following basic snippet illustrates how to use the C API instead of the main interface of Foma to construct a finite-state machine encoding the language a+b+ and check whether a string matches it : 1. void check_word ( char *s ) { 2. fsm_t *network ; 3. fsm_match_result *result ; 4 . 5. network = fsm_regex ( `` a+ b+ '' ) ; 6. result = fsm_match ( fsm , s ) ; 7. if ( result- > num_matches > 0 ) 8. printf ( `` Regex matches '' ) ; 9 . 10 } Here , instead of calling the fsm regex ( ) function to construct the machine from a regular expressions , we could instead have accessed the beforementioned low-level routines and built the network entirely without regular expressions by combining low-level primitives , as follows , replacing line 5 in the above : network = fsm_concat ( fsm_kleene_plus ( fsm_symbol ( `` a '' ) ) , fsm_kleene_plus ( fsm_symbol ( `` b '' ) ) ) ; The API is currently under active development and future functionality is likely to include conversion of networks to 8-bit letter transducers/automata for maximum speed in regular expression matching and transduction . educational use Foma has support for visualization of the machines it builds through the AT & T Graphviz library . For educational purposes and to illustrate automata construction methods , there is some support for changing the behavior of the algorithms . For instance , by default , for efficiency reasons , Foma determinizes and minimizes automata between nearly every incremental operation . Operations such as unions of automata are also constructed by default with the product construction method that directly produces deterministic automata . However , this on-the-fly minimization and determinization can be relaxed , and a Thompson construction method chosen in the interface so that automata remain non-deterministic and non- minimized whenever possible—non-deterministic automata naturally being easier to inspect and analyze . Though the main concern with Foma has not been that of efficiency , but of compatibility and extendibility , from a usefulness perspective it is important to avoid bottlenecks in the underlying algorithms that can cause compilation times to skyrocket , especially when constructing and combining large lexical transducers . With this in mind , some care has been taken to attempt to optimize the underlying primitive algorithms . One the whole , Foma seems to perform particularly well with pathological cases that involve exponential growth in the number of states when determinizing non- deterministic machines . For general usage patterns , this advantage is not quite as dramatic , and for average use Foma seems to perform comparably with e.g . the Xerox/PARC toolkit , perhaps with the exception of certain types of very large lexicon descriptions ( > 100,000 words ) . The Foma project is multipurpose multi-mode finite-state compiler geared toward practical construction of large-scale finite-state machines such as may be needed in natural language processing as well as providing a framework for research in finite-state automata . Several wide- coverage morphological analyzers specified in the LEXC/xfst format have been compiled successfully with Foma . Foma is free software and will remain under the GNU General Public License . As the source code is available , collaboration is encouraged . GNU AT & T Foma xfst flex fsm 4 Σ∗aΣ15 0.216s 16.23s 17.17s 1.884s Σ∗aΣ20 8.605s nf nf 153.7s North Sami 14.23s 4.264s N/A N/A 8queens 0.188s 1.200s N/A N/A sudoku2x3 5.040s 5.232s N/A N/A lexicon.lex 1.224s 1.428s N/A N/A 3sat30 0.572s 0.648s N/A N/A Table 2 : The first and second entries are short regular expressions that exhibit exponential behavior . The second results in a FSM with 221 states and 222 arcs . The others are scripts that can be run on both Xerox/PARC and Foma . The file lexicon.lex is a LEXC format English dictionary with 38418 entries . North Sami is a large lexicon ( lexc file ) for the North Sami language available from http : //divvun.no . Discovering Corpus-Specific Word Senses This paper presents an unsupervised algorithm which automatically discovers word senses from text . The algorithm is based on a graph model representing words and relationships between them . Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word . Discrimination against previously extracted sense clusters enables us to discover new senses . We use the same data for both recognising and resolving ambiguity . This paper describes an algorithm which automatically discovers word senses from free text and maps them to the appropriate entries of existing dictionaries or taxonomies . Automatic word sense discovery has applications of many kinds . It can greatly facilitate a lexicographer 's work and can be used to automatically construct corpus-based taxonomies or to tune existing ones . The same corpus evidence which supports a clustering of an ambiguous word into distinct senses can be used to decide which sense is referred to in a given context ( Schiitze , 1998 ) . This paper is organised as follows . In section 2 , we present the graph model from which we discover word senses . Section 3 describes the way we divide graphs surrounding ambiguous words into different areas corresponding to different senses , using Markov clustering ( van Dongen , 2000 ) . The quality of the Markov clustering depends strongly on several parameters such as a granularity factor and the size of the local graph . In section 4 , we outline a word sense discovery algorithm which bypasses the problem of parameter tuning . We conducted a pilot experiment to examine the performance of our algorithm on a set of words with varying degree of ambiguity . Section 5 describes the experiment and presents a sample of the results . Finally , section 6 sketches applications of the algorithm and discusses future work . The model from which we discover distinct word senses is built automatically from the British National corpus , which is tagged for parts of speech . Based on the intuition that nouns which co-occur in a list are often semantically related , we extract contexts of the form Noun , Noun , ... and/or Noun , e.g . `` genomic DNA from rat , mouse and dog '' . Following the method in ( Widdows and Dorow , 2002 ) , we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1 . Following Lin 's work ( 1998 ) , we are currently investigating a graph with verb-object , verb-subject and modifier-noun-collocations from which it is possible to infer more about the senses of systematically polysemous words . The word sense clustering algorithm as outlined below can be applied to any kind of similarity measure based on any set of features . 1 Si mple cutoff functions proved unsatisfactory because of the bias they give to more frequent words . Instead we link each word to its top n neighbors where n can be determined by the user ( cf . section 4 ) .. 41=0 441=P .4161. sz44 , CD miltrA , litrepate inovio.� h , ) Ambiguous words link otherwise unrelated areas of meaning . There are , of course , many more types of polysemy ( cf . ( Kilgarriff , 1992 ) ) . The same happens with wing `` part of a building '' and wing `` political group '' which are linked via policy . However , whereas there are many edges within an area of meaning , there is only a small number of ( weak ) links between different areas of meaning . To detect the different areas of meaning in our local graphs , we use a cluster algorithm for graphs ( Markov clustering , MCL ) developed by van Dongen ( 2000 ) . The idea underlying the MCL-algorithm is that random walks within the graph will tend to stay in the same cluster rather than jump between clusters . The following notation and description of the MCL algorithm borrows heavily from van Dongen ( 2000 ) . Let G� , denote the local graph around the ambiguous word w. The adjacency matrix MG� 4111 ) 11� 41 4Wit ler,1110.1/.17 , cgtoserek�Ilt G� , is defined by setting ( 111G� ) pq equal to the weight of the edge between nodes v and v q . Normalizing the columns of A/G� results in the Markov Matrix Taw whose entries ( Thi , ) pq can be interpreted as transition probability from v q to vv . It can easily be shown that the k-th power of TG� lists the probabilities ( TL ) pq of a path of length k starting at node vq and ending at node V. The MCL-algorithm simulates flow in Gw by iteratively recomputing the set of transition probabilities via two steps , expansion and inflation . The expansion step corresponds with taking the k-th power of TG� as outlined above and allows nodes to see new neighbours . The inflation step takes each matrix entry to the r-th power and then rescales each column so that the entries sum to 1.Vi a inflation , popular neighbours are further supported at the expense of less popular ones . Flow within dense regions in the graph is concentrated by both expansion and inflation . Eventually , flow between dense regions will disappear , the matrix of transition probabilities TG� will converge and the limiting matrix can be interpreted as a clustering of the graph . The output of the MCL-algorithm strongly depends on the inflation and expansion parameters r and k as well as the size of the local graph which serves as input to MCL . An appropriate choice of the inflation param 80 eter r can depend on the ambiguous word w to be clustered . In case of homonymy , a small inflation parameter r would be appropriate . However , there are ambiguous words with more closely related senses which are metaphorical or metonymic variations of one another . In that case , the different regions of meaning are more strongly interlinked and a small power coefficient r would lump different meanings together . Usually , one sense of an ambiguous word w is much more frequent than its other senses present in the corpus . If the local graph handed over to the MCL process is small , we might miss some of w 's meanings in the corpus . On the other hand , if the local graph is too big , we will get a lot of noise . Below , we outline an algorithm which circumvents the problem of choosing the right parameters . In contrast to pure Markov clustering , we do n't try to find a complete clustering of G into senses at once . Instead , in each step of the iterative process , we try to find the most disctinctive cluster c of G w ( i.e . the most distinctive meaning of w ) only . We then recompute the local graph Gw by discriminating against c 's features . This is achieved , in a manner similar to Pantel and Lin 's ( 2002 ) sense clustering approach , by removing c 's features from the set of features used for finding similar words . The process is stopped if the similarity between w and its best neighbour under the reduced set of features is below a fixed threshold . Let F be the set of w 's features , and let L be the output of the algorithm , i.e . a list of sense clusters initially empty . The algorithm consists of the following steps : 1 . Compute a small local graph Gw around w using the set of features F. If the similarity between w and its closest neighbour is below a fixed threshold go to 6 . Recursively remove all nodes of degree one . Then remove the node corresponding with w from G. 3 . Apply MCL to Gw with a fairly big inflation parameter r which is fixed . Take the `` best '' cluster ( the one that is most strongly connected to w in Gw before removal of w ) , add it to the final list of clusters L and remove/devalue its features from F. 5 . Go back to 1 with the reduced/devalued set of features F. 6 . Go through the final list of clusters L and assign a name to each cluster using a broad-coverage taxonomy ( see below ) . Merge semantically close clusters using a taxonomy-based semantic distance measure ( Budanitsky and Hirst , 2001 ) and assign a class-label to the newly formed cluster . Output the list of class-labels which best represent the different senses of w in the corpus . The local graph in step 1 consists of w , the ni neighbours of w and the n9 neighbours of the neighbours of w. Since in each iteration we only attempt to find the `` best '' cluster , it suffices to build a relatively small graph in 1 . Step 2 removes noisy strings of nodes pointing away from G. The removal of w from G w might already separate the different areas of meaning , but will at least significantly loosen the ties between them . In our simple model based on noun co-occurrences in lists , step 5 corresponds to rebuilding the graph under the restriction that the nodes in the new graph not co-occur ( or at least not very often ) with any of the cluster members already extracted . The class-labelling ( step 6 ) is accomplished using the taxonomic structure of WordNet , using a robust algorithm developed specially for this purpose . The hypemym which subsumes as many cluster members as possible and does so as closely as possible in the taxonomic tree is chosen as class-label . The family of such algorithms is described in ( Widdows , 2003 ) . In this section , we describe an initial evaluation experiment and present the results . We will soon carry out and report on a more thorough analysis of our algorithm . We used the simple graph model based on co-occurrences of nouns in lists ( cf . section 2 ) for our experiment . We gathered a list of nouns with varying degree of ambiguity , from homonymy ( e.g . arms ) to systematic polysemy ( e.g . Our algorithm was applied to each word in the list ( with parameters Iii = 20 , n2 = 10 , r = 2.0 , k = 2.0 ) in order to extract the top two sense clusters only . We then determined the WordNet synsets which most adequately characterized the sense clusters . The benefits of automatic , data-driven word sense discovery for natural language processing and lexicography would be very great . Here we only mention a few direct results of our work . Our algorithm does not only recognise ambiguity , but can also be used to resolve it , because the features shared by the members of each sense cluster provide strong indication of which reading of an ambiguous word is appropriate given a certain context . This gives rise to an automatic , unsupervised word sense disambiguation algorithm which is trained on the data to be disambiguated . The ability to map senses into a taxonomy using the class-labelling algorithm can be used to ensure that the sense-distinctions discovered correspond to recognised differences in meaning . This approach to disambiguation combines the benefits of both Yarowsky 's ( 1995 ) and Schtitze 's ( 1998 ) approaches . Off-the-shelf lexical resources are rarely adequate for NLP tasks without being adapted . They often contain many rare senses , but not the same ones that are relevant for specific domains or corpora . The problem can be addressed by using word sense clustering to attune an existing resource to accurately describe the meanings used in a particular corpus . We prepare an evaluation of our algorithm as applied to the collocation relationships ( cf . section 2 ) , and we plan to evaluate the uses of our clustering algorithm for unsupervised disambiguation more thoroughly . We present an unsupervised model for coreference resolution that casts the problem as a clustering task in a directed labeled weighted multigraph . The model outperforms most systems participating in the English track of the CoNLL 12 shared task . Coreference resolution is the task of determining which mentions in a text refer to the same entity . With the advent of machine learning and the availability of annotated corpora in the mid 1990s the research focus shifted from rule-based approaches to supervised machine learning techniques . Quite recently , however , rule-based approaches regained popularity due to Stanford s multi-pass sieve approach which exhibits state- of-the-art performance on many standard coreference data sets ( Raghunathan et al. , 2010 ) and also won the CoNLL2011 shared task on coreference resolution ( Lee et al. , 2011 ; Pradhan et al. , 2011 ) . These results show that carefully crafted rule-based systems which employ suitable inference schemes can achieve competitive performance . Such a system can be considered unsupervised in the sense that it does not employ training data for optimizing parameters . In this paper we present a graph-based approach for coreference resolution that models a document to be processed as a graph . The nodes are mentions and the edges correspond to relations between mentions . Coreference resolution is performed via graph clustering . Our approach belongs to a class of recently proposed graph models for coreference resolution ( Cai and Strube , 2010 ; Sapena et al. , 2010 ; Martschat et al. , 2012 ) and is designed to be a simplified version of existing approaches . In contrast to previous models belonging to this class we do not learn any edge weights but perform inference on the graph structure only which renders our model unsupervised . On the English data of the CoNLL 12 shared task the model outperforms most systems which participated in the shared task . While not developed within a graph-based framework , factor-based approaches for pronoun resolution ( Mitkov , 1998 ) can be regarded as greedy clustering in a multigraph , where edges representing factors for pronoun resolution have negative or positive weight . This yields a model similar to the one presented in this paper though Mitkov s work has only been applied to pronoun resolution . Nicolae and Nicolae ( 2006 ) phrase coreference resolution as a graph clustering problem : they first perform pairwise classification and then construct a graph using the derived confidence values as edge weights . In contrast , work by Culotta et al . ( 2007 ) , Cai and Strube ( 2010 ) and Sapena et al . ( 2010 ) omits the classification step entirely . ( 2010 ) and Cai and Strube ( 2010 ) perform coreference resolution in one step using graph partitioning approaches . These approaches participated in the recent CoNLL 11 shared task ( Pradhan et al. , 2011 ; Sapena et al. , 2011 ; Cai et al. , 2011b ) with excellent results . The approach by Cai et al . ( 2011b ) has been modified by Martschat et al . ( 2012 ) and ranked second in the English track at the CoNLL 12 shared task ( Pradhan et al. , 2012 ) . The top performing system at the CoNLL 12 shared task ( Fernandes et al. , 2012 ) 81 Proceedings of the ACL Student Research Workshop , pages 81 88 , Sofia , Bulgaria , August 49 2013 . Qc 2013 Association for Computational Linguistics also represents the problem as a graph by performing inference on trees constructed using the multi-pass sieve approach by Raghunathan et al . ( 2010 ) and Lee et al . ( 2011 ) , which in turn won the CoNLL 11 shared task . Cardie and Wagstaff ( 1999 ) present an early approach to unsupervised coreference resolution based on a cent sentences ( both in the subject slot ) , which is also a weak coreference indicator . We denote these relations as N Number , P AnaPron and P Subject respectively . ( 2004 ) build on their approach and devise more sophisticated clustering algorithms . Haghighi and Klein ( 2007 ) , Ng ( 2008 ) and Charniak and Elsner ( 2009 ) employ unsupervised generative models . Poon and Domingos ( 2008 ) present a Markov Logic Network approach to unsupervised coreference resolution . These approaches reach competitive performance on gold mentions but not on system mentions ( Ng , 2008 ) . The multi-pass sieve Leaders recent developments P AnaPron P Subject P AnaPron Paris They approach by Raghunathan et al . ( 2010 ) can also be viewed as unsupervised . We aim for a model which directly represents the relations between mentions in a graph structure . Clusters in the graph then correspond to entities . To motivate the choice of our model , let us consider a simple made-up example . Leaders met in Paris to discuss recent developments . They left the city today . We want to model that Paris is not a likely candidate antecedent for They due to number disagreement , but that Leaders and recent developments are potential antecedents for They . We want to express that Leaders is the preferred antecedent , since Leaders and They are in a parallel construction both occupying the subject position in their respective sentences . In other words , our model should express the following relations for this example : number disagreement for ( They , Paris ) , which indicates that the mentions are not coreferent , the anaphor being a pronoun for ( They , Lead ers ) , ( They , recent developments ) and ( They , Paris ) , which is a weak indicator for coreference if the mentions are close to each other , syntactic parallelism for ( They , Leaders ) : A directed edge from a mention m to n indicates that n precedes m and that there is some relation between m and n that indicates coreference or non-coreference . Labeled edges describe the relations between the mentions , multiple relations can hold between a pair . 3.2 Multigraphs for Coreference Resolution . Formally , the model is a directed labeled weighted multigraph . That is a tuple D = ( R , V , A , w ) where R is the set of labels ( in our case relations such as P Subject that hold between mentions ) , V is the set of nodes ( the mentions extracted from a document ) , A V V R is the set of edges ( relations between two mentions ) , w is a mapping w : A R { } ( weights for edges ) . Many graph models for coreference resolution operate on A = V V . Our multigraph model allows us to have multiple edges with different labels between mentions . To have a notion of order we employ a directed graph : We only allow an edge from m to n if m appears later in the text than n. To perform coreference resolution for a document d , we first construct a directed labeled multi- graph ( Section 3.3 ) . We then assign a weight to each edge ( Section 3.4 ) . The resulting graph is clustered to obtain the mentions that refer to the same entity ( Section 3.5 ) . Given a set M of mentions extracted from a document d , we set V = M , i.e . the nodes of the graph are the mentions . To construct the edges A , we consider each pair ( m , n ) of mentions with n m. We then check for every relation r R if r holds for the pair ( m , n ) . If this is the case we add the edge ( m , n , r ) to A . For simplicity , we restrict ourselves to binary relations that hold between pairs of mentions ( see Section 4 ) . Depending on whether a relation r R is indicative for non-coreference ( e.g . number disagree ment ) or for coreference ( e.g . string matching ) it should be weighted differently . We therefore divide R into a set of negative relations R and a set of positive relations R+ . Previous work on multigraphs for coreference resolution disallows any edge between mentions for which a negative relations holds ( Cai et al. , 2011b ; Martschat et al. , 2012 ) . We take a similar approach and set w ( m , n , r ) = for ( m , n , r ) A when r R 1 . Work on graph-based models similar to ours report robustness with regard to the amount of training data used ( Cai et al. , 2011b ; Cai et al. , 2011a ; Martschat et al. , 2012 ) . Motivated by their observations we treat every positive relation equally and set w ( m , n , r ) = 1 for ( m , n , r ) A if r R+ . In contrast to previous work on similar graph models we do not learn any edge weights from training data . We compare this unsupervised scheme with supervised variants empirically in Section 5 . To describe the clustering algorithm used in this work we need some additional terminology . If there exists an edge ( m , n , r ) A we say that n is a child of m. 1 We experimented with different weighting schemes for negative relations on development data ( e.g . setting w ( m , n , r ) = 1 ) but did not observe a gain in performance . In the graph constructed according to the procedure described in Section 3.3 , all children of a mention m are candidate antecedents for m. The relations we employ are indicators for coreference ( which get a positive weight ) and indicators for non-coreference ( which get a negative weight ) . We aim to employ a simple and efficient clustering scheme on this graph and therefore choose 1-nearest-neighbor clustering : for every m , we choose as antecedent m s child n such that the sum of edge weights is maximal and positive . We break ties by choosing the closest mention . In the unsupervised setting described in Section 3.4 this algorithm reduces to choosing the child that is connected via the highest number of positive relations and via no negative relation . The graph model described in Section 3 is based on expressing relations between pairs of mentions via edges built from such relations . We now describe the relations currently used by our system . They are well-known indicators and constraints for coreference and are taken from previous work ( Cardie and Wagstaff , 1999 ; Soon et al. , 2001 ; Rahman and Ng , 2009 ; Lee et al. , 2011 ; Cai et al. , 2011b ) . All relations operate on pairs of mentions ( m , n ) , where m is the anaphor and n is a candidate antecedent . If a relation r holds for ( m , n ) , the edge ( m , n , r ) is added to the graph . We finalized the set of relations and their distance thresholds on development data . Negative relations receive negative weights . They allow us to introduce well-known constraints such as agreement into our model . ( 1 ) N Gender , ( 2 ) N Number : Two mentions do not agree in gender or number . We compute number and gender for common nouns using the number and gender data provided by Bergsma and Lin ( 2006 ) . ( 3 ) N SemanticClass : Two mentions do not agree in semantic class ( we only use the top categories Object , Date and Person from WordNet ( Fellbaum , 1998 ) ) . ( 4 ) N ItDist : The anaphor is it or they and the sentence distance to the antecedent is larger than one . ( 5 ) N Speaker12Pron : Two first person pronouns or two second person pronouns with different speakers , or one first person pronoun and one second person pronoun with the same speaker2 . ( 6 ) N ContraSubObj : Two mentions are in the subject/object positions of the same verb , the anaphor is a non-possessive/reflexive pronoun . ( 7 ) N Mod : Two mentions have the same syntactic heads , and the anaphor has a nominal modifier which does not occur in the antecedent . ( 8 ) N Embedding : Two mentions where one embeds the other , which is not a reflexive or possessive pronoun . ( 9 ) N 2PronNonSpeech : Two second person pronouns without speaker information and not in direct speech . Positive relations are coreference indicators which are added as edges with positive weights . ( 10 ) P NonPron StrMatch : Applies only if the anaphor is definite or a proper name3 . This relation holds if after discarding stop words the strings of mentions completely match . ( 11 ) P HeadMatch : If the syntactic heads of mentions match . ( 12 ) P Alias : If mentions are aliases of each other ( i.e . proper names with partial match , full names and acronyms , etc . ) . ( 13 ) P Speaker12Pron : If the speaker of the second person pronoun is talking to the speaker of the first person pronoun ( applies only to first/second person pronouns ) . ( 14 ) P DSPron : One mention is a speak verb s subject , the other mention is a first person pronoun within the corresponding direct speech . ( 15 ) P ReflPronSub : If the anaphor is a reflexive pronoun , and the antecedent is the subject of the sentence . ( 16 ) P PossPronSub : If the anaphor is a possessive pronoun , and the antecedent is the subject of the anaphor s sentence or subclause . ( 17 ) P PossPronEmb : The anaphor is a posses 2 Like all relations using speaker information , this relation depends on the gold speaker annotation layer in the corpus . 3 This condition is necessary to cope with the high-recall output of the mention tagger . sive pronoun embedded in the antecedent . ( 18 ) P AnaPron : If the anaphor is a pronoun and none of the mentions is a first or second person pronoun . This relation is restricted to a sentence distance of 3 . ( 19 ) P VerbAgree : If the anaphor is a third person pronoun and has the same predicate as the antecedent . This relation is restricted to a sentence distance of 1 . ( 20 ) P Subject , ( 21 ) P Object : The anaphor is a third person pronoun and both mentions are subjects/objects . These relations are restricted to a sentence distance of 1 . ( 22 ) P Pron StrMatch : If both mentions are pronouns and their strings match . ( 23 ) P Pron Agreement : If both mentions are different pronoun tokens but agree in number , gender and person . 5.1 Data and Evaluation Metrics . We use the data provided for the English track of the CoNLL 12 shared task on multilingual coreference resolution ( Pradhan et al. , 2012 ) which is a subset of the upcoming OntoNotes 5.0 release and comes with various annotation layers provided by state-of-the-art NLP tools . We used the official dev/test split for development and evaluation . We evaluate the model in a setting that corresponds to the shared task s closed track , i.e . we use only WordNet ( Fellbaum , 1998 ) , the number and gender data of Bergsma and Lin ( 2006 ) and the provided annotation layers . To extract system mentions we employ the mention extractor described in Martschat et al . We evaluate our system with the coreference resolution evaluation metrics that were used for the CoNLL shared tasks on coreference , which are MUC ( Vilain et al. , 1995 ) , B3 ( Bagga and Baldwin , 1998 ) and CEAFe ( Luo , 2005 ) . We also report the unweighted average of the three scores , which was the official evaluation metric in the shared tasks . To compute the scores we employed the official scorer supplied by the shared task organizers . 35 me dia n 62 . 9 thi s wo rk ( w eig hts fra cti on ) 64 . 63 thi s wo rk ( w eig hts Ma xE nt ) 63 . 36 this wo rk ( u ns up erv ise d ) 64 . 05 CoNLL 12 English test data be st 65 . 37 me dia n 62 . 68 thi s wo rk ( w eig hts fra cti on ) 64 . 87 thi s wo rk ( w eig hts Ma xE nt ) 63 . 28 this wo rk ( u ns up erv ise d ) 63 . 10 CoNLL 12 shared task , which are denoted as best and median respectively . best employs a structured prediction model with learned combinations of 70 basic features . We also compare with two supervised variants of our model which use the same relations and the same clustering algorithm as the unsupervised model : weights fraction sets the weight of a relation to the fraction of positive instances in training data ( as in Martschat et al . weights MaxEnt trains a mention-pair model ( Soon et al. , 2001 ) via the maximum entropy classifier implemented in the BART toolkit ( Versley et al. , 2008 ) and builds a graph where the weight of an edge connecting two mentions is the classifier s prediction4 . We use the official CoNLL 12 English training set for training . Our unsupervised model performs considerably better than the median system from the CoNLL 12 shared task on both data sets according to all metrics . It also seems to be able to accommodate well for the relations described in Section 4 since it outperforms both supervised variants5 . The model performs worse than best , the gap according to B3 and CEAFe being considerably smaller than according to MUC . While we observe a decrease of 1 point average score when evaluating on test data the model still would have ranked fourth in the English track of the CoNLL 12 shared task with only 0.2 points difference in average score to the second ranked system . 4 The classifier s output is a number p [ 0 , 1 ] . In order to have negative weights we use the transformation pi = 2p 1 . 5 Compared with the supervised variants all improvements in F1 score are statistically significant according to a paired t-test ( p < 0.05 ) except for the difference in MUC F1 to weights fraction . In order to understand weaknesses of our model we perform an error analysis on the development data . We distinguish between precision and recall errors . For an initial analysis we split the errors according to the mention type of anaphor and antecedent ( name , nominal and pronoun ) . Our system operates in a pairwise fashion . We therefore count one precision error whenever the clustering algorithm assigns two non-coreferent mentions to the same cluster . number of clustering decisions made according to the mention type and in brackets the fraction of decisions that erroneously assign two non-coreferent mentions to the same cluster . We see that two main sources of error are nominal-nominal pairs and the resolution of pronouns . We now focus on gaining further insight into the system s performance for pronoun resolution by investigating the performance per pronoun type . We obtain good performance for I and my which in the majority of cases can be resolved unambiguously by the speaker relations employed by our system . The relations we use also seem Anaphor all anaphoric I 1260 ( 13 % ) 1239 ( 11 % ) my 192 ( 14 % ) 181 ( 9 % ) he 824 ( 14 % ) 812 ( 13 % ) . they 764 ( 29 % ) 725 ( 26 % ) . you 802 ( 41 % ) 555 ( 15 % ) it 1114 ( 64 % ) 720 ( 44 % ) Rows are pronoun surfaces , columns number of clustering decisions and percentage of wrong decisions for all and only anaphoric pronouns respectively . to work well for he . In contrast , the local , shallow approach we currently employ is not able to resolve highly ambiguous pronouns such as they , you or it in many cases . The reduction in error rate when only considering anaphoric pronouns shows that our system could benefit from an improved detection of expletive it and you . Estimating recall errors by counting all missing pairwise links would consider each entity many times . Therefore , we instead count one recall error for a pair ( m , n ) of anaphor m and antecedent n if ( i ) m and n are coreferent , ( ii ) m and n are not assigned to the same cluster , ( iii ) m is the first mention in its cluster that is coreferent with n , and ( iv ) n is the closest mention coreferent with m that is not in m s cluster . This can be illustrated by an example . , m5 , assume that m1 , m3 , m4 and m5 are coreferent but the system clusters are { m2 , m3 } and { m4 , m5 } . We then count two recall errors : one for the missing link from m3 to m1 and one for the missing link from m4 to m3 . According to this definition we count 3528 recall errors on the development set . We see that NA M N O M PR O NA M 32 1 22 0 24 7 N O M 30 6 79 7 33 0 PR O 30 6 47 6 52 5 the main source of recall errors are missing links of nominal-nominal pairs . We randomly extracted 50 of these errors and manually assigned them to different categories . 29 errors : missing semantic knowledge . In these cases lexical or world knowledge is needed to build coreference links between mentions with different heads . For example our system misses the link between the sauna and the hotbox sweatbox . 14 errors : too restrictive N Mod . In these cases the heads of the mentions matched but no link was built due to N Mod . An example is the missing link between our island s last remaining forest of these giant trees and the forest of Chilan . 4 errors : too cautious string match . We only apply string matching for common nouns when the noun is definite . Three errors could not be attributed to any of the above categories . We presented an unsupervised graph-based model for coreference resolution . Experiments show that our model exhibits competitive performance on the English CoNLL 12 shared task data sets . An error analysis revealed that two main sources of errors of our model are the inaccurate resolution of highly ambiguous pronouns such as it and missing links between nominals with different heads . Future work should investigate how semantic knowledge and more complex relations capturing deeper discourse properties such as coherence or information status can be added to the model . Processing these features efficently may require a more sophisticated clustering algorithm . We are surprised by the good performance of this unsupervised model in comparison to the state-of-the-art which uses sophisticated machine learning techniques ( Fernandes et al. , 2012 ) or well-engineered rules ( Lee et al. , 2011 ) . We are not sure how to interpret these results and want to leave different interpretations for discussion : our unsupervised model is really that good ( hopefully ) , the evaluation metrics employed are to be questioned ( certainly ) , efficiently making use of annotated trainingdata still remains a challenge for the state-of the-art ( likely ) . This work has been funded by the Klaus Tschira Foundation , Germany . The author has been supported by a HITS PhD scholarship . Named Entity Recognition : A Maximum Entropy Approach Using Global Information This paper presents a maximum entropy-based named entity recognizer ( NER ) . It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word , with just one classifier . Previous work that involves the gathering of information from the whole document often uses a secondary classifier , which corrects the mistakes of a primary sentence- based classifier . In this paper , we show that the maximum entropy framework is able to make use of global information directly , and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data . Considerable amount of work has been done in recent years on the named entity recognition task , partly due to the Message Understanding Conferences ( MUC ) . A named entity recognizer ( NER ) is useful in many NLP applications such as information extraction , question answering , etc . On its own , a NER can also provide users who are looking for person or organization names with quick information . In MUC6 and MUC7 , the named entity task is defined as finding the following classes of names : person , organization , location , date , time , money , and percent ( Chinchor , 1998 ; Sundheim , 1995 ) Machine learning systems in MUC6 and MUC 7 achieved accuracy comparable to rule-based systems on the named entity task . Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence , and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information ( e.g. , the same named entity occurring in different sentences of the same document ) , but they usually consist of incorporating an additional classifier , which tries to correct the errors in the output of a first NER ( Mikheev et al. , 1998 ; Borthwick , 1999 ) . We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing . Our system is built on a maximum entropy classifier . By making use of global context , it has achieved excellent results on both MUC6 and MUC7 official test data . We will refer to our system as MENERGI ( Maximum Entropy Named Entity Recognizer using Global Information ) . As far as we know , no other NERs have used information from the whole document ( global ) as well as information within the same sentence ( local ) in one framework . The use of global features has improved the performance on MUC6 test data from 90.75 % to 93.27 % ( 27 % reduction in errors ) , and the performance on MUC7 test data from 85.22 % to 87.24 % ( 14 % reduction in errors ) . These results are achieved by training on the official MUC6 and MUC7 training data , which is much less training data than is used by other machine learning systems that worked on the MUC6 or MUC7 named entity task ( Bikel et al. , 1997 ; Bikel et al. , 1999 ; Borth- wick , 1999 ) . We believe it is natural for authors to use abbreviations in subsequent mentions of a named entity ( i.e. , first “ President George Bush ” then “ Bush ” ) . As such , global information from the whole context of a document is important to more accurately recognize named entities . Although we have not done any experiments on other languages , this way of using global features from a whole document should be applicable to other languages . Recently , statistical NERs have achieved results that are comparable to hand-coded systems . Since MUC6 , BBN ' s Hidden Markov Model ( HMM ) based IdentiFinder ( Bikel et al. , 1997 ) has achieved remarkably good performance . MUC7 has also seen hybrids of statistical NERs and hand-coded systems ( Mikheev et al. , 1998 ; Borthwick , 1999 ) , notably Mikheev ' s system , which achieved the best performance of 93.39 % on the official NE test data . MENE ( Maximum Entropy Named Entity ) ( Borth- wick , 1999 ) was combined with Proteus ( a hand- coded system ) , and came in fourth among all MUC 7 participants . MENE without Proteus , however , did not do very well and only achieved an F measure of 84.22 % ( Borthwick , 1999 ) . Among machine learning-based NERs , Identi- Finder has proven to be the best on the official MUC6 and MUC7 test data . MENE ( without the help of hand-coded systems ) has been shown to be somewhat inferior in performance . By using the output of a hand-coded system such as Proteus , MENE can improve its performance , and can even outperform IdentiFinder ( Borthwick , 1999 ) . ( 1998 ) did make use of information from the whole document . However , their system is a hybrid of hand-coded rules and machine learning methods . Another attempt at using global information can be found in ( Borthwick , 1999 ) . He used an additional maximum entropy classifier that tries to correct mistakes by using reference resolution . Reference resolution involves finding words that co-refer to the same entity . In order to train this error-correction model , he divided his training corpus into 5 portions of 20 % each . MENE is then trained on 80 % of the training corpus , and tested on the remaining 20 % . This process is repeated 5 times by rotating the data appropriately . Finally , the concatenated 5 * 20 % output is used to train the reference resolution component . We will show that by giving the first model some global features , MENERGI outperforms Borthwick ' s reference resolution classifier . On MUC6 data , MENERGI also achieves performance comparable to IdentiFinder when trained on similar amount of training data . both MENE and IdentiFinder used more training data than we did ( we used only the official MUC 6 and MUC7 training data ) . On the MUC6 data , Bikel et al . ( 1997 ; 1999 ) do have some statistics that show how IdentiFinder performs when the training data is reduced . Our results show that MENERGI performs as well as IdentiFinder when trained on comparable amount of training data . The system described in this paper is similar to the MENE system of ( Borthwick , 1999 ) . It uses a maximum entropy framework and classifies each word given its features . Each name class is subdivided into 4 sub-classes , i.e. , N begin , N continue , N end , and N unique . Hence , there is a total of 29 classes ( 7 name classes 4 sub-classes 1 not-a-name class ) . The maximum entropy framework estimates probabilities based on the principle of making as few assumptions as possible , other than the constraints imposed . Such constraints are derived from training data , expressing some relationship between features and outcome . The probability distribution that satisfies the above property is the one with the highest entropy . It is unique , agrees with the maximum-likelihood distribution , and has the exponential form ( Della Pietra et al. , 1997 ) : where refers to the outcome , the history ( or context ) , and is a normalization function . In addition , each feature function is a binary function . For example , in predicting if a word belongs to a word class , is either true or false , and refers to the surrounding context : if = true , previous word = the otherwise The parameters are estimated by a procedure called Generalized Iterative Scaling ( GIS ) ( Darroch and Ratcliff , 1972 ) . This is an iterative method that improves the estimation of the parameters at each iteration . We have used the Java-based opennlp maximum entropy package1 . In Section 5 , we try to compare results of MENE , IdentiFinder , and MENERGI . However , 1 http : //maxent.sourceforge.net 3.2 Testing . During testing , it is possible that the classifier produces a sequence of inadmissible classes ( e.g. , person begin followed by location unique ) . To eliminate such sequences , we define a transition probability between word classes to be equal to 1 if the sequence is admissible , and 0 otherwise . The probability of the classes assigned to the words in a sentence in a document is defined as follows : where is determined by the maximum entropy classifier . A dynamic programming algorithm is then used to select the sequence of word classes with the highest probability . The features we used can be divided into 2 classes : local and global . Local features are features that are based on neighboring tokens , as well as the token itself . Global features are extracted from other occurrences of the same token in the whole document . The local features used are similar to those used in BBN ' s IdentiFinder ( Bikel et al. , 1999 ) or MENE ( Borthwick , 1999 ) . However , to classify a token , while Borthwick uses tokens from to ( from two tokens before to two tokens after ) , we used only the tokens , , and . Even with local features alone , MENERGI outperforms MENE ( Borthwick , 1999 ) . This might be because our features are more comprehensive than those used by Borthwick . In IdentiFinder , there is a priority in the feature assignment , such that if one feature is used for a token , another feature lower in priority will not be used . In the maximum entropy framework , there is no such constraint . Multiple features can be used for the same token . Feature selection is implemented using a feature cutoff : features seen less than a small count during training will not be used . We group the features used into feature groups . Each feature group can be made up of many binary features . For each token , zero , one , or more of the features in each feature group are set to 1 . The local feature groups are : Non-Contextual Feature : This feature is set to 1 for all tokens . This feature imposes constraints Zone : MUC data contains SGML tags , and a document is divided into zones ( e.g. , headlines and text zones ) . The zone to which a token belongs is used as a feature . For example , in MUC6 , there are four zones ( TXT , HL , DATELINE , DD ) . Hence , for each token , one of the four features zone-TXT , zone- HL , zone-DATELINE , or zone-DD is set to 1 , and the other 3 are set to 0 . This group consists of ( 3 total number of possible zones ) features . Case and Zone of and : Similarly , if ( or ) is initCaps , a feature ( initCaps , zone ) ( or ( initCaps , zone ) ) is set to 1 , etc . First Word : This feature group contains only one feature firstword . If the token is the first word of a sentence , then this feature is set to 1 . Otherwise , it is set to 0 . Lexicon Feature : The string of the token is used as a feature . This group contains a large number of features ( one for each token string present in the training data ) . At most one feature in this group will be set to 1 . If is seen infrequently during training ( less than a small count ) , then will not be selected as a feature and all features in this group are set to 0 . Lexicon Feature of Previous and Next Token : The string of the previous token and the next token is used with the initCaps information of . If has initCaps , then a feature ( initCaps , ) is set to 1 . If is not initCaps , then ( not-initCaps , ) is set to 1 . In the case where the next token is a hyphen , then is also used as a feature : ( init- Caps , ) is set to 1 . This is because in many cases , the use of hyphens can be considered to be optional ( e.g. , third-quarter or third quarter ) . Out-of-Vocabulary : We derived a lexicon list from WordNet 1.6 , and words that are not found in this list have a feature out-of-vocabulary set to 1 . Dictionaries : Due to the limited amount of training material , name dictionaries have been found to be useful in the named entity task . The importance of dictionaries in NERs has been investigated in the literature ( Mikheev et al. , 1999 ) . For all lists except locations , the lists are processed into a list of tokens ( unigrams ) . Location list is processed into a list of unigrams and bigrams ( e.g. , New York ) . For locations , tokens are matched against unigrams , and sequences of two consecutive tokens are matched against bigrams . A list of words occurring more than 10 times in the training data is also collected ( commonWords ) . If they are found in a list , then a feature for that list will be set to 1 . For example , if Barry is not in commonWords and is found in the list of person first names , then the feature PersonFirstName will be set to 1 . Similarly , the tokens and are tested against each list , and if found , a corresponding feature will be set to 1 . For example , if is found in the list of person first names , the feature PersonFirstName is set to 1 . Month Names , Days of the Week , and Numbers : If is initCaps and is one of January , February , . , December , then the feature MonthName is set to 1 . If is one of Monday , Tuesday , . , Sun day , then the feature DayOfTheWeek is set to 1 . If is a number string ( such as one , two , etc ) , then the feature NumberString is set to 1 . Suffixes and Prefixes : This group contains only two features : Corporate-Suffix and Person-Prefix . Two lists , Corporate-Suffix-List ( for corporate suffixes ) and Person-Prefix-List ( for person prefixes ) , are collected from the training data . For corporate suffixes , a list of tokens cslist that occur frequently as the last token of an organization name is collected from the training data . Frequency is calculated by counting the number of distinct previous tokens that each token has ( e.g. , if Electric Corp. is seen 3 times , and Manufacturing Corp. is seen 5 times during training , and Corp. is not seen with any other preceding tokens , then the “ frequency ” of Corp. is 2 ) . The most frequently occurring last words of organization names in cslist are compiled into a list of corporate suffixes , Corporate-Suffix- List . A Person-Prefix-List is compiled in an analogous way . For MUC6 , for example , Corporate- Suffix-List is made up of ltd. , associates , inc. , co , corp , ltd , inc , committee , institute , commission , university , plc , airlines , co. , corp. and Person-Prefix- List is made up of succeeding , mr. , rep. , mrs. , secretary , sen. , says , minister , dr. , chairman , ms. . For a token that is in a consecutive sequence of init then a feature Corporate-Suffix is set to 1 . If any of the tokens from to is in Person-Prefix- List , then another feature Person-Prefix is set to 1 . Note that we check for , the word preceding the consecutive sequence of initCaps tokens , since person prefixes like Mr. , Dr. , etc are not part of person names , whereas corporate suffixes like Corp. , Inc. , etc are part of corporate names . Context from the whole document can be important in classifying a named entity . A name already mentioned previously in a document may appear in abbreviated form when it is mentioned again later . Previous work deals with this problem by correcting inconsistencies between the named entity classes assigned to different occurrences of the same entity ( Borthwick , 1999 ; Mikheev et al. , 1998 ) . We often encounter sentences that are highly ambiguous in themselves , without some prior knowledge of the entities concerned . For example : McCann initiated a new global system . ( 1 ) CEO of McCann . ( 2 ) Description Source Location Names http : //www.timeanddate.com http : //www.cityguide.travel-guides.com http : //www.worldtravelguide.net Corporate Names http : //www.fmlx.com Person First Names http : //www.census.gov/genealogy/names Person Last Names The McCann family . ( 3 ) In sentence ( 1 ) , McCann can be a person or an orga nization . Sentence ( 2 ) and ( 3 ) help to disambiguate one way or the other . If all three sentences are in the same document , then even a human will find it difficult to classify McCann in ( 1 ) into either person or organization , unless there is some other information provided . The global feature groups are : InitCaps of Other Occurrences ( ICOC ) : There are 2 features in this group , checking for whether the first occurrence of the same word in an unambiguous position ( non first-words in the TXT or TEXT zones ) in the same document is initCaps or not-initCaps . For a word whose initCaps might be due to its position rather than its meaning ( in headlines , first word of a sentence , etc ) , the case information of other occurrences might be more accurate than its own . For example , in the sentence that starts with “ Bush put a freeze on . ” , because Bush is the first word , the initial caps might be due to its position ( as in “ They put a freeze on . If somewhere else in the document we see “ restrictions put in place by President Bush ” , then we can be surer that Bush is a name . Corporate Suffixes and Person Prefixes of Other Occurrences ( CSPP ) : If McCann has been seen as Mr. McCann somewhere else in the document , then one would like to give person a higher probability than organization . On the other hand , if it is seen as McCann Pte . Ltd. , then organization will be more probable . With the same Corporate- Suffix-List and Person-Prefix-List used in local features , for a token seen elsewhere in the same document with one of these suffixes ( or prefixes ) , another feature Other-CS ( or Other-PP ) is set to 1 . Acronyms ( ACRO ) : Words made up of all capitalized letters in the text zone will be stored as acronyms ( e.g. , IBM ) . The system will then look for sequences of initial capitalized words that match the acronyms found in the whole document . Such sequences are given additional features of A begin , A continue , or A end , and the acronym is given a feature A unique . For example , if FCC and Federal Communications Commission are both found in a document , then Federal has A begin set to 1 , Communications has A continue set to 1 , Commission has A end set to 1 , and FCC has A unique set to 1 . Sequence of Initial Caps ( SOIC ) : In the sentence Even News Broadcasting Corp. , noted for its accurate reporting , made the erroneous announcement. , a NER may mistake Even News Broadcasting Corp. as an organization name . However , it is unlikely that other occurrences of News Broadcasting Corp. in the same document also co-occur with Even . This group of features attempts to capture such information . For every sequence of initial capitalized words , its longest substring that occurs in the same document as a sequence of initCaps is identified . For this example , since the sequence Even News Broadcasting Corp. only appears once in the document , its longest substring that occurs in the same document is News Broadcasting Corp . In this case , News has an additional feature of I begin set to 1 , Broadcasting has an additional feature of I continue set to 1 , and Corp. has an additional feature of I end set to 1 . Unique Occurrences and Zone ( UNIQ ) : This group of features indicates whether the word is unique in the whole document . needs to be in initCaps to be considered for this feature . If is unique , then a feature ( Unique , Zone ) is set to 1 , where Zone is the document zone where appears . MUC6 MUC7 Baseline 90.75 % 85.22 % + ICOC 91.50 % 86.24 % + CSPP 92.89 % 86.96 % + ACRO 93.04 % 86.99 % + SOIC 93.25 % 87.22 % + UNIQ 93.27 % 87.24 % Systems MUC6 MUC7 No . of Tokens MENERGI 318 160,000 200 180,000 IdentiFinder – 650,000 – 790,000 MENE – – 350 321,000 2 ICOC and CSPP contributed the greatest im provements . The effect of UNIQ is very small on both data sets . All our results are obtained by using only the official training data provided by the MUC conferences . The reason why we did not train with both MUC6 and MUC7 training data at the same time is because the task specifications for the two tasks are not identical . IdentiFinder ' 99 ' s results are considerably better than IdentiFinder ' 97 ' s. IdentiFinder ' s performance in MUC7 is published in ( Miller et al. , 1998 ) . Besides size of training data , the use of dictionaries is another factor that might affect performance . ( 1999 ) did not report using any dictionaries , but mentioned in a footnote that they have added list membership features , which have helped marginally in certain domains . Borth 2MUC data can be obtained from the Linguistic Data Consortium : http : //www.ldc.upenn.edu 3Training data for IdentiFinder is actually given in words ( i.e. , 650K & 790K words ) , rather than tokens wick ( 1999 ) reported using dictionaries of person first names , corporate names and suffixes , colleges and universities , dates and times , state abbreviations , and world regions . In ( Bikel et al. , 1997 ) and ( Bikel et al. , 1999 ) , performance was plotted against training data size to show how performance improves with training data size . We have estimated the performance of IdentiFinder ' 99 at 200K words of training data from the graphs . In fact , training on the official training data is not suitable as the articles in this data set are entirely about aviation disasters , and the test data is about air vehicle launching . Both BBN and NYU have tagged their own data to supplement the official training data . Even with less training data , MENERGI outperforms Borthwick ' s MENE + reference resolution ( Borthwick , 1999 ) . The effect of a second reference resolution classifier is not entirely the same as that of global features . A secondary reference resolution classifier has information on the class assigned by the primary classifier . Such a classification can be seen as a not-always-correct summary of global features . The secondary classifier in ( Borthwick , 1999 ) uses information not just from the current article , but also from the whole test corpus , with an additional feature that indicates if the information comes from the same document or from another document . We feel that information from a whole corpus might turn out to be noisy if the documents in the corpus are not of the same genre . Moreover , if we want to test on a huge test corpus , indexing the whole corpus might prove computationally expensive . Hence we decided to restrict ourselves to only information from the same document . ( 1998 ) have also used a maximum entropy classifier that uses already tagged entities to help tag other entities . The overall performance of the LTG system was outstanding , but the system consists of a sequence of many hand-coded rules and machine-learning modules . We have shown that the maximum entropy framework is able to use global information directly . This enables us to build a high performance NER without using separate classifiers to take care of global consistency or complex formulation on smoothing and backoff models ( Bikel et al. , 1997 ) . Using less training data than other systems , our NER is able to perform as well as other state-of-the-art NERs . Information from a sentence is sometimes insufficient to classify a name correctly . Global context from the whole document is available and can be exploited in a natural manner with a maximum entropy classifier . We believe that the underlying principles of the maximum entropy framework are suitable for exploiting information from diverse sources . Borth- wick ( 1999 ) successfully made use of other hand- coded systems as input for his MENE system , and achieved excellent results . However , such an approach requires a number of hand-coded systems , which may not be available in languages other than English . We believe that global context is useful in most languages , as it is a natural tendency for authors to use abbreviations on entities already mentioned previously . In this paper we present a new , multi lingual data-driven method for coreference resolution as implemented in the SWIZZLE system . The results obtained after training this system on a bilingual corpus of English and Romanian tagged texts , outperformed coreference resolution in each of the indi vidual languages . The recent availability of large bilingual corpora has spawned interest in several areas of multilingual text processing . Most of the research has focused on bilingual terminology identification , either as par allel multiwords forms ( e.g . the Champollion sys tem ( Smadja et al.1996 ) ) , technical terminology ( e.g . the Termight system ( Dagan and Church , 1994 ) or broad-coverage translation lexicons ( e.g . the SABLE system ( Resnik and Melamed , 1997 ) ) . In addition , the Multilingual Entity Task ( MET ) from the TIP STER program1 ( http : //wwwnlpir.nist.govjrelated projectsjtipsterjmet.htm ) challenged the partici pants in the Message Understanding Conference ( MUC ) to extract named entities across several for eign language corpora , such as Chinese , Japanese and Spanish . In this paper we present a new application of aligned multilingual texts . Since coreference reso lution is a pervasive discourse phenomenon causing performance impediments in current IE systems , we considered a corpus of aligned English and Roma nian texts to identify coreferring expressions . Our task focused on the same kind of coreference as considered in the past MUC competitions , namely 1The TIPSTER Text Program was a DARPA-led government effort to advance the state of the art in text processing technologies . Steven J. Maiorano IPO Washington , D.C. 20505 maiorano cais.com the identity coreference . Identity coreference links nouns , pronouns and noun phrases ( including proper names ) to their corresponding antecedents . We created our bilingual collection by translating the MUC6 and MUC7 coreference training texts into Romanian using native speakers . The train ing data set for Romanian coreference used , wher ever possible , the same coreference identifiers as the English data and incorporated additional tags as needed . Our claim is that by adding the wealth of coreferential features provided by multilingual data , new powerful heuristics for coreference resolu tion can be developed that outperform monolingual coreference resolution systems . For both languages , we resolved coreference by using SWIZZLE , our implementation of a bilingual coreference resolver . SWIZZLE is a multilingual en hancement of COCKTAIL ( Harabagiu and Maiorano , 1999 ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information2 When COCKTAIL was applied separately on the English and the Ro manian texts , coreferring links were identified for each English and Romanian document respectively . When aligned referential expressions corefer with nonaligned anaphors , SWIZZLE derived new heuris tics for coreference . Our experiments show that SWIZZLE outperformed COCKTAIL on both English and Romanian test documents . The rest of the paper is organized as follows . Sec tion 2 presents COCKTAIL , a monolingual coreference resolution system used separately on both the En glish and Romanian texts . Section 3 details the data-driven approach used in SWIZZLE and presents some of its resources . Section 4 reports and discusses the experimental results . Section 5 summarizes the 2 The name of COCKTAIL is a pun on CogNIAC be cause COCKTAIL combines a larger number of heuristics than those reported in ( Baldwin , 1997 ) . SWIZZLE , more over , adds new heuristics , discovered from the bilingual aligned corpus . Currently , some of the best-performing and most robust coreference resolution systems employ knowledge-based techniques . Traditionally , these techniques have combined extensive syntactic , se mantic , and discourse knowledge . The acquisition of such knowledge is time-consuming , difficult , and error-prone . Nevertheless , recent results show that knowledge-poor methods perform with amazing ac curacy ( cf . ( Mitkov , 1998 ) , ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ) . For example , CogNIAC ( Baldwin , 1997 ) , a system based on seven ordered heuristics , generates high-precision resolution ( over 90 % ) for some cases of pronominal reference . For this research , we used a coreference resolution sys tem ( ( Harabagiu and Maiorano , 1999 ) ) that imple ments different sets of heuristics corresponding to various forms of coreference . This system , called COCKTAIL , resolves coreference by exploiting several textual cohesion constraints ( e.g . term repetition ) combined with lexical and textual coherence cues ( e.g . subjects of communication verbs are more likely to refer to the last person mentioned in the text ) . These constraints are implemented as a set of heuristics ordered by their priority . Moreover , the COCKTAIL framework uniformly addresses the prob lem of interaction between different forms of coref erence , thus making the extension to multilingual coreference very natural . In general , we define a data-driven methodology as a sequence of actions that captures the data pat terns capable of resolving a problem with both a high degree of precision and recall . Our data-driven methodology reported here generated sets of heuris tics for the coreference resolution problem . Precision is the number of correct references out of the total number of coreferences resolved , whereas the recall measures the number of resolved references out of the total number of keys , i.e. , the annotated coref erence data . The data-driven methodology used in COCKTAIL is centered around the notion of a coreference chain . Due to the transitivity of coreference relations , k coreference relations having at least one common ar gument generate k + 1 core/erring expressions . The text position induces an order among coreferring ex pressions . A coreference structure is created when a set of coreferring expressions are connected in an oriented graph such that each node is related only to one of its preceding nodes . In turn , a corefer ence chain is the coreference structure in which ev ery node is connected to its immediately preceding node . Clearly , multiple coreference structures for the same set of coreferring expressions can be mapped to a single coreference chain . As an example , both coreference structures illustrated in ( a ) and ( c ) are cast into the coreference chain illustrated in ( b ) . TEXT TEXT TEXT : Three coreference structures . Given a corpus annotated with coreference data , the data-driven methodology first generates all coreference chains in the data set and then con siders all possible combinations of coreference re lations that would generate the same coreference chains . For a coreference chain of length l with nodes n1 , n2 , ... nl+1 , each node nk ( 1 k l ) can be connected to any of the l - k nodes preceding it . From this observation , we find that a number of 1 x 2 x ... x ( l - k ) ... x l = l ! coreference struc tures can generate the same coreference chain . This result is very important , since it allows for the auto matic generation of coreference data . For each coref erence relation n from an annotated corpus we cre ated a median of ( l- 1 ) ! new coreference relations , where l is the length of the coreference chain contain ing relation n. This observation gave us the possi bility of expanding the test data provided by the coreference keys available in the MUC6 and MUC 7 competitions ( MUC6 1996 ) , ( MUC7 1998 ) . The MUC6 coreference annotated corpus contains 1626 coreference relations , while the MUC7 corpus has 2245 relations . The average length of a coreference chain is 7.21 for the MUC6 data , and 8.57 for the MUC7 data . We were able to expand the number of annotated coreference relations to 6,095,142 for the MUC6 corpus and to 8,269,403 relations for the MUC7 corpus ; this represents an expansion factor of 3,710 . We are not aware of any other automated way of creating coreference annotated data , and we believe that much of the COCKTAIL 's impressive per formance is due to the plethora of data provided by this method . He uri sti cs for 3r d pe rs on pr on ou ns He uri sti cs for no mi na l ref er en ce oH eu ris tic 1 Pr on ou n ( Hl Pr on ) Se ar ch in the sa me se nte nc e for the sa m e 3 r d p e r s o n p r o n o u n P r o n ' i f ( P r o n ' b e l o n g s t o c o r e f e r e n c e c h a i n C C ) a n d t h e r e i s a n e l e m e n t f r o m C C w h i c h i s c l o s e s t t o P r o n i n T e x t , P i c k t h a t e l e m e n t . e l s e P i c k P r o n ' . oH eur isti c 2 Pr on ou n ( H2 Pr on ) Se ar ch for PN , the clo ses t pr op er na me fro m Pr on if ( P N agr ees in nu mb er an d ge nd er wit h Pr on ) i f ( P N b e l o n g s t o c o r e f e r e n c e c h a i n C C ) t h e n P i c k t h e e l e m e n t f r o m C C w h i c h i s c l o s e s t t o P r o n i n T e x t . e l s e P i c k P N . o He uri sti c 3 Pr on ou n ( H3 Pr on ) Se arc h for No un , the clo ses t no un fro m Pr on if ( N ou n agr ees in nu mb er an d ge nd er wit h Pr on ) i f ( N o u n b e l o n g s t o c o r e f e r e n c e c h a i n C C ) a n d t h e r e i s a n e l e m e n t f r o m C C w h i c h i s c l o s e s t t o P r o n i n T e x t , P i c k t h a t e l e m e n t . e l s e P i c k N o u n oH eu ris tic 1 No mi na lC Hl No m ) if ( N ou n is the he ad of an ap po siti ve ) t h e n P i c k t h e p r e c e d i n g N P . e l s e P i c k N o u n ' . oH eu ris tic 3 No mi na l ( H3 No m ) if No un is the he ad of an N P t h e n S e a r c h f o r p r o p e r n a m e P N s u c h t h a t h e a d ( P N ) = N o u n i f ( P N b e l o n g s t o c o r e f e r e n c e c h a i n C C ) a n d t h e r e i s a n e l e m e n t f r o m C C w h i c h i s c l o s e s t t o N o u n i n T e x t , P i c k t h a t e l e m e n t . e l s e P i c k P N . : Best performing heuristics implemented in COCKTAIL 2.2 Knowledge-Poor Coreference . Resolution The result of our data-driven methodology is the set of heuristics implemented in COCKTAIL which cover both nominal and pronoun coreference . Each heuristic represents a pattern of coreference that was mined from the large set of coreference data . COCKTAIL uses knowledge-poor methods because ( a ) it is based only on a limited number of heuristics and ( b ) text processing is limited to part-of-speech tagging , named-entity recognition , and approximate phrasal parsing . The heuristics from COCKTAIL can be classified along two directions . First of all , they can be grouped according to the type of corefer ence they resolve , e.g. , heuristics that resolve the anaphors of reflexive pronouns operate differently than those resolving bare nominals . Currently , in COCKTAIL there are heuristics that resolve five types of pronouns ( personal , possessive , reflexive , demon strative and relative ) and three forms of nominals ( definite , bare and indefinite ) . Secondly , for each type of coreference , there are three classes of heuristics categorized according to their suitability to resolve coreference . The first class is comprised of strong indicators of coreference . This class resulted from the analysis of the distribu tion of the antecedents in the MUC annotated data . For example , repetitions of named entities and ap positives account for the majority of the nominal coreferences , and , therefore , represent anchors for the first class of heuristics . The second class of coreference covers cases in which the arguments are recognized to be seman tically consistent . COCKTAIL 's test of semantic con sistency blends together information available from WordNet and statistics gathered from Treebank . Different consistency checks are modeled for each of the heuristics . Ex a m pl e of th e ap pli ca tio n of he ur ist ic H 2 Pr on M r. A da m s 1 , 69 ye ar s ol d , is th e re tir ed ch ai r m an o f C a n a d ia n b a s e d E m c o L t d . , a m a k e r o f p l u m b i n g a n d p et r o le u m e q u i p m e n t ; h e 1 h a s s e r v e d o n t h e W o o l w o rt h b o a r d si n c e 1 9 8 1 . Ex a m pl e of th e ap pli ca tio n of he ur ist ic H 3 Pr on `` W e ha ve go t to st op po in ti ng ou r fi ng er s at th es e k i d s 2 w h o h a v e n o f u t u r e , `` h e s a i d , `` a n d r e a c h o u r h a n d s o u t t o t h e m 2 . Ex a m pl e of th e ap pli ca tio n of he ur ist ic H 2 N o m T he ch air m an an d th e ch ief ex ec uti ve of fi ce r3 o f W o o l w o r t h C o r p . h a v e t e m p o r a r i l y r e l i n q u i s h e d t h e i r p o s t s w h i l e t h e r e t a i l e r c o n d u c t s i t s i n v e s t i g a t i o n i n t o a l l e g e d a c c o u n t i n g i r r e g u l a r i t i e s 4 . W o o l w o rt h ' s b o a r d n a m e d J o h n W . A d a m s , a n o u ts i d e r , t o s e r v e a s i n t e ri m c h a ir m a n a n d e x e c u ti v e o ff i c e r 3 , w h il e a s p e c i a l c o m m it t e e , a p p o i n t e d b y t h e b o a r d l a st w e e k a n d l e d b y M r. A d a m s , i n v e st i g a t e s t h e al le g e d ir r e g u l a ri ti e s 4 . : Examples of coreference resolution . The same annotated index indicates coreference . The third class of heuristics resolves coreference by coercing nominals . Sometimes coercions involve only derivational morphology - linking verbs with their nominalizations . On other occasions , coercions are obtained as paths of meronyms ( e.g . is-part re lations ) and hypernyms ( e.g . Con sistency checks implemented for this class of coref erence are conservative : either the adjuncts must be identical or the adjunct of the referent must be less specific than the antecedent . lists the top performing heuristics of COCKTAIL for pronominal and nominal coreference . Examples of the heuristics operation on the MUC data are presented presented in Details of the top performing heuris tics of COCKTAIL were reported in ( Harabagiu and Maiorano , 1999 ) . Resolution One of the major drawbacks of existing corefer ence resolution systems is their inability to recog nize many forms of coreference displayed by many real-world texts . Recall measures of current systems range between 36 % and 59 % for both knowledge based and statistical techniques . Knowledge based systems would perform better if more coreference constraints were available whereas statistical meth ods would be improved if more annotated data were available . Since knowledge-based techniques out perform inductive methods , we used high-precision coreference heuristics as knowledge seeds for ma chine learning techniques that operate on large amounts of unlabeled data . One such technique is bootstrapping , which was recently presented in ( Riloff and Jones 1999 ) , ( Jones et al.1999 ) as an ideal framework for text learning tasks that have knowledge seeds . The method does not require large training sets . We extended COCKTAIL by using meta bootstrapping of both new heuristics and clusters of nouns that display semantic consistency for corefer ence . The coreference heuristics are the seeds of our bootstrapping framework for coreference resolution . When applied to large collections of texts , the heuristics determine classes of coreferring expres sions . By generating coreference chains out of all these coreferring expressions , often new heuristics are uncovered . For example , illustrates the application of three heuristics and the generation of data for a new heuristic rule . In COCKTAIL , after a heuristic is applied , a new coreference chain is cal culated . For the example illustrated in , if the reference of expression A is sought , heuristic Hl the FOIL-Gain measure , as introduced by the FOIL inductive algorithm ( CameronJones and Quinlan 1993 ) . Let H0 be the new heuristic and H1 a heuris tic that is already in the seed set . Let Po be the num ber of positive coreference examples of Hnew ( i.e . the number of coreference relations produced by the heuristic that can be found in the test data ) and n0 the number of negative examples of Hnew ( i.e . the number of relations generated by the heuristic which can not be found in the test data ) . Similarly , P1 and n1 are the positive and negative examples of H1 The new heuristics are scored by their FOJL_Gain distance to the existing set of heuristics , and the best scoring one is added to the COCKTAIL system . The FOILGain formula is : FOILGain ( H1 , Ho ) = k ( log2 Pl log2 Po ) P1 + n1 Po+ no where k is the number of positive examples cov ered by both H1 and Ho . Heuristic Ho is added to the seed set if there is no other heuristic providing larger FOILGain to any of the seed heuristics . Since in COCKTAIL , semantic consistency of core ferring expressions is checked by comparing the sim ilarity of noun classes , each new heuristic deter mines the adjustment of the similarity threshold of all known coreferring noun classes . The steps of the bootstrapping algorithm that learns both new heuristics and adjusts the similarity threshold of coreferential expressions is : MUTUAL BOOTSTRAPPING LOOP 1 . Score all candidate heuristics with FOIL- . heuristics ( COCKTAIL ) heuristics ( COCKTAIL ) semantic consistency of core/erring nouns not degrade under minimal performance . indicates expression B to be the antecedent . When the coreference chain is built , expression A is di rectly linked to expression D , thus uncovering a new heuristic HO . As a rule of thumb , we do not consider a new heuristic unless there is massive evidence of its cov erage in the data . To measure the coverage we use ( Riloff and Jones 1999 ) note that the bootstrap ping algorithm works well but its performance can deteriorate rapidly when non coreferring data enter as candidate heuristics . To make the algorithm more robust , a second level of bootstrapping can be intro duced . The outer bootstrapping mechanism , called metabootstrapping compiles the results of the inner ( mutual ) bootstrapping process and identifies the k most reliable heuristics , where k is a number de termined experimentally . These k heuristics are re tained and the rest of them are discarded . To study the performance of a data-driven multi lingual coreference resolution system , we prepared a corpus of Romanian texts by translating the MUC6 and MUC7 coreference training texts . The transla tions were performed by a group of four Romanian native speakers , and were checked for style by a cer tified translator from Romania . In addition , the Ro manian texts were annotated with coreference keys . Two rules were followed when the annotations were done : o1 : Whenever an expression ER represents a trans lation of an expression EE from the corresponding English text , if EE is tagged as a coreference key with identification number ID , then the Romanian expression ER is also tagged with the same ID num ber . This rule allows for translations in which the textual position of the referent and the antecedent have been swapped . o2 : Since the translations often introduce new coreferring expressions in the same chain , the new expressions are given new , unused ID numbers . For example , lists corresponding English and Romanian fragments of coreference chains from the original MUC6 Wall Street Journal document DOCNO : 9307290143. also shows the original MUC coreference SGML annotations . Whenever present , the REF tag indicates the ID of the antecedent , whereas the MIN tag indicates the minimal reference expression . The multilingual coreference resolution method im plemented in SWIZZLE incorporates the heuristics de rived from COKCTAIL 's monolingual coreference res olution processing in both languages . To this end , COCKTAIL required both sets of texts to be tagged for part-of-speech and to recognize the noun phrases . The English texts were parsed with Brill 's part-of speech tagger ( Brill1992 ) and the noun phrases were identified by the grammar rules implemented in the phrasal parser of FASTUS ( Appelt et al. , 1993 ) . Cor responding resources are not available in Romanian . To minimize COCKTAIL 's configuration for process ing Romanian texts , we implemented a Romanian part-of-speech rule-based tagger that used the same Economic adviser Gene Sperling described < COREF ID= '' 29 '' TYPE= '' IDENT '' REF= '' 30 '' > it < /COREF > as `` a true full-court press '' to pass < COREF ID= '' 31 '' TYPE= '' IDENT '' REF= '' 26 '' MIN= '' bill '' > the < COREF ID= '' 32 '' TYPE= '' IDENT '' REF= '' lO '' MIN= '' reduction '' > < COREF ID= '' 33 '' TYPE= '' IDENT '' REF= '' 12 '' > deficit < /COREF > -reduction < /COREF > bill , the final version of which is now being hammered out by < COREF ID= '' 43 '' > House < /COREF > and < COREF ID= '' 41 '' > Senate < JCOREF > negotiators < /COREF > . < COREF ID= '' 34 '' TYPE= '' IDENT '' REF= '' 2 '' > The executives < /COREF > ' backing- however tepid - gives the administration a way to counter < COREF ID= '' 35 '' TYPE= '' IDENT '' REF= '' 36 '' > business < /COREF > critics of < COREF ID= '' 500 '' TYPE= '' IDENT '' REF= '' 31 '' MIN= '' package '' STATUS= '' OPT '' > the overall package < JCOREF > , ... Consilierul cu probleme economice Gene Sperling a descris- < COREF ID= '' 29 '' TYPE= '' IDENT '' REF= '' 30 '' > o < /COREF > cape un efort de avengura menit sa promoveze < COREF ID= '' 1125 '' TYPE= '' IDENT '' REF= '' 26 '' MIN= '' legea '' > legea < JCOREF > pentru < COREF TYPE= '' IDENT '' REF= '' 10 '' MIN= '' reducerea '' > reducerea < /COREF > < COREF ID= '' 33 '' TYPE= '' IDENT '' REF= '' 12 '' > deficitului in bugetul SUA < /COREF > . Versiunea finala a acestei < COREF ID= '' 1126 '' TYPE= '' IDENT '' REF= '' 1125 '' MIN= '' legi '' > legi < JCOREF > este desfiin ata chiar in aceste zile in cadrul dezbaterilor ce au loc in < COREF ID= '' 43 '' > Camera Reprezentativilor < /COREF > i in < COREF ID= '' 41 '' > Senat < /COREF > < /COREF > . Sprijinirea < COREF ID= '' 127 '' TYPE= '' IDENT '' REF= '' ll26 '' MIN= '' legii '' > legii > /COREF > de catre speciali ti ineconomiede i in maniera moderataofera administratiei o modalitate de a contrabalansa criticile aduse < COREF ID= '' 500 '' TYPE= '' IDENT '' REF= '' 31 '' MIN= '' legii '' STATUS= '' OPT '' > legii < /COREF > de catre companiile americane , ... : Example of parallel English and Romanian text annotated for coreference . The elements from a coreference chain in the respective texts are under lined . The English text has only two elements in the coreference chain , whereas the Romanian text con tains four different elements . The two additional ele ments of the Romanian coreference chain are derived due to ( 1 ) the need to translate the relative clause from the English fragment into a separate sentence in Romanian ; and ( 2 ) the reordering of words in the second sentence . 146 tags as generated by the Brill tagger . In addition , we implemented rules that identify noun phrases in Romanian . To take advantage of the aligned corpus , SWIZZLE also relied on bilingual lexical resources that help translate the referential expressions . For this purpose , we used a core Romanian WordNet ( Harabagiu , 1999 ) which encoded , wherever possi ble , links between the English synsets and their Ro manian counterparts . This resource also incorpo rated knowledge derived from several bilingual dic tionaries ( e.g . ( Banta , 1969 ) ) . Having the parallel coreference annotations , we can easily identify their translations because they have the same identification coreference key . Look ing at the example given in , the expres sion `` legii '' , with ID=500 is the translation of the implemented , several other principles were applied . In our experiment , we were satisfied with the qual ity of the translations recognized by following only these two principles . Resolution The SWIZZLE system was run on a corpus of 2335 referential expressions in English ( 927 from MUC 6 and 1408 from MUC7 ) and 2851 Romanian ex pressions ( 1219 from MUC6 and 1632 from MUC 7 ) . Initially , the heuristics implemented in COCKTAIL were applied separately to the two textual collec tions . English Text Romanian Text expression `` package '' , having the same ID in the English text . However , in the test set , the REF fields are intentionally voided , entrusting COCKTAIL to identify the antecedents . The bilingual corefer Reference ' ' - f onre Translation ence resolution performed in SWIZZLE , however , re quires the translations of the English and Romanian antecedents . The principles guiding the translations of the English and Romanian antecedents ( AE-R and ARE , respectively ) are : Circularity : Given an English antecedent , due to semantic ambiguity , it can belong to several English WordNet sysnsets . For each such sysnset Sf we con sider the Romanian corresponding sysnet ( s ) Sf . We filter out all Sf that do not contain AE-R . If only one Romanian sysnset is left , then we identified a translation . Otherwise , we start from the Roma nian antecedent , find all synsets Sf ! to which it be longs , and obtain the corresponding English sysnets Sf . Similarly , all English synsets not containing the English antecedent are filtered out . If only one synset remains , we have again identified a transla tion . Finally , in the last case , the intersection of the multiple synsets in either language generates a legal translation . For example , the English synset sE = { bill , measure } translates into the Romanian synset sR = { lege } . First , none of the dictionary translations of bill into Romanian ( e.g . politif. , hac nota , afi ) translate back into any of the elements of sE . However the translation of measure into the Romanian lege translates back into bill , its synonym . Semantic density : Given an English and a Roma nian antecedent , to establish whether they are trans lations of one another , we disambiguate them by first collapsing all sysnsets that have common elements . Then we apply the circularity principle , relying on the semantic alignment encoded in the Romanian WordNet . When this core lexical database was first : Case 1 of multilingual coreference Case 1 , which is the ideal case , is shown in Fig ure 3 . It occurs when two referential expressions have antecedents that are translations of one an other . This situation occurred in 63.3 % of the refer ential expressions from MUC6 and in 58.7 % of the MUC7 references . Over 50 % of these are pronouns or named entities . However , all the non-ideal cases are more interesting for SWIZZLE , since they port knowledge that enhances system performance . RA H4 R R Translation ER : English reference RR : Romanian reference EA : English antecedent RA : Romanian antecedent ET : English translation RT : Romanian translation of Romanian antecedent of English antecedent : Case 2 of multilingual coreference Case 2 occurs when the antecedents are not trans lations , but belong to or corefer with elements of some coreference chains that were already estab lished . Moreover , one of the antecedents is textually 147 closer to its referent . illustrates the case when the English antecedent is closer to the referent than the Romanian one . SWIZZLE Solutions : ( 1 ) If the heuristic H ( E ) used to resolve the reference in the English text has higher priority than H ( R ) , which was used to resolve the reference from the Romanian text , then we first search for RT , the Romanian translation of EA , the English antecedent . In the next step , we add heuris tic Hl that resolves RR into RT , and give it a higher priority than H ( R ) . Finally , we also add heuristic H2 that links RT to RA when there is at least one trans lation between the elements of the coreference chains containing EA and ET respectively . ( 2 ) If H ( R ) has higher priority than H ( E ) , heuris tic H3 is added while H ( E ) is removed . We also add H4 that relates ER to ET , the English translation of RA . Case 3 occurs when at least one of the antecedents starts a new coreference chain ( i.e. , no coreferring antecedent can be found in the current chains ) . SWIZZLE Solution : If one of the antecedents corefers with an element from a coreference chain , then the antecedent in the opposite language is its translation . Otherwise , SWIZZLE chooses the an tecedent returned by the heuristic with highest pri ority . The foremost contribution of SWIZZLE was that it improved coreference resolution over both English and Romanian texts when compared to monolingual coreference resolution performance in terms of preci sion and recall . Also relevant was the contribution of SWIZZLE to the process of understanding the cultural differences expressed in language and the way these differences influence coreference resolution . Because we do not have sufficient space to discuss this issue in detail here , let us state , in short , that English is more economical than Romanian in terms of referen tial expressions . However the referential expressions in Romanian contribute to the resolution of some of the most difficult forms of coreference in English . summarizes the precision results for both English and Romanian coreference . The results in dicate that the English coreference is more pre cise than the Romanian coreference , but SWIZZLE improves coreference resolution in both languages . There were 64 % cases when the English coreference was resolved by a heuristic with higher priority than the corresponding heuristic for the Romanian coun terpart . This result explains why there is better pre cision enhancement for the English coreference . N o mi na l Pr on om ina l To tal E ng lis h 7 3 % 8 9 % 84 % R o m an ia n 6 6 % 7 8 % 72 % S W IZ Z L E on E ng lis h 7 6 % 9 3 % 87 % S W IZ Z L E on R o m an ia n 7 1 % 8 2 % 76 % : Coreference precision N o mi na l Pr on o mi nal To tal E ng lis h 6 9 % 8 9 78 % R o m an ia n 6 3 % 8 3 % 72 % S W IZ Z L E on E ng lis h 6 6 % 8 7 % 77 % S W IZ Z L E on R o m an ia n 6 1 % 8 0 % 70 % : Coreference recall also illustrates the recall results . The advantage of the data-driven coreference resolution over other methods is based on its better recall per formance . This is explained by the fact that this method captures a larger variety of coreference pat terns . Even though other coreference resolution sys tems perform better for some specific forms of refer ence , their recall results are surpassed by the data driven approach . Multilingual coreference in turn improves more the precision than the recall of the monolingual data-driven coreference systems . In addition , shows that the English coref erence results in better recall than Romanian coref erence . However , the recall shows a decrease for both languages for SWIZZLE because imprecise coreference links are deleted . As is usually the case , deleting data lowers the recall . All results were obtained by using the automatic scorer program developed for the MUC evaluations . We have introduced a new data-driven method for multilingual coreference resolution , implemented in the SWIZZLE system . The results of this method are encouraging since they show clear improvements over monolingual coreference resolution . Currently , we are also considering the effects of a bootstrap ping algorithm for multilingual coreference resolu tion . Through this procedure we would learn con currently semantic consistency knowledge and bet ter performing heuristic rules . To be able to de velop such a learning approach , we must first develop a method for automatic recognition of multilingual referential expressions . 148 We also believe that a better performance evalu ation of SWIZZLE can be achieved by measuring its impact on several complex applications . We intend to analyze the performance of SWIZZLE when it is used as a module in an IE system , and separately in a Question/ Answering system . Acknowledgements This paper is dedicated to the memory of our friend Megumi Kameyama , who in spired this work . We present a novel approach to automatic metaphor identification , that discovers both metaphorical associations and metaphorical expressions in unrestricted text . Our system first performs hierarchical graph factorization clustering ( HGFC ) of nouns and then searches the resulting graph for metaphorical connections between concepts . It then makes use of the salient features of the metaphorically connected clusters to identify the actual metaphorical expressions . In contrast to previous work , our method is fully unsupervised . Despite this fact , it operates with an encouraging precision ( 0.69 ) and recall ( 0.61 ) . Our approach is also the first one in NLP to exploit the cognitive findings on the differences in or- ganisation of abstract and concrete concepts in the human brain . Metaphor has traditionally been viewed as a form of linguistic creativity , that gives our expression more vividness , distinction and artistism . While this is true on the surface , the mechanisms of metaphor have a much deeper origin in our reasoning . Today metaphor is widely understood as a cognitive phenomenon operating at the level of mental processes , whereby one concept or domain is systematically viewed in terms of the properties of another ( Lakoff and Johnson , 1980 ) . Consider the examples ( 1 ) He shot down all of my arguments and ( 2 ) He attacked every weak point in my argument . They demonstrate a metaphorical mapping of the concept of argument to that of war . The argument , which is the target concept , is viewed in terms of a battle ( or a war ) , the source concept . The existence of such a link allows us to systematically describe arguments using the war terminology , thus leading to a number of metaphorical expressions . Lakoff and Johnson call such generalisations a source target domain mapping , or conceptual metaphor . The ubiquity of metaphor in language has been established in a number of corpus studies ( Cameron , 2003 ; Martin , 2006 ; Steen et al. , 2010 ; Shutova and Teufel , 2010 ) and the role it plays in human reasoning has been confirmed in psychological experiments ( Thibodeau and Boroditsky , 2011 ) . This makes metaphor an important research area for computational and cognitive linguistics , and its automatic processing indispensable for any semantics- oriented NLP application . The problem of metaphor modeling is gaining interest within NLP , with a growing number of approaches exploiting statistical techniques ( Mason , 2004 ; Gedigian et al. , 2006 ; Shutova , 2010 ; Shutova et al. , 2010 ; Turney et al. , 2011 ; Shutova et al. , 2012 ) . Compared to more traditional approaches based on hand-coded knowledge ( Fass , 1991 ; Martin , 1990 ; Narayanan , 1997 ; Narayanan , 1999 ; Feldman and Narayanan , 2004 ; Barnden and Lee , 2002 ; Agerri et al. , 2007 ) , these more recent methods tend to have a wider coverage , as well as be more efficient , accurate and robust . However , even the statistical metaphor processing approaches so far often focused on a limited domain or a subset of phenomena ( Gedigian et al. , 2006 ; Krishnakumaran and Zhu , 2007 ) , and only addressed one of the metaphor processing sub- tasks : identification of metaphorical mappings ( Mason , 2004 ) or identification of metaphorical expressions ( Shutova et al. , 2010 ; Turney et al. , 2011 ) . In this paper , we present the first computational method 978 Proceedings of NAACLHLT 2013 , pages 978 988 , Atlanta , Georgia , 9 14 June 2013 . Qc 2013 Association for Computational Linguistics that identifies the generalisations that govern the production of metaphorical expressions , i.e . conceptual metaphors , and then uses these generalisations to identify metaphorical expressions in unrestricted text . As opposed to previous works on statistical metaphor processing that were supervised or semi-supervised , and thus required training data , our method is fully unsupervised . It relies on building a hierarchical graph of concepts connected by their association strength ( using hierarchical clustering ) and then searching for metaphorical links in this graph . ( 2010 ) introduced the hypothesis of clustering by association and claimed that in the course of distributional noun clustering , abstract concepts tend to cluster together if they are associated with the same source domain , while concrete concepts cluster by meaning similarity . We share this intuition , but take this idea a significant step further . Our approach is theoretically grounded in the cognitive science findings suggesting that abstract and concrete concepts are organised differently in the human brain ( Crutch and Warrington , 2005 ; Binder et al. , 2005 ; WiemerHastings and Xu , 2005 ; Huang et al. , 2010 ; Crutch and Warring- ton , 2010 ; Adorni and Proverbio , 2012 ) . According to Crutch and Warrington ( 2005 ) , these differences emerge from their general patterns of relation with other concepts . However , most NLP systems to date treat abstract and concrete concepts as identical . In contrast , we incorporate this distinction into our model by creating a network ( or a graph ) of concepts , and automatically learning the different patterns of association of abstract and concrete concepts with other concepts . We expect that , while concrete concepts would tend to naturally organise into a tree-like structure ( with more specific terms descending from the more general terms ) , abstract concepts would exhibit a more complex pattern of associations . One can see from this graph that if concrete concepts , such as bike or engine tend to be connected to only one concept at the higher level in the hierarchy ( mechanism ) , abstract concepts may have multiple higher-level associates : the literal ones and the metaphorical ones . For ex ample , the abstract concept of democracy is literally associated with a more general concept of political system , as well as metaphorically associated with the concept of mechanism . Such multiple associations are due to the fact that political systems are metaphorically viewed as mechanisms , they can function , break , they can be oiled etc . We often discuss them using mechanism terminology , and thus a corpus-based distributional learning approach would learn that they share features with political systems ( from their literal uses ) , as well as with mechanisms ( from their metaphorical uses , as shown next to the respective graph edges in the . Our system discovers such association patterns within the graph and uses them to identify metaphorical connections between the concepts . To the best of our knowledge , our method is the first one to use a hierarchical clustering model for the metaphor processing task . The original graph of concepts is built using hierarchical graph factorization clustering ( HGFC ) ( Yu et al. , 2006 ) of nouns , yielding a network of clusters with different levels of generality . The weights on the edges of the graph indicate association between the clusters ( concepts ) . HGFC has not been previously employed for noun clustering in NLP , but showed successful results in the verb clustering task ( Sun and Korhonen , 2011 ) . In summary , our system ( 1 ) builds a graph of concepts using HGFC , ( 2 ) traverses it to find metaphorical associations between clusters using weights on the edges of the graph , ( 3 ) generates lists of salient features for the metaphorically connected clusters and ( 4 ) searches the British National Corpus ( BNC ) ( Burnard , 2007 ) for metaphorical expressions describing the target domain concepts using the verbs from the set of salient features . We evaluated the performance of the system with the aid of human judges in precision- and recall-oriented settings . In addition , we compared its performance to that of two baselines , an unsupervised baseline using agglomerative clustering ( AGG ) and a supervised baseline built upon WordNet ( Fellbaum , 1998 ) ( WN ) . 2.1 Dataset and Feature Extraction . Our noun dataset used for clustering contains 2000 most frequent nouns in the BNC ( Burnard , 2007 ) . We extracted the features from the Gigaword corpus ( Graff et al. , 2003 ) , which was first parsed using the RASP parser ( Briscoe et al. , 2006 ) . The verb lemmas in VERB SUBJECT , VERB DIRECT OBJECT and VERB INDIRECT OBJECT relations with the nouns in the dataset were then extracted from the GR output of the parser . The feature values were the relative frequencies of the features . Clustering The most widely used method for hierarchical word clustering is AGG ( Schulte im Walde and Brew , 2001 ; Stevenson and Joanis , 2003 ; Ferrer , 2004 ; Devereux and Costello , 2005 ) . The method treats each word as a singleton cluster and then successively merges two closest clusters until all the clusters have been merged into one . The cluster similarity is measured using linkage criteria ( e.g . Ward ( 1963 ) measures the decrease in variance for the clusters being merged ) . As opposed to this , HGFC derives probabilistic bipartite graphs from the similarity matrix ( Yu et al. , 2006 ) . Since we require a graph of concepts , our task is rather different from standard hierarchical word clustering that produces a tree of concepts . In a tree , each word can only erarchical clustering tasks ( Yu et al. , 2006 ; Sun and Korhonen , 2011 ) , but its hierarchical graph output is also a more suitable representation of the concept graph . In addition , HGFC can detect the number of levels and the number of clusters on each level of the hierarchical graph automatically . This is essential for our task as these settings are difficult to pre- define for a general-purpose concept graph . Given a set of nouns , V = { vn } N , the similarity matrix for HGFC is constructed using Jensen Shannon Divergence . The graph G and the cluster structure can be represented by a bipartite graph K ( V , U ) . V are the vertices on G. U = { up mrepresent the hidden m clusters . The matrix B denotes then m adjacency matrix , with bip being the connec tion weight between the vertex vi and the cluster up . Thus , B represents the connections between clusters at an upper and lower level of clustering . A flat clustering algorithm can be induced by assigning a lower level node to the parent node that has the largest connection weight . The number of clusters at any level can be determined by only counting the number of nonempty nodes ( namely the nodes that have at least one lower level node associated ) . The bipartite graph K also induces a similarityhave a unique parent cluster at each level . Our con ( W t ) between vi and vj : wt m bip bjp ij = 2 : p=1 p = cept graph does not have this constraint : at any level a word can be associated with an arbitrary number of parent clusters . Therefore , not only HGFC outperformed agglomerative clustering methods in hi ( B 1BT ) ij where = diag ( 1 , ... , m ) . Therefore , B can be found by minimizing the divergence distance ( ) between the similarity matrices W and W t : v 1 u 1 2 v v v v 1 2 1 2 1 v v v v v 3 7 v u v 8 v 8 5 2 9 9 v 1 v 2 u 1 v u u 3 q 1 2 v 1 4 u 5 2 v 2 6 6 6 v 3 6 v v 5 v 5 7 4 7 4 u 3 u 2 v 3 v 3 8 8 v v 9 9 ( a ) ( b ) ( c ) ( d ) ( e ) : ( a ) An undirected graph G representing the similarity matrix ; ( b ) The bipartite graph showing three clusters on G ; ( c ) The induced clusters U ; ( d ) The new graph G1 over clusters U ; ( e ) The new bipartite graph over G1 n min ( W , H H T ) , s.t . hip = 1 ( 1 ) H , i=1 value of m0 to 800 . For the subsequent levels , ml is set to the number of nonempty clusters ( bipartite H = B 1 ; ( X , Y ) = ( xij log ij xij yij xij + yij ) graph nodes ) on the parent level . The matrices B and are initialized randomly . We found that the Yu et al . ( 2006 ) showed that this cost function is non-increasing under the update rule : wij actual initialization values have little impact on the final result . The rows in B are normalized after the initialization so the values in each row add up to one . For a word vi , the probability of assigning it to clus h ip hip j ( H H T ) ij p hjp s.t . h ip = 1 ( 2 ) i ter x ( l ) X at level l is given by : p p wij ( H H T ) ij hip hjp s.t . p = wij ( 3 ) p ( x ( l ) |vi ) = ... p ( x ( l ) |x ( l 1 ) ) ... p ( x ( 1 ) |vi ) j p ij p Xl 1 p x ( 1 ) X1 The cost function can thus be optimized by updating h and alternately . The similarity between clusters p ( up , uq ) can be induced from B , as follows : p ( up , uq ) = p ( up ) p ( up |uq ) = ( BT D 1 B ) pq ( 4 ) m D = diag ( d1 , ... , dn ) where di = bip p=0 We can then construct a new graph G1 ( ( d ) ) with the clusters U as vertices , and the cluster similarity p as the connection weight . The clustering algorithm can now be applied again ( e ) ) . This process can go on iteratively , leading to a hierarchical graph . The number of levels ( L ) and the number of clusters ( ml ) are detected automatically , using the method of Sun and Korhonen ( 2011 ) . Clustering starts with an initial setting of number of clusters ( m0 ) for the first level . In our experiment , we set the = ( D ( 1 ) 1 1 1 B1 D2 B2 ... Dl Bl ) ip ( 5 ) Due to the random walk property of the graph , ml is non-increasing for higher levels ( Sun and Korhonen , 2011 ) . The algorithm can thus terminate when all nouns are assigned to one cluster . We run 1000 iterations of updates of h and ) for each two adjacent levels . The resulting graph is composed of a set of bipartite graphs defined by Bl , Bl 1 , ... , B1 . A bipartite graph has a similar structure as in For a given noun , we can rank the clusters at any level according to the soft assignment probability ( eq . The clusters that have no member noun were hidden from the ranking since they do not explicitly represent any concept . However , these clusters are still part of the organisation of conceptual space within the model and they contribute to the probability for the clusters on upper levels ( eq . We call the view of the hierarchical graph where these empty clusters are hidden an explicit graph . The whole algorithm can be summarized as follows : SOURCE : fire TARGET 1 : sense hatred emotion passion enthusiasm sentiment hope interest feeling resentment optimism Require : N nouns V , initial number of clusters m1 Compute the similarity matrix W0 from V Build the graph G0 from W0 , l 1 while ml > 1 do Factorize Gl 1 to obtain bipartite graph Kl with the adjacency matrix Bl ( eq . 1 , 2 and 3 ) Build a graph Gl with similarity matrix Wl = hostility excitement anger TARGET 2 : coup violence fight resistance clash rebellion battle drive fighting riot revolt war confrontation volcano row revolution struggle TARGET 3 : alien immigrant TARGET 4 : prisoner hostage inmate BT 1 l Dl Bl according to l l + 1 ; ml No . 5 ) end while return Bl , Bl 1 ... B1 2.3 Identification of Metaphorical Associations . Once we obtained the explicit graph of concepts , we can now identify metaphorical associations based on the weights connecting the clusters at different levels ( eq . Taking a single noun ( e.g . fire ) as input , the system computes the probability of its cluster membership for each cluster at each level , using these weights . We expect the cluster membership probabilities to indicate the level of association of the input noun with the clusters . The system can then rank the clusters at each level based on these probabilities . We chose level 3 as the optimal level of generality for our experiments , based on our qualitative analysis of the graph . The system selects 6 top- ranked clusters from this level ( we expect an average source concept to have no more than 5 typical target associates ) and excludes the literal cluster containing the input concept ( e.g . The remaining clusters represent the target concepts associated with the input source concept . Example output for the input concepts of fire and disease is shown in One can see from the that each of the noun-to-cluster mappings represents a new conceptual metaphor , e.g . EMOTION is FIRE , VIOLENCE is FIRE , CRIME is a DISEASE etc . These mappings are exemplified in language by a number of metaphorical expressions ( e.g . His anger will burn him , violence flared again , it s time they found a cure for corruption ) . 2.4 Identification of Salient Features and . Metaphorical Expressions After extracting the source target domain mappings , we now move on to the identification of the cor SOURCE : disease TARGET 1 : fraud outbreak offense connection leak count crime violation abuse conspiracy corruption terrorism suicide TARGET 2 : opponent critic rival TARGET 3 : execution destruction signing TARGET 4 : refusal absence fact failure lack delay Discovered metaphorical associations rage-ncsubj engulf -ncsubj erupt-ncsubj burn-ncsubj light-dobj consume-ncsubj flare-ncsubj sweep-ncsubj spark-dobj battle-dobj gut-idobj smolder-ncsubj ignite-dobj destroy-idobj spread-ncsubj damage-idobj light-ncsubj ravage-ncsubj crackle-ncsubj open-dobj fuel-dobj spray-idobj roar-ncsubj perish-idobj destroy- ncsubj wound-idobj start-dobj ignite-ncsubj injure- idobj fight-dobj rock-ncsubj retaliate-idobj devastate- idobj blaze-ncsubj ravage-idobj rip-ncsubj burn-idobj spark-ncsubj warm-idobj suppress-dobj rekindle-dobj : Salient features for fire and the violence cluster responding metaphorical expressions . The system does this by harvesting the salient features that lead to the input noun being strongly associated with the extracted clusters . The salient features are selected by ranking the features according to the joint probability of the feature ( f ) occurring both with the input noun ( w ) and the cluster ( c ) . Under a simplified independence assumption , p ( w , c|f ) = p ( w|f ) p ( c|f ) . p ( w|f ) and p ( c|f ) are calculated as the ra tio of the frequency of the feature f to the total frequency of the input noun and the cluster respectively . The features ranked higher are expected to represent the source domain vocabulary that can be used to metaphorically describe the target concepts . We selected the top 50 features from the ranked list . Example features ( verbs and their grammatical relations ) extracted for the source domain noun fire and the violence cluster are shown in We then refined the lists of features by means of selectional preference ( SP ) filtering . We use SPs to FEELING IS FIRE hope lit ( Subj ) , anger blazed ( Subj ) , optimism raged ( Subj ) , enthusiasm engulfed them ( Subj ) , hatred flared ( Subj ) , passion flared ( Subj ) , interest lit ( Subj ) , fuel resentment ( Dobj ) , anger crackled ( Subj ) , feelings roared ( Subj ) , hostility blazed ( Subj ) , light with hope ( Iobj ) CRIME IS A DISEASE cure crime ( Dobj ) , abuse transmitted ( Subj ) , eradicate terrorism ( Dobj ) , suffer from corruption ( Iobj ) , diagnose abuse ( Dobj ) , combat fraud ( Dobj ) , cope with crime ( Iobj ) , cure abuse ( Dobj ) , eradicate corruption Identified metaphorical expressions for the mappings FEELING IS FIRE and CRIME IS A DISEASE quantify how well the extracted features describe the source domain ( e.g . We extracted nominal argument distributions of the verbs in our feature lists for VERB SUBJECT , VERB DIRECT OBJECT and VERB INDIRECT OBJECT relations . We used the algorithm of Sun and Korhonen ( 2009 ) to create SP classes and the measure of Resnik ( 1993 ) to quantify how well a particular argument class fits the verb . Resnik measures selectional preference strength SR ( v ) of a predicate as a KullbackLeibler distance between two distributions : the prior probability of the noun class P ( c ) and the posterior probability of the noun class given the verb P ( c|v ) . SR ( v ) = P ( c|v ) 3 Evaluation and Discussion . AGG : the agglomerative clustering baseline is constructed using SciPy implementation ( Oliphant , 2007 ) of Ward s linkage method ( Ward , 1963 ) . The output tree is cut according to the number of levels and the number of clusters of the explicit graph detected by HGFC . The resulting tree is converted into a graph by adding connections from each cluster to all the clusters one level above . The connection weight ( the cluster distance ) is measured using JensenShannon Divergence between the cluster centroids . This graph is used in place of the HGFC graph in the metaphor identification experiments . WN : in the WN baseline , the WordNet hierarchy is used as the underlying graph of concepts , to which the metaphor extraction method is applied . Given a source concept , the system extracts all its sense 1 hypernyms two levels above and subsequently all of their sister terms . The hypernyms themselves are considered to represent the literal sense of the source noun and are , therefore , removed . The sister terms are kept as potential target domains . 3.2 Evaluation of Metaphorical Associations . To create our dataset , we extracted 10 common D ( P ( c|v ) ||P ( c ) ) = 2 : c P ( c|v ) log P ( c ) . In order source concepts that map to multiple targets from to quantify how well a particular argument class fits the verb , Resnik defines selectional association as AR ( v , c ) = 1 P ( c|v ) log P ( c|v ) . We rank the the Master Metaphor List ( Lakoff et al. , 1991 ) and linguistic analyses of metaphor ( Lakoff and Johnson , 1980 ; Shutova and Teufel , 2010 ) . These SR ( v ) P ( c ) nominal arguments of the verbs in our feature lists using their selectional association with the verb , and then only retain the features whose top 5 arguments contain the source concept . For example , the verb start , that is a common feature for both fire and the violence cluster ( e.g . start a war , start a fire ) , would be filtered out in this way , whereas the verbs flare or blaze would be retained as descriptive source domain vocabulary . We then search the RASP-parsed BNC for grammatical relations , in which the nouns from the target domain cluster appear with the verbs from the source domain vocabulary ( e.g . war blazed ( subj ) , to fuel violence ( dobj ) for the mapping VIOLENCE is FIRE ) . The system thus annotates metaphorical expressions in text , as well as the corresponding conceptual metaphors , as shown in included FIRE , CHILD , SPEED , WAR , DISEASE , BREAKDOWN , CONSTRUCTION , VEHICLE , SYSTEM , BUSINESS . Each of the three systems identified 50 source target domain mappings for the given source domains , resulting in a set of 150 conceptual metaphors ( each representing a number of submappings since all the target concepts are clusters or synsets ) . These were then evaluated against human judgements in two different experimental settings . Setting 1 : The judges were presented with a set of conceptual metaphors identified by the three systems , randomized . They were asked to annotate the mappings they considered valid . In all our experiments , the judges were encouraged to rely on their own intuition of metaphor , but they also reviewed the metaphor annotation guidelines of Shutova and Teufel ( 2010 ) . Two independent judges , both na tive speakers of English , participated in this experiment . Their agreement on the task was = 0.60 ( n = 2 , N = 150 , k = 2 ) ( Siegel and Castellan , 1988 ) . The main differences in the annotators judgements stem from the fact that some metaphorical associations are less obvious and common than others , and thus need more context ( or imaginative effort ) to establish . Such examples , where the judges disagreed included metaphorical mappings such as INTENSITY is SPEED , GOAL is a CHILD , COLLECTION is a SYSTEM , ILLNESS is a BREAKDOWN . The system performance was then evaluated against these judgements in terms of precision ( P ) , i.e . the proportion of the valid metaphorical mappings among those identified . We calculated system precision ( in all experiments ) as an average over both annotations . HGFC operates with a precision of P = 0.69 , whereas the baselines attain P = 0.36 ( AGG ) and P = 0.29 ( WN ) . The precision of an- notator judgements against each other ( the human ceiling ) is P = 0.80 , suggesting that this is a challenging task . Setting 2 : To measure recall , R , of the systems we asked two annotators ( both native speakers with a background in metaphor , different from Setting 1 ) to write down up to 5 target concepts they strongly associated with each of the 10 source concepts . Their annotations were then aggregated into a single metaphor association gold standard , consisting of 63 mappings in total . The recall of the systems was measured against this gold standard , resulting in HGFC R = 0.61 , AGG R = 0.11 and WN R = 0.03 . As expected , HGFC outperforms both AGG and WN baselines in both settings . AGG has been previously shown to be less accurate than HGFC in the verb clustering task ( Sun and Korhonen , 2011 ) . Our analysis of the noun clusters indicated that HGFC tends to produce more pure and complete clusters than AGG . Another important reason AGG fails is that it by definition organises all concepts into tree and optimises its solution locally , taking into account a small number of clusters at a time . However , being able to discover connections between more distant domains and optimising globally over all concepts is crucial for metaphor identification . This makes AGG less suitable for the task , as demonstrated by our results . However , AGG identified a number of interesting mappings missed by HGFC , e.g . CAREER IS A CHILD , LANGUAGE IS A SYSTEM , CORRUPTION IS A VEHICLE , EMPIRE IS A CONSTRUCTION , as well as a number of mappings in common with HGFC , e.g . DEBATE IS A WAR , DESTRUCTION IS A DISEASE . The WN system also identified a few interesting metaphorical mappings ( e.g . COGNITION IS FIRE , EDUCATION IS CONSTRUCTION ) , but its output is largely dominated by the concepts similar to the source noun and contains some unrelated concepts . The comparison of HGFC to WN shows that HGFC identifies meaningful properties and relations of abstract concepts that can not be captured in a tree-like classification ( even an accurate , manually created one ) . The latter is more appropriate for concrete concepts , and a more flexible representation is needed to model abstract concepts . The fact that both baselines identified some valid metaphorical associations , relying on less suitable conceptual graphs , suggests that our way of traversing the graph is a viable approach in principle . HGFC identifies valid metaphorical associations for a range of source concepts . On of them ( CRIME IS A VIRUS ) happened to have been already validated in psychological experiments ( Thibodeau and Boroditsky , 2011 ) . The most frequent type of error of HGFC is the presence of target clusters similar or closely related to the source noun ( e.g . the parent cluster for child ) . The clusters from the same domain can , however , be filtered out if their nouns frequently occur in the same documents with the source noun ( in a large corpus ) , i.e . The latter is less likely for the metaphorically connected nouns . We intend to implement this improvement in the future version of the system . 3.3 Evaluation of Metaphorical Expressions . For each of the identified conceptual metaphors , the three systems extracted a number of metaphorical expressions from the corpus ( average of 430 for HGFC , 148 for AGG , and 855 for WN ) . The expressions were also evaluated against human judgements . The judges were presented with a set of randomly sampled sentences containing metaphorical expressions as annotated by the system and by the baselines ( 200 each ) , randomized . They were asked to mark the tagged expressions that were metaphorical in their judgement as correct . Their agreement on the task was = 0.56 ( n = 2 , N = 600 , k = 2 ) , HLJ 26 [ .. ] effective action was needed to eradicate terrorism , drug-trafficking and corruption . EG0 275 In the 1930s the words means test was a curse , fuelling the resistance against it both among the unemployed and some of its administrators . CRX 1054 [ .. ] if the rehabilitative approach were demonstrably successful in curing crime . HL3 1206 [ .. ] he would strive to accelerate progress towards the economic integration of the Caribbean . HXJ 121 [ .. ] it is likely that some industries will flourish in certain countries as the market widens . The system performance against these annotations is P = 0.65 ( HGFC ) , P = 0.47 ( AGG ) and P = 0.12 ( WN ) . The human ceiling for this task was measured at P = 0.79 . The performance of our unsupervised approach is close to the previous supervised systems of Mason ( 2004 ) ( accuracy of 0.73 ) and Shutova et al . ( 2010 ) ( precision of 0.79 ) , however , the results are not directly comparable due to different experimental settings . The system errors in this task stem from multiple word senses of the salient features or the source and target sharing some physical properties ( e.g . one can die from crime and die from a disease ) . Some identified expressions invoke a chain of mappings ( e.g . ABUSE IS A DISEASE , DISEASE IS AN ENEMY for combat abuse ) , however , such chains are not yet incorporated into the system . The performance of AGG is higher than in the mappings identification task , since it outputs only few expressions for the incorrect mappings . In contrast , WN tagged a large number of literal expressions due to the incorrect prior identification of the underlying associations . Since there is no large metaphor-annotated corpus available , it was impossible for us to reliably evaluate the recall of metaphorical expressions . However , we estimated it as a recall of salient features . We manually compiled sets of typical features for the 10 source domains , and measured their recall among the top 50 HGFC features at R = 0.70 . However , in practice the coverage in this task would directly depend on that of the metaphorical associations . One of the first attempts to identify and interpret metaphorical expressions in text is the met* system of Fass ( 1991 ) , that utilizes hand-coded knowledge and detects non-literalness via selectional preference violation . In case of a violation , the respective phrase is first tested for being metonymic using hand-coded patterns ( e.g . If this fails , the system searches the knowledge base for a relevant analogy in order to discriminate metaphorical relations from anomalous ones . The system of Krishnakumaran and Zhu ( 2007 ) uses WordNet ( the hyponymy relation ) and word bigram counts to predict verbal , nominal and adjectival metaphors at the sentence level . The authors discriminate between conventional metaphors ( included in WordNet ) and novel metaphors . Birke and Sarkar ( 2006 ) present a sentence clustering approach that employs a set of seed sentences annotated for literalness and computes similarity between the new input sentence and all of the seed sentences . The system then tags the sentence as literal or metaphorical according to the annotation in the most similar seeds , attaining an f-score of 53.8 % . The first system to discover source target domain mappings automatically is CorMet ( Mason , 2004 ) . It does this by searching for systematic variations in domain-specific verb selectional preferences . For example , pour is a characteristic verb in both LAB and FINANCE domains . In the LAB domain it has a strong preference for liquids and in the FINANCE domain for money . From this the system infers the domain mapping FINANCE LAB and the concept mapping money liquid . ( 2006 ) trained a maximum entropy classifier to discriminate between literal and metaphorical use . They annotated the sentences from PropBank ( Kingsbury and Palmer , 2002 ) containing the verbs of MOTION and CURE for metaphoricity . They used PropBank annotation ( arguments and their semantic types ) as features for classification and report an accuracy of 95.12 % ( however , against a majority baseline of 92.90 % ) . The metaphor identification system of Shutova et al . ( 2010 ) starts from a small seed set of metaphorical expressions , learns the analogies involved in their production and extends the set of analogies by means of verb and noun clustering . As a result , the system can recognize new metaphorical expressions in unrestricted text ( e.g . from the seed stir excitement it infers that swallow anger is also a metaphor ) , achieving a precision of 79 % . ( 2011 ) classify verbs and adjectives as literal or metaphorical based on their level of concreteness or abstractness in relation to a noun they appear with . They learn concreteness rankings for words automatically ( starting from a set of examples ) and then search for expressions where a concrete adjective or verb is used with an abstract noun ( e.g . dark humour is tagged as a metaphor and dark hair is not ) . They report an accuracy of 73 % . Previous research on metaphor addressed a number of different aspects of the phenomenon , and has shown that these aspects can be successfully modeled using statistical techniques . However , the methods often focused on a limited domain and needed manually-labeled training data . This made them difficult to apply in a real-world setting with the goal of improving semantic interpretation in NLP at large . Our method takes a step towards this direction . It is fully unsupervised , and thus more robust , and can perform accurate metaphor identification in unrestricted text . It identifies metaphor with a precision of 69 % and a recall of 61 % , which is a very encouraging result for an unsupervised method . We believe that this work has important implications for computational and cognitive modeling of metaphor , but is also applicable to a range of other semantic tasks within NLP . Integrating different representations of abstract and concrete concepts into NLP systems may improve their performance , as well as make the models more cognitively plausible . One of our key future research objectives is to investigate the use and adaptation of the created conceptual graph to perform metaphor interpretation . In addition , we plan to extend this work to cover nominal and adjectival metaphors , by harvesting salient nominal and adjectival features . This work was funded by the MetaNet project ( grant number W911NF12-C-0022 ) and the Dorothy Hodgkin Postgraduate Award . Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags . It is based on three ideas : ( 1 ) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities , ( 2 ) estimation of the contextual probabilities with decision trees , and ( 3 ) use of high-order HMMs . In experiments on German and Czech data , our tagger outperformed state- of-the-art POS taggers . A Hidden-Markov-Model part-of-speech tagger ( Brants , 2000 , e.g . ) computes the most probable POS tag sequence tˆN = tˆ1 , ... , tˆN for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags . Tagsets of this size contain little or no information about number , gender , case and similar morphosyntac- tic features . For languages with a rich morphology such as German or Czech , more fine-grained tagsets are often considered more appropriate . The additional information may also help to disambiguate the ( base ) part of speech . Without gender information , for instance , it is difficult for a tagger to correctly disambiguate the German sentence Ist das Realita¨ t ? ( Is that reality ? ) . The word das is ambiguous between an article and a demonstrative . Because of the lack of gender agreement between das ( neuter ) and the noun Realita¨ t ( feminine ) , the article reading must be wrong . The German Tiger treebank ( Brants et al. , 2002 ) is an example of a corpus with a more fine-grained tagset ( over 700 tags overall ) . Large tagsets aggravate sparse data problems . As an example , take the German sentence Das zu versteuernde Einkommen sinkt ( “ The to be taxed income decreases ” ; The tˆN N N 1 = arg max p ( t1 , w1 ) 1 N taxable income decreases ) . Das ART.Def.Nom.Sg.Neut zu PART.Zu versteuernde ADJA.Pos.Nom.Sg.Neut Einkommen N.Reg.Nom.Sg.Neut p ( tN , wN ) = n 1 1 i=1 p ( ti|ti−1 ) i−k p ( wi|ti ) le . ( 1 ) context prob . xical prob HMM taggers are fast and were successfully applied to a wide range of languages and training corpora . Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license ( http : //creativecommons.org/licenses/by-nc-sa/3.0/ ) . Unfortunately , the POS trigram consisting of the tags of the first three words does not occur in the Tiger corpus . ( Neither does the pair consisting of the first two tags . ) The unsmoothed 777 Proceedings of the 22nd International Conference on Computational Linguistics ( Coling 2008 ) , pages 777–784 Manchester , August 2008 context probability of the third POS tag is therefore 0 . If the probability is smoothed with the backoff distribution p ( •|P ART .Z u ) , the most probable tag is ADJA.Pos.Acc.Sg.Fem rather than ADJA.Pos.Nom.Sg.Neut . Thus , the agreement between the article and the adjective is not checked anymore . A closer inspection of the Tiger corpus reveals that it actually contains all the information needed to completely disambiguate each component of the POS tag ADJA.Pos.Nom.Sg.Neut : • All words appearing after an article ( ART ) and the infinitive particle zu ( PART.zu ) are attributive adjectives ( ADJA ) ( 10 of 10 cases ) . • All adjectives appearing after an article and a particle ( PART ) have the degree positive ( Pos ) ( 39 of 39 cases ) . • All adjectives appearing after a nominative article and a particle have nominative case ( 11 of 11 cases ) . • All adjectives appearing after a singular article and a particle are singular ( 32 of 32 cases ) . • All adjectives appearing after a neuter article and a particle are neuter ( 4 of 4 cases ) . Hence the context probability of the whole tag is . Without having observed the given context , it is possible to deduce that the observed POS tag is the only possible tag in this context . These considerations motivate an HMM tagging approach which decomposes the POS tags into a set of simple attributes , and uses decision trees to estimate the probability of each attribute . Decision trees are ideal for this task because the identification of relevant attribute combinations is at the heart of this method . The backoff smoothing methods of traditional n-gram POS taggers require an ordering of the reduced contexts which is not available , here . Discriminatively trained taggers , on the other hand , have difficulties to handle the huge number of features which are active at the same time if any possible combination of context attributes defines a separate feature . Decision trees ( Breiman et al. , 1984 ; Quinlan , 1993 ) are normally used as classifiers , i.e . they assign classes to objects which are represented as attribute vectors . The non-terminal nodes are labeled with attribute tests , the edges with the possible outcomes of a test , and the terminal nodes are labeled with classes . An object is classified by evaluating the test of the top node on the object , following the respective edge to a daughter node , evaluating the test of the daughter node , and so on until a terminal node is reached whose class is assigned to the object . Decision Trees are turned into probability estimation trees by storing a probability for each possible class at the terminal nodes instead of a single result class . 2.1 Induction of Decision Trees . Decision trees are incrementally built by first selecting the test which splits the manually annotated training sample into the most homogeneous subsets with respect to the class . This test , which maximizes the information gain1 wrt . the class , is following expression for the context probability : 1 The information gain measures how much the test de- . p ( ADJA | ART , PART.Zu ) ∗ p ( Pos | 2 : ART , 1 : PART , 0 : ADJA ) ∗ p ( Nom | 2 : ART.Nom , 1 : PART.Zu , 0 : ADJA ) creases the uncertainty about the class . It is the difference between the entropy of the empirical distribution of the class variable in the training set and the weighted average entropy yes 0 : N.Name yes no 1 : ART.Nom no 1 : ADJA.Nom yes no which returns a probability of 0.3 . The third tree for neuter has one non terminal and two terminal nodes returning a probability of 0.3 and 0.5 , respectively . The sum of probabilities is therefore either 0.9 or 1.1 , but never exactly 1 . This problem 2 : N.Reg p=0.999 0 : N.Name 0 : N.Name yes no p=0.571 p=0.938 yes no p=0.948 p=0.998 ... . is solved by renormalizing the probabilities . The probability of an attribute ( such as “ Nom ” ) is always conditioned on the respective base POS ( such as “ N ” ) ( unless the predicted attribute is the assigned to the top node . The tree is recursively expanded by selecting the best test for each subset and so on , until all objects of the current subset belong to the same class . In a second step , the decision tree may be pruned in order to avoid overfit- ting to the training data . Our tagger generates a predictor for each feature ( such as base POS , number , gender etc . ) Instead of using a single tree for the prediction of all possible values of a feature ( such as noun , article , etc . for base POS ) , the tagger builds a separate decision tree for each value . The motivation was that a tree which predicts a single value ( say verb ) does not fragment the data with tests which are only relevant for the distinction of two other values ( e.g . article and possessive pronoun ) .2 Furthermore , we observed that such two-class decision trees require no optimization of the pruning threshold ( see also section 2.2 . ) The tree induction algorithm only considers binary tests , which check whether some particular attribute is present or not . The best test for each node is selected with the standard information gain criterion . The recursive tree building process terminates if the information gain is 0 . The decision tree is pruned with the pruning criterion described below . Since the tagger creates a separate tree for each attribute , the probabilities of a set of competing attributes such as masculine , feminine , and neuter will not exactly sum up to 1 . To understand why , assume that there are three trees for the gender attributes . Two of them ( say the trees for masculine and feminine ) consist of a single terminal node base POS ) in order to make sure that the probability of an attribute is 0 if it never appeared with the respective base POS . All context attributes other than the base POS are always used in combination with the base POS . A typical context attribute is “ 1 : ART.Nom ” which states that the preceding tag is an article with the attribute “ Nom ” . “ 1 : ART ” is also a valid attribute specification , but “ 1 : Nom ” is not . The tagger further restricts the set of possible test attributes by requiring that some attribute of the POS tag at position i-k ( i=position of the predicted POS tag , k ≥ 1 ) must have been used be fore an attribute of the POS tag at position i- ( k+1 ) may be examined . This restriction improved the tagging accuracy for large contexts . The tagger applies3 the critical-value pruning strategy proposed by ( Mingers , 1989 ) . A node is pruned if the information gain of the best test multiplied by the size of the data subsample is below a given threshold . To illustrate the pruning , assume that D is the data of the current node with 50 positive and 25 negative elements , and that D1 ( with 20 positive and 20 negative elements ) and D2 ( with 30 positive and 5 negative elements ) are the two subsets induced by the best test . The entropy of D is −2/3 log22/3 − 1/3 log21/3 = 0.92 , the entropy of D1 is −1/2 log21/2−1/2 log21/2 = 1 , and the entropy of D2 is −6/7 log26/7 − 1/7 log21/7 = 0.59 . The information gain is therefore 0.92 − ( 8/15 ∗ 1 − 7/15 ∗ 0.59 ) = 0.11 . The resulting score is 75 ∗ 0.11 = 8.25 . Given a threshold of 6 , the node is therefore not pruned . We experimented with pre-pruning ( where a node is always pruned if the gain is below the in the two subsets . The weight of each subset is proportional to its size . 2 We did not directly compare the two alternatives ( two- valued vs. multi-valued tests ) , because the implementational effort required would have been too large . 3 We also experimented with a pruning criterion based on binomial tests , which returned smaller trees with a slightly lower accuracy , although the difference in accuracy was never larger than 0.1 % for any context size . Thus , the simpler pruning strategy presented here was chosen . threshold ) as well as post-pruning ( where a node is only pruned if its sub-nodes are terminal nodes or pruned nodes ) . The performance of pre-pruning was slightly better and it was less dependent on the choice of the pruning threshold . A threshold of 6 consistently produced optimal or near optimal results for pre-pruning . Thus , pre-pruning with a threshold of 6 was used in the experiments . The tagger treats dots in POS tag labels as attribute separators . The first attribute of a POS tag is the main category . The number of additional attributes is fixed for each main category . The additional attributes are category-specific . The singular attribute of a noun and an adjective POS tag are therefore two different attributes.4 Each position in the POS tags of a given category corresponds to a feature . The attributes occurring at a certain position constitute the value set of the feature . Our tagger is a HMM tagger which decomposes the context probabilities into a product of attribute probabilities . The probability of an attribute given the attributes of the preceding POS tags as well asand that the context probability p ( ti|ti−1 ) is internally computed as a product of attribute probabili ties . In order to increase the speed , the tagger also applies a beam-search strategy which prunes all search paths whose probability is below the probability of the best path times a threshold . With athreshold of 10−3 or lower , the influence of prun ing on the tagging accuracy was negligible . The tagger may use an external lexicon which supplies entries for additional words which are not found in the training corpus , and additional tags for words which did occur in the training data . If an external lexicon is provided , the lexical probabilities are smoothed as follows : The tagger computes the average tag probabilities of all words with the same set of possible POS tags . The Witten-Bell method is then applied to smooth the lexical probabilities with the average probabilities . If the word w was observed with N different tags , and f ( w , t ) is the joint frequency of w and POS tag t , and p ( t| [ w ] ) is the average probability of t among words with the same set of possible tags as w , then the smoothed probability of t given w is defined as follows : f ( w , t ) + N p ( t| [ w ] ) the preceding attributes of the predicted POS tag is estimated with a decision tree as described be p ( t|w ) = f ( w ) + N fore . The probabilities at the terminal nodes of the decision trees are smoothed with the parent node probabilities ( which themselves were smoothed in the same way ) . The smoothing is implemented by adding the weighted class probabilities pp ( c ) of the parent node to the frequencies f ( c ) before normalizing them to probabilities : p ( c ) = f ( c ) + αpp ( c ) α + �c f ( c ) The weight α was fixed to 1 after a few experiments on development data . This smoothing strategy is closely related to Witten-Bell smoothing . The probabilities are normalized by dividing them by the total probability of all attribute values of the respective feature ( see section 2.1 ) . The best tag sequence is computed with the Viterbi algorithm . The main differences of our tag- ger to a standard trigram tagger are that the order of the Markov model ( the k in equation 1 ) is not fixed 4 This is the reason why the attribute tests in figure 1 used complex attributes such as ART.Nom rather than Nom.The smoothed estimates of p ( tag|word ) are di vided by the prior probability p ( tag ) of the tag and used instead of p ( word|tag ) .5 4.2 Unknown Words . The lexical probabilities of unknown words are obtained as follows : The unknown words are divided into four disjoint classes6 with numeric expressions , words starting with an uppercase letter , words starting with a lowercase letter , and a fourth class for the other words . The tagger builds a suffix trie for each class of unknown words using the known word types from that class . The maximal length of the suffixes is 7 . The suffix tries are pruned until ( i ) all suffixes have a frequency of at least 5 and ( ii ) the information gain multiplied by the suffix frequency and di 5 p ( word|tag ) is equal to p ( tag|word ) p ( word ) /p ( tag ) and p ( word ) is a constant if the tokenization is unambiguous . Therefore dropping the factor p ( word ) has no influence on the ranking of the different tag sequences . 6 In earlier experiments , we had used a much larger number of word classes . Decreasing their number to 4 turned out to be better . More precisely , if Tα is the set of POS tags that occurred with suffix α , |T | is the size of the set T , fα is the frequency of suffix α , and pα ( t ) is the probability of POS tag t among the words with suffix α , then the following condition must hold : tion between definite and indefinite articles , and the distinction between hyphens , slashes , left and right parentheses , quotation marks , and other symbols which the Tiger treebank annotates with “ $ ( ” . A supplementary lexicon was created by analyzing a word list which included all words from the faα paα ( t ) log paα ( t ) < 1 training , development , and test data with a German computationa l morphology . The analyses gener |Taα| t∈Taα pα ( t ) ated by the morphology were mapped to the Tiger The POS probabilities are recursively smoothed with the POS probabilities of shorter suffixes using Witten-Bell smoothing . Our tagger was first evaluated on data from the German Tiger treebank . The results were compared to those obtained with the TnT tagger ( Brants , 2000 ) and the SVMTool ( Gime´nez and Ma ` rquez , 2004 ) , which is based on support vector machines.7 The training of the SVMTool took more than a day . Therefore it was not possible to optimize the parameters systematically . We took standard features from a 5 word window and M4LRL training without optimization of the regular- ization parameter C. In a second experiment , our tagger was also evaluated on the Czech Academic corpus 1.0 ( Hladka´ et al. , 2007 ) and compared to the TnT tag- ger . The German Tiger treebank ( Brants et al. , 2002 ) contains over 888,000 tokens . It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number , gender , case , person , degree , tense , and mood . After deleting problematic sentences ( e.g . with an incomplete annotation ) and automatically correcting some easily detectable errors , 885,707 tokens were left . The first 80 % were used as training data , the first half of the rest as development data , and the last 10 % as test data . Some of the 54 STTS labels were mapped to new labels with dots , which reduced the number of main categories to 23 . Examples are the nominal POS tags NN and NE which were mapped to N.Reg and N.Name . Some lexically decidable distinctions missing in the Tiger corpus have been tagset . Note that only the words , but not the POS tags from the test and development data were used , here . Therefore , it is always possible to create a supplementary lexicon for the corpus to be processed . In case of the TnT tagger , the entries of the supplementary lexicon were added to the regular lexicon with a default frequency of 1 if the word/tag- pair was unknown , and with a frequency proportional to the prior probability of the tag if the word was unknown . This strategy returned the best results on the development data . In case of the SVM- Tool , we were not able to successfully integrate the supplementary lexicon . 5.1.1 Refined Tagset Prepositions are not annotated with case in the Tiger treebank , although this information is important for the disambiguation of the case of the next noun phrase . In order to provide the tagger with some information about the case of prepositions , a second training corpus was created in which prepositions which always select the same case , such as durch ( through ) , were annotated with this case ( APPR.Acc ) . Prepositions which select genitive case , but also occur with dative case8 , were tagged with APPR.Gen . The more frequent ones of the remaining prepositions , such as in ( in ) , were lexicalized ( APPR.in ) . The refined tagset also distinguished between the auxiliaries sein , haben , and werden , and used lexicalized tags for the coordinating conjunctions aber , doch , denn , wie , bis , noch , and als whose distribution differs from the distribution of prototypical coordinating conjunctions such as und ( and ) or oder ( or ) . For evaluation purposes , the refined tags are mapped back to the original tags . 7 It was planned to include also the Stanford tagger . ( Toutanova et al. , 2003 ) in this comparison , but it was not possible to train it on the Tiger data . 8 In German , the genitive case of arguments is more and . more replaced by the dative . The first result was obtained with TnT trained on Tiger data which was mapped to STTS before . The second row contains the results for the TnT tagger when it is trained on the Tiger data and the output is mapped to STTS . The third row gives the corresponding figures for our tagger . 5.1.2 Results A tag is considered correct if all attributes are correct . The SVMTool is slightly better than the TnT tagger on the default tagset , but shows little improvement from the tagset refinement . Apparently , the lexical features used by the SVMTool encode most of the information of the tagset refinement . With a context of two preceding POS tags ( similar to the trigram tagger TnT ) , our tagger outperforms TnT by 0.7 % on the default tagset , by 1 % on the refined tagset , and by 1.1 % on the refined tagset plus the additional lexicon . A larger context of up to 10 preceding POS tags further increased the accuracy by 0.6 , 0.6 , and 0.7 % , respectively . de fa ult refined ref.+lexicon T n T S T T S T n T Ti g e r 1 0 t a g s 9 7 . 1 7 97.26 97.51 9 7 . 3 9 97.57 97.97 These figures are considerably lower than e.g . the 96.7 % accuracy reported in Brants ( 2000 ) for the Negra treebank which is annotated with STTS tags without agreement features . This is to 9 Unknown words are tagged by choosing the most frequent tag of words with the same capitalization . The best results are obtained with a context size of 10 . What type of information is relevant across a distance of ten words ? A good example is the decision tree for the attribute first person of finite verbs , which looks for a first person pronoun at positions -1 through -10 ( relative to the position of the current word ) in this order . Since German is a verb-final language , these tests clearly make sense . Our tagger was used with a context size of 10 . The suffix length parameter of the TnT tagger was set to 6 without lexicon and to 3 with lexicon . These values were optimal on the development data . The accuracy of our tagger is lower than on the development data . This could be due to the higher rate of unknown words ( 10.0 % vs. 7.7 % ) . Relative to the TnT tagger , however , the accuracy is quite similar for test and development data . The differences between the two taggers are significant.10 ta gg er de fa ult refined ref.+lexicon Tn T ou r ta gg er 8 3 . 4 5 84.11 89.14 8 5 . 0 0 85.92 91.07 If 10 726 sentences were better tagged by TnT ( i.e . with few errors ) , 1450 sentences were better tagged by our tagger . The resulting score of a binomial test is below 0.001 . Our tagger is quite fast , although not as fast as the TnT tagger . With a context size of 3 ( 10 ) , it annotates 7000 ( 2000 ) tokens per second on a computer with an Athlon X2 4600 CPU . The training with a context size of 10 took about 4 minutes . We also evaluated our tagger on the Czech Academic corpus ( Hladka´ et al. , 2007 ) which contains 652.131 tokens and about 1200 different POS tags . The data was divided into 80 % training data , 10 % development data and 10 % test data . 89 88.9 88.8 Provost & Domingos ( 2003 ) noted that well- known decision tree induction algorithms such as C4.5 ( Quinlan , 1993 ) or CART ( Breiman et al. , 1984 ) fail to produce accurate probability estimates . They proposed to grow the decision trees to their maximal size without pruning , and to smooth the probability estimates with add-1 smoothing ( also known as the Laplace correction ) . ( 2003 ) describe a more complex backoff smoothing method . Contrary to them , we applied pruning and found that some pruning ( threshold=6 ) gives better results than no pruning ( threshold=0 ) . Another difference is that we used N two- class trees with normalization to predict the probabilities of N classes . These two-class trees can be pruned with a fixed pruning threshold . Hence there is no need to put aside training data for parameter tuning . 88.7 88.6 88.5 ’ c o n t e x t d a t a 2 ’ 2 3 4 5 6 7 8 9 10 A n ope n que stio n is wh eth er the SV MT ool ( or oth er dis cri min ativ ely trai ned tag ger s ) cou ld out - perf orm the pre sen ted tag ger if the sa me dec om positi on of PO S tag s and the sa me con text size was The best accuracy of our tagger on the development set was 88.9 % obtained with a context of 4 preceding POS tags . The best accuracy of the TnT tagger was 88.2 % with a maximal suffix length of 5 . Our tagger combines two ideas , the decomposition of the probability of complex POS tags into a product of feature probabilities , and the estimation of the conditional probabilities with decision trees . A similar idea was previously presented in Kempe ( 1994 ) , but apparently never applied again . The tagging accuracy reported by Kempe was below that of a traditional trigram tagger . Unlike him , we found that our tagging method outperformed state-of-the-art POS taggers on fine-grained POS tagging even if only a trigram context was used . Schmid ( 1994 ) and Ma ` rquez ( 1999 ) used decision trees for the estimation of contextual tag probabilities , but without a decomposition of the tag probability . Magerman ( 1994 ) applied probabilistic decision trees to parsing , but not with a generative model . We think that this might be the case if the SVM features are restricted to the set of relevant attribute combinations discovered by the decision tree , but we doubt that it is possible to train the SVMTool ( or other discriminatively trained tag- gers ) without such a restriction given the difficulties to train it with the standard context size . Czech POS tagging has been extensively studied in the past ( Hajicˇ and Vidova´-Hladka´ , 1998 ; Hajicˇ et al. , 2001 ; Votrubec , 2006 ) . ( 2007 ) compared several POS taggers including an n-gram tagger and a discriminatively trained tagger ( Morcˇe ) , and evaluated them on the Prague Dependency Treebank ( PDT 2.0 ) . Morcˇe ’ s tagging accuracy was 95.12 % , 0.3 % better than the n-gram tagger . A hybrid system based on four different tagging methods reached an accuracy of 95.68 % . Because of the different corpora used and the different amounts of lexical information available , a direct comparison to our results is difficult . Furthermore , our tagger uses no corpus-specific heuristics , whereas Morcˇe e.g . is optimized for Czech POS tagging . The German tagging results are , to the best of our knowledge , the first published results for fine- grained POS tagging with the Tiger tagset . We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees . In experiments with German and Czech corpora , this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers ( TnT and SVMTool ) . This paper proposes a new approach to dynamically determine the tree span for tree kernel-based semantic relation extraction . It exploits constituent dependencies to keep the nodes and their head children along the path connecting the two entities , while removing the noisy information from the syntactic parse tree , eventually leading to a dynamic syntactic parse tree . This paper also explores entity features and their combined features in a unified parse and semantic tree , which integrates both structured syntactic parse information and entity-related semantic information . Evaluation on the ACE RDC 2004 corpus shows that our dynamic syntactic parse tree outperforms all previous tree spans , and the composite kernel combining this tree kernel with a linear state-of-the-art feature-based kernel , achieves the so far best performance . Information extraction is one of the key tasks in natural language processing . It attempts to identify relevant information from a large amount of natural language text documents . Of three sub- tasks defined by the ACE program1 , this paper focuses exclusively on Relation Detection and Characterization ( RDC ) task , which detects and classifies semantic relationships between predefined types of entities in the ACE corpus . Licensed under the Creative Commons Attribution- Noncommercial-Share Alike 3.0 Unported license ( http : //creativecommons.org/licenses/by-nc-sa/3.0/ ) . 1 http : //www.ldc.upenn.edu/Projects/ACE/ example , the sentence Microsoft Corp. is based in Redmond , WA conveys the relation GPEAFF.Based between Microsoft Corp. [ ORG ] and Redmond [ GPE ] . Due to limited accuracy in state-of-the-art syntactic and semantic parsing , reliably extracting semantic relationships between named entities in natural language documents is still a difficult , unresolved problem . In the literature , feature-based methods have dominated the research in semantic relation extraction . Featured-based methods achieve promising performance and competitive efficiency by transforming a relation example into a set of syntactic and semantic features , such as lexical knowledge , entity-related information , syntactic parse trees and deep semantic information . However , detailed research ( Zhou et al. , 2005 ) shows that it s difficult to extract new effective features to further improve the extraction accuracy . Therefore , researchers turn to kernel-based methods , which avoids the burden of feature engineering through computing the similarity of two discrete objects ( e.g . From prior work ( Zelenko et al. , 2003 ; Culotta and Sorensen , 2004 ; Bunescu and Mooney , 2005 ) to current research ( Zhang et al. , 2006 ; Zhou et al. , 2007 ) , kernel methods have been showing more and more potential in relation extraction . The key problem for kernel methods on relation extraction is how to represent and capture the structured syntactic information inherent in relation instances . While kernel methods using the dependency tree ( Culotta and Sorensen , 2004 ) and the shortest dependency path ( Bunescu andMooney , 2005 ) suffer from low recall perform ance , convolution tree kernels ( Zhang et al. , 2006 ; Zhou et al. , 2007 ) over syntactic parse trees achieve comparable or even better performance than feature-based methods . However , there still exist two problems regarding currently widely used tree spans . ( 2006 ) discover that the Shortest Path 697 Proceedings of the 22nd International Conference on Computational Linguistics ( Coling 2008 ) , pages 697 704 Manchester , August 2008 enclosed Tree ( SPT ) achieves the best performance . ( 2007 ) further extend it to Context-Sensitive Shortest Path-enclosed Tree ( CS- SPT ) , which dynamically includes necessary predicate-linked path information . One problem with both SPT and CS-SPT is that they may still contain unnecessary information . The other problem is that a considerable number of useful context-sensitive information is also missing from SPT/CS-SPT , although CS-SPT includes some contextual information relating to predicate- linked path . This paper proposes a new approach to dynamically determine the tree span for relation extraction by exploiting constituent dependencies to remove the noisy information , as well as keep the necessary information in the parse tree . Our motivation is to integrate dependency information , which has been proven very useful to relation extraction , with the structured syntactic information to construct a concise and effective tree span specifically targeted for relation extraction . Moreover , we also explore interesting combined entity features for relation extraction via a unified parse and semantic tree . The other sections in this paper are organized as follows . Previous work is first reviewed in Section 2 . Then , Section 3 proposes a dynamic syntactic parse tree while the entity-related semantic tree is described in Section 4 . Evaluation on the ACE RDC corpus is given in Section 5 . Finally , we conclude our work in Section 6 . Due to space limitation , here we only review kernel-based methods used in relation extraction . For those interested in feature-based methods , please refer to Zhou et al . ( 2005 ) for more details . ( 2003 ) described a kernel between shallow parse trees to extract semantic relations , where a relation instance is transformed into the least common sub-tree connecting the two entity nodes . The kernel matches the nodes of two corresponding sub-trees from roots to leaf nodes recursively layer by layer in a top- down manner . Their method shows successful results on two simple extraction tasks . Culotta and Sorensen ( 2004 ) proposed a slightly generalized version of this kernel between dependency trees , in which a successful match of two relation instances requires the nodes to be at the same layer and in the identical path starting from the roots to the current nodes . These strong constraints make their kernel yield high precision but very low recall on the ACE RDC 2003 corpus . Bunescu and Mooney ( 2005 ) develop a shortest path dependency tree kernel , which simply counts the number of common word classes at each node in the shortest paths between two entities in dependency trees . Similar to Culotta and Sorensen ( 2004 ) , this method also suffers from high precision but low recall . ( 2006 ) describe a convolution tree kernel ( CTK , Collins and Duffy , 2001 ) to investigate various structured information for relation extraction and find that the Shortest Path- enclosed Tree ( SPT ) achieves the F-measure of 67.7 on the 7 relation types of the ACE RDC 2004 corpus . One problem with SPT is that it loses the contextual information outside SPT , which is usually critical for relation extraction . ( 2007 ) point out that both SPT and the convolution tree kernel are context-free . Theyexpand SPT to CS-SPT by dynamically includ ing necessary predicate-linked path information and extending the standard CTK to context- sensitive CTK , obtaining the F-measure of 73.2 on the 7 relation types of the ACE RDC 2004 corpus . However , the CS-SPT only recovers part of contextual information and may contain noisy information as much as SPT . In order to fully utilize the advantages of feature-based methods and kernel-based methods , researchers turn to composite kernel methods . Zhao and Grishman ( 2005 ) define several feature-based composite kernels to capture diverse linguistic knowledge and achieve the F-measure of 70.4 on the 7 relation types in the ACE RDC 2004 corpus . ( 2006 ) design a composite kernel consisting of an entity linear kernel and a standard CTK , obtaining the F-measure of 72.1 on the 7 relation types in the ACE RDC 2004 corpus . ( 2007 ) describe a composite kernel to integrate a context-sensitive CTK and a state-of-the-art linear kernel . It achieves the so far best F-measure of 75.8 on the 7 relation types in the ACE RDC 2004 corpus . In this paper , we will further study how to dynamically determine a concise and effective tree span for a relation instance by exploiting constituent dependencies inherent in the parse tree derivation . We also attempt to fully capture both the structured syntactic parse information and entity-related semantic information , especially combined entity features , via a unified parse and semantic tree . Finally , we validate the effectiveness of a composite kernel for relation extraction , which combines a tree kernel and a linear kernel . This section discusses how to generate dynamic syntactic parse tree by employing constituent dependencies to overcome the problems existing in currently used tree spans . 3.1 Constituent Dependencies in Parse Tree . ( 2006 ) explore five kinds of tree spans and find that the Shortest Path-enclosed Tree ( SPT ) achieves the best performance . ( 2007 ) further propose Context-Sensitive SPT ( CS-SPT ) , which can dynamically determine the tree span by extending the necessary predicate-linked path information outside SPT . However , the key problem of how to represent the structured syntactic parse tree is still partially resolved . As we indicate as follows , current tree spans suffer from two problems : ( 1 ) Both SPT and CS-SPT still contain unnecessary information . For example , in the sentence bought one of town s two meat-packing plants , the condensed information one of plants is sufficient to determine DISC relationship between the entities one [ FAC ] and plants [ FAC ] , while SPT/CS-SPT include the redundant underlined part . Therefore more unnecessary information can be safely removed from SPT/CS-SPT . ( 2 ) CS-SPT only captures part of context- sensitive information relating to predicate-linked structure ( Zhou et al. , 2007 ) and still loses much context-sensitive information . Let s take the same example sentence bought one of town s two meat-packing plants , where indeed there is no relationship between the entities one [ FAC ] and town [ GPE ] . Nevertheless , the information contained in SPT/CS-SPT ( one of town ) may easily lead to their relationship being misclassified as DISC , which is beyond our expectation . Therefore the underlined part outside SPT/CS- SPT should be recovered so as to differentiate it from positive instances . Since dependency plays a key role in many NLP problems such as syntactic parsing , semantic role labeling as well as semantic relation extraction , our motivation is to exploit dependency knowledge to distinguish the necessary evidence from the unnecessary information in the structured syntactic parse tree . On one hand , lexical or word-word dependency indicates the relationship among words occurring in the same sentence , e.g . predicate- argument dependency means that arguments are dependent on their target predicates , modifier head dependency means that modifiers are dependent on their head words . This dependency relationship offers a very condensed representation of the information needed to assess the relationship in the forms of the dependency tree ( Culotta and Sorensen , 2004 ) or the shortest dependency path ( Bunescu and Mooney , 2005 ) that includes both entities . On the other hand , when the parse tree corresponding to the sentence is derived using derivation rules from the bottom to the top , the word- word dependencies extend upward , making a unique head child containing the head word for every non-terminal constituent . As indicated as follows , each CFG rule has the form : P Ln L1 H R1 Rm Here , P is the parent node , H is the head child of the rule , Ln L1 and R1 Rm are left and right modifiers of H respectively , and both n and m may be zero . In other words , the parent node P depends on the head child H , this is what we call constituent dependency . Vice versa , we can also determine the head child of a constituent in terms of constituent dependency . Our hypothesis stipulates that the contribution of the parse tree to establishing a relationship is almost exclusively concentrated in the path connecting the two entities , as well as the head children of constituent nodes along this path . 3.2 Generation of Dynamic Syntactic Parse . Tree Starting from the Minimum Complete Tree ( MCT , the complete sub-tree rooted by the nearest common ancestor of the two entities under consideration ) as the representation of each relation instance , along the path connecting two entities , the head child of every node is found according to various constituent dependencies . Then the path nodes and their head children are kept while any other nodes are removed from the tree . Eventually we arrive at a tree called Dynamic Syntactic Parse Tree ( DSPT ) , which is dynamically determined by constituent dependencies and only contains necessary information as expected . There exist a considerable number of constituent dependencies in CFG as described by Collins ( 2003 ) . However , since our task is to extract the relationship between two named entities , our focus is on how to condense Noun-Phrases ( NPs ) and other useful constituents for relation extraction . Therefore constituent dependencies can be classified according to constituent types of the CFG rules : ( 1 ) Modification within base-NPs : base-NPs mean that they do not directly dominate an NP themselves , unless the dominated NP is a possessive NP . The noun phrase right above the entity headword , whose mention type is nominal or name , can be categorized into this type . In this case , the entity headword is also the headword of the noun phrase , thus all the constituents before the headword are dependent on the headword , and may be removed from the parse tree , while the headword and the constituents right after the headword remain unchanged . In this way the parse tree one of plants could capture the DISC relationship more concisely and precisely . For both SPT and CS-SPT , this example would be condensed to one of town and therefore easily misclassified as the DISC relationship between the two entities . In the contrast , our DSPT can avoid this problem by keeping the constituent s and the headword plants . ( 2 ) Modification to NPs : except base-NPs , other modification to NPs can be classified into this type . Usually these NPs are recursive , meaning that they contain another NP as their child . The CFG rules corresponding to these modifications may have the following forms : NP NP SBAR [ relative clause ] NP NP VP [ reduced relative ] NP NP PP [ PP attachment ] Here , the NPs in bold mean that the path connecting the two entities passes through them . For every right hand side , the NP in bold is modified by the constituent following them . That is , the latter is dependent on the former , and may be reduced to a single NP . Since the reduced relative nominated for modifies and is therefore dependent on the people , they can be removed from the parse tree , that is , the right side ( NP VP ) can be reduced to the left hand side , which is exactly a single NP . An argument represents the subject or object of a verb , while an adjunct indicates the location , date/time or way of the action corresponding to the verb . They depend on the verb and can be removed if they are not included in the path connecting the two entities . However , when the parent tag is S or SBAR , and its child VP is not included in the path , this VP should be recovered to indicate the predicate verb . While PP can be removed from the rule ( VP VBZ PP ) , the VP should be kept in the rule ( S NP VP ) . Consequently , the tree span looks more concise and precise for relation extraction . ( 4 ) Coordination conjunctions : In coordination constructions , several peer conjuncts may be reduced into a single constituent . Although the first conjunct is always considered as the headword ( Collins , 2003 ) , actually all the conjuncts play an equal role in relation extraction . ( 2007 ) further indicates that among these entity features , entity type , subtype , and mention type , as well as the base form of predicate verb , contribute most while the contribution of other features , such as entity class , headword and GPE role , can be ignored . In the example sentence they re here , which is excerpted from the ACE RDC 2004 corpus , there exists a relationship Physical.Located between the entities they [ PER ] and here [ GPE.Population-Center ] . The features are encoded as TP , ST , MT and PVB , which denote type , subtype , mention-type of the two entities , and the base form of predicate verb if existing ( nearest to the 2nd entity along the path connecting the two entities ) respectively . For example , the tag TP1 represents the type of the 1st entity , and the tag ST2 represents the sub- type of the 2nd entity . The three entity-related semantic tree setups are depicted as follows : ENT sentence ( governors from connecticut , south TP1 ST1 MT1 TP2 ST2 MT2 PVB dakota , and montana ) can be reduced to a single NP ( governors from montana ) by keeping the conjunct in the path while removing the other conjuncts . PER null PRO GPE Pop . PRO be ( a ) Bag Of Features ( BOF ) ENT ( 5 ) Modification to other constituents : except for the above four types , other CFG rules fall into this type , such as modification to PP , ADVP and PRN etc . These cases are similar to arguments/adjuncts to verbs , but less frequent than them , so we will not detail this scenario.In fact , SPT ( Zhang et al. , 2006 ) can be ar TP ST MT TP1 TP2 ST1 ST2 MT1 MT2 PER GPE null Pop . PRO PRO ( b ) Feature Paired Tree ( FPT ) ENT E1 E2 PVB be PVBrived at by carrying out part of the above re TP1 ST1 MT1 TP2 ST2 MT2 be moval operations using a single rule ( i.e . all the constituents outside the linking path should be removed ) and CS-CSPT ( Zhou et al. , 2007 ) further recovers part of necessary context-sensitive information outside SPT , this justifies that SPT performs well , while CS-SPT outperforms SPT . Entity semantic features , such as entity headword , entity type and subtype etc. , impose a strong constraint on relation types in terms of relation definition by the ACE RDC task . Experiments by Zhang et al . ( 2006 ) show that linear kernel using only entity features contributes much when combined with the convolution parse tree kernel . PER null PRO GPE Pop . Different setups for entity-related se mantic tree ( EST ) ( a ) Bag of Features ( BOF , e.g . 2 ( a ) ) : all feature nodes uniformly hang under the root node , so the tree kernel simply counts the number of common features between two relation instances . This tree setup is similar to linear entity kernel explored by Zhang et al . ( b ) Feature-Paired Tree ( FPT , e.g . 2 ( b ) ) : the features of two entities are grouped into different types according to their feature names , e.g . TP1 and TP2 are grouped to TP . This tree setup is aimed to capture the additional similarity of the single feature combined from different entities , i.e. , the first and the second entities . ( c ) Entity-Paired Tree ( EPT , e.g . 2 ( c ) ) : all the features relating to an entity are grouped to nodes E1 or E2 , thus this tree kernel can further explore the equivalence of combined entity features only relating to one of the entities between two relation instances . In fact , the BOF only captures the individual entity features , while the FPT/EPT can additionally capture the bi-gram/tri-gram features respectively . Rather than constructing a composite kernel , we incorporate the EST into the DSPT to produce a Unified Parse and Semantic Tree ( UPST ) to investigate the contribution of the EST to relation extraction . However , detailed evaluation ( Qian et al. , 2007 ) indicates that the UPST achieves the best performance when the feature nodes are attached under the top node . Hence , we also attach three kinds of entity-related semantic trees ( i.e . BOF , FPT and EPT ) under the top node of the DSPT right after its original children . Thereafter , we employ the standard CTK ( Collins and Duffy , 2001 ) to compute the similarity between two ( Moschitti , 2004 ) 2 is selected as our classifier . For efficiency , we apply the one vs. others strategy , which builds K classifiers so as to separate one class from all others . For comparison purposes , the training parameters C ( SVM ) and ( tree kernel ) are also set to 2.4 and 0.4 respectively . The MCT with only entity-type information is first used as the baseline , and various constituent dependencies are then applied sequentially to dynamically reshaping the tree in two different modes : -- [ M1 ] Respective : every constituent dependency is individually applied on MCT . -- [ M2 ] Accumulative : every constituent dependency is incrementally applied on the previously derived tree span , which begins with the MCT and eventually gives rise to a Dynamic Syntactic Parse Tree ( DSPT ) . Dependency types P ( % ) R ( % ) F MCT ( baseline ) 75.1 53.8 62.7 UPSTs , since this CTK and its variations are Modification within 76.5 59.8 67.1successfully applied in syntactic parsing , seman tic role labeling ( Moschitti , 2004 ) and relation base-NPs ( 59.8 ) ( 59.8 ) ( 67.1 ) extraction ( Zhang et al. , 2006 ; Zhou et al. , 2007 ) Modification to NPs 77.0 63.2 69.4 as well . ( 76.2 ) ( 56.9 ) ( 65.1 ) Arguments/adjuncts to verb 77.1 63.9 69.9 ( 76.1 ) ( 57.5 ) ( 65.5 ) Coordination conjunctions 77.3 65.2 70.8 This section will evaluate the effectiveness of the ( 77.3 ) ( 55.1 ) ( 63.8 ) DSPT and the contribution of entity-related se Other modifications 77.4 65.4 70.9 mantic information through experiments . For evaluation , we use the ACE RDC 2004 corpus as the benchmark data . This data set contains 451 documents and 5702 relation instances . It defines 7 entity types , 7 major relation types and 23 subtypes . For comparison with previous work , evaluation is done on 347 ( nwire/bnews ) documents and 4307 relation instances using 5-fold cross-validation . Here , the corpus is parsed using Charniak s parser ( Charniak , 2001 ) and relation instances are generated by iterating over all pairs of entity mentions occurring in the same sentence with given true mentions and coreferential information . This indicates that reshaping the tree by exploiting constituent dependencies may significantly improve extraction accuracy largely due to the increase in recall . It further suggests that constituent dependencies knowledge is very effec 2 http : //ainlp.info.uniroma2.it/moschitti/ tive and can be fully utilized in tree kernel-based relation extraction . This indicates the local characteristic of semantic relations , which can be effectively captured by NPs near the two involved entities in the DSPT . ( 2 ) All the other three dependencies show minor contribution to performance enhancement , they improve the F-measure only by 2.8/0.9/-0.1 units in mode M1 and 0.5/0.9/0.1 units in mode M2 . This may be due to the reason that these dependencies only remove the nodes far from the two entities . It shows that : ( 1 ) All the three unified parse and semantic tree kernels significantly outperform the DSPT kernel , obtaining an average increase of ~4 units in F-measure . This means that they can effectively capture both the structured syntactic information and the entity-related semantic features . ( 2 ) The Unified Parse and Semantic Tree with Feature-Paired Tree achieves the best performance of 80.1/70.7/75.1 in P/R/F respectively , with an increase of F-measure by 0.4/0.3 units over BOF and EPT respectively . This suggests that additional bi-gram entity features capturedby FPT are more useful than tri-gram entity fea tures captured by EPT . Performance of Unified Parse and Semantic Trees ( UPSTs ) on the 7 relation types of the ACE RDC 2004 corpus tree spans . It also shows that the Unified Parse and Semantic Tree with Feature-Paired Tree perform significantly better than the other two tree setups ( i.e. , CS-SPT and DSPT ) by 6.7/4.2 units in F-measure respectively . This implies that the entity-related semantic information is very useful and contributes much when they are incorporated into the parse tree for relation extraction . It shows that our UPST outperforms all previous tree setups using one single kernel , and even better than two previous composite kernels ( Zhang et al. , 2006 ; Zhao and Grishman , 2005 ) . Furthermore , when the UPST ( FPT ) kernel is com bined with a linear state-of-the-state feature- based kernel ( Zhou et al. , 2005 ) into a composite one via polynomial interpolation in a setting similar to Zhou et al . ( 2007 ) ( i.e . polynomial degree d=2 and coefficient =0.3 ) , we get the so far best performance of 77.1 in F-measure for 7 relation types on the ACE RDC 2004 data set . Systems P ( % ) R ( % ) F Ours : It shows that in a similar setting , our DSPT outperforms SPT by 3.8 units in F-measure , while CS-SPT outperforms SPT by 1.3 units in F-measure . This suggests that the DSPT performs best among these 3 We arrive at these values by subtracting P/R/F . ( 79.6/5.6/71.9 ) of 4 There might be some typing errors for the performance . reported in Zhao and Grishman ( 2005 ) since P , R and F do not match . This paper further explores the potential of structured syntactic information for tree kernel-based relation extraction , and proposes a new approach to dynamically determine the tree span ( DSPT ) for relation instances by exploiting constituent dependencies . We also investigate different ways of how entity-related semantic features and their combined features can be effectively captured in a Unified Parse and Semantic Tree ( UPST ) . Evaluation on the ACE RDC 2004 corpus shows that our DSPT is appropriate for structured representation of relation instances . We also find that , in addition to individual entity features , combined entity features ( especially bi-gram ) contribute much when they are combined with a DPST into a UPST . And the composite kernel , combining the UPST kernel and a linear state-of- the-art kernel , yields the so far best performance . For the future work , we will focus on improving performance of complex structured parse trees , where the path connecting the two entities involved in a relationship is too long for current kernel methods to take effect . Our preliminary experiment of applying certain discourse theory exhibits certain positive results . This research is supported by Project 60673041 under the National Natural Science Foundation of China , Project 2006AA01Z147 under the 863 National High-Tech Research and Development of China , and the National Research Foundation for the Doctoral Program of Higher Education of China under Grant No . We would also like to thank the excellent and insightful comments from the three anonymous reviewers . We propose semantic role features for a Tree-to-String transducer to model the reordering/deletion of source-side semantic roles . These semantic features , as well as the Tree-to-String templates , are trained based on a conditional log-linear model and are shown to significantly outperform systems trained based on Max-Likelihood and EM . We also show significant improvement in sentence fluency by using the semantic role features in the log-linear model , based on manual evaluation . Syntax-based statistical machine translation ( SSMT ) has achieved significant progress during recent years ( Galley et al. , 2006 ; May and Knight , 2007 ; Liu et al. , 2006 ; Huang et al. , 2006 ) , showing that deep linguistic knowledge , if used properly , can improve MT performance . Semantics-based SMT , as a natural extension to SSMT , has begun to receive more attention from researchers ( Liu and Gildea , 2008 ; Wu and Fung , 2009 ) . Semantic structures have two major advantages over syntactic structures in terms of helping machine translation . First of all , semantic roles tend to agree better between two languages than syntactic constituents ( Fung et al. , 2006 ) . This property motivates the approach of using the consistency of semantic roles to select MT outputs ( Wu and Fung , 2009 ) . Secondly , the set of semantic roles of a predicate models the skeleton of a sentence , which is crucial to the readability of MT output . By skeleton , we mean the main structure of a sentence including the verbs and their arguments . In spite of the theoretical potential of the semantic roles , there has not been much success in using them to improve SMT systems . Liu and Gildea ( 2008 ) proposed a semantic role based Tree-to-String ( TTS ) transducer by adding semantic roles to the TTS templates . Their approach did not differentiate the semantic roles of different predicates , and did not always improve the TTS transducer s performance . Wu and Fung ( 2009 ) took the output of a phrase-based SMT system Moses ( Koehn et al. , 2007 ) , and kept permuting the semantic roles of the MT output until they best matched the semantic roles in the source sentence . This approach shows the positive effect of applying semantic role constraints , but it requires re-tagging semantic roles for every permuted MT output and does not scale well to longer sentences . This paper explores ways of tightly integrating semantic role features ( SRFs ) into an MT system , rather than using them in post-processing or n- best re-ranking . Semantic role labeling ( SRL ) systems usually use sentence-wide features ( Xue and Palmer , 2004 ; Pradhan et al. , 2004 ; Toutanova et al. , 2005 ) ; thus it is difficult to compute target- side semantic roles incrementally during decoding . Noticing that the source side semantic roles are easy to compute , we apply a compromise approach , where the target side semantic roles are generated by projecting the source side semantic roles using the word alignments between the source and target sentences . Since this approach does not perform true SRL on the target string , it can not fully evaluate whether the source and target semantic structures are consistent . However , the approach does capture the semantic-level reordering of the sentences . We assume here that the MT system is capable of providing word alignment ( or equivalent ) information during decoding , which is generally true for current statistical MT systems . Specifically , two types of semantic role features are proposed in this paper : a semantic role reordering feature designed to capture the skeleton- level permutation , and a semantic role deletion fea 716 Proceedings of the 23rd International Conference on Computational Linguistics ( Coling 2010 ) , pages 716 724 , Beijing , August 2010 ture designed to penalize missing semantic roles in the target sentence . To use these features during decoding , we need to keep track of the semantic role sequences ( SRS ) for partial translations , which can be generated based on the source-side semantic role sequence and the corresponding word alignments . Since the SRL system and the MT system are separate , a translation rule ( e.g. , a phrase pair in phrase-based SMT ) could cover two partial source-side semantic roles . In such cases partial SRSs must be recorded in such a way that they can be combined later with other partial SRSs . Dealing with this problem will increase the complexity of the decoding algorithm . Fortunately , Tree-to- String transducer based MT systems ( Liu et al. , 2006 ; Huang et al. , 2006 ) can avoid this problem by using the same syntax tree for both SRL and MT . Such an arrangement guarantees that a TTS template either covers parts of one source-side semantic role , or a few complete semantic roles . This advantage motivates us to use a TTS transducer as the MT system with which to demonstrate the use of the proposed semantic role features . Since it is hard to design a generative model to combine both the semantic role features and the TTS templates , we use a log-linear model to estimate the feature weights , by maximizing the conditional probabilities of the target strings given the source syntax trees . The log-linear model with latent variables has been discussed by Blunsom et al . ( 2008 ) ; we apply this technique to combine the TTS templates and the semantic role features . The remainder of the paper is organized as follows : Section 2 describes the semantic role features proposed for machine translation ; Section 3 describes how semantic role features are used and trained in a TTS transducer ; Section 4 presents the experimental results ; and Section 5 gives the conclusion . Translation 2.1 Defining Semantic Roles . There are two semantic standards with publicly available training data : PropBank ( Palmer et al. , 2005 ) and FrameNet ( Johnson et al. , 2002 ) . Prop- Bank defines a set of semantic roles for the verbs in the Penn TreeBank using numbered roles . These roles are defined individually for each verb . For example , for the verb disappoint , the role name arg1 means experiencer , but for the verb wonder , role name arg1 means cause . FrameNet is motivated by the idea that a certain type of verbs can be gathered together to form a frame , and in the same frame , a set of semantic roles is defined and shared among the verbs . For example , the verbs boil , bake , and steam will be in frame apply heat , and they have the semantic roles of cook , food , and heating instrument . Of these two semantic standards , we choose PropBank over FrameNet for the following reasons : 1 . PropBank has a simpler semantic definition . than FrameNet and thus is easier for automatic labeling . PropBank is built upon the Penn TreeBank . and is more consistent with statistical parsers , most of which are trained on the Penn Tree- Bank . PropBank is a larger corpus than FrameNet . Note that the semantic standard/corpus is not cru cial in this paper . Any training corpus that can be used to automatically obtain the set of semantic roles of a verb could be used in our approach . Ideally , we want to use features based on the true semantic roles of the MT candidates . Considering there is no efficient way of integrating SRL and MT , accurate target-side semantic roles can only be used in post-processing and re-ranking the MT outputs , where a limited number of MT candidates are considered . On the other hand , it is much easier to obtain reliable semantic roles for the source sentences . This paper uses a compromise approach , where the target-side semantic roles are projected from the source-side semantic roles using the word alignment derived from the translation process . More specifically , we define two types of semantic role features : 1 . Semantic Role Reordering ( SRR ) This fea- . ture describes reordering of the source-side semantic roles ( including the predicate ) in the target side . It takes the following form : arg0 arg neg arg1 arg1 arg0 I did not see the book you borrowed SrcP red : SrcRole1 , ... , SrcRolen T arRole1 , ... , T arRolen SRR : where SrcP red and SrcRole denotes the central verb and semantic roles in the source side , and T arRole denotes the target-side roles . The source/target SRSs do not need be continuous , but there should be a one-to-one alignment between the roles in the two sides . Compared to the general reordering models used in statistical MT systems , this type of feature is capable of modeling skeleton-level reordering , which is crucial to the fluency of MT output . Because a predicate can have different semantic role sequences in different voices , passive/active are tagged for each occurrence of the verbs based on their POS and preceding words . Deleted Roles ( DR ) are the individual source- . side semantic roles which are deleted in the MT outputs , taking the form of : SrcP red : SrcRole deleted DR is meant to penalize the deletion of the semantic roles . Though most statistical MT systems have penalties for word deletion , it is still useful to make separate features for the deletion of semantic roles , which is considered more harmful than the deletion of non-core components ( e.g. , modifiers ) and deserves more serious penalty . Both types of features can be made non-lexicalized by removing the actual verb but retaining its voice information in the features . Non-lexicalized features are used in the system to alleviate the problem of sparse verbs . see active : arg neg verb arg neg verb borrowed active : arg1 arg0 arg0 arg1 borrowed active : arg1 verb verb arg1 borrowed active : arg0 verb arg0 verb borrowed active : arg1 arg0 verb arg0 verb arg1 DR : see active : arg0 deleted We first briefly describe the basic Tree-to-String translation model used in our experiments , and then describe how to modify it to incorporate the semantic role features . A Tree-to-String transducer receives a syntax tree as its input and , by recursively applying TTS templates , generates the target string . A TTS template is composed of a left-hand side ( LHS ) and a right-hand side ( RHS ) , where the LHS is a sub- tree pattern and the RHS is a sequence of variables and translated words . The variables in the RHS of a template correspond to the bottom level non- terminals in the LHS s subtree pattern , and their relative order indicates the permutation desired at the point where the template is applied to translate one language to another . The variables are further transformed , and the recursive process goes on until there are no variables left . The formal description of a TTS transducer is given by Graehl and Knight ( 2004 ) , and our baseline approach follows the Extended Tree-to-String Transducer defined by Huang et al . For a given derivation ( decomposition into templates ) of a syntax tree , the translation probability is computed as the product of the templates which generate both the source syntax trees and the target translations . n Machine Translation P r ( S | T , D ) = t D P r ( t ) This section describes how to use the proposed semantic role features in a Tree-to-String transducer , Here , S denotes the target sentence , T denotes the source syntax tree , and D denotes the derivation of T . In addition to the translation model , the function DECODE ( T ) for tree node v of T in bottom-up order do for template t applicable at v do { c1 , c2 } =match ( v , t ) ; s.lef tw = c1 .lef tw ; s.rightw = c2 .rightw ; s.val = c1 .val c2 .val ; s.val = P r ( t ) ; s.val = P r ( c2 .lef tw|c1 .rightw ) ; add s to v s beam ; lef tw/rightw denote the left/right boundary word of s. c1 , c2 denote the descendants of v , ordered based on RHS of t. VBG [ giving : verb ] giving VBG [ giving : verb ] giving VP NP [ giving : arg2 ] VP [ giving : arg2 arg1 ] NP [ giving : arg2 ] NP [ giving : arg1 ] NP [ giving : arg1 ] TTS system includes a trigram language model , a deletion penalty , and an insertion bonus . To incorporate the n-gram language model , states in the algorithm denote a tree node s best translations with different left and right boundary words . We use standard beam-pruning to narrow the search space . It is straightforward to generalize the algorithm for larger n-gram models and TTS templates with any number of children in the bottom using target-side binarized combination ( Huang et al. , 2006 ) . TTS template : ( VP ( VBG giving ) NP # 1 NP # 2 ) NP # 1 NP # 2 Triggered SRR : giving active : arg2 arg1 arg2 arg1 Triggered DR : giving active : verb deleted Above/middle is the state information before/after applying the TTS template , and bottom is the used TTS template and the triggered SRFs during the combination . Now we show how to incorporate the two types of semantic role features into a TTS transducer . To use the semantic role reordering feature SRR , the states in the decoding algorithm need to be expanded to encode the target-side SRSs . The SRSs are initially attached to the translation states of the source tree con 3.2 Modified Tree-to-String Transducer with . Semantic Role Features Semantic role features can be used as an auxiliary translation model in the TTS transducer , which focuses more on the skeleton-level permutation . The model score , depending on not only the input source tree and the derivation of the tree , but also the semantic roles of the source tree , can be formulated as : VBZ [ bring : verb ] VP NP [ bring : arg1 ] NNP NN new test PP [ bring : arg3 ] P r ( S | T , D ) = n f F ( S , T .role , D ) P r ( f ) where T denotes the source syntax tree with semantic roles , T .role denotes the semantic role sequence in the source side and F ( S.role , T .role , D ) denotes the set of defined semantic role features over T .role and the target side semantic role sequence S.role . Note that given T .role and the derivation D , S.role can Median = 3 ar^g1 Combined SRS arg3 verb arg1 stituents which are labeled as semantic roles for some predicate . These semantic roles are then accumulated with reordering and deletion operations specified by the TTS templates as the decoding process goes bottom-up . The model component corresponding to the feature SRR is computed when combining two translation states . I.e. , the probabilities of the SRR features composed based on the semantic roles of function DECODE ( T ) for tree node v of T in bottom-up order do for template t applicable at v do { c1 , c2 } =match ( v , t ) ; s.lef tw = c1 .lef tw ; s.rightw = c2 .rightw ; s.role = concatenate ( c1 .role , c2 .role ) ; if v is a semantic role then set s.role to v.role ; s.val = c1 .val c2 .val ; s.val = P r ( t ) ; s.val = P r ( c2 .lef tw|c1 .rightw ) ; t > Compute the probabilities associated with semantic roles the two combining states will be added into the = Q f Sema ( c1 .role , c2 .role , t ) add s to v s beam ; P r ( f ) ; combined state . Sema ( c1 .role , c2 .role , t ) denotes the triggered semantic ity is O ( N M 4 ( n 1 ) R ( LC C ! ) V ) , the number of nodes in the source syntax tree , M is the vocabulary size of the target language , n is the order of the n-gram language model , R is the maximum number of TTS templates which can be matched at a tree node , C is the maximum number of roles of a verb , and V is the maximum number of verbs in a sentence . In this formula , LC C ! is the number of role sequences obtained by first choosing i out of C possible roles and then permuting the i roles . This theoretical upper bound is not reached in practice , because the number of possible TTS templates applicable at a tree node is very limited . Furthermore , since we apply beam pruning at each tree node , the running time is controlled by the beam size , and is linear in the size of the tree . Since we are primarily interested in the relative order of the semantic roles , we approximate each semantic role s target side position by the median of the word positions that is aligned to . If more than one semantic role is mapped to the same position in the target side , their source side order will be used as their target side order , i.e. , monotonic translation is assumed for those semantic roles . The word alignments in the TTS templates are also used to compute the deletion feature DR . Whenever a semantic role is deleted in a TTS template s RHS , the corresponding deletion penalty will be applied . We describe two alternative methods for training the weights for the model s features , including both the individual TTS templates and the semantic role features . The first method maximizes data likelihood as is standard in EM , while the second method maximizes conditional likelihood for a log- linear model following Blunsom et al . 3.3.1 Maximizing Data Likelihood The standard way to train a TTS translation model is to extract the minimum TTS templates using GHKM ( Galley et al. , 2004 ) , and then normalize the frequency of the extracted TTS templates ( Galley et al. , 2004 ; Galley et al. , 2006 ; Liu et al. , 2006 ; Huang et al. , 2006 ) . The probability of the semantic features SRR and DR can be computed similarly , given that SRR and DR can be derived from the paired source/target sentences and the word alignments between them . We refer to this model as max-likelihood training and normalize the counts of TTS templates and semantic features based on their roots and predicates respectively . The difficult part of the EM algorithm is the E- step , which computes the expected counts of the TTS templates and the semantic features by summing over all possible derivations of the source trees and target strings . The standard inside- outside algorithm ( Graehl and Knight , 2004 ) can be used to compute the expected counts of the TTS templates . Similar to the modification made in the TTS decoder , we can add the target-side semantic where the features f include both the TTS templates and the semantic role features . The numerator in the formula above can be computed using the same dynamic programming algorithm used to compute the expected counts in the EM algorithm . However , the partition function ( denominator ) requires summing over all possible source trees and target strings , and is infeasible to compute . Instead of approximating the partition function using methods such as sampling , we change the objective function from the data likelihood to the conditional likelihood : role sequence to the dynamic programming states D exp i i fi ( S , T , D ) of the inside-outside algorithm to compute the ex P r ( S | T ) = P P exp P f ( S , T , D ) pected counts of the semantic features . This way St all ( T ) Dt i i i each state ( associated with a source tree node ) represents a target side span and the partial SRSs . To speed up the training , a beam is created for each target span and only the top rated SRSs in the beam are kept . 3.3.2 Maximizing Conditional Likelihood A log-linear model is another way to combine the TTS templates and the semantic features together . Again , to simplify the illustration , only binary TTS templates are used . Using the conditional probability as the objective function not only reduces the computational cost , but also corresponds better to the TTS decoder , where the best MT output is selected only among the possible candidates which can be generated from the input source tree using TTS templates . The derivative of the logarithm of the objective function ( over the entire training corpus ) w.r.t . a feature weight can be computed as : ( n-gram language model , TTS templates , SRR , DR ) weights of the transducer are tuned based on the development set using a grid-based line search , and the translation results are evaluated based on a single Chinese reference using BLEU4 ( Papineni et al. , 2002 ) . ( 2006 ) used character- based BLEU as a way of normalizing inconsistent log Q P r ( S | T ) = X EC ( fi ) EC t ( fi ) } Chinese word segmentati on , but we avoid this prob i { S , T D|S , T S |T lem as the training , developmen t , and test data are from the same source . With the objective function and its derivatives , a variety of optimization methods can be used to obtain the best feature weights ; we use LBFGS ( Zhu et al. , 1994 ) in our experiments . To prevent the model from overfitting the training data , a weighted Gaussian prior is used with the objective function . The variance of the Gaussian prior is tuned based on the development set . We train an English-to-Chinese translation system using the FBIS corpus , where 73,597 sentence pairs are selected as the training data , and 500 sentence pairs with no more than 25 words on the Chinese side are selected for both the development and test data.1 Charniak ( 2000 ) s parser , trained on the Penn Treebank , is used to generate the English syntax trees . To compute the semantic roles for the source trees , we use an in-house maxent classifier with features following Xue and Palmer ( 2004 ) and Pradhan et al . The semantic role labeler is trained and tuned based on sections 2 21 and section 24 of PropBank respectively . The standard role-based F-score of our semantic role labeler is 88.70 % . Modified KneserNey trigram models are trained using SRILM ( Stolcke , 2002 ) on the Chinese portion of the training data . The baseline system in our experiments uses the TTS templates generated by using GHKM and the union of the two single-direction alignments generated by GIZA++ . Unioning the two single-direction alignments yields better performance for the SSMT systems using TTS templates ( Fossum et al. , 2008 ) than the two single-direction alignments and the heuristic diagonal combination ( Koehn et al. , 2003 ) . The two single-direction word alignments as well as the union are used to generate the initial TTS template set for both the EM algorithm and the log-linear model . The initial TTS templates probabilities/weights are set to their normalized counts based on the root of the TTS template ( Galley et al. , 2006 ) . To test semantic role features , their initial weights are set to their normalized counts for the EM algorithm and to 0 for the log-linear model . We can see that the EM algorithm , based only on TTS templates , is slightly better than the baseline system . Adding semantic role features to the EM algorithm actually hurts the performance , which is not surprising since the combination of the TTS templates and semantic role features does not yield a sound generative model . The log-linear model based on TTS templates achieves significantly better results than both the baseline system and the EM algorithm . Both improvements are significant at p < 0.05 based on 2000 iterations of paired bootstrap re- sampling of the test set ( Koehn , 2004 ) . Adding semantic role features to the log-linear model further improves the BLEU score . One problem in our approach is the sparseness of the verbs , which makes it difficult for the log-linear model to tune the lexicalized semantic role features . One way to alleviate this problem is to make features based on verb classes . We first tried using the verb TTS Templates + SRF + Verb Class Union 15.6 EM 15.9 15.5 15.6 Log-linear 17.1 17.4 17.6 Unfortu SRF On 123 4 6,7 14,15 9 11,12 SRF Off 123 4 14,15 , 6,7 11,12 9 Source A1 gratifying2 change3 also4 occurred5 in6 the7 structure8 of9 ethnic10 minority11 cadres12 SRF On 10,11 8 4 5 2 3 nately , VerbNet only covers about 34 % of the verb SRF Off , tokens in our training corpus , and does not improve the system s performance . We then resorted 1 2 3 4 10,11 8 to automatic clustering based on the aspect model ( Hofmann , 1999 ; Rooth et al. , 1999 ) . The training corpus used in clustering is the English portion of the selected FBIS corpus . Though automatically obtained verb clusters lead to further improvement in BLEU score , the total improvement from the semantic role features is not statistically significant . Because BLEU4 is biased towards the adequacy of the MT outputs and may not effectively evaluate their fluency , it is desirable to give a more accurate evaluation of the sentence s fluency , which is the property that semantic role features are supposed to improve . To do this , we manually compare the outputs of the two log-linear models with and without the semantic role features . Our evaluation focuses on the completeness and ordering of the semantic roles , and better , equal , worse are tagged for each pair of MT outputs indicating the impact of the semantic role features . This paper proposes two types of semantic role features for a Tree-to-String transducer : one models the reordering of the source-side semantic role sequence , and the other penalizes the deletion of a source-side semantic role . The first and second example shows that SRFs improve the completeness and the ordering of the MT outputs respectively , the third example shows that SRFs improve both properties . The subscripts of each Chinese phrase show their aligned words in English . and the Tree-to-String templates , trained based on a conditional log-linear model , are shown to significantly improve a basic TTS transducer s performance in terms of BLEU4 . To avoid BLEU s bias towards the adequacy of the MT outputs , manual evaluation is conducted for sentence fluency and significant improvement is shown by using the semantic role features in the log-linear model . Considering our semantic features are the most basic ones , using more sophisticated features ( e.g. , the head words and their translations of the source- side semantic roles ) provides a possible direction for further experimentation . Acknowledgments This work was funded by NSF IIS0546554 and IIS0910611 . Binding constraints form one of the most robust modules of grammatical knowledge . Despite their crosslinguistic generality and practical relevance for anaphor resolution , they have resisted full integration into grammar processing . The ultimate reason for this is to be found in the original exhaustive coindexation rationale for their specification and verification . As an alternative , we propose an approach which , while permitting a unification-based specification of binding constraints , allows for a verification methodology that helps to overcome previous drawbacks . This alternative approach is based on the rationale that anaphoric nominals can be viewed as binding machines . Since the so-called integrative approach to anaphor resolution was developed in the late 1980s ( Carbonell and Brown 1988 ; Rich and LuperFoy 1988 ; Asher and Wada 1989 ) , and its practical viability extensively tested ( e.g. , Lappin and Leass 1994 ; Mitkov 1997 , 1998 ) , it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences . The former exclude impossible an tecedents and help to circumscribe the set of antecedent candidates ; the latter help to pick the most likely candidate , which will be proposed as the antecedent.Binding constraints are a significant subset of such filters . As they delimit the rel ative positioning of anaphors and their possible antecedents in grammatical geometry , these constraints are crucial to restricting the search space for antecedents and enhanc ing the performance of anaphor resolvers.1 From an empirical perspective , they stemfrom quite robust generalizations and exhibit a universal character , given their param eterized validity across natural languages . From a conceptual point of view , in turn , the relations among binding constraints involve nontrivial symmetry , which lends them a modular nature . Accordingly , they have been considered one of the most robust and intriguing grammar submodules , usually referred to as binding theory . However , in contrast to this , the formal and computational handling of binding constraints has presented considerable resistance . Anaphor resolution typically builds on many sources of information among them , information about the grammatical structure of the sentence so that the different fil ters and preferences may be used . Consequently , it must in general be regarded as a postgrammatical process , in the sense that it is completed after sentences are parsed . Binding constraints , as a subset of the filters for anaphor resolution , are a special case Department of Informatics , Faculdade de Ci encias de Lisboa , Campo Grande , 1700 Lisboa , Portugal . Email : Antonio.Branco @ di.fc.ul.pt . 1 See the Appendix for a specification of binding constraints . We adhere to the following terminological . convention : anaphors divide into reflexives and nonreflexives ; reflexives form a class that includes short-distance ( ruled by Constraint A ; e.g. , himself ) and long-distance reflexives ( Constraint Z ; e.g. , Chinese ziji ) ; nonreflexives include pronouns ( Constraint B ; e.g. , he ) and nonpronouns ( Constraint C ; e.g. , the student ) . Oc 2002 Association for Computational Linguistics in this respect . Given that they form a submodule of grammar , they are specified on a par with other grammatical submodules and constraints , and they are thus expected to be integrated already into the processing of grammar . Nevertheless , this integration can not be considered to have been adequately achieved . As we will discuss at length , the original methodology for verifying the compliance of grammatical representations with binding constraints requires extragrammatical processing steps delivering a forest of indexed trees to anaphor resolvers ( Chomsky 1981 ) . More recently , constraint-based grammatical frameworks either require special- purpose extensions of the description formalism , though ensuring only a partial handling of these constraints , as in Lexical-Functional Grammar ( LFG ; Dalrymple 1993 ) , or do not offer a solution yet to integrate them into grammar , as in Head-Driven Phrase Structure Grammar ( HPSG ; Pollard and Sag 1994 ) .2 Our primary goal here is thus to bridge the gap between the grammatical nature of binding constraints and their full integration into grammar processing . In particular , we aim at achieving this in such a way that a lean interface between grammar and reference processing emerges . In Section 2 , we first underline the distinction , seldom taken into account , between specification and verification of binding constraints . We then review advances proposed in the literature concerning the completion of the verification task . We observe that three major lines of progress can be identified : packing of anaphoric ambiguity , packing of nonlocal context , and lexicalization of binding constraints . Building on these contributions , in Section 3 we argue that the remaining step forward is to harmonize these different advances . We suggest that a more accurate , semantics-driven comprehension of the nature of binding constraints is a relevant move toward this harmonization . On the basis of this revision , we introduce a methodology for verifying these constraints , which rests on the new concept of binding machine , to be defined . In Section 4 , in the light of this new methodology , we show how binding constraints can be given a unification-based specification and can be fully integrated into grammar . In Section 5 , we present an illustrative example and discuss in detail how binding constraints and reference-processing systems are coordinated , and how the previously identified drawbacks are overcome . In recent decades , great strides have been made toward an empirically adequate specification of binding constraints , this being an important research issue in theoretical linguistics . Many aspects of this issue a parameterizable definition of local domain , the existence of a fourth constraint for long-distance reflexives , the possible subject- orientedness of some anaphors , and the degree of universality of binding constraints , to name just a few have come under intense scrutiny . In contrast , the verification task has been studied much less extensively . Even though important problems also remain to be solved in this more applied dimension 2 The fragment of grammar developed and extensively discussed in Pollard and Sag ( 1994 ) is formally . specified in its Appendix with the HPSG unification-based description language . Binding constraints escape such encoding . While noting that these constraints have yet to be accommodated in HPSG grammars , Bredenkamp s ( 1996 ) and Backofen et al . s ( 1996 ) subsequent elaboration of this issue implies that some kind of essential limitation of the unification-based formalism might have been reached , a suggestion we seek to contradict here . 2 of the so-called binding theory , the issue of determining whether a given grammatical representation complies with binding constraints has not attracted similar attention . In this section , we briefly review major advances reported in resolving this issue . 2.1 Exhaustive Coindexing for Filtering . The first formulation of a verification procedure , based on exhaustive coindexation , dates back to Chomsky ( 1980 , Appendix ; 1981 , Section 3.2.3 ) . The basics of this ap proach can be outlined as follows : After the grammatical parsing of a sentence with n NPs has been completed , for every parse tree t : a . Indexation : Generate a new , annotated tree by assigning indices to the NPs in t. b. Filtering : Store this annotated tree if the indexation of NPs respects binding constraints ; otherwise , delete it . c. Iteration : Repeat ( a ) ( b ) until all type-different assignments of n possibly different indices have been exhausted . As discussed in Correa ( 1988 ) , this procedure is grossly inefficient : its complexity was shown in Fong ( 1990 ) to be of exponential order . Moreover , this approach is conceptually awkward , given that a submodule of the grammar , the set of binding constraints , is not operative during grammatical processing , but functions as an extragrammatical add-on.3 This proposal also disregards the need to interface grammar with systems for reference processing . The input for such systems will not be a grammatical representation to be refined visa ` -vis the preferences for anaphor resolution , but a forest of differently labeled trees that have to be internally searched and compared with each other by anaphor resolvers . A first proposal for improving the exhaustive coindexation-driven methodology is due to Correa ( 1988 ) , whose goal was to enhance the integration of binding constraints into grammar and obtain a tractable verification procedure . Simplifying some details , the proposed algorithm can be outlined as follows : Let t be a constituency tree where every NP has a type-distinct index . Start from the top node of t with two empty stacks , A and B , where indices will be collected , respectively local c-commanding4 indices and nonlocal c-commanding indices , while descending the tree . When an NPj is found : a . Copy : Leave a copy of A ( if NPj is a short-distance reflexive ) or B ( if it is a pronoun ) at the NPj . 3 Correa ( 1988 , page 123 ) observes that although the integration of binding constraints into rules which . may be used to derive structure that already satisfies the [ constraints ] is not a straightforward task , that should be the path to follow , a point also strongly stressed in subsequent elaboration on this issue by Merlo ( 1993 ) . 4 C-command is a configurational version of the command relation where x c-commands y iff the first branching node that dominates x dominates y ( Barker and Pullum 1990 ) . Assign : Take the first index i of the stack copied into the NPj node , and annotate NPj with j = i. c. Collect : Add index j to A in each sister node of NPj . When a local domain border is crossed : d. Reset : Reset B to A B . This algorithm has been given two different implementations , one by Correa ( 1988 ) , the other by Ingria and Stallard ( 1989 ) . Further elaboration by Giorgi , Pianesi , and Satta ( 1990 ) and Pianesi ( 1991 ) offers a variant in terms of formal language techniques , where the stack copied into pronouns contains the antecedent candidates excluded by Principle B . The do-it-while-parsing approach of Correa s implementation has the advantage of discarding a special-purpose postgrammatical module for binding . Nevertheless , this solution turns out to be dependent on a top-down parsing strategy . On the other hand , while Ingria and Stallard s implementation is independent of the parsing strategy adopted , its independence comes at the cost of still requiring a special-purpose postgrammatical parsing module for binding . Besides incorporating binding theory into grammar , Correa s development inside the coindexation-driven methodology presents other significant improvements . If one disregards step ( b ) a disguised recency preference mixed with binding constraints and considers the result of verifying these constraints to be the assignment to an NP of the set of indices of its grammatically admissible antecedents , then it is possible to discard the proliferation of indexed trees as a way to express anaphoric ambiguity . Moreover , this packing of anaphoric ambiguity provides for a neat interface with anaphor resolvers , whose preferences will then pick the most likely antecedent candidate from the relevant stack of indices . These advances permit a verification procedure of tractable complexity ( Correa 1988 , page 127 ; Giorgi , Pianesi , and Satta 1990 , page 5 ) . This results crucially from the move toward the lexicalization of the constraining effect of binding principles , a solution also adopted in subsequent proposals by other authors , as we will discuss below . The binding constraint of each anaphor is now enforced independently of how the surrounding anaphors happen to be resolved . This implies that there is no need to anticipate all the different resolutions for every relevant anaphor with a process of exhaustive coindexation . It also implies that cases of undesired transitive anaphoricity are handled by other filters during the anaphor resolution process.5 However , these positive results regarding the verification task seem to be obtainedat the cost of some negative consequences regarding the specification task and em pirical adequacy . The above algorithm is acknowledged not to be able to cope with 5 Consider the sentence John said that he shaved him . Ignoring how other anaphors are resolved , in the light . of Binding Constraint B , he can take John as its antecedent , as empirically replicated in other minimally different examples such as Johni said that he shaved Peter ; likewise , him can take John as its antecedent . A point worth noting is that , if he actually ends up resolved against John , the latter can not be the antecedent of him , and vice versa . This specific resolution of he and him , out of the many possible resolutions , blocks two anaphoric links that would otherwise have been admissible . It induces a contingent violation of binding constraint B due to an accidental , transitive anaphoric relationship between he and him . This issue is not discussed in Correa ( 1988 ) , since this paper is strictly focused on syntax and binding . See footnote 13 below for a suggestion on how this issue may be handled in a grammatical framework integrating syntactic and semantic representations . 4 constraints involving nonlocal dependencies . It does not account for Principle C , and it only partially accommodates the anaphoric potential of anaphors complying with Principle B . As Stack B only contains indices of the nonlocal c-commanders rather than all indices except those of the local c-commanders the algorithm does not correctly account for the constraining effect of Principle B . Also this approach does not account for backward anaphora or crossover cases ( Correa 1988 , page 127 ; Ingria and Stallard 1989 , page 268 ) .6 2.3 Packing Nonlocality . Other improvements in the task of verifying binding constraints are due to Dalrymple ( 1993 ) and Johnson ( 1995 ) . Instead of being concerned with packing ambiguity , they are concerned with packing nonlocality . 2.3.1 Trees in Nodes of Trees . Johnson s ( 1995 ) algorithm is embodied in Prolog code . Abstracting away from details associated with that format , it can be outlined as follows : Let t be a constituency tree where every NP has a type-distinct index . For every NPi in t , traverse the tree from NPi upward until the top node is reached . When a locally c-commanding NPj is found : a. Annotate NPi with i = j if NPi is a short-distance reflexive . b. Annotate NPi with i /= j if NPi is a nonreflexive . When a nonlocally c-commanding NPj is found : c. Annotate NPi with i /= j if NPi is a nonpronoun . Although this outline renders the algorithm in a bottom-up fashion , Johnson ingeniously develops an implementation of it that is independent of the parsing strategy by resorting to delaying mechanisms . Consequently , despite its postgrammatical flavor , this implementation does not require postgrammatical processing , thus incorporating the task of binding constraint verification into grammar processing . These results are obtained with some auxiliary devices . Each node in the tree is conceptualized as a pair consisting of a tree and a vertex in that tree ( Johnson 1995 , page 62 ) . Consequently , the whole tree where a given NP appears is locally accessible to be walked up since its replica is present at the pair ( Category , Tree ) , which is the NP node itself . This algorithm makes the verification of binding constraints more efficient because it does not resort to exhaustive indexation . However , it does so at the cost of highly complicating the grammatical representation , since the tree is replicated at each one of its nodes . While avoiding exhaustive indexation , this approach does not fully eliminate the proliferation of trees . For a given ambiguous reflexive , with more than one admissible 6 See the Appendix for the notion of locality and local domain and other auxiliary notions in the . Backward anaphora occurs in cases where the anaphor is resolved against an antecedent that occurs linearly after the anaphor , as in If hei is around , Peteri will do it . An example of so-called crossover cases is the ungrammatical construction *Whoi did Peter think shei saw ? or *Peteri , hei said you like , where the fronted phrase is meant to be the antecedent of some pronoun c-commanding the position from which this phrase is displaced . 5 antecedent , each antecedent candidate corresponds to a different coindexation and , consequently , to a different tree . That is what generally happens with long-distance reflexives , whose antecedents can be found in any of the binding domains induced by the local or by the upward predicators , but it may also happen with short-distance ones , as in ( 2 ) below . As to the interface with reference processing , problems arise with reflexives and nonreflexives , though of different nature . Reflexives , if ambiguous , give rise to proliferation of trees , thus requiring comparison between trees during subsequent anaphor resolution . As to nonreflexives pronouns and nonpronouns their analysis does not give rise to proliferation of trees , but the representation of their ambiguity is not fully made explicit in the grammatical representation of the sentence being parsed . This is so because they end up associated with negative information , that is , information about what NPs can not be their antecedents . The index of a pronoun is made unequal with the indices of its local c-commanders ; it is not made equal with the indices of its grammatically admissible antecedents . The same holds for nonpronouns with respect to their c-commanders . Consequently , in this case , the task of determining the antecedent candidates that satisfy the relevant binding constraint of nonreflexives remains to be completed after grammatical processing is finished . This will involve some postgrammatical rescanning of the parse tree generated for extracting the indices that do not enter in the inequalities obtained during the parsing . Finally , like Correa s approach , Johnson s does not account for backward anaphora , as only surface c-commanders are visible to the tree-climbing procedure . 2.3.2 Equations with Regular Expressions . The basic LFG account of binding , set forth by Dalrymple ( 1993 ) , adopts a different approach to generalize over the possible nonlocality of intrasentential anaphoric dependencies . This approach makes crucial use of a special-purpose extension of the LFG description formalism , the so-called binding equations , which are lexically associated with anaphors . Building on Kaplan and Maxwell s ( 1988 ) proposal concerning functional uncertainty , binding equations are designed to encode the uncertainty concerning the long-distance path between the positions of the anaphor and its permissible antecedent in the grammatical structure.7 Given that uncertainty concerning long-distance dependencies involves a ( possibly infinite ) disjunction of possibilities , the basic idea is to encode such a disjunction in finite terms by the use of regular expressions over feature structures . An example of a binding equation encoding functional uncertainty is given in ( 1 ) , preceded by an example with the corresponding long-distance subject-oriented reflexive , Chinese ziji . ( 1 ) Zhangsani yiwei [ Lisij yiwei [ ... zijii/j/ ... Zhangsani thought [ Lisij thought [ ... himi/j/ ... ] ] ziji : ( ( COMP* OBJ ) SUBJ ) = The right-hand side of the equation stands for the semantic representation ( ) of the anaphor ( ) , while the left-hand side stands for the semantic representation of the antecedent . The description of the antecedent indicates that the long-distance reflexive is an object and that this object is constrained to be part of a feature structure where 7 Koenig ( 1999 ) introduces a device in HPSG description language for stating inside-out constraints . would help in developing an HPSG emulation of the LFG approach for the verification of binding constraints . 6 its antecedent may be one of the possibly many upward subjects . The Kleene operator * allows abbreviation of the set of paths consisting of zero or more occurrences of COMP corresponding to possible successive clausal embeddings followed by one occurrence of OBJ . While regular expressions may be used in binding equations , such expressions are not necessary if the grammatical relation between the anaphor and its admissible antecedents does not involve a long-distance dependency . That is the case in ( 2 ) , which displays the binding equation for the short-distance reflexive himself . Given that both the subject and the object are admissible antecedents for the reflexive , in the binding equation the use of the attribute GF , which stands for any grammatical function , underspecifies the grammatical functions of the admissible antecedents ( Dalrymple 1993 , Section 4.4.2 ) . ( 2 ) Johni described Billj to himselfi/j . himself : ( ( OBLGoal ) GF ) = Binding equations may also express negative constraints , as in ( 3 ) , where the semantic representation of the pronoun is constrained to be different from that of its local coarguments . ( 3 ) Johni described Billj to himi/j . him : ( ( OBLGoal ) GF ) /= As noted in Dalrymple ( 1993 , Section 3.3 ) , a few aspects of this approach for binding need to be fully worked out . For instance , the positive equations for reflexives do not require identity of indices of anaphorically related expressions , but instead impose identity of semantic representations . Without further elaboration , this will incorrectly enforce any type of anaphoric link ( coreference , bound , bridging , e-type , etc . ) to the sole mode of coreference . Another important issue is the account of nonlexical anaphoric NPs : it is not clear how this type of NP ( e.g. , anaphoric definite descriptions , ruled by Principle C ) may be assigned the corresponding binding equation . However these difficulties turn out to be resolved , the LFG approach for binding , though building on a different strategy for handling nonlocality , presents the same sort of problems as Johnson s proposal . The interfacing of grammar with reference-processing systems is problematic since the proliferation of representations is not avoided . Constructions with reflexives , if these are ambiguous , end up associated with several grammatical representations . In the case of long-distance reflexives , as exemplified in ( 1 ) , these representations result from the possibly many solutions for the functional uncertainty encoded by the regular expression in the binding equation . In the case of short-distance reflexives , as exemplified in ( 2 ) , they result from the different solutions for the unification of the different grammatical functions of the admissible antecedents with the attribute GF in the binding equation . Likewise , the anaphoric capacity of pronouns and nonpronouns , typically ambiguous , is not explicitly captured in the final grammatical representation . These anaphors are lexically associated with negative equations , and for this type of equation there is only one possible solution , namely , the grammatical structure where the semantic representation of the anaphor is not identical to the semantic representations of any of the phrases complying with the description of the antecedent in the left-hand side 7 of the equation ( Dalrymple 1993 , Section 4.1.5 ) . Therefore , for these anaphors the final grammatical representation provides no information about what their admissible antecedents are according to the relevant binding constraints . The contributions assessed above share a common point of departure with regard to the verification algorithm first proposed by Chomsky ( 1981 ) , each addressing and solving some of its more significant drawbacks . The common move toward the lexical- ization of binding constraints represents an important shift in the verification strategy : verifying binding constraints is not a matter of inspecting final grammatical representations , but instead a matter of some local operation triggered by information lexically associated with anaphors about their anaphoric class . This move has allowed binding constraint verification to be incorporated into grammar processing and permitted tractable verification procedures . From the discussion in the previous section , it follows also that these contributions have been partially successful in overcoming other problems of the verification methodology based on exhaustive coindexation . Though partially successful , they have brought to the fore important dimensions of binding that have to be concomitantly accounted for . Accordingly , an alternative method for the verification of binding constraints has to find a way to harmonize all those different dimensions lexicalization , anaphoric ambiguity packing , and nonlocal context packing while providing adequate empirical coverage and neatly interfacing grammar with reference processing . Against this background , a breakthrough depends , in our view , on reconsidering some primitives underlying the conception of binding constraints . In the previous section , we made a clear distinction between specification and verification of binding constraints , so that the latter task could be isolated and better assessed . We will argue now that further progress on the verification task depends on bridging this distinction and possibly changing the way the specification of binding constraints is understood . 3.1 Patterns in the Semantics of Anaphors . Binding constraints have generally been viewed as well-formedness conditions on syntactic representations , thus belonging to the realm of syntax . In line with Gawron and Peters ( 1990 ) , however , we think these constraints should rather be understood as conditions on semantic representations , since they primarily delimit ( nonlocal ) aspects of semantic composition , rather than aspects of syntactic composition.8Like other types of constraints on semantic composition , binding constraints im pose conditions on the interpretation of certain expressions anaphors , in the present case based on syntactic geometry . However , this can not be viewed as implying that they express grammaticality requirements . By replacing a pronoun with a reflexive in a given sentence , for instance , we do not turn a grammatical construction into anungrammatical one , even if we assign to the reflexive the antecedent appropriately se 8 As implied by the title of this section , and as will become clear in the following discussion , this does . not mean that we are claiming that binding theory can be built without any reference to syntactic constructs . In the argument in the following paragraphs , we are assuming a notion of semantic composition not in its strict sense , as used for example in Montague Grammar , but in the broader sense that the intermediate semantic representations of the expressions are composed from other representations , as used in Discourse Representation Theory ( DRT ) . Note that reformulations of frameworks like DRT can be worked out that result in a semantic system adhering to strict compositionality ; see Janssen ( 1997 , Section 4.4 ) for references and a thorough discussion of this issue . 8 lected for the pronoun . In that case , we are simply asking the hearer to try to assign to that sentence a meaning it can not express just as if we were to ask whether someone could interpret The red book is on the white table as describing a situation where a white book is on a red table . In this example , given how they happen to be syntactically related , the semantic values of red and table can not be composed in such a way that the sentence could be used to describe a situation concerning a red table , rather than a white table . Likewise , in the sentence John thinks Peter shaved him , given how they happen to be syntactically related , the semantic values of Peter and him can not be composed in such a way that this sentence could be used to describe a situation where John thinks that Peter shaved himself ( i.e. , Peter ) , rather than a situation where John thinks that Peter shaved other people ( e.g. , Paul , Bill , or John himself ) . The difference between these two cases is that in the former , the composition of the semantic contributions of white and table ( for the interpretation of the NP white table ) is constrained by local syntactic geometry , while in the latter , the composition of the semantic contributions of John and him ( for the interpretation of the NP him ) is constrained by nonlocal syntactic geometry . This discussion leads us to consider that , semantically , an anaphor should be specified in the lexicon as a function whose argument is a suitable representation of the context providing a semantic representation of the NPs available in the discourse vicinity and its value is the set of the grammatically admissible antecedents for that anaphor . This rationale is in line with other approaches to the meaning of anaphors that , building in other sorts of arguments or research concerns , understand it also as a projection from some relevant representation of contexts to entities.9 But given the specific focus of the present study , what should be noted is that , all in all , there will be four such functions available to be lexically associated with anaphors , each corresponding to one of the four different classes of anaphors , in accordance with the four binding constraints A , B , C , and Z.10 3.2 Binding Machines . Given these considerations , we can show that this conceptual shift to a semantics driven approach for the verification of binding constraints provides an adequate basis for harmonizing the advances put forward in the literature and discussed above . To make this alternative rationale for binding perspicuous , we suggest envisioning an anaphoric NP as a binding machine , which operates by receiving an input , changing its internal state , and returning an output . More specifically , an anaphoric NP can be 9 See , among others , Gawron and Peters ( 1990 ) , Lappin and Francez ( 1994 ) , and the discussion in . Adopting Lo bner s ( 1987 ) duality criterion for quantification in natural language , and the formal tools he developed for the analysis of phase quantification , we showed in Branco ( 2000 ) that the four binding constraints can be seen as the effect of four binding quantifiers . These phase quantifiers can be viewed as being expressed by the nominals of the four binding classes , and they quantify over the reference markers organized in the obliqueness order . A full-fledged account of the empirical support and justification for these results , and of their implications , is beyond the scope of this article . For an abridged presentation of the core argument , see Branco ( 1998 ) . 10 As there are different grammatical frameworks , binding constraints have been specified under different . Some differences between versions are due just to this fact that binding constraints are supposed to be accommodated into different grammatical frameworks ; some other differences , however , are real differences of specification in the sense that different variants may not have the same empirical coverage or be aimed at predicting the same ( un ) grammatical constructions . In the Appendix , we present a common and fairly well empirically tested version of binding theory given the current state of the art in this area , a version presently adopted in the HPSG framework . For an alternative , see for example Reinhart and Reuland ( 1993 ) . 9 viewed as a binding machine that ( 1 ) takes a representation of its context ; ( 2 ) updates its own semantic value in response both to its context and to its intrinsic anaphoric potential ( i.e. , in accordance with its binding constraint ) ; and ( 3 ) contributes to the makeup of the context , which the other binding machines read as input ( i.e. , against which the other anaphoric NPs are interpreted ) .11 The output of an anaphoric nominal n viewed as a binding machine is simply the incrementing of the context with a copy of its reference marker.12 The internal state of the machine after its operation is a representation of the con- textualized anaphoric capacity of n under the form of the set of reference markers of the grammatically admissible antecedents of n. This internal state results when the binding constraint associated with n is applied to the input , and it is the interface point between grammar and reference processing . This set of reference markers collects the antecedent candidates , and its elements are submitted to other filters and preferences by the anaphor resolvers so that one of them ends up being chosen as the antecedent . The input , in turn , is a representation of the aspects of the context relevant to help circumscribe the anaphoric potential of nominal anaphors . It is coded under the form of three lists of reference markers , A , Z , and U . In list A , the reference markers of the local o-commanders of n are ordered according to their relative grammatical obliqueness ; Z includes the o-commanders of n , possibly observing a multiclausal obliqueness hierarchy ; and U is the list of all reference markers in the discourse context , including those not linguistically introduced . Given this setup , the contribution of binding constraints in circumscribing the an- aphoric potential of nominals is explicitly acknowledged . The particular contextualized instantiation of that potential and the verification of binding constraints coincide and consist of a few simple steps . If n is a short-distance reflexive , its internal state is set up as At , where At contains the reference markers of the o-commanders of n in A . If n is a long-distance reflexive , its semantic representation includes Zt , such that Zt contains the o-commanders of n in Z . If n is a pronoun , B = U \ ( At [ r-markn ] ) is encoded into its representation , where r-markn is the reference marker of n. Finally , if n is a nonpronoun , its updated semantics keeps a copy of C = U \ ( Zt [ r-markn ] ) . Besides adhering to an empirically grounded conception of binding constraints , this approach embodies , and harmonizes , the crucial contributions of previous pro posals concerning the verification of these constraints . It assumes the lexicalization of binding constraints . Concomitantly , it builds on specific strategies for the packing of anaphoric ambiguity ( viz. , list of reference markers ) and nonlocal context ( viz. , setof lists of reference markers ) . Moreover , it achieves this while avoiding the above mentioned problems related to the proliferation of grammatical representations and to the interfacing of grammar with reference processing , as well as the problems of ensuring complete empirical coverage.What remains to be discussed is whether , given this new format for the verifica tion of binding constraints , they can still be specified and integrated into grammar processing with currently affordable formal and computational tools . This new approach to binding constraints can be integrated into grammar easily and in a principled manner . In what follows , we outline how these constraints can be specified and handled in a unification-based grammatical framework such as HPSG . 11 This rationale is in line with the insights of Johnson and Klein ( 1990 ) concerning the processing of the . 12 See Kamp and Reyle ( 1993 ) for the notion of reference marker.. 10 As a proposal for that integration , we designed an extension to the Underspecified Discourse Representation Theory ( UDRT ) semantics component for HPSG developed by Frank and Reyle ( 1995 ) . This component is encoded as the value of the feature CONT ( ENT ) , which is now extended with the feature ANAPH ( ORA ) ; see ( 4 ) . This new feature keeps information about the anaphoric potential of the corresponding nominal n : its subfeature ANTEC ( EDENTS ) keeps a record of how that potential is updated when the anaphor enters a grammatical construction ; and its subfeature R ( EFERENCE ) -MARK ( ER ) indicates the reference marker of n , to be contributed to the context . Similarly , and still assuming Pollard and Sag s ( 1994 ) feature geometry as a starting point , the NONLOC value is also extended with a new feature , BIND ( ING ) , with subfeatures LIST-A , LIST-Z , and LIST-U . These lists provide a specification of the relevant context and correspond to the lists A , Z , and U above . Subfeature LIST-LU is a fourth , auxiliary list for encoding the contribution of local context to the global , nonlocal context . The SYNSEM value of a pronoun , for instance , can now be designed as shown in ( 4 ) . ( 4 ) ls l m a x 1 1 l m i n 1 subord { } label 1 1 loc | cont conds a r g r 2 r m a r k 2 anaph antec 5 principleB ( 4 , 3 , 2 '\ list-a 3 list-z list nonloc bind l i s t u 4 list-lu 2 Given this feature structure , the binding constraint associated with pronouns is specified as the relational constraint principleB . This relational constraint returns list B as the value of ANTEC . It is defined to take ( in the first argument ) all markers in the discourse context , given in LIST-U value , and remove from them both the local o-commanders of the pronoun ( included in the second argument ) and the marker corresponding to the pronoun ( in the third argument ) . The SYNSEMs of other anaphors , ruled by Principles A , C , and Z , are similar to the one above.13 The only difference lies in the relational constraint in the ANTEC value , which encodes the appropriate binding constraint and returns the updated anaphoric potential under the form of list At , C , or Zt , respectively , as discussed in the previous section . Turning to the specification of the context ( i.e. , the values of LIST-A , LIST-Z , LIST-U , and LIST-LU ) , this is handled by means of a new HPSG principle , which can be termed the Binding Domains Principle . This principle consists of three clauses constraining 13 Binding constraints for nonlexical anaphoric nominals are lexically stated in the corresponding . A constraint for pronominal anaphoric transitivity may also be introduced at the lexical representation of pronouns , by including in the CONDS value in ( 4 ) Discourse Representation Structure conditions expressing that ra , rb ( ( 2 = anaph ra rb = anaph ra ) ( [ rb ] 5 = 5 ) ) . 11 signs and their values with respect to these lists of reference markers . Due to space limitations , we illustrate this principle simply by stating Clause I , which constrains LIST-U and LIST-LU.14 ( 5 ) Binding Domains Principle , Clause I a . In every sign , the LIST-LU value is identical to the concatenation of the LIST-LU values of its daughters . In a sign of sort discourse , the LIST-LU and LIST-U values are token identical . c. In a non-NP sign , the LIST-U value is token identical to each LIST-U value of its daughters . d. In an NP sign k , i . In Spec-daughter , the LIST-U value is the result of removing the elements of the LIST-A value of Head-daughter from the LIST-U value of k ; ii . In Head-daughter , the LIST-U value is the result of removing the value of R-MARK of Spec-daughter from the LIST-U value of k. LIST-LU collects , up to the outermost sign of sort discourse , all the markers contributed by the different NPs for the context . At this sign , they are passed to LIST-U , by means of which they are propagated to every NP . The HPSG ontology was extended with the sort discourse , which corresponds to sequences of sentential signs and at whose signs reference markers from the nonlinguistic context may be introduced in the semantic representation.15 Subclause ( d ) is meant to avoid what is known in the literature as the i-within-i effect . The above unification-based specification of binding constraints , while ensuring their integration into grammar , allows the binding module to be suitably hooked up with systems of reference processing . Feature ANTEC is the interface point between them . 14 Clauses II and III constrain LIST-A and LIST-Z , respectively . Roughly , Clause II ensures that the LIST-A . value is passed from the lexical head to its successive projections , and also from the head-daughters to their arguments . Note that exemption occurs when principleA ( 1 , 2 ) is the empty list , in which case the reflexive should find its antecedent outside any binding constraint ( Pollard and Sag 1994 , Chapter 6 ) . Clause III ensures that , at the top node of the grammatical representation , LIST-Z is set up as the LIST-A value of that node , and that LIST-Z is successively incremented at the suitable downstairs nodes by appending its value with the LIST-A value of those nodes . At the lexical entry of a predicator , LIST-A is defined as the concatenation of the R-MARK values of its subcategorized arguments specified in the ARG-S value . For a detailed specification of the Binding Domains Principle , see Branco ( 2000 ) . 15 Reference markers can be introduced linguistically , by the utterance of the corresponding expressions , . or nonlinguistically , by means of their cognitive availability in the context of the discourse . Theories of natural language semantics can be used to represent these two types of reference markers . Nevertheless , only a global theory encompassing natural language and cognition seems to be able to pursue the ambitious goal of providing an integrated account of how both types of markers , and not only those linguistically evoked , are introduced into semantic representation . 12 We are following a distinction between the notions of anaphor resolution and reference processing commonly assumed in the literature . Anaphor resolution is seen as being concerned with the task of identifying the antecedents of anaphors . It is therefore part of a reference-processing system , whose overall goal , in turn , is to determine the interpretation of the anaphors . This involves determining the appropriate semantic type of the anaphoric link between an anaphor and its antecedent ( coreference , bridging , e-type , bound anaphora , etc . ) and providing a suitable semantic representation for this link . Being the interface point between grammatical representation and reference processing , the list value of the feature ANTEC has just to be reduced by anaphor resolvers , given the relevant preferences and filters other than binding constraints , until the most likely antecedent is isolated . It is thus a process concerning selection in a list , rather than search in a set of indexed trees . As to reference processing in general , the specification suggested in the previous section provides a suitable framework for the correct representation of the semantically different types of anaphoric links , the range of options not being restricted to coreference only . After the anaphor has been resolved , the reference marker of the anaphor and the reference marker selected as the antecedent can be related in accordance with the mode of anaphora determined by the reference-processing system . This semantic relation between anaphorically related reference markers can be represented simply as another DRS condition in the CONDS value . This makes possible a mainstream DRT representation for the resolved anaphoric link , thus building on the substantial number of already worked out solutions available in the literature for DRT-based semantic representation of anaphora.16 This specification of binding theory for HPSG was tested with a computational implementation using ProFIT ( Erbach 1995 ) . In this implementation , the relational constraints corresponding to binding principles were straightforwardly encoded by means of Prolog predicates associated to the lexical clauses of anaphoric expressions , and defined in terms of simple auxiliary predicates ensuring the component operations of list appending , list difference , and so on . It is worth noting that some of these predicates have arguments for example , the LIST-U value , whose value is computed when the whole relevant grammatical representation is built up . This is a consequence of packing nonlocal information in such lists . As in Johnson s approach , it requires that some delaying device be used , which in this computational grammar was done by resorting to the Prolog built-in predicate freeze/2 . For the sake of the example , consider the following multiclausal sentence from Portuguese displaying backward anaphora between a topicalized reflexive and a pronoun : ( 6 ) De si pro prio , cada estudante disse que ele gosta . of him self every student said that he likes Himself , every student said that he likes . overview concerning the semantic representation of different modes of anaphora . ..CONT|CONDS ... ARGR LIST_A 415 . LIST_A LIST_Z LIST_U LIST_LU , , , , , , , , LIST_A LIST_Z LIST_U LIST_LU 5 4 5 4 4 1 5 , 2 4 , 2 4 7 , 5 4 , 3 9 2 , , , . ..|BINDING LIST_Z LIST_U LIST_LU 415 , , , , ctx LIST_A LIST_Z LIST_U LIST_LU 5 4 5 4 , , , , , , . ..|ANAPHORA REFMARK 392 himse lf ANTEC 24 . ..|BINDING LIST_A LIST_Z LIST_U LIST_LU 24 , 392 54 , 24 , 392 415 , 24 , 247 , 54 , 392 392 LIST_A LIST_Z LIST_U LIST_LU 24 , 392 , , , , , , 392 . ..|ANAPHORA REFMARK 247 every stu d e nt . ..|BINDING VAR LIST_A LIST_Z LIST_U 54 54 , , , , said that LIST_A LIST_Z LIST_U LIST_LU 24 , 392 54 , 24 , 392 415 , 24 , 247 , 54 , 392 LIST_LU 247 , 54 REFMARK he ... |ANAPHORA . ..|BINDING ANTEC LIST_A LIST_Z LIST_U LIST_LU , , , 24 , 392 , , , , , , 24 likes trace in the leaves of the tree , while the ones above the tree correspond to partial representations of some nonterminal nodes . In the outer nodes of the matrix clause , due to the effect of the Binding Domains Principle , Clause III , the LIST-Z value is obtained from the value of LIST-A , with which it is token identical , comprising the list with a single element ( 54 ) . In the nodes of the embedded clause , the LIST-Z value is the concatenation of that upper LIST-Z value and the LIST-A value ( 24 , 392 ) in the embedded clause , from which the list ( 54 , 24 , 392 ) is the result . LIST-A values are obtained from the representation of the subcategorization frames of the verbal predicators . Reading upward , note that at each higher level in the constituency representation , the list gets longer ; by the effect of the Binding Domains Principle , Clause I , the LIST-LU value at a given node gathers the reference markers of the nodes dominated by it . At the discourse top node , LIST-LU includes all the reference markers of the NPs in the example , the list ( 415 , 24 , 247 , 54 , 392 ) . The Binding Domains Principle , Clause I , also ensures that this list of all reference markers is passed to the LIST-U value of the top node and that it is then percolated down to all relevant nodes of the grammatical representation . Taking a closer look at the NPs , it is easy to check that every phrase contributes to the global anaphoric potential of its linguistic context by passing the tag of its reference marker into its own LIST-LU . In the case of the quantificational NP every student , two tags are passed , corresponding to the REFMARK value , providing for e-type anaphora , and the VAR value , providing for bound anaphora interpretations . And in the case of the ctx node , to illustrate how the nonlinguistic context may be taken into account in the linguistic representation , the reference marker ( 415 ) is obtained from the set of semantic conditions that conventionally may capture the nonlinguistic context.On the other hand , the context also contributes to establishing the anaphoric po tential of each NP . This is ensured by the different clauses of the Binding Domains Principle , which enforce the presence of suitable values of LIST-A , LIST-Z , and LIST-U at the different nodes . Finally , token identity is ensured between the ANTEC value and the outcome of the different relational constraints that are lexically associated with each NP and express binding constraints . The value of ANTEC is a list that , at this stage of anaphor resolution , records the grammatically admissible antecedents of the corresponding anaphor only in the light of binding constraints . Departing from the coindexation-driven approach for encoding anaphoric dependencies in grammatical representations , we have proposed an alternative methodology where binding constraints are viewed as contributing to circumscribing their contextually determined semantic value . This semantics-driven approach allows a principled integration of binding constraints into grammar that supports both a specification format and a verification methodology free from previous difficulties . Importantly , it also permits a neat interface between the grammatical module of binding and systems of reference processing . Appendix In this article , we consider the version of binding constraints formulated within Head- Driven Phrase Structure Grammar ( Pollard and Sag 1994 , Chapter 6 ) . Recent developments indicate that there are four binding constraints ( Xue , Pollard , and Sag 1994 ; 15 Branco and Marrafa 1999 ) . Here , the definition of each binding constraint is followed by an illustrative example . ( 7 ) Principle A A locally o-commanded short-distance reflexive must be locally o-bound . Leei thinks [ Maxj saw himself i/j ] . ( 8 ) Principle Z An o-commanded long-distance reflexive must be o-bound . [ O amigo do Ruii ] j acha que o Pedrok gosta dele pro prio i/j/k . [ the friend of the Rui ] thinks that the Pedro likes of he PRO PRIO [ Rui s friend ] j thinks that Pedrok likes himj /himselfk . ( Portuguese ) ( 9 ) Principle B A pronoun must be locally o-free . Leei thinks [ Maxj saw himi/ j ] . ( 10 ) Principle C A nonpronoun must be o-free . [ Kimi s friend ] j thinks [ Lee saw Kimi/ j ] . These constraints are defined on the basis of some auxiliary notions.The notion of local domain involves the partition of sentences and associated gram matical geometry into two zones of greater or lesser proximity with respect to the anaphor . The exact definition of the boundary separating the local from the nonlocal domain may vary from language to language . Typically , the local domain tends to correspond to the structure in the grammatical representation that is affected by the selectional capacity and requirements of a predicator . O-command is a partial order under which , in a clause , the subject o-commands the direct object , the direct object o-commands the indirect object , and so on , following the usual obliqueness hierarchy of grammatical functions , while in a multiclausal sentence , the upward arguments o-command the successively embedded arguments . The notion of o-binding is such that x o-binds y iff x o-commands y and x and y are coindexed , where coindexation is meant to represent anaphoric links . I am grateful to Hans Uszkoreit for advice and helpful discussion , and to Mark Johnson for clarifying criticisms . I am solely responsible for remaining errors . The results presented here were obtained while I was on leave at the Language Technology Group of the DFKIGerman Research Center on Artificial Intelligence , Saarbru cken , Germany , whose hospitality and enthusiastic atmosphere I was very fortunate to enjoy and I hereby gratefully acknowledge . Many types of polysemy are not word specific , but are instances of general sense alternations such as ANIMAL-FOOD . Despite their pervasiveness , regular alternations have been mostly ignored in empirical computational semantics . This paper presents ( a ) a general framework which grounds sense alternations in corpus data , generalizes them above individual words , and allows the prediction of alternations for new words ; and ( b ) a concrete unsupervised implementation of the framework , the Centroid Attribute Model . We evaluate this model against a set of 2,400 ambiguous words and demonstrate that it outperforms two baselines . One of the biggest challenges in computational semantics is the fact that many words are polysemous . For instance , lamb can refer to an animal ( as in The lamb squeezed through the gap ) or to a food item ( as in Sue had lamb for lunch ) . Polysemy is pervasive in human language and is a problem in almost all applications of NLP , ranging from Machine Translation ( as word senses can translate differently ) to Textual Entailment ( as most lexical entailments are sense-specific ) . The field has thus devoted a large amount of effort to the representation and modeling of word senses . The arguably most prominent effort is Word Sense Disambiguation , WSD ( Navigli , 2009 ) , an in-vitro task whose goal is to identify which , of a set of predefined senses , is the one used in a given context . In work on WSD and other tasks related to pol- ysemy , such as word sense induction , sense alternations are treated as word-specific . As a result , a model for the meaning of lamb that accounts for the relation between the animal and food senses can not predict that the same relation holds between instances of chicken or salmon in the same type of contexts . A large number of studies in linguistics and cognitive science show evidence that there are regulari- ties in the way words vary in their meaning ( Apresjan , 1974 ; Lakoff and Johnson , 1980 ; Copestake and Briscoe , 1995 ; Pustejovsky , 1995 ; Gentner et al. , 2001 ; Murphy , 2002 ) , due to general analogical processes such as regular polysemy , metonymy and metaphor . Most work in theoretical linguistics has focused on regular , systematic , or logical polysemy , which accounts for alternations like ANIMAL-FOOD . Sense alternations also arise from metaphorical use of words , as dark in dark glass-dark mood , and also from metonymy when , for instance , using the name of a place for a representative ( as in Germany signed the treatise ) . Disregarding this evidence is empirically inadequate and leads to the well-known lexical bottleneck of current word sense models , which have serious problems in achieving high coverage ( Navigli , 2009 ) . We believe that empirical computational semantics could profit from a model of polysemy1 which ( a ) is applicable across individual words , and thus capable of capturing general patterns and generalizing to new 1 Our work is mostly inspired in research on regular polysemy . However , given the fuzzy nature of regularity in meaning variation , we extend the focus of our attention to include other types of analogical sense construction processes . 151 First Joint Conference on Lexical and Computational Semantics ( *SEM ) , pages 151 160 , Montre al , Canada , June 78 , 2012 . Qc 2012 Association for Computational Linguistics words , and ( b ) is induced in an unsupervised fashion from corpus data . This is a long-term goal with many unsolved subproblems . The current paper presents two contributions towards this goal . First , since we are working on a relatively unexplored area , we introduce a formal framework that can encompass different approaches ( Section 2 ) . Second , we implement a concrete instantiation of this framework , the unsupervised Centroid Attribute Model ( Section 3 ) , and evaluate it on a new task , namely , to detect which of a set of words in- stantiate a given type of polysemy ( Sections 4 and 5 ) . We finish with some conclusions and future work ( Section 7 ) . In addition to introducing formal definitions for terms commonly found in the literature , our framework provides novel terminology to deal with regular polysemy in a general fashion ( cf . As an example , let lambanm denote the ANIMAL sense of lamb , lambfod the FOOD sense , and lambhum the PERSON sense . Then , an appropriate model of meta alternations should predict that score ( animal , food , lambanm , lambfod ) is greater than score ( animal , food , lambanm , lambhum ) . Meta alternations are defined as unordered pairs of meta senses , or crossword senses like ANIMAL . The meta senses M can be defined a priori or induced from data . They are equivalence classes of senses to which they are linked through the function meta . A sense s instantiates a meta sense m iff meta ( s ) = m. Functions inst and sns allow us to define meta senses and lemma-specific senses in terms of actual instances , or occurrences of words in context . 2 We reuse inst as a function that returns the set of instances . We decompose the score function into two parts : a representation function repA that maps a meta alternation into some suitable representation for meta alternations , A , and a compatibility function comp that compares the relation between the senses of a word to the meta alternation s representation . Thus , comp repA = score . The Centroid Attribute Model ( CAM ) is a simple instantiation of the framework defined in Section 2 , designed with two primary goals in mind . First , it is a data-driven model . Second , it does not require any manual sense disambiguation , a notorious bottleneck . To achieve the first goal , CAM uses a distributional approach . It represents the relevant entities as co-occurrence vectors that can be acquired from a large corpus ( Turney and Pantel , 2010 ) . To achieve the second goal , CAM represents meta senses using monosemous words only , that is , words whose senses all correspond to one meta sense . 4 Examples are cattle and robin for the meta sense ANIMAL . We define the vector for a meta sense as the centroid ( average vector ) of the monosemous words instantiating it . In turn , meta alternations are represented by the centroids of their meta senses vectors . This strategy is not applicable to test lemmas , which instantiate some meta alternation and are by definition ambiguous . To deal with these without for a sense : SL ( IL ) and assume that senses partition lemmas instances : l : inst ( l ) = Us sns ( l ) inst ( s ) . 3 Consistent with the theoretical literature , this paper focuses on two-way polysemy . See Section 7 for further discussion . and 2.3 % are disemous , while , on a token level , 23.3 % are monosemous and 20.2 % disemous . CoreLex : A Semantic Inventory . CAM uses CoreLex ( Buitelaar , 1998 ) as its meta sense inventory . CoreLex is a lexical resource that was designed specifically for the study of polysemy . It builds on WordNet ( Fellbaum , 1998 ) , whose sense distinctions are too fine-grained to describe general sense alternations . These classes are linked to one or more Wordnet anchor nodes , the definitions for vecL and repA . First , vecL defines a lemma s vector as the centroid of its instances : vecL ( l ) = C { vecI ( i ) | i inst ( l ) } ( 1 ) Before defining repA , we specify a function repM that computes vector representations for meta senses m. In CAM , this vector is defined as the centroid of the vectors for all monosemous lemmas whose WordNet sense maps onto m : repM ( m ) = C { vecL ( l ) | meta ( sns ( l ) ) = { m } } ( 2 ) Now , repA can be defined simply as the centroid of the meta senses instantiating a : repA ( m1 , m2 ) = C { repM ( m1 ) , repM ( m2 ) } ( 3 ) Predicting Meta Alternations . The final component of CAM is an instantiation of comp ( cf . Since CAM does not represent these senses separately , we define comp as comp ( a , s1 , s2 ) = sim ( a , vecL ( l ) ) which define a mapping from WordNet synsets onto basic types : A synset s maps onto a basic type b if b so that { s1 , s2 } = sns ( l ) ( 4 ) has an anchor node that dominates s and there is no other anchor node on the path from b and s.5 We adopt the WordNet synsets as S , the set of senses , and the CoreLex basic types as our set of meta senses M . The meta function ( mapping word senses onto meta senses ) is given directly by the anchor mapping defined in the previous paragraph . This means that the set of meta alternations is given by the set of pairs of basic types . Although basic types do not perfectly model meta senses , they constitute an approximation that allows us to model many prominent alternations such as ANIMAL-FOOD . Vectors for Meta Senses and Alternations . All representations used by CAM are co-occurrence vec tors in Rk ( i.e. , A : = Rk ) . vecI returns a vector for a lemma instance , vecL a ( type ) vector for a lemma , and C the centroid of a set of vectors . We leave vecI and C unspecified : we will experiment with these functions in Section 4 . CAM does fix 5 This is necessary because some classes have non-disjoint anchor nodes : e.g. , ANIMALs are a subset of LIVING BEINGs . The complete model , score , can now be stated as : score ( m , m ! , s , s ! ) = sim ( repA ( m , m ! ) , vecL ( l ) ) so that { s , s ! } = sns ( l ) ( 5 ) CAM thus assesses how well a meta alternation a = ( m , m ! ) explains a lemma l by comparing the centroid of the meta senses m , m ! The central feature of CAM is that it avoids word sense disambiguation , although it still relies on a predefined sense inventory ( Word- Net , through CoreLex ) . Our use of monosemous words to represent meta senses and meta alternations goes beyond previous work which uses monosemous words to disambiguate polysemous words in context ( Izquierdo et al. , 2009 ; Navigli and Velardi , 2005 ) . Because of its focus on avoiding disambiguation , CAM simplifies the representation of meta alternations and polysemous words to single centroid vectors . In the future , we plan to induce word senses ( Schu tze , 1998 ; Pantel and Lin , 2002 ; Reisinger and Mooney , 2010 ) , which will allow for more flexible and realistic models . ab s AB STR AC TIO N e nt EN TIT Y l o c LO CAT ION p rt PAR T ac t AC T ev t EV ENT l o g GE O . We test CAM on the task of identifying which lemmas of a given set instantiate a specific meta alternation . We let the model rank the lemmas through the score function ( cf . ( 5 ) ) and evaluate the ranked list using Average Precision . While an alternative would be to rank meta alternations for a given polysemous lemma , the method chosen here has the benefit of providing data on the performance of individual meta senses and meta alternations . All modeling and data extraction was carried out on the written part of the British National Corpus ( BNC ; Burnage and Dunlop ( 1992 ) ) parsed with the C & C sen so that they match targets in frequency . For each meta alternation , we randomly select 40 lemmas as experimental items ( 10 targets and 10 distractors of each type ) so that a total of 2,400 lemmas is used in the evaluation.7 Table 4 shows four targets and their distractors for the meta alternation ANIMAL-FOOD.8 4.2 Evaluation Measure and Baselines . To measure success on this task , we use Average Precision ( AP ) , an evaluation measure from IR that reaches its maximum value of 1 when all correct items are ranked at the top ( Manning et al. , 2008 ) .It interpolates the precision values of the top-n prediction lists for all positions n in the list that con tools ( Clark and Curran , 2007 ) . 6 For the evaluation , we focus on disemous words , tain a target . Let T = q1 , . , qm be the list of words which instantiate exactly two meta senses according to WordNet . For each meta alternationtargets , and let P = p1 , . , pn be the list of pre dictions as ranked by the model . Let I ( xi ) = 1 if pi T , and zero otherwise . Then AP ( P , T ) = ( m , m ! ) , we evaluate CAM on a set of disemous tar 1 ' m i j=1 I ( xi ) gets ( lemmas that instantiate ( m , m ! ) ) and disemous m i=1 I ( xi ) i . AP measures the quality distractors ( lemmas that do not ) . We define three types of distractors : ( 1 ) distractors sharing m with the targets ( but not m ! ) , ( 2 ) distractors sharing m ! with the targets ( but not m ) , and ( 3 ) distractors shar ing neither . In this way , we ensure that CAM can not obtain good results by merely modeling the similarity of targets to either m or m ! , which would rather be a coarse-grained word sense modeling task . To ensure that we have enough data , we evaluate CAM on all meta alternations with at least ten targets that occur at least 50 times in the corpus , discarding nouns that have fewer than 3 characters or contain non-alphabetical characters . The distractors are cho 6 The C & C tools were able to reliably parse about 40M words.. of the ranked list for a single meta alternation . The overall quality of a model is given by Mean Average Precision ( MAP ) , the mean of the AP values for all meta alternations . We consider two baselines : ( 1 ) A random baseline that ranks all lemmas in random order . This baseline is the same for all meta alternations , since the distribution is identical . We estimate it by sampling . ( 2 ) A meta alternation-specific frequency baseline which orders the lemmas by their corpus frequencies . This 7 Dataset available at http : //www.nlpado.de/ . 8 Note that this experimental design avoids any overlap be- . tween the words used to construct sense vectors ( one meta sense ) and the words used in the evaluation ( two meta senses ) . There are four more parameters to set . We instantiate the vecI function in three ways . All three are based on dependency-parsed spaces , following our intuition that topical similarity as provided by window-based spaces is insufficient for this task . The functions differ in the definition of the space s dimensions , incorporating different assumptions about distributional differences among meta alternations . The first option , gram , uses grammatical paths of lengths 1 to 3 as dimensions and thus characterizes lemmas and meta senses in terms of their grammatical context ( Schulte im Walde , 2006 ) , with a total of 2,528 paths . The second option , lex , uses words as dimensions , treating the dependency parse as a co-occurrence filter ( Pado and Lapata , 2007 ) , and captures topical distinctions . The third option , gramlex , uses lexicalized dependency paths like obj see to mirror more fine-grained semantic properties ( Grefenstette , 1994 ) . Both lex and gramlex use the 10,000 most frequent items in the corpus . We use raw corpus co- occurrence frequencies as well as log-likelihood- transformed counts ( Lowe , 2001 ) as elements of the co-occurrence vectors . There are three centroid computations in CAM : to combine instances into lemma ( type ) vectors ( function vecL in Eq . ( 1 ) ) ; to combine lemma vectors into meta sense vectors ( function repM in Eq . ( 2 ) ) ; and to combine meta sense vectors into meta alternation vectors ( function repA in Eq . For vecL , the obvious definition of the centroid function is as a micro-average , that is , a simple average over all instances . For repM and repA , there is a design choice : The centroid can be computed by micro-averaging as well , which assigns a larger weight to more frequent lemmas ( repM ) or meta senses ( repA ) . Alternatively , it can be computed by macro-averaging , that is , by normalizing the individual vectors before averaging . This gives equal weight to the each lemma or meta sense , respectively . Macro-averaging in repA thus assumes that senses are equally distributed , which is an oversimplification , as word senses are known to present skewed distributions ( McCarthy et al. , 2004 ) and vectors for words with a predominant sense will be similar to the dominant meta sense vector . Micro-averaging partially models sense skewedness under the assumption that word frequency correlates with sense frequency . As the vector similarity measure in Eq . ( 5 ) , we use the standard cosine similar ity ( Lee , 1999 ) . It ranges between 1 and 1 , with 1 denoting maximum similarity . In the current model where the vectors do not contain negative counts , the range is [ 0 ; 1 ] . Effect of Parameters The four parameters of Section 4.3 ( three space types , macro-/micro-averaging for repM and repA , and log-likelihood transformation ) correspond to 24 instantiations of CAM . The only significant difference is tied to the use of lexicalized vector spaces ( gramlex / lex are better than gram ) . The statistical significance of this difference was verified by a t-test ( p < 0.01 ) . This indicates that meta alternations can be characterized better through fine-grained semantic distinctions than by syntactic ones . The choice of micro- vs. macro-average does not have a clear effect , and the large variation observed in False True LL transformation firmed that the difference to the frequency baseline is significant at p < 0.01 for all 24 models . The difference to the random baseline is significant at p < 0.01 for 23 models and at p < 0.05 for the remaining model . This shows that the models cap ture the meta alternations to some extent . The best model uses macro averaging for repM and repA in a log likelihood transformed gramlex space and achieves a MAP of 0.399 . It shows an encouraging picture : CAM outperforms the frequency baseline for 49 of the 60 meta alternations and both baselines for 44 ( 73.3 % ) of all alternations . The performance shows a high degree of variance , however , ranging from 0.22 to 0.71 . A data point is the mean AP ( MAP ) across all meta alternations for a specific setting . Focusing on meta alternations , whether the two intervening meta senses should be balanced or not can be expected to depend on the frequencies of the concepts denoted by each meta sense , which vary for each case . Indeed , for AGENT-HUMAN , the alternation which most benefits from the micro-averaging setting , the targets are much more similar to the HUMAN meta sense ( which is approximately 8 times as frequent as AGENT ) than to the AGENT meta sense . The latter contains anything that can have an effect on something , e.g . emulsifier , force , valium . The targets for AGENT-HUMAN , in contrast , contain words such as engineer , manipulator , operative , which alternate between an agentive role played by a person and the person herself . While lacking in clear improvement , log- likelihood transformation tends to reduce variance , consistent with the effect previously found in selectional preference modeling ( Erk et al. , 2010 ) . Overall Performance Although the performance of the CAM models is still far from perfect , all 24 models obtain MAP scores of 0.35 or above , while the random baseline is at 0.313 , and the overall frequency baseline at 0.291 . Thus , all models consistently outperform both baselines . A bootstrap resampling test ( Efron and Tibshirani , 1994 ) con Analysis by Meta Alternation Coherence Meta alternations vary greatly in their difficulty . Since CAM is an attribute similarity-based approach , we expect it to perform better on the alternations whose meta senses are ontologically more similar . We next test this hypothesis . Let Dmi = { dij } be the set of distractors for the targets T = { tj } that share the meta sense mi , and DR = { d3j } the set of random distractors . We define the coherence of an alternation a of meta senses m1 , m2 as the mean ( ) difference between the similarity of each target vector to a and the similarity of the corresponding distractors to a , or formally ( a ) = sim ( repA ( m1 , m2 ) , vecL ( tj ) ) sim ( repA ( m1 , m2 ) , vecL ( dij ) ) , for 1 i 3 and 1 j 10 . That is , measures how much more similar , on average , the meta alternation vector is to the target vectors than to the distractor vectors . For a meta alternation with a higher , the targets should be easier to distinguish from the distractors . As we expect from the definition of , AP is strongly correlated with . However , there is a marked Y shape , i.e. , a divergence in behavior between high- and mid-AP alternations ( upper right corner ) and mid- and high-AP alternations ( upper left corner ) . In the first case , meta alternations perform worse than expected , and we find that this typically points to missing senses , that is , problems in the underlying lexical resource ( WordNet , via CoreLex ) . For instance , the FOOD-PLANT distractor almond is given grs ps y 0.7 09 com ev t 0.5 01 art co m 0.4 00 a tr c o m 0 . 36 1 a r t f r m 0 . 28 6 pro st a 0.6 78 art gr s 0.4 98 act po s 0.3 96 a t r s t a 0 . 36 1 a ct h u m 0 . 28 1 fod pl t 0.6 45 hum ps y 0.4 86 phm st a 0.3 88 a ct p h m 0 . 33 9 a r t f o d 0 . 28 0 psy st a 0.6 30 hum na t 0.4 56 atr p sy 0.3 84 a n m a rt 0 . 33 5 g r s h u m 0 . 27 2 hum pr t 0.6 02 anm hu m 0.4 48 fod hu m 0.3 83 a r t a t r 0 . 33 3 a c t a r t 0 . 26 7 grp ps y 0.5 74 com ps y 0.4 43 plt s u b 0.3 83 a ct p s y 0 . 33 3 a r t g r p 0 . 25 8 grs lo g 0.5 73 act gr s 0.4 41 act co m 0.3 82 a gt h u m 0 . 31 9 a r t n a t 0 . 24 8 act ev t 0.5 39 atr re l 0.4 40 grp gr s 0.3 79 a r t e v t 0 . 31 4 a c t a t r 0 . 24 6 evt ps y 0.5 26 art q ui 0.4 33 art p sy 0.3 73 a t r e v t 0 . 31 2 a rt h u m 0 . 24 0 act tm e 0.5 23 act st a 0.4 13 art pr t 0.3 64 a r t s t a 0 . 30 2 a r t l o c 0 . 23 8 art p h o 0.5 20 art su b 0.4 12 evt st a 0.3 64 a ct g r p 0 . 29 6 a rt p o s 0 . 22 8 act pr o 0.5 13 art lo g 0.4 07 anm fo d 0.3 61 c o m h u m 0 . 29 2 c o m s t a 0 . 21 9 The random baseline performs at 0.313 while the frequency baseline ranges from 0.255 to 0.369 with a mean of 0.291 . Alternations for which the model outperforms the frequency baseline are in boldface ( mean AP : 0.399 , standard deviation : 0.119 ) . grspsy democracy , faculty , humanism , regime , pro-sta bondage , dehydration , erosion , urbanization psysta anaemia , delight , pathology , sensibility hum-prt bum , contractor , peter , subordinate grppsy category , collectivism , socialism , underworld grs psy pro sta psy sta hum prt grp psy eavctt pesvyt grs log fod plt a PLANT sense by WordNet , but no FOOD sense . In the case of SOCIAL GROUP-GEOGRAPHICAL LOCATION , distractors laboratory and province are missing SOCIAL GROUP senses , which they clearly possess ( cf . The whole laboratory celebrated Christmas ) . This suggests that our approach can help in Word Sense Induction and thesaurus construction . In the second case , meta alternations perform better than expected : They have a low , but a high AP . These include grspsy , pro-sta , psysta , hum-prt and grppsy . These meta alternations involve fairly abstract meta senses such as PSYCHOLOGICAL FEATURE and STATE.9 The targets are clearly similar to each other on the level of their meta senses . However , they can occur in very different semantic contexts . Thus , here it is the underlying model ( the gramlex space ) that can explain the lower than average coherence . It is striking that CAM can account for abstract words and meta alternations between these , given that it uses first-order co-occurrence information only . 9 An exception is hum-prt . It has a low coherence because many WordNet lemmas with a PART sense are body parts . aacctt ptrmoeart pho arcto mg resvt psy hum nat gartstr qrueil araarctat r ltso tgsaub fgor dpc ohgmuprmlst sub atrr t sa cstnpotmrma tfod aacrattn mpa shtaymrt aagattrr t h euevmvtt aacrrottm fgshrrtumpam art agrrp tatt r ltanho taucprtmos 0.00 0.05 0.10 0.15 0.20 0.25 coherence Correlation : r = 0.743 ( p < 0.001 ) As noted in Section 1 , there is little work in empirical computational semantics on explicitly modeling sense alternations , although the notions that we have formalized here affect several tasks across NLP sub- fields . Most work on regular sense alternations has focused on regular polysemy . A pioneering study is Buitelaar ( 1998 ) , who accounts for regular polysemy through the CoreLex resource ( cf . A similar effort is carried out by Tomuro ( 2001 ) , but he represents regular polysemy at the level of senses . Recently , Utt and Pado ( 2011 ) explore the differences between between idiosyncratic and regular polysemy patterns building on CoreLex . Lapata ( 2000 ) focuses on the default meaning arising from word combinations , as opposed to the polysemy of single words as in this study . Meta alternations other than regular polysemy , such as metonymy , play a crucial role in Information Extraction . For instance , the meta alternation SOCIAL GROUP-GEOGRAPHICAL LOCATION corresponds to an ambiguity between the LOCATION- ORGANIZATION Named Entity classes which is known to be a hard problem in Named Entity Recognition and Classification ( Markert and Nissim , 2009 ) . Metaphorical meta alternations have also received attention recently ( Turney et al. , 2011 ) On a structural level , the prediction of meta alternations shows a clear correspondence to analogy prediction as approached in Turney ( 2006 ) ( carpenter : wood is analogous to mason : stone , but not to photograph : camera ) . The framework defined in Section 2 conceptualizes our task in a way parallel to that of analogical reasoning , modeling not first-order semantic similarity , but second-order semantic relations . However , the two tasks can not be approached with the same methods , as Turney s model relies on contexts linking two nouns in corpus sentences ( what does A do to B ? ) . In contrast , we are interested in relations within words , namely between word senses . We can not expect two different senses of the same noun to co-occur in the same sentence , as this is discouraged for pragmatic reasons ( Gale et al. , 1992 ) . A concept analogous to our notion of meta sense ( i.e. , senses beyond single words ) has been used in previous work on class-based WSD ( Yarowsky , 1992 ; Curran , 2005 ; Izquierdo et al. , 2009 ) , and indeed , the CAM might be used for class-based WSD as well . However , our emphasis lies rather on modeling polysemy across words ( meta alternations ) , something that is absent in WSD , class-based or not . The only exception , to our knowledge , is Ando ( 2006 ) , who pools the labeled examples for all words from a dataset for learning , implicitly exploiting regularities in sense alternations . Meta senses also bear a close resemblance to the notion of semantic class as used in lexical acquisition ( Hindle , 1990 ; Merlo and Stevenson , 2001 ; Schulte im Walde , 2006 ; Joanis et al. , 2008 ) . However , in most of this research polysemy is ignored . A few exceptions use soft clustering for multiple assignment of verbs to semantic classes ( Pereira et al. , 1993 ; Rooth et al. , 1999 ; Korhonen et al. , 2003 ) , and Boleda et al . ( to appear ) explicitly model regular polysemy for adjectives . We have argued that modeling regular polysemy and other analogical processes will help improve current models of word meaning in empirical computational semantics . We have presented a formal framework to represent and operate with regular sense alternations , as well as a first simple instantiation of the framework . We have conducted an evaluation of different implementations of this model in the new task of determining whether words match a given sense alternation . All models significantly outperform the baselines when considered as a whole , and the best implementation outperforms the baselines for 73.3 % of the tested alternations . We have two next steps in mind . The first is to become independent of WordNet by unsupervised induction of ( meta ) senses and alternations from the data . This will allow for models that , unlike CAM , can go beyond disemous words . Other improvements on the model and evaluation will be to develop more informed baselines that capture semantic shifts , as well as to test alternate weighting schemes for the co-occurrence vectors ( e.g . PMI ) and to use larger corpora than the BNC . The second step is to go beyond the limited in-vitro evaluation we have presented here by integrating alternation prediction into larger NLP tasks . Knowledge about alternations can play an important role in counteracting sparseness in many tasks that involve semantic compatibility , e.g. , testing the applicability of lexical inference rules ( Szpektor et al. , 2008 ) . This research is partially funded by the Spanish Ministry of Science and Innovation ( FFI201015006 , TIN200914715-C0404 ) , the AGAUR ( 2010 BPA00070 ) , the German Research Foundation ( SFB 732 ) , and the EU ( PASCAL2 ; FP7ICT-216886 ) . It is largely inspired on a course by Ann Copestake at U. Pompeu Fabra ( 2008 ) . We thank Marco Baroni , Katrin Erk , and the reviewers of this and four other conferences for valuable feedback . A phrase-based statistical machine translation approach the alignment template approach is described . This translation approach allows for general many-to-many relations between words . Thereby , the context of words is taken into account in the translation model , and local changes in word order from source to target language can be learned explicitly . Thereby , the model is easier to extend than classical statistical machine translation systems . We describe in detail the process for learning phrasal translations , the feature functions used , and the search algorithm . The evaluation of this approach is performed on three different tasks . For the German English speech Verbmobil task , we analyze the effect of various system components . On the French English Canadian Hansards task , the alignment template system obtains significantly better results than a single-word-based translation model . In the Chinese English 2002 National Institute of Standards and Technology ( NIST ) machine translation evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems . Machine translation ( MT ) is a hard problem , because natural languages are highly complex , many words have various meanings and different possible translations , sentences might have various readings , and the relationships between linguistic entities are often vague . In addition , it is sometimes necessary to take world knowledge into account . The number of relevant dependencies is much too large and those dependencies are too complex to take them all into account in a machine translation system . Given these boundary conditions , a machine translation system has to make decisions ( produce translations ) given incomplete knowledge . In such a case , a principled approach to solving that problem is to use the concepts of statistical decision theory to try to make optimal decisions given incomplete knowledge . This is the goal of statistical machine translation . The use of statistical techniques in machine translation has led to dramatic improvements in the quality of research systems in recent years . For example , the statistical approaches of the Verbmobil evaluations ( Wahlster 2000 ) or the U.S. National 1600 Amphitheatre Parkway , Mountain View , CA 94043 . Email : och @ google.com . Lehrstuhl fu r Informatik VI , Computer Science Department , RWTH Aachen University of Technology , Ahornstr . 55 , 52056 Aachen , Germany . Email : ney @ cs.rwthaachen.de . Submission received : 19 November 2002 ; Revised submission received : 7 October 2003 ; Accepted for publication : 1 June 2004 c 2004 Association for Computational Linguistics Institute of Standards and Technology ( NIST ) /TIDES MT evaluations 2001 through 20031 obtain the best results . In addition , the field of statistical machine translation israpidly progressing , and the quality of systems is getting better and better . An im portant factor in these improvements is definitely the availability of large amounts of data for training statistical models . Yet the modeling , training , and search methods have also improved since the field of statistical machine translation was pioneered by IBM in the late 1980s and early 1990s ( Brown et al . 1990 ; Brown et al . This article focuses on an important improvement , namely , the use of ( gen eralized ) phrases instead of just single words as the core elements of the statistical translation model . We describe in Section 2 the basics of our statistical translation model . In Section 3 , we describe the statistical alignment models used to obtain a word alignment and techniques for learning phrase translations from word alignments . Here , the term phrase just refers to a consecutive sequence of words occurring in text and has to be distinguished from the use of the term in a linguistic sense . The learned bilingual phrases are not constrained by linguistic phrase boundaries . Compared to the word-based statistical translation models in Brown et al . ( 1993 ) , this model is based on a ( statistical ) phrase lexicon instead of a single-word-based lexicon . Looking at the results of the recent machine translation evaluations , this approach seems currently to give the best results , and an increasing number of researchers are working on different methods for learning phrase translation lexica for machine translation purposes ( Marcu and Wong 2002 ; Venugopal , Vogel , and Waibel 2003 ; Tillmann 2003 ; Koehn , Och , and Marcu 2003 ) . Our approach to learning a phrase translation lexicon works in two stages : In the first stage , we compute an alignment between words , and in the second stage , we extract the aligned phrase pairs . In our machine translation system , we then use generalized versions of these phrases , called alignment templates , that also include the word alignment and use word classes instead of the words themselves . In Section 4 , we describe the various components of the statistical translationmodel . The backbone of the translation model is the alignment template feature function , which requires that a translation of a new sentence be composed of a set of align ment templates that covers the source sentence and the produced translation . Other feature functions score the well-formedness of the produced target language sentence ( i.e. , language model feature functions ) , the number of produced words , or the or der of the alignment templates . Note that all components of our statistical machine translation model are purely data-driven and that there is no need for linguisticallyannotated corpora . This is an important advantage compared to syntax-based trans lation models ( Yamada and Knight 2001 ; Gildea 2003 ; Charniak , Knight , and Yamada 2003 ) that require a parser for source or target language . In Section 5 , we describe in detail our search algorithm and discuss an efficient implementation . We use a dynamic-programming-based beam search algorithm that allows a trade-off between efficiency and quality . We also discuss the use of heuristic functions to reduce the number of search errors for a fixed beam size . In Section 6 , we describe various results obtained on different tasks . For theGerman English Verbmobil task , we analyze the effect of various system compo 1 http : //www.nist.gov/speech/tests/mt/ . On the French English Canadian Hansards task , the alignment template system obtains significantly better results than a single-word-based translation model . In the Chinese English 2002 NIST machine translation evaluation it yields results that are significantly better statistically than all competing research and commercial translation systems . We are given a source ( French ) sentence f = f J = f1 , ... , fj , ... , fJ , which is to be trans lated into a target ( English ) sentence e = eI = e1 , ... , ei , ... , eI . :2 1 1 1 I 1 . As an alternative to the often used source channel approach ( Brown et al . 1993 ) , ( Och and Ney 2002 ) . An es pecially well-founded framework for doing this is the maximum-entropy framework ( Berger , Della Pietra , and Della Pietra 1996 ) . In this framework , we have a set of M fea ture functions hm ( eI , f J ) , m = 1 , ... , M. For each feature function , there exists a model 1 1 2 The notational convention employed in this article is as follows . We use the symbol Pr ( ) to denote . general probability distributions with ( nearly ) no specific assumptions . In contrast , for model-based probability distributions , we use the generic symbol p ( ) . 419 parameter m , m = 1 , ... , M. e Hence , . .. , S } for log-linear models is the maximum class posterior probability criterion , which can be derived from the maximum-entropy principle : . This direct optimization of the posterior probability in Bayes decision rule is referred to as discriminative training ( Ney 1995 ) because we directly take into account the overlap in the probability distributions . The optimization problem under this criterion has very nice properties : There is one unique global optimum , and there are algorithms ( e.g . gradient descent ) that are guaranteed to converge to the global optimum . Yet the ultimate goal is to obtain good translation quality on unseen test data . An alternative training criterion therefore directly optimizes translation quality as measured by an automatic evaluation criterion ( Och 2003 ) . To include these dependencies in our log-linear model , we extend the feature functions to include the dependence on the additional hidden variable . Using for example the alignment aJ as hidden variable , we obtain M feature functions of the form hm ( eI , f J , aJ ) , m = 1 , ... , M and the following model : 1 1 1 exp M m hm ( eI , f J , aJ ) Pr ( eI , aJ | f J ) = m=1 1 1 1 1 1 1 M I J J e I J exp m hm ( e 1 , f , a ) 1 , a 1 m=1 1 1 Obviously , we can perform the same step for translation models with an even richer set of hidden variables than only the alignment aJ . In this section , we describe methods for learning the single-word and phrase-based translation lexica that are the basis of the machine translation system described in 420 Section 4 . First , we introduce the basic concepts of statistical alignment models , which are used to learn word alignment . Then , we describe how these alignments can be used to learn bilingual phrasal translations . In ( statistical ) a hidden alignment a = aJis intro 1 1 1 1 duced that describes a mapping from a source position j to a target position aj . The relationship between the translation model and the alignment model is given by The alignment aJ may contain alignments aj = 0 with the empty word e0 to account for source words that are not aligned with any target word . In general , the statistical model depends on a set of unknown parameters that is learned from training data . To express the dependence of the model on the parameter set , we use the following notation : A detailed description of different specific statistical alignment models can be found in Brown et al . ( 1993 ) and Och and Ney ( 2003 ) . Here , we use the hidden Markov model ( HMM ) alignment model ( Vogel , Ney , and Tillmann 1996 ) and Model 4 of Brown et al . ( 1993 ) to compute the word alignment for the parallel training corpus . To train the unknown parameters , we are given a parallel training corpus consisting of S sentence pairs { ( fs , es ) : s = 1 , ... , S } . For each sentence pair ( fs , es ) , the alignment variable is denoted by a = aJ . T For a given sentence pair there are a large number of alignments . : A detailed comparison of the quality of these Viterbi alignments for various statistical alignment models compared to human-made word alignments can be found in Och and Ney ( 2003 ) . The baseline alignment model does not allow a source word to be aligned with two or more target words . Therefore , lexical correspondences like the German compound word Zahnarzttermin for dentist s appointment cause problems because a single source word must be mapped onto two or more target words . Therefore , the resulting Viterbi alignment of the standard alignment models has a systematic loss in recall . Here , we 421 F describe various methods for performing a symmetrization of our directed statistical alignment models by applying a heuristic postprocessing step that combines the alignments in both translation directions ( source to target , target to source ) . To solve this problem , we train in both translation directions . For each sentence pair , we compute two Viterbi alignments aJ and bI . Let A1 = { ( aj , j ) | aj > 0 } and A2 = { ( i , bi ) | bi > 0 } denote the sets of alignments in the two Viterbi alignments . To increase the quality of the alignments , we can combine ( symmetrize ) A1 and A2 into one alignment matrix A using one of the following combination methods : Intersection : A = A1 A2 . Union : A = A1 A2 . Refined method : In a first step , the intersection A = A1 A2 is determined . The elements of this intersection result from both Viterbi alignments and are therefore very reliable . Then , we extend the alignment A iteratively by adding alignments ( i , j ) occurring only in the 422 alignment A1 or in the alignment A2 if neither fj nor ei have an alignment in A , or if the following conditions both hold : The alignment ( i , j ) has a horizontal neighbor ( i 1 , j ) , ( i + 1 , j ) or a vertical neighbor ( i , j 1 ) , ( i , j + 1 ) that is already in A . The set A { ( i , j ) } does not contain alignments with both horizontal and vertical neighbors . Obviously , the intersection yields an alignment consisting of only one-to-one alignments with a higher precision and a lower recall . The union yields a higher recall and a lower precision of the combined alignment . The refined alignment method is often able to improve precision and recall compared to the nonsymmetrized alignments . Whether a higher precision or a higher recall is preferred depends on the final application of the word alignment . For the purpose of statistical MT , it seems that a higher recall is more important . Therefore , we use the union or the refined combination method to obtain a symmetrized alignment matrix . The resulting symmetrized alignments are then used to train single-word-based translation lexica p ( e | f ) by computing relative frequencies using the count N ( e , f ) of how many times e and f are aligned divided by the count N ( f ) of how many times the word f occurs : p ( e | f ) = N ( e , f ) N ( f ) 3.3 Bilingual Contiguous Phrases . In this section , we present a method for learning relationships between whole phrases of m source language words and n target language words . This algorithm , whichwill be called phrase-extract , takes as input a general word alignment matrix ( Sec tion 3.2 ) . The output is a set of bilingual phrases . : Hence , the set of all bilingual phrases that are consistent with the alignment is constituted by all bilingual phrase pairs in which all words within the source language phrase are aligned only with the words of the target language phrase and the words of the target language phrase are aligned only with the words of the source language phrase . Note that we require that at least one word in the source language phrase be aligned with at least one word of the target language phrase . As a result there are no empty source or target language phrases that would correspond to the empty word of the word-based statistical alignment models . These phrases can be computed straightforwardly by enumerating all possible phrases in one language and checking whether the aligned words in the other language are consecutive , with the possible exception of words that are not aligned at all . In principle , this approach to learning phrases from a word-aligned corpus could be extended straightforwardly to handle nonconsecutive phrases in source and target language as well . Informal experiments have shown that allowing for nonconsecutive phrases significantly increases the number of extracted phrases and especially increases the percentage of wrong phrases . Therefore , we consider only consecutive phrases . In the following , we add generalization capability to the bilingual phrase lexicon by replacing words with word classes and also by storing the alignment information for each phrase pair . These generalized and alignment-annotated phrase pairs are called alignment templates . Here quasi-consecutive ( TP ) is a predicate that tests whether the set of words TP is consecutive , with the possible exception of words that are not aligned . that describes the alignment A between a source class sequence FJ and a target class sequence EI . If each word corresponds to one class , an alignment template corresponds to a bilingual phrase together with an alignment within this phrase . The alignment A is represented as a matrix with J ( I + 1 ) binary elements . A matrix element with value 1 means that the words at the corresponding positions are aligned , and the value 0 means that the words are not aligned . If a source word is not aligned with a target word , then it is aligned with the empty word e0 , which is at the imaginary position i = 0 . The classes used in FJ and EI are automatically trained bilingual classes using the method described in Och ( 1999 ) and constitute a partition of the vocabulary of source and target language . In general , we are not limited to disjoint classes as long as each specific instance of a word is disambiguated , that is , uniquely belongs to a specific class . In the following , we use the class function C to map words to their classes . Hence , it would be possible to employ parts-of-speech or semantic categories instead of the automatically trained word classes used here . The use of classes instead of the words themselves has the advantage of better generalization . For example , if there exist classes in source and target language that contain town names , it is possible that an alignment template learned using a specific town name can be generalized to other town names . In the following , e and f denote target and source phrases , respectively . To train 1 1 | the probability of applying an alignment template p ( z = ( FJ , EI , A ) f ) , we use an extended version of the algorithm phrase-extract from Section 3.3 . All bilingualphrases that are consistent with the alignment are extracted together with the align 425 . ment within this bilingual phrase . Thus , we obtain a count N ( z ) of how often an alignment template occurred in the aligned training corpus . : ) 426 Depending on the size of the corpus , the maximal length in the experiments is between four and seven words . In addition , we remove alignment templates that have a probability lower than a certain threshold . In the experiments , we use a threshold of 0.01 . It should be emphasized that this algorithm for computing aligned phrase pairs and their associated probabilities is very easy to implement . The joint translation model suggested by Marcu and Wong ( 2002 ) tries to learn phrases as part of a full EM algorithm , which leads to very large memory requirements and a rather complicated training algorithm . A comparison of the two approaches can be found in Koehn , Och , and Marcu ( 2003 ) . To describe our translation model based on the alignment templates described in the previous section in a formal way , we first decompose both the source sentence f J and the target sentence eI into a sequence of phrases ( k = 1 , ... , . .. , ) Note that there are a large number of possible segmentations of a sentence pair into K phrase pairs . In the following , we will describe the model for a specific segmentation . Eventually , however , a model can be described in which the specific segmentation is not known when new text is translated . Hence , as part of the overall search process ( Section 5 ) , we also search for the optimal segmentation . To allow possible reordering of phrases , we introduce an alignment on the phrase level K between the source phrases f K and the target phrases eK . Hence , K is a 1 1 1 1 permutation of the phrase positions 1 , ... , K and indicates that the phrases ek and f are transl ations of one anoth er . We assum e that for the transl ation betwe en these phrases a specific alignment template zk is used : e zk f Hence , our model has the following hidden variables : K K 1 , z1 . Hence , all knowl edge sources are described as feature functions that include the given source language string f J , the target language string eI , and the above-stated hidden variables . Hence , 1 1 we have the following functional form of all feature functions : h ( eI , f J , K , zK ) 1 1 1 1 Finally , the sequence of phrases eK constitutes the sequence of words eI . l. 428 4.1 Feature Functions . To score the use of an alignment template , we use the probability p ( z | f ) defined in Section 3. : Here , j k 1 + 1 is the position of the first word of alignment template zk in the source language sentence and j k is the position of the last word of that alignment template . Note that this feature function requires that a translation of a new sentence be composed of a set of alignment templates that covers both the source sentence and the produced translation . There is no notion of empty phrase that corresponds to the empty word in word-based statistical alignment models . The alignment on the phrase level is actually a permutation , and no insertions or deletions are allowed . For scoring the use of target language words , we use a lexicon probability p ( e | f ) , which is estimated using relative frequencies as described in Sec tion 3.2 . The target word e depends on the aligned source words . If we denote the resulting word alignment matrix by A : = A K K and the predicted word class for word 1 , z1 ei by Ei , then the feature function hWRD is defined as follows : I For p ( ei | { fj | ( i , j ) A } ) we use a uniform mixture of a single-word model p ( e | f ) , which is constrained to predict only words that are in the predicted word class Ei : j| ( i , j ) A } p ( ei | fj ) p ( ei | { fj | ( i , j ) A } , Ei ) = { | { j | ( i , j ) A } | ( C ( ei ) , Ei ) A disadvantage of this model is that the word order is ignored in the translation model . The translations the day after tomorrow or after the day tomorrow for the German word u bermorgen receive an identical probability . Yet the first one should obtain a significantly higher probability . Hence , we also include a dependence on the word positions in the lexicon model p Here , [ ( i , j ) A ] is 1 if ( i , j ) A and 0 otherwise . As a result , the word ei depends not only on the aligned French word fj , but also on the number of preceding French words aligned with ei and on the number of the preceding English words aligned with fj . This model distinguishes the positions within a phrasal translation . The number of parameters of p ( e | f , i , j ) is significantly higher than that of p ( e | f ) alone . Hence , there is a data estimation problem especially for words that rarely occur . Therefore , we linearly interpolate the models p ( e | f ) and p ( e | f , i , j ) . The phrase alignment feature simply takes into account that very often a monotone alignment is a correct alignment . Hence , the feature function hAL measures the amount of nonmonotonicity by summing over the distance ( in the 429 source language ) of alignment templates that are consecutive in the target language : K+1 Here , j 0 is defined to equal 0 and j K+1 1 is defined to equal J . The above-stated sum includes k = K + 1 to include the distance from the end position of the last phrase to the end of sentence . The sequence of K = 6 alignment templates in Figure 5 corresponds to the following sum of seven jump distances : 0 + 0 + 1 + 3 + 2 + 0 + 0 = 6 . As a default language model feature , we use a standard backing-off word-based trigram language model ( Ney , Generet , and Wessel 1995 ) : In addition , we use a 5-gram class-based language model : . To improve the scoring for different target sentence lengths , we also use as a feature the number of produced target language words ( i.e. , the length of the produced target language sentence ) : hWP ( eI , f J , K , zK ) = I ( 19 ) 1 1 1 1 Without this feature , we typically observe that the produced sentences tend to be too short . We also use a feature that counts how many entries of a conventional lexicon co-occur in the given sentence pair . Therefore , the weight for the provided conventional dictionary can be learned : The intuition is that the conventional dictionary LEX is more reliable than the automatically trained lexicon and therefore should get a larger weight . A major advantage of the log-linear modeling approach used is that we can add numerous features that deal with specific problems of the baseline statistical MT system . Here , we will restrict ourselves to the described set of features . Yet we could use grammatical features that relate certain grammatical dependencies of source and target language . For example , using a function k ( ) that counts how many arguments the main verb of a sentence has in the source or target sentence , we can define the following feature , which has a nonzero value if the verb in each of the two sentences has the same number of arguments : h ( f J , eI , K , zK ) = ( k ( f J ) , k ( eI ) ) ( 21 ) 1 1 1 1 1 1 In the same way , we can introduce semantic features or pragmatic features such as the dialogue act classification . For the three different tasks on which we report results , we use two different training approaches . For the French English Hansards task and the Chinese English NIST task , we simply tune the modelparameters by coordinate descent on held-out data with respect to the automatic eval uation metric employed , using as a starting point the model parameters obtained on the Verbmobil task . Note that this tuning depends on the starting point of the model parameters and is not guaranteed to converge to the global optimum on the trainingdata . As a result , this approach is limited to a very small number of model parame ters . An efficient algorithm for performing this tuning for a larger number of model parameters can be found in Och ( 2003 ) . A standard approach to training the log-linear model parameters of the maximum class posterior probability criterion is the GIS ( Generalized Iterative Scaling ) algorithm ( Darroch and Ratcliff 1972 ) . To apply this algorithm , we have to solve various practi cal problems . Hence , we ap proximate this sum by extracting a large set of highly probable sentences as a sample from the space of all possible sentences ( n-best approximation ) . The set of considered sentences is computed by means of an appropriately extended version of the search algorithm described in Section 5 . Using an n-best approximation , we might face the problem that the parameters trained with the GIS algorithm yield worse translation results even on the training corpus . This can happen because with the modified model scaling factors , the n-best list can change significantly and can include sentences that have not been taken into account in training . Using these sentences , the new model parameters might perform worse than the old model parameters . To avoid this problem , we proceed as follows . In a first step , we perform a search , compute an n-best list , and use this n-best list to train the model parameters . Second , we use the new model parameters in a new search and compute a new n-best list , which is combined with the existing n-best list . Third , using this extended n-best list , new model parameters are computed . This process is iterated until the resulting n-best list does not change . In this algorithm , convergence is guaranteed , as in the limit the n-best list will contain all possible translations . In practice , the algorithm converges after five to seven iterations . In our experiments this final n-best list contains about 500 1000 alternative translations . We might have the problem that none of the given reference translations is part of the n-best list because the n-best list is too small or because the search algorithmperforms pruning which in principle limits the possible translations that can be pro duced given a certain input sentence . To solve this problem , we define as reference translation for maximum-entropy training each sentence that has the minimal number of word errors with respect to any of the reference translations in the n-best list . More details of the training procedure can be found in Och and Ney ( 2002 ) . In this section , we describe an efficient search architecture for the alignment template model . In general , the search problem for statistical MT even using only Model 1 of Brown et al . ( 1993 ) is NP-complete ( Knight 1999 ) . Therefore , we can not expect to develop 431 efficient search algorithms that are guaranteed to solve the problem without search errors . Yet for practical applications it is acceptable to commit some search errors ( Section 6.1.2 ) . Hence , the art of developing a search algorithm lies in finding suitable approximations and heuristics that allow an efficient search without committing too many search errors . In the development of the search algorithm described in this section , our main aim is that the search algorithm should be efficient . It should be possible to translate a sentence of reasonable length within a few seconds of computing time . We accept that the search algorithm sometimes results in search errors , as long as the impact on translation quality is minor . Yet it should be possible to reduce the number of search errors by increasing computing time . In the limit , it should be possible to search without search errors . The search algorithm should not impose any principal limitations . We also expect that the search algorithm be able to scale up to very long sentences with an acceptable computing time . To meet these aims , it is necessary to have a mechanism that restricts the search effort . We accomplish such a restriction by searching in a breadth-first manner with pruning : beam search . In pruning , we constrain the set of considered translation candidates ( the beam ) only to the promising ones . We compare in beam search those hypotheses that cover different parts of the input sentence . This makes the comparison of the probabilities problematic . Therefore , we integrate an admissible estimation of the remaining probabilities to arrive at a complete translation ( Section 5.6 ) Many of the other search approaches suggested in the literature do not meet the described aims : Neither optimal A* search ( Och , Ueffing , and Ney 2001 ) nor optimal integer programming ( Germann et al . 2001 ) for statistical MT allows efficient search for long sentences . Greedy search algorithms ( Wang 1998 ; Germann et al . 2001 ) typically commit severe search errors ( Germann et al . Other approaches to solving the search problem obtain polynomial time algorithms by assuming monotone alignments ( Tillmann et al . 1997 ) or imposing a simplified recombination structure ( Nie en et al . Others make simplifying assumptions about the search space ( Garc a-Varea , Casacuberta , and Ney 1998 ; Garc a-Varea et al . 2001 ) , as does the original IBM stack search decoder ( Berger et al . All these simplifications ultimately make the search problem simpler but introduce fundamental search errors . In the following , we describe our search algorithm based on the concept of beam search , which allows a trade-off between efficiency and quality by adjusting the size of the beam . The search algorithm can be easily adapted to other phrase-based translation models . For single-word-based search in MT , a similar algorithm has been described in Tillmann and Ney ( 2003 ) . The class-based 5-gram language model ( CLM ) can be included like the trigram language model . Note that all these feature functions decompose nicely into contributions for each produced target language word or for each covered source language word . This makes it possible to develop an efficient dynamic programming search algorithm . Not all feature functions have this nice property : For the conventional lexicon feature function ( LEX ) , we obtain an additional term in our decision rule which depends on the full sentence . Therefore , this feature function will not be integrated in the dynamic programming search but instead will be used to rerank the set of candidate translations produced by the search . 5.3 Structure of Search Space . We have to structure the search space in a suitable way to search efficiently . In our search algorithm , we generate search hypotheses that correspond to prefixes of target language sentences . Each hypothesis is the translation of a part of the source languagesentence . A hypothesis is extended by appending one target word . The set of all hy potheses can be structured as a graph with a source node representing the sentencestart , goal nodes representing complete translations , and intermediate nodes repre senting partial translations . There is a directed edge between hypotheses n1 and n2 if the hypothesis n2 is obtained by appending one word to hypothesis n1 . Each edge has associated costs resulting from the contributions of all feature functions . Finally , our search problem can be reformulated as finding the optimal path through this graph.In the first step , we determine the set of all source phrases in f for which an appli cable alignment template exists . Every possible application of an alignment template z = ( FJ , EI , A ) to a subsequence f j+J 1 of the source sentence is called an alignment 1 1 j template instantiation Z = ( z , j ) . Hence , the set of all alignment template instantiations for the source sentence f J is Z = ( z , j ) | z = ( FJ , EI , A ) j : p ( z f j+J 1 ) > 0 ( 27 ) 1 1 | j 3 Note that here some of the simplifying notation of Section 4 has been used.. 433 If the source sentence contains words that have not been seen in the training data , we introduce a new alignment template that performs a one-to-one translation of each of these words by itself . In the second step , we determine a set of probable target language words for each target word position in the alignment template instantiation . Only these words are then hypothesized in the search . We call this selection of highly probable words observation pruning ( Tillmann and Ney 2000 ) . As a criterion for a word e at position i in the alignment template instantiation , we use ( Ei , C ( e ) ) J j=0 A ( i , j ) A ( i , j ) p ( e | fj ) ( 28 ) In our experiments , we hypothesize only the five best-scoring words . A decision is a triple d = ( Z , e , l ) consisting of an alignment template instantiation Z , the generated word e , and the index l of the generated word in Z . A hypothesis n corresponds to a valid sequence of decisions di . The possible decisions are as follows : 1 . Start a new alignment template : di = ( Zi , ei , 1 ) . In this case , the index l = . This decision can be made only if the previous decision di 1 finished . an alignment template and if the newly chosen alignment template instantiation does not overlap with any previously chosen alignment template instantiation . The resulting decision score corresponds to the contribution of the LM and the WRD features ( expression ( 24 ) ) for the produced word and the contribution of AL and AT features ( expression ( 25 ) ) for the started alignment template . Extend an alignment template : di = ( Zi , ei , l ) . This decision can be made . only if the previous decision uses the same alignment template instantiation and has as index l 1 : di 1 = ( Zi , ei 1 , l 1 ) . The resulting decision score corresponds to the contribution of the LM and the WRD features ( expression ( 24 ) ) . Finish the translation of a sentence : di = ( EOS , EOS , 0 ) . In this case , the . hypothesis is marked as a goal hypothesis . This decision is possible only if the previous decision di 1 finished an alignment template and if the alignment template instantiations completely cover the input sentence . The resulting decision score corresponds to the contribution of expression ( 26 ) . Any valid and complete sequence of decisions dI+1 uniquely corresponds to a certain translation eI , a segmentation into K phrases , a phrase alignment K , and a sequence 1 1 of alignment template instantiations zK . The sum of the decision scores is equal to the corresponding score described in expressions ( 24 ) ( 26 ) . A straightforward representation of all hypotheses would be the prefix tree of all possible sequences of decisions . Obviously , there would be a large redundancy in thissearch space representation , because there are many search nodes that are indistin guishable in the sense that the subtrees following these search nodes are identical . We can recombine these identical search nodes ; that is , we have to maintain only the most probable hypothesis ( Bellman 1957 ) . In general , the criterion for recombining a set of nodes is that the hypotheses can be distinguished by neither language nor translation model . In performing recombination , 434 INPUT : implicitly defined search space ( functions Recombine , Extend ) H = { initial hypothesis } WHILE H = Hext : = FOR n H IF hypothesis n is final THEN Hfin : = Hfin { n } EL SE Hext : = Hext Extend ( n ) H : = Recombine ( Hext ) Q = maxn H Q ( n ) H : = { n H : Q ( n ) > log ( tp ) + Q } H : = HistogramPrun ing ( H , Np ) n = a r g m a x Q ( n ) n H f i n OUTPUT : n . we obtain a search graph instead of a search tree . The exact criterion for performing recombination for the alignment templates is described in Section 5.5 . Theoretically , we could use any graph search algorithm to search the optimal path in the search space . We use a breadth-first search algorithm with pruning . This approach offers very good possibilities for adjusting the trade-off between quality and efficiency . In pruning , we always compare hypotheses that have produced the same number of target words . Therefore , we represent the searchspace implicitly , using the functions Extend and Recombine . The function Extend produces new hypotheses extending the current hypothesis by one word . Some hypothe ses might be identical or indistinguishable by the language and translation models . These are recombined by the function Recombine . We expand the search space such that only hypotheses with the same number of target language words are recombined.In the pruning step , we use two different types of pruning . First , we perform prun ing relative to the score Q of the current best hypothesis . We ignore all hypotheses that have a probability lower than log ( tp ) + Q , where tp is an adjustable pruning parameter . This type of pruning can be performed when the hypothesis extensions are computed . Second , in histogram pruning ( Steinbiss , Tran , and Ney 1994 ) , we maintain only the best Np hypotheses . The two pruning parameters tp and Np have to be optimized with respect to the trade-off between efficiency and quality . In this section , we describe various issues involved in performing an efficient imple mentation of a search algorithm for the alignment template approach . A very important design decision in the implementation is the representation of a hypothesis . Theoretically , it would be possible to represent search hypotheses only by the associated decision and a back-pointer to the previous hypothesis . Yet this would be a very inefficient representation for the implementation of the operations 435 that have to be performed in the search . The hypothesis representation should contain all information required to perform efficiently the computations needed in the search but should contain no more information than that , to keep the memory consumption small . In search , we produce hypotheses n , each of which contains the following information : 1. e : the final target word produced 2. h : the state of the language model ( to predict the following word ) 3. c = cJ : the coverage vector representing the already covered positions of the source sentence ( cj = 1 means the position j is covered , cj = 0 means the position j is not covered ) 4 . Z : a reference to the alignment template instantiation that produced the final target word 5. l : the position of the final target word in the alignment template instantiation 7. n : a reference to the previous hypothesis Using this representation , we can perform the following operations very efficiently : Determining whether a specific alignment template instantiation can be used to extend a hypothesis . To do this , we check whether the positions of the alignment template instantiation are still free in the hypothesis coverage vector . Checking whether a hypothesis is final . To do this , we determine whether the coverage vector contains no uncovered position . Using a bit vector as representation , the operation to check whether a hypothesis is final can be implemented very efficiently . Checking whether two hypotheses can be recombined . The criterion for recombining two hypotheses n1 = ( e1 , h1 , c1 , Z1 , l1 ) and n2 = ( e2 , h2 , c2 , Z2 , l2 ) is h1 = h2 identical language model state c1 = c2 identical coverage vector ( ( Z1 = Z2 l1 = l2 ) alignment template instantiation is identical ( J ( Z1 ) = l1 J ( Z2 ) = l2 ) ) alignment template instantiation finished We compare in beam search those hypotheses that cover different parts of the input sentence . This makes the comparison of the probabilities problematic . Therefore , we integrate an admissible estimation of the remaining probabilities to arrive at a complete translation . Details of the heuristic function for the alignment templates are provided in the next section . To improve the comparability of search hypotheses , we introduce heuristic functions . A heuristic function estimates the probabilities of reaching the goal node from a certain 436 search node . An admissible heuristic function is always an optimistic estimate ; that is , for each search node , the product of edge probabilities of reaching a goal node is always equal to or smaller than the estimated probability . For an A*-based search algorithm , a good heuristic function is crucial to being able to translate long sentences . For a beam search algorithm , the heuristic function has a different motivation . It is used to improve the scoring of search hypotheses . The goal is to make the probabilities of all hypotheses more comparable , in order to minimize the chance that the hypothesis leading to the optimal translation is pruned away . Heuristic functions for search in statistical MT have been used in Wang and Waibel ( 1997 ) and Och , Ueffing , and Ney ( 2001 ) . Wang and Waibel ( 1997 ) have described a simple heuristic function for Model 2 of Brown et al . ( 1993 ) that was not admissible . Och , Ueffing , and Ney ( 2001 ) have described an admissible heuristic function for Model 4 of Brown et al . ( 1993 ) and an almost-admissible heuristic function that is empirically obtained . We have to keep in mind that a heuristic function is helpful only if the overhead introduced in computing the heuristic function is more than compensated for by the gain obtained through a better pruning of search hypotheses . The heuristic functions described in the following are designed such that their computation can be performed efficiently . The basic idea for developing a heuristic function for an alignment model is that all source sentence positions that have not been covered so far still have to be translated to complete the sentence . If we have an estimation rX ( j ) of the optimal score for translating position j , then the value of the heuristic function RX ( n ) for a node n can be inferred by summing over the contribution for every position j that is not in the coverage vector c ( n ) ( here X denotes different possibilities to choose the heuristic function ) : RX ( n ) = j c ( n ) rX ( j ) ( 29 ) The situation in the case of the alignment template approach is more complicated , as not every word is translated alone , but typically the words are translated in context . Therefore , the basic quantity for the heuristic function in the case of the alignment template approach is a function r ( Z ) that assigns to every alignment template in- stantiation Z a maximal probability . Using r ( Z ) , we can induce a position-dependent heuristic function r ( j ) : r ( j ) : = max Z : j ( Z ) j j ( Z ) +J ( Z ) 1 r ( Z ) /J ( Z ) ( 30 ) Here , J ( Z ) denotes the number of source language words produced by the alignment template instantiation Z and j ( Z ) denotes the position of the first source language word . It can be easily shown that if r ( Z ) is admissible , then r ( j ) is also admissible . We have to show that for all nonoverlapping sequences ZK the following holds : K k=1 r ( Zk ) j c ( ZK ) r ( j ) ( 31 ) Here , c ( ZK ) denotes the set of all positions covered by the sequence of alignment templates ZK . This can be shown easily : K r ( Zk ) = K J ( Zk ) r ( Zk ) /J ( Zk ) ( 32 ) k=1 k=1 j=1 437 INPUT : covera ge vector cJ , previou sly covere d positio n j 1 ff = min ( { j | cj = 0 } ) mj = |j ff| WHILE ff = ( J + 1 ) fo : = mi n ( { j | j > ff cj = 1 } ) ff : = mi n ( { j | j > fo cj = 0 j = J + 1 } ) mj : = mj + |ff fo| OUTP UT : mj . = j c ( ZK ) r ( Zk ( j ) ) /J ( Zk ( j ) ) ( 33 ) j c ( ZK ) max Z : j ( Z ) j j ( Z ) +J ( Z ) 1 r ( Z ) /J ( Z ) ( 34 ) Here , k ( j ) denotes the phrase index k that includes the target language word position j . In the following , we develop various heuristic functions r ( Z ) of increasing complexity . The simplest realization of a heuristic function r ( Z ) takes into account only the prior probability of an alignment template instantiation : RAT ( Z = ( z , j ) ) = AT log p ( z | fj , j+J ( z ) The lexicon model can be integrated as follows : 1 ) ( 35 ) RWRD ( Z ) = WRD j ( Z ) +J ( Z ) 1 j =j ( Z ) max log p ( e fj ) ( 36 ) e The language model can be incorporated by considering that for each target word there exists an optimal language model probability : pL ( e ) = max p ( e e , e ) ( 37 ) e , e Here , we assume a trigram language model . In general , it is necessary to maximize over all possible different language model histories . We can also combine the language model and the lexicon model into one heuristic function : RWRD+LM ( Z ) = j ( Z ) +J ( Z ) 1 j =j ( Z ) max WRD log ( p ( e | fj ) ) + LM log ( pL ( e ) ) ( 38 ) To include the phrase alignment probability in the heuristic function , we compute the minimum sum of all jump widths that is needed to complete the translation . Then , an admissible heuristic function for the jump width is obtained by RAL ( c , j ) = AL D ( c , j ) ( 39 ) 438 Table 2 Statistics for Verbmobil task : training corpus ( Train ) , conventional dictionary ( Lex ) , development corpus ( Dev ) , test corpus ( Test ) ( Words* : words without punctuation marks ) . 3 Test S e n t e n c e s W o r d s 2 , 6 2 8 2 , 8 7 1 25 1 2 , 6 4 0 2 , 8 6 2 T r i g r a m p e r p l e x i t y 3 0 . 9 Combining all the heuristic functions for the various models , we obtain as final heuristic function for a search hypothesis n 6 . R ( n ) = RAL ( c ( n ) , j ( n ) ) + j c ( n ) RAT ( j ) + RWRD+LM ( j ) ( 40 ) 6.1 Results on the Verbmobil Task . We present results on the Verbmobil task , which is a speech translation task in the domain of appointment scheduling , travel planning , and hotel reservation ( Wahlster 2000 ) . T We use a training corpus , which is used to train the alignment template model and the language models , a development corpus , which is used to estimate the model scaling factors , and a test corpus . On average , 3.32 reference translations for the development corpus and 5.14 reference translations for the test corpus are used . A standard vocabulary had been defined for the various speech recognizers usedin Verbmobil . However , not all words of this vocabulary were observed in the train ing corpus . Therefore , the translation vocabulary was extended semiautomatically byadding about 13,000 German English entries from an online bilingual lexicon avail able on the Web . The resulting lexicon contained not only word-word entries , but also multi-word translations , especially for the large number of German compound words . To counteract the sparseness of the training data , a couple of straightforward rule-based preprocessing steps were applied before any other type of processing : normalization of numbers time and date phrases spelling ( e.g. , don t do not ) splitting of German compound words . 439 So far , in machine translation research there is no generally accepted criterion for the evaluation of experimental results . Therefore , we use various criteria . In the following experiments , we use : WER ( word error rate ) /mWER ( multireference word error rate ) : The WER is computed as the minimum number of substitution , insertion , and deletion operations that have to be performed to convert the generated sentence into the target sentence . In the case of the multireference word error rate for each test sentence , not just a single reference translation is used , as for the WER , but a whole set of reference translations . For each translation hypothesis , the edit distance to the most similar sentence is calculated ( Nie en et al . PER ( position-independent WER ) : A shortcoming of the WER is the fact that it requires a perfect word order . An acceptable sentence can have a word order that is different from that of the target sentence , so the WER measure alone could be misleading . To overcome this problem , we introduce as an additional measure the position-independent word error rate . This measure compares the words in the two sentences , ignoring the word order . BLEU ( bilingual evalutation understudy ) score : This score measures the precision of unigrams , bigrams , trigrams , and 4-grams with respect to a whole set of reference translations , with a penalty for too-short sentences ( Papineni et al . Unlike all other evaluation criteria used here , BLEU measures accuracy , that is , the opposite of error rate . Hence , the larger BLEU scores , the better . In the following , we analyze the effect of various system components : alignment template length , search pruning , and language model n-gram size . A systematic evaluation of the alignment template system comparing it with other translation approaches ( e.g. , rule-based ) has been performed in the Verbmobil project and is described in Tessiore and von Hahn ( 2000 ) . There , the alignment-template-based system achieved a significantly larger number of approximately correct translations than the competing translation systems ( Ney , Och , and Vogel 2001 ) . 6.1.1 Effect of Alignment Template Length . Typically , it is necessary to restrict the alignment template length to keep memory requirements low . We see that using alignment templates with only one or two words in the source languages results in very bad translation quality . Yet using alignment templates with lengths as small as three words yields optimal results . 6.1.2 Effect of Pruning and Heuristic Function . In the following , we analyze the effect of beam search pruning and of the heuristic function . We use the following criteria : Number of search errors : A search error occurs when the search algorithm misses the most probable translation and produces a translation which is less probable . As we typically can not efficiently compute the probability of the optimal translation , we can not efficiently compute the number of search errors . Yet we can compute a lower bound on the number of search errors by comparing the translation 440 Table 3 Effect of alignment template length on translation quality . AT len gth P E R [ % ] m W E R [ % ] B L E U [ % ] 1 2 9 . 1 Table 4 Effect of pruning parameter tp and heuristic function on search efficiency for direct-translation model ( Np = 50,000 ) . no heuristic function AT+WRD +LM +AL tp 10 2 10 4 10 6 10 8 10 10 10 12 found under specific pruning thresholds with the best translation that we have found using very conservative pruning thresholds . Average translation time per sentence : Pruning is used to adjust the trade-off between efficiency and quality . Hence , we present the average time needed to translate one sentence of the test corpus . Translation quality ( mWER , BLEU ) : Typically , a sentence can have many different correct translations . Therefore , a search error does not necessarily result in poorer translation quality . It is even possible that a search error can improve translation quality . Hence , we analyze the effect of search on translation quality , using the automatic evaluation criteria mWER and BLEU . The first is an estimate of the alignment template and the lexicon probability ( AT+WRD ) , the second adds an estimate of the language model ( +LM ) probability , and the third also adds the alignment probability ( +AL ) . These heuristic functions are described in Section 5.6 . Without a heuristic function , even more than a hundred seconds per sentence can not guarantee search-error-free translation . We draw the conclusion that a good heuristic function is very important to obtaining an efficient search algorithm . 441 tp 10 2 10 4 10 6 10 8 10 10 10 12 e r r o r r a t e s [ % ] no heuristic function AT+WRD +LM +AL . no heuristic function AT+WRD +LM +AL N p t i m e [ s ] s e a r c h e r r o r s t i m e [ s ] s e a r c h e r r o r s t i m e [ s ] s e a r c h e r r o r s t i m e [ s ] se a rc h e rr o rs 1 0 . 0 2 3 7 0 . 0 2 3 8 0 . 0 2 3 8 0 . 0 2 3 2 10 0 . 0 1 6 9 0 . 0 1 5 4 0 . 0 1 4 8 0 . 0 98 10 0 0 . 3 1 0 1 0 . 2 21 1,0 00 2 . 0 5 10,0 00 1 8 . 3 1 0 2 1 . 3 1 50,0 00 11 4 . 2 0 error rates [ % ] no heuristic function AT+WRD +LM +AL N p m W E R B L E U m W E R B L E U m W E R B L E U m W E R B L E U 1 6 4 . 3 10 0 4 1 . 6 1,0 00 3 7 . 0 10,0 00 3 5 . 0 50,0 00 3 5 . 0 442 Lan gua ge mo del typ e P P P E R [ % ] m W E R [ % ] B L E U [ % ] Zer ogr am 4 7 8 1 . 0 Uni gra m 2 0 3 . 7 Bigr am 3 8 . 0 Trig ram 2 9 . 2 Trig ram + CL M 2 6 . 1 In addition , the search errors have a more severe effect on the error rates if we do not use a heuristic function . The reason is that without a heuristic function , often the easy part of the input sentence is translated first . This yields severe reordering errors . 6.1.3 Effect of the Length of the Language Model History . In this work , we use only n-gram-based language models . Ideally , we would like to take into account long-range dependencies . Yet long n-grams are seen rarely and are therefore rarely used on unseen data . Therefore , we expect that extending the history length will at some point not improve further translation quality . If we perform log-linear interpolation of a trigram model with a class-based 5-gram model , we observe an additional small improvement in translation quality to an mWER of 30.9 % . 6.2 Results on the Hansards task . The Hansards task involves the proceedings of the Canadian parliament , which are kept by law in both French and English . About three million parallel sentences of this bilingual data have been made available by the Linguistic Data Consortium ( LDC ) . Here , we use a subset of the data containing only sentences of up to 30 words . Because of memory limitations , the maximum alignment template length has been restricted to four words . We compare here against the single-word-based search for Model 4 described in Tillmann ( 2001 ) . We see that the alignment template approach obtains significantly better results than the single-word-based search . 6.3 Results on Chinese English . Various statistical , example-based , and rule-based MT systems for a Chinese English news domain were evaluated in the NIST 2002 MT evaluation.4 Using the alignment 4 Evaluation home page : http : //www.nist.gov/speech/tests/mt/mt2001/index.htm.. 443 ) . F r e n c h E n g l i s h Trai nin g S e n t e n c e s W o r d s 2 4 , 3 3 8 , 1 9 5 1,4 70 , 47 3 22 ,1 63 ,0 92 W o r d s * 2 2 , 1 7 5 , 0 6 9 20 ,0 63 ,3 78 V o c a b u l a r y 1 0 0 , 2 6 9 7 8 , 3 3 2 S i n g l e t o n s 4 0 , 1 9 9 3 1 , 3 1 9 Test S e n t e n c e s W o r d s 9 7 , 6 4 6 5 , 4 3 2 8 8 , 7 7 3 T r i g r a m p e r p l e x i t y 1 7 9 . French English English French Translation approach WER [ % ] PER [ % ] WER [ % ] PER [ % ] Ali gn me nt tem plat es 6 1 . 9 Single word bas ed : mo not one sear ch 6 5 . 3 Single word bas ed : reor deri ng sear ch 6 4 . 4 Table 11 Corpus statistics for Chinese English corpora large data track ( Words* : words without punctuation marks ) . The problem domain is the translation of Chinese news text into English . The English vocabulary consists of full- form words that have been converted to lowercase letters . The number of sentences has been artificially increased by adding certain parts of the original training material more than once to the training corpus , in order to give larger weight to those parts of the training corpus that consist of high-quality aligned Chinese news text and are therefore expected to be especially helpful for the translation of the test data . 444 Table 12 Results of Chinese English NIST MT evaluation , June 2002 , large data track ( NIST09 score : larger values are better ) . System NIST09 score Alignment template approach 7.65 Competing research systems 5.03 7.34 Best of six commercial off-the-shelf systems 6.08 The Chinese language poses special problems because the boundaries of Chinese words are not marked . Chinese text is provided as a sequence of characters , and it is unclear which characters have to be grouped together to obtain entities that can be interpreted as words . For statistical MT , it would be possible to ignore this fact and treat the Chinese characters as elementary units and translate them into English . Yet preliminary experiments showed that the existing alignment models produce better results if the Chinese characters are segmented in a preprocessing step into single words . We use the LDC segmentation tool.5 For the English corpus , the following preprocessing steps are applied . First , the corpus is tokenized ; it is then segmented into sentences , and all uppercase characters are converted to lowercase . As the final evaluation criterion does not distinguish case , it is not necessary to deal with the case information . Then , the preprocessed Chinese and English corpora are sentence aligned in which the lengths of the source and target sentences are significantly different . From the resulting corpus , we automatically replace translations . In addition , only sentences with less than 60 words in English and Chinese are used . To improve the translation of Chinese numbers , we use a categorization of Chinese number and date expressions . For the statistical learning , all number and date expressions are replaced with one of two generic symbols , $ number or $ date . The number and date expressions are subjected to a rule-based translation by simple lexicon lookup . The translation of the number and date expressions is inserted into the output using the alignment information . For Chinese and English , this categorization is implemented independently of the other language . To evaluate MT quality on this task , NIST made available the NIST09 evaluation tool . This tool provides a modified BLEU score by computing a weighted precision of n-grams modified by a length penalty for very short translations . Table 12 shows the results of the official evaluation performed by NIST in June 2002 . With a score of 7.65 , the results obtained were statistically significantly better than any other competing approach . Differences in the NIST score larger than 0.12 are statistically significant at the 95 % level . We conclude that the developed alignment template approach is also applicable to unrelated language pairs such as Chinese English and that the developed statistical models indeed seem to be largely language-independent . Table 13 shows various example translations . We have presented a framework for statistical MT for natural languages which is more general than the widely used source channel approach . It allows a baseline MT 5 The LDC segmentation tool is available at . http : //morph.ldc.upenn.edu/Projects/Chinese/LDC ch.htm # cseg . 445 Table 13 Example translations for Chinese English MT . Reference Significant Accomplishment Achieved in the Economic Construction of the Fourteen Open Border Cities in China Translation The opening up of the economy of China s fourteen City made significant achievements in construction Reference Xinhua News Agency , Beijing , Feb. 12 Exciting accomplishment has been achieved in 1995 in the economic construction of China s fourteen border cities open to foreigners . Translation Xinhua News Agency , Beijing , February 12 China s opening up to the outside world of the 1995 in the fourteen border pleased to obtain the construction of the economy . Reference Foreign Investment in Jiangsu s Agriculture on the Increase Translation To increase the operation of foreign investment in Jiangsu agriculture Reference According to the data provided today by the Ministry of Foreign Trade and Economic Cooperation , as of November this year , China has actually utilized 46.959 billion US dollars of foreign capital , including 40.007 billion US dollars of direct investment from foreign businessmen . Translation The external economic and trade cooperation Department today provided that this year , the foreign capital actually utilized by China on November to US $ 46.959 billion , including of foreign company direct investment was US $ 40.007 billion . Reference According to officials from the Provincial Department of Agriculture and Forestry of Jiangsu , the Three-Capital ventures approved by agencies within the agricultural system of Jiangsu Province since 1994 have numbered more than 500 and have utilized over 700 million US dollars worth of foreign capital , respectively three times and seven times more than in 1993 . Translation Jiangsu Province for the Secretaries said that , from the 1994 years , Jiangsu Province system the approval of the three-funded enterprises , there are more than 500 , foreign investment utilization rate of more than US $ 700 million , 1993 years before three and seven . Reference The actual amount of foreign capital has also increased more than 30 % as compared with the same period last year . Translation The actual amount of foreign investment has increased by more than 30 % compared with the same period last year . Reference Import and Export in Pudong New District Exceeding 9 billion US dollars This Year Translation Foreign trade imports and exports of this year to the Pudong new Region exceeds US $ 9 billion system to be extended easily by adding new feature functions . We have described the alignment template approach for statistical machine translation , which uses two different alignment levels : a phrase-level alignment between phrases and a word- level alignment between single words . As a result the context of words has a greater influence , and the changes in word order from source to target language can be learned explicitly . An advantage of this method is that machine translation is learned fully automatically through the use of a bilingual training corpus . We have shown that the presented approach is capable of achieving better translation results on various tasks compared to other statistical , example-based , or rule-based translation systems . This is especially interesting , as our system is structured simpler than many competing systems . 446 We expect that better translation can be achieved by using models that go beyond the flat phrase segmentation that we perform in our model . A promising avenue is to gradually extend the model to take into account to some extent the recursive structure of natural languages using ideas from Wu and Wong ( 1998 ) or Alshawi , Bangalore , and Douglas ( 2000 ) . We expect other improvements as well from learning nonconsecutive phrases in source or target language and from better generalization methods for the learned-phrase pairs . The work reported here was carried out while the first author was with the Lehrstuhl fu r Informatik VI , Computer Science Department , RWTH Aachen University of Technology . In some societies , internet users have to create information morphs ( e.g . Peace West King to refer to Bo Xilai ) to avoid active censorship or achieve other communication goals . In this paper we aim to solve a new problem of resolving entity morphs to their real targets . We exploit temporal constraints to collect cross- source comparable corpora relevant to any given morph query and identify target candidates . Then we propose various novel similarity measurements including surface features , meta-path based semantic features and social correlation features and combine them in a learning-to-rank framework . Experimental results on Chinese Sina Weibo data demonstrate that our approach is promising and significantly outperforms baseline methods1 . Language constantly evolves to maximize communicative success and expressive power in daily social interactions . The proliferation of online social media significantly expedites this evolution , as new phrases triggered by social events may be disseminated rapidly in social media . To automatically analyze such fast evolving language in social media , new computational models are demanded . In this paper , we focus on one particular language evolution that creates new ways to communicate sensitive subjects because of the existence of internet information censorship . We call this 1 Some of the resources and open source programs developed in this work are made freely available for research purpose at http : //nlp.cs.qc.cuny.edu/Morphing.tar.gz phenomenon information morph . For example , when Chinese online users talk about the former politician Bo Xilai , they use a morph Peace West King instead , a historical figure four hundreds years ago who governed the same region as Bo . Morph can be considered as a special case of alias used for hiding true entities in malicious environment ( Hsiung et al. , 2005 ; Pantel , 2006 ) . However , social network plays an important role in generating morphs . Usually morphs are generated by harvesting the collective wisdom of the crowd to achieve certain communication goals . Aside from the purpose of avoiding censorship , other motivations for using morph include expressing sarcasm/irony , positive/negative sentiment or making descriptions more vivid toward some entities or events . We can see that a morph can be either a regular term with new meaning or a newly created term . Morph Target Motivation Peace West King Bo Xilai Sensitive Blind Man Chen Guangcheng Sensitive Miracle Brother Wang Yongping Irony Kim Fat Kim Joingil Negative Kimchi Country South Korea Vivid Table 1 : Morph Examples and Motivations . We believe that successful resolution of morphs is a crucial step for automated understanding of the fast evolving social media language , which is important for social media marketing ( Bar- wise and Meehan , 2010 ) . Another application is to help common users without enough background/cultural knowledge to understand internet language for their daily use . Furthermore , our approaches can also be applied for satire or other implicit meaning recognition , as well as information extraction ( Bollegala et al. , 2011 ) . 1083 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics , pages 1083 1093 , Sofia , Bulgaria , August 49 2013 . Qc 2013 Association for Computational Linguistics However , morph resolution in social media is challenging due to the following reasons . First , the sensitive real targets that exist in the same data source under active censorship are often automatically filtered . Thus , the co-occurrence of a morph and its target is quite low in the vast amount of information in social media . Second , most morphs were not created based on pronunciations , spellings or other encryptions of their original targets . Instead , they were created according to semantically related entities in historical and cultural narratives ( e.g . Peace West King as morph of Bo Xilai ) and thus very difficult to capture based on typical lexical features . Third , tweets from Twitter/Chinese Weibo are short ( only up to the similarity measures to generate global semantic features . We model social user behaviors and use social correlation to assist in measuring semantic similarities because the users who posted a morph and its corresponding target tend to share similar interests and opinions . Our experiments demonstrate that the proposed approach significantly outperforms traditional alias detection methods ( Hsiung et al. , 2005 ) . Morph Query Comparable Data Acquisition 140 characters ) and noisy , resulting in difficult extraction of rich and accurate evidences due to the Censored Data Uncensored Data lack of enough contexts . Semantic Annotatio n and Target Candidate Identification Target Candidate Ranking Surface Features Semantic Features Learning to Rank Social Features To the best of our knowledge , this is the first work to use NLP and social network analysis techniques to automatically resolve morphed information . To address the above challenges , our paper offers the following novel contributions . We detect target candidates by exploiting the dynamics of the social media to extract temporal distribution of entities , based on the assumption that the popularity of an individ ual is correlated between censored and uncensored text within a certain time window . Our approach builds and analyzes heterogeneous information networks from multiple sources , such as Twitter , Sina Weibo and web documents in formal genre ( e.g . news ) be cause a morph and its target tend to appear in similar contexts . We propose two new similarity measures , as Target Figure 1 : Overview of Morph Resolution Given a morph query m , the goal of morph resolution is to find its real target . Figure 1 depicts the general procedure of our approach . It consists of two main sub-tasks : Target Candidate Identification : For each m , discover a list of target candidates E = { e1 , e2 , ... , eN } . First , relevant comparable data sets that include m are retrieved . In this paper we collect comparable censored data from Weibo and uncensored data from Twitter and Web documents such as news articles . We then apply various annotations such as word segmentation , part-of-speech tagging , noun phrase chunking , name tagging and event extraction to these data sets . Target Candidate Ranking : Rank the target candidates in E. We explore various features including surface , semantic and social fea rank framework . Finally , the top ranked candidate is produced as the resolved target . The general goal of the first step is to identify a list of target candidates for each morph query from the comparable corpora including Sina Weibo , Chinese News websites and English Twitter . However , obviously we can not consider all of the named entities in these sources as target candidates due to the sheer volume of information . In addition , morphs are not limited to named entity forms . In order to narrow down the scope of target candidates , we propose a Temporal Distribution Assumption as follows . The intuition is that a morph m and its real target e should have similar temporal distributions in terms of their occurrences . Suppose the data sets are separated into Z temporal slots ( e.g . by day ) , the assumption can 4.2 Semantic Features . 4.2.1 Motivations Fortunately , although a morph and its target may have very different orthographic forms , they tend to be embedded in similar semantic contexts which involve similar topics and events . Figure 2 presents some example messages under censorship ( Weibo ) and not under censorship ( Twitter and Chinese Daily ) . We can see that they include similar topics , events ( e.g. , fell from power , gang crackdown , sing red songs ) , and semantic relations ( e.g. , family relations with Bo Guagua ) . Therefore if we can automatically extract and exploit these indicative semantic contexts , we can narrow down the real targets effectively . Peace West King from Chongqing Bo Xilai : ten thousand letters of accusation have been received during be stated as : fell from power , still need to sing red songs ? There is no difference between that The webpage of Tianze Economic Let Tm = { tm1 , tm2 , ... , tmZm } be the set of temporal slots each morph m occurs , and Te = { te1 , te2 , ... , teZe } be the set of slots a target can didate e occurs . Then e is considered as a target candidate of m if and only if , for each tmi Tm ( i = 1 , 2 , ... , Zm ) , there exist a j { 1 , 2 , ... , Ze } guy s plagiarism and Buhou s gang crackdown . Remember that Buhou said that his family was not rich at the press conference a few days before he fell from power . His son Bo Guagua is supported by his scholarship . Study Institute owned by the liberal party has been closed . This is the first affected website of the liberal party after Bo Xilai fell from power . Bo Xilai gave an explanation about the source of his son , Bo Guagua s tuition . Bo Xilai led Chongqing city leaders and 40 district and county party and government leaders to sing red songs . such that tmi tej , where is a threshold value ( in this paper we set the threshold to 7 days , which is optimized from a development set ) . For comparison we also attempted topic modeling approach to detect target candidates , as shown in section 5.3 . Next , we propose a learning-to-rank framework to rank target candidates based on various levels of novel features based on surface , semantic and social analysis . We first extract surface features between the morph and the candidate based on measuring orthographic similarity measures which were commonly used in entity coreference resolution ( e.g . ( Ng , 2010 ; Hsiung et al. , 2005 ) ) . The measures we use include string edit distance , normalized string edit distance ( Wagner and Fischer , 1974 ) and longest common subsequence ( Hirschberg , 1977 ) . Weibo ( censored ) Twitter and Chinese News ( uncensored ) Figure 2 : Cross-source Comparable Data Example ( each morph and target pair is shown in the same color ) 4.2.2 Information Network Construction We define an information network as a directed graph G = ( V , E ) with an object type mapping function : V A and a link type mapping function : E R , where each object v V belongs to one particular object type ( v ) A , each link e E belongs to a particular relation ( e ) R. If two links belong to the same relation type , then they share the same starting object type as well as the same ending object type . An information network is homogeneous if and only if there is only one type for both objects and links , and an information network is heterogeneous when the objects are from multiple distinct types or there exist more than one type of links . In order to construct the information networks for morphs , we apply the Standford Chinese word segmenter with Chinese Penn Treebank segmentation standard ( Chang et al. , 2008 ) and Stanford part-of-speech tagger ( Toutanova et al. , 2003 ) to process each sentence in the comparable data sets . Then we apply a hierarchical Hidden Markov Model ( HMM ) based Chinese lexical analyzer ICTCLAS ( Zhang et al. , 2003 ) to extract named entities , noun phrases and events . We have also attempted using the results from Dependency Parsing , Relation Extraction and Event Extraction tools ( Ji and Grishman , 2008 ) Buhou Morph Peace West King Morph Bo Guagua Entity Chongqing Entity Gang Crackdown Event Fell From Power Event Sing Red Songs Event Bo Xilai Entity to enrich the link types . Unfortunately the state- of-the-art techniques for these tasks still perform poorly on social media in terms of both accuracy and coverage of important information , these sophisticated semantic links all produced negative impact on the target ranking performance . Therefore we limited the types of vertices into : Morph ( M ) , Entity ( E ) , which includes target candidates , Event ( EV ) , and NonEntity Noun Phrases ( NP ) ; and used co-occurrence as the edge type . We extract entities , events , and nonentity noun phrases that occur in more than one tweet as neighbors . And for two vertices xi and xj , the weight wij of their edge is the frequency they co-occur together within the tweets . A network schema of such networks is shown in Figure 3 . Figure 4 M E EV NP Figure 3 : Network Schema of Morph-Related Heterogeneous Information Network presents an example of a heterogeneous information network from the motivation examples following the above network schema , which connects the morphs Peace West King , Buhou and their corresponding target Bo Xilai . 4.2.3 Meta-Path-Based Semantic Similarity Measurements Given the constructed network , a straightforward solution for finding the target for a morph is to use link-based similarity search . However , now objects are linked to different types of neighbors , if all neighbors are treated as the same , it may cause information loss problems . For example , the en tity ( Chongqing ) is a very important aspect Figure 4 : Example of Morph-Related Heterogeneous Information Network since he governed it , and if a morph m which is also highly correlated with ( Chongqing ) , it is very likely that Bo Xilai is the real target of m. Therefore , the semantic features generated from neighbors such as the entity ( Chongqing ) should be treated differently from other types of neighbors such as ( talented people ) . In this work , we propose to measure the similarity of two nodes over heterogeneous networks as shown in Figure 3 , by distinguishing neighbors into three types according to the network schema ( i.e . entities , events , nonentity noun phrases ) . We then adopt meta-path-based similarity measures ( Sun et al. , 2011a ; Sun et al. , 2011b ) , which are defined over heterogeneous networks to extract semantic features . A meta-path is a path defined over a network , and composed of a sequence of relations between different object types . For example , as shown in Figure 3 , a morph and its target candidate can be connected by three meta-paths , including ME - E , MEV - E , and M - NP - E . Intuitively , each meta-path provides a unique angle to measure how similar two objects are . For the determined meta-paths , we extract semantic features using the similarity measures proposed in ( Sun et al. , 2011a ; Hsiung et al. , 2005 ) . We denote the neighbor sets of certain type for a morph m and a target candidate e as ( m ) and ( e ) , and a meta-path as P. We now list several meta-path-based similarity measures below . Common neighbors ( CN ) . It measures the number of common neighbors that m and e share as | ( m ) ( e ) | . Path count ( PC ) . It measures the number of path instances between m and e following meta-path P.Pairwise random walk ( PRW ) . For a meta meta-paths with the same length P = ( P1P2 ) , pairwise random walk measures the probabil ity of the pairwise random walk starting from both m and e and reaching the same middle object . More formally , it is computed as into similarity measures to generate global semantic features . Let T = t1 t2 ... tN be a set of temporal slots ( i.e . by day ) , E be the set of target candidates for each morph m. Then for each ti T , and each 1 1 ( p1 p2 ) ( P1 P2 ) prob ( p1 ) prob ( p2 ) , where p2 is the inverse of p2 . KullbackLeibler distance ( KLD ) . For m and e , the pairwise random walk probability of their neighbors can be represented as two probability vectors , then KullbackLeibler distance ( Hsiung et al. , 2005 ) can be used to compute sim ( m , e ) . Beyond the above similarity measures , we also propose to use cosine-similarity-style normalization method to modify common neighbor and pair e E , the local semantic features simti ( m , e ) is extracted based only on the information posted within ti using one of the similarity measures introduced in Section 4.2.3 . Then we propose two approaches to generate global semantic features . The first approach is adding the similarity score between m and e in each temporal slot to attain the first set of global features : simglobal sum ( m , e ) = ) simt wise random walk measures so that we can ensure ti T i ( m , e ) . the morph node and the target candidate node are strongly connected and also have similar popularity . The modified algorithms penalize features involved with the highly popular objects , since they are more likely to have accidental interactions with each other . The second method first normalizes the similarity score in each temporal slot ti , them sum the normalized scores to generate the second set of global features , which can be calculated as simglobal norm ( m , e ) = ) normt ( m , e ) .Normalized common neighbors ( NCN ) . Nor malized common neighbors can be measured as ti T sim ( m , e ) = | ( m ) ( e ) | . where normt ( m , e ) = si mti ( m , e ) . | ( m ) | | ( e ) | i e E simti ( m , e ) counting of common neighbors by avoiding bias to highly visible or concentrated objects . Pairwise random walk/cosine ( PRW/cosine ) . Pairwise random walk measures linkage weights disproportionately with their visibility to their neighbors , which may be too strong . Instead , we propose to use a tamer normalization method as 4.2.5 Integrate Cross Source/Cross Genre Information Due to internet information censorship or surveillance , users may need to use morphs to post sensitive information . For example , the Chinese Weibo message , ( Already ( p1 p2 ) ( P1 P2 ) f ( p1 ) f ( p2 ) , where . count ( m , x ) put in prison , still need to serve Buhou ? include a morph ( Buhou ) . In contrast , users are less restricted in some other uncensored social media f ( p1 ) = f ( p2 ) = , x count ( m , x ) count ( e , x ) , x count ( e , x ) such as Twitter . For example , the tweet from Twitter ... ... ( ... call Bo Xilai peace west king or buhou ... ) contains both the morph and the real target and is the set of middle objects connecting the decomposed meta-paths p1 and p 1 , count ( y , x ) is the total number of paths between y and the middle object x , y could be m or e. The above similarity measures can also be applied to homogeneous networks that do not differentiate the neighbor types . 4.2.4 Global Semantic Feafure Generation A morph tends to have higher temporal correlation with its real target , and share more similar topics compared to other irrelevant targets . Therefore , we propose to incorporate temporal information ( Bo Xilai ) . Therefore , we propose to integrate information from another source ( e.g . Twitter ) to help resolution of sensitive morphs in Weibo . Another difficulty from morph resolution in micro-blogging is that tweets are only allowed to contain maximum 140 characters with a lot of noise and diverse topics . The shortness and diversity of tweets may limit the power of content analysis for semantic feature extraction . However , formal genres such as web documents are cleaner and contain richer contexts , thus can provide more topically related information . In this work , we also exploit the background web documents from the embedded URLs in tweets to enrich information network construction . After applying the same annotation techniques as tweets for uncensored data sets , sentence-level co-occurrence relations are extracted and integrated into the network as shown in Figure 3 . It has been shown that there exist correlation between neighbors in social networks ( Anagnostopoulos et al. , 2008 ; Wen and Lin , 2010 ) . Because of such social correlation , close social neighbors in social media such as Twitter and Weibo may post similar information , or share similar opinion . Therefore , we can utilize social correlation to assist in resolving morphs . As social correlation can be defined as a function of social distance between a pair of users , we use social distance as a proxy to social correlation in our approach . The social distance between user i and j is defined by considering the degree of separation in their interaction ( e.g . retweeting and mentioning ) and the amount of the interaction . Similar definition has been shown effective in characterizing social distance in social networks extracted from communication data ( Lin et al. , 2012 ; Wen and Lin , 2010 ) . Specifically , it is dist ( i , j ) = K 1 1 , where 4.4 Learning-to-Rank . Similar to ( Hsiung et al. , 2005 ; Sun et al. , 2011a ) , we then model the probability of linkage prediction between a morph m and its target candidate e as a function incorporating the surface , semantic and social features . Given a training pair ( m , e ) , we choose the standard logistic regression model to learn weights for the features defined above . The learnt model is used to predict the probability of linking an unseen morph and its target candidate . Based on the descending ranking order of the probability , we select top k candidates as the final answers based on the answer size k. Next , we present the experiment under various settings shown in Table 3 , and the impacts of cross source and cross genre information . 5.1 Data and Evaluation Metric . We collected 1 , 553 , 347 tweets from Chinese Sina Weibo from May 1 to June 30 to construct the censored data set , and retrieved 66 , 559 web documents from the embedded URLs in tweets as the initial uncensored data set . Retweets and redun k=1 strength ( vk , vk+1 ) v1 , ... , vk are the nodes on the shortest path from user i to user j , and strength ( vk , vk+1 ) measures the strength of interactions between vk and vk+1 dant web documents are filtered to ensure more reliable frequency counting of co-occurrence relations . We asked two native Chinese annotators to as : strength ( i , j ) = log ( Xij ) j ij , where Xij is analyze the data , and construct a test set consisted of 107 morph entities ( 81 persons and 26 locathe total interactions between user i and j , includ ing both retweeting and mentioning ( If Xij < 10 , we set strength ( i , j ) = 0 ) . We integrate social correlation and temporal information to define our social features . The intuition is that when a morph is used by an user , the real target may also in the posts by the user or his/her close friends within a certain time period . Let T be the set of temporal slots a morph m occurs , Ut be the set of users whose posts include m in slot t where t T , and Uc be the set of close friends ( i.e. , social distance < 0.5 ) for Ut . The social features are defined as s ( m , e ) = t T f ( e , t , Ut , Uc ) . |T | where f ( e , t , Ut , Uc ) is a indicator function which return 1 if one of the users in Ut or Uc posts tweets tions ) and their real targets as our references . We verified the references by Web resources including the summary of popular morphs in Wikipedia 2 . In addition , we used 23 sensitive morphs and . the entities that appear in the tweets as queries and retrieved 25 , 128 Chinese tweets from 10 % Twitter feeds within the same time period , as well as 7 , 473 web documents from the embedded URLs and added them into the uncensored data set . To evaluate the system performance , we use leave-one-out cross validation by computing accuracy as Acc @ k = Ck , where Ck is the total number of correctly resolved morphs at top k ranked answers , and Q is the total number of morph queries . We consider a morph as correctly resolved at the top k answers if the top k answer set contains the real target of the morph . include the target candidate e within 7 days before t. 2 http : //zh.wikipedia.org/wiki/ Fe atu re set s De scr ipti on s Su rf Sur fac e fea tur es Ho m B Se ma ntic fea tur es ext rac ted fro m ho mo ge ne ou s CN , PC , PR W , an d KL D Ho m E Ho m B + se ma ntic fea tur es ext rac ted fro m ho mo ge ne ous N C N an d PR W/ co sin e He tB Se ma ntic fea tur es ext rac ted fro m het ero ge ne ous CN , PC , PR W an d KL D He tE He tB + Se ma ntic fea tur es ext rac ted fro m het ero ge ne ous N C N an d PR W/ co sin e Gl ob Glo bal se ma ntic fea tur es So cia l So cial net wo rk fea tur es Table 3 : Description of feature sets . Glob only uses the same set of similarity measures when combined with other semantic features . 5.2.1 Single Genre Information We first study the contributions of each set of surface and semantic features , as shown in the first five rows in Table 4 . The poor performance based on surface features shows that morph resolution task is very challenging since 70 % of morphs are not orthographically similar to their real targets . Thus , capturing a morph s semantic meaning is crucial . Overall , the results demonstrate the effectiveness of our proposed methods . Specifically , comparing HomB and HetB , HomE and HetE , we can see that the semantic features based on heterogeneous networks have advantages over those based on homogeneous networks . This corroborates that different neighbor sets contribute differently , and such discrepancies should be captured . And comparisions of HomB and HomE , HetB and HetE demonstrate the effectiveness of our two new proposed measures . To evaluate the importance of each similarity measures , we delete the semantic features obtained from each measure in HetE and reevaluate the system . We find that NCN is the most effective measure , while KLD is the least important one . Further adding the global semantic features significantly improves the performance . This indicates that capturing both temporal correlations and semantics of morphing simultaneously are important for morph resolution . Table 5 shows that combination of surface and semantic features further improves the performance , showing that they are complementary . For example , using only surface features , the real tar get Steve Jobs of the morph ( Qiao Boss ) is not top ranked since some othercandidates such as ( George ) are more or thographically similar . However , Steve Jobs is ranked top when combined with semantic features . Table 4 : The System Performance Based on Each Single Feature Set . Fe atu res Su rf + Ho m B Su rf + Ho m E Su rf + He tB Su rf + He tE Ac c @ 1 0.2 34 0.2 38 0.2 62 0.2 76 Ac c @ 5 0.4 16 0.4 44 0.4 81 0.5 19 Ac c @ 10 0.4 77 0.5 05 0.5 33 0.5 70 Ac c @ 20 0.5 19 0.5 61 0.5 65 0.5 98 Fe atu res + Gl ob + Gl ob + Gl ob + Gl ob Ac c @ 1 0.2 90 0.3 41 0.3 22 0.3 46 Ac c @ 5 0.5 05 0.4 95 0.5 28 0.5 33 Ac c @ 10 0.5 51 0.5 51 0.5 79 0.5 84 Ac c @ 20 0.5 94 0.6 03 0.6 36 0.6 31 Table 5 : The System Performance Based on Combinations of Surface and Semantic Features . 5.2.2 Cross Source and Cross Genre Information We integrate the cross source information from Twitter , and the cross genre information from web documents into Weibo tweets for information network construction , and extract a new set of semantic features . Table 6 shows that further gains can be achieved . Notice that integrating tweets from Twitter mainly improves the ranking for top k where k > 1 . This is because Weibo dominates our dataset , and in Weibo many of these sensitive morphs are mostly used with their traditional meanings instead of the morph senses . Further performance improvement is achieved by integrating information from background formal web documents which can provide richer context and relations . speed up the system 5 times , while retain 98.5 % of the morph candidates that can be detected . Sy ste m Ac c @ 1 Ac c @ 5 Ac c @ 10 Ac c @ 20 Wit ho ut 0 . 6 9 6 Wit h 0 . 7 0 1 Table 6 : The System Performance of Integrating Cross Source and Cross Genre Information . 5.2.3 Effects of Social Features Table 7 shows that adding social features can improve the best performance achieved so far . This is because a group of people with close relationships may share similar opinion . As an example , two tweets ... of course the reputation of Buhou is a little too high ! // @ User1 : // @ User2 : Chongqing event tells us ... ) and ... do not follow Bo Xilai ... @ User1 ... ) are from two users in the same social group.One includes a morph Buhou and the other includes its target Bo Xilai . Table 8 : The Effects of Temporal Constraint We also attempted using topic modeling approach to detect target candidates . Due to the large amount of data , we first split the data set on a daily basis , then applied Probabilistic Latent Semantic Analysis ( PLSA ) ( Hofmann , 1999 ) . Named entities which co-occur at least times with a morph query in the same topic are selected as its target candidates . As shown in Table9 ( K is the number of predefined topics ) , PLSA is not quite effective mainly because traditional topic modeling approaches do not perform well on short texts from social media . Therefore , in this paper we choose a simple method based on temporal distribution to detect target candidates . Table 9 : Accuracy of Target Candidate Detection Table 7 : The Effects of Social Features . 5.3 Effects of Candidate Detection . The performance with and without candidate detection step ( using all features ) is shown in Table 8 . The gain is small since the combination of all features in the learning to rank framework can already well capture the relationship between a morph and a target candidate . Nevertheless , the temporal distribution assumption is effective . It helps to filter out 80 % of unrelated targets and 5.4 Discussions . Compared with the standard alias detection ( Surf+HomB ) approach ( Hsiung et al. , 2005 ) , our proposed approach achieves significantly better performance ( 99.9 % confidence level by the Wilcoxon Matched-Pairs Signed-Ranks Test for Acc @ 1 ) . We further explore two types of factors which may affect the system performance as follows . One important aspect affecting the resolution performance is the morph & non-morph ambiguity . We categorize a morph query as Unique if the string is mainly used as a morph when it occurs , such as ( Bodu ) which is used to re fer to Bo Xilai ; otherwise as Common ( e.g . ( Baby ) , ( President ) ) . Table 10 presents the separate scores for these two categories . We can see that the morphs in Unique category have much better resolution performance than those in Common category . Cat eg ory Nu mb er Ac c @ 1 Ac c @ 5 Acc @ 1 0 Acc @ 2 0 Un iqu e 72 0.4 79 0.7 15 0.7 71 0.8 19 Co mm on 35 0.1 71 0.3 43 0.4 0 0.4 29 Table 10 : Performance of Two Categories We also investigate the effects of popularity of morphs on the resolution performance . We split the queries into 5 bins with equal size based on the non-descending frequency , and evaluate Acc @ 1 separately . As shown in Table11 , we can see that the popularity is not highly correlated with the performance . Ra nk 0 20 % 20 % 40 % 40 % 60 % 60 % 80 % 80 % 10 0 % All 0.3 33 0.4 76 0.3 41 0.4 29 0.3 18 Un iqu e 0.3 21 0.6 79 0.3 79 0.5 71 0.4 83 Co mm on 0.2 14 0.2 14 0.0 71 0.0 71 0.2 86 Table 11 : Effects of Popularity of Morphs To analyze social media behavior under active censorship , ( Bamman et al. , 2012 ) automatically discovered politically sensitive terms from Chinese tweets based on message deletion analysis . In contrast , our work goes beyond target idendification by resolving implicit morphs to their real targets . Our work is closely related to alias detection ( Hsiung et al. , 2005 ; Pantel , 2006 ; Bollegala et al. , 2011 ; Holzer et al. , 2005 ) . We demonstrated that state-of-the-art alias detection methods did not perform well on morph resolution . In this paper we exploit cross-genre information and social correlation to measure semantic similarity . ( Yang et al. , 2011 ; Huang et al. , 2012 ) also showed the effectiveness of exploiting information from formal web documents to enhance tweet summarization and tweet ranking . Other similar research lines are the TACKBP Entity Linking ( EL ) ( Ji et al. , 2010 ; Ji et al. , 2011 ) , which links a named entity in news and web documents to an appropriate knowledge base ( KB ) entry , the task of mining name translation pairs from comparable corpora ( Udupa et al. , 2009 ; Ji , 2009 ; Fung and Yee , 1998 ; Rapp , 1999 ; Shao and Ng , 2004 ; Hassan et al. , 2007 ) and the link prediction problem ( Adamic and Adar , 2001 ; LibenNowell and Kleinberg , 2003 ; Sun et al. , 2011b ; Hasan et al. , 2006 ; Wang et al. , 2007 ; Sun et al. , 2011a ) . Most of the work focused on unstructured or structured data with clean and rich relations ( e.g . In contrast , our work constructs heterogeneous information networks from unstructured , noisy multi-genre text without explicit entity attributes . To the best of our knowledge , this is the first work of resolving implicit information morphs from the data under active censorship . Our promising results can well serve as a benchmark for this new problem . Both of the Meta-path based and social correlation based semantic similarity measurements are proven powerful and complementary . In this paper we have focused on entity morphs . In the future we will extend our method to discover other types of information morphs , such as events and nominal mentions . In addition , automatic identification of candidate morphs is another challenging task , especially when the mentions are ambiguous and can also refer to other real entities . Our ongoing work includes identifying candidate morphs from scratch , as well as discovering morphs for a given target based on anomaly analysis and textual coherence modeling . Thanks to the three anonymous reviewers for their insightful comments . This work was supported by the U.S. Army Research Laboratory under Cooperative Agreement No . W911NF09-20053 ( NSCTA ) , the U.S. NSF CAREER Award under Grant IIS0953149 , the U.S. NSF EAGER Award under Grant No . IIS1144111 , the U.S. DARPA FA875013-20041 - Deep Exploration and Filtering of Text ( DEFT ) Program , the U.S. DARPA under Agreement No . W911NF12-C-0028 , CUNY Junior Faculty Award , NSF IIS0905215 , CNS 0931975 , CCF0905014 , and MIAS , a DHS-IDS Center for Multimodal Information Access and Synthesis at UIUC . The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies , either expressed or implied , of the U.S. Government . The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on . We present a statistical model of Japanese unknown words consisting of a set of length and spelling models classified by the character types that con stitute a word . The point is quite simple : differ ent character sets should be treated differently and the changes between character types are very im portant because Japanese script has both ideograms like Chinese ( kanji ) and phonograms like English ( katakana ) . Both word segmentation accuracy and part of speech tagging accuracy are improved by the proposed model . The model can achieve 96.6 % tag ging accuracy if unknown words are correctly seg mented . In Japanese , around 95 % word segmentation ac curacy is reported by using a word-based lan guage model and the Viterbi-like dynamic program ming procedures ( Nagata , 1994 ; Yamamoto , 1996 ; Takeuchi and Matsumoto , 1997 ; Haruno and Mat sumoto , 1997 ) . About the same accuracy is reported in Chinese by statistical methods ( Sproat et al. , 1996 ) . But there has been relatively little improve ment in recent years because most of the remaining errors are due to unknown words . There are two approaches to solve this problem : to increase the coverage of the dictionary ( Fung and Wu , 1994 ; Chang et al. , 1995 ; Mori and Nagao , 1996 ) and to design a better model for unknown words ( Nagata , 1996 ; Sproat et al. , 1996 ) . We take the latter approach . To improve word segmenta tion accuracy , ( Nagata , 1996 ) used a single general purpose unknown word model , while ( Sproat et al. , 1996 ) used a set of specific word models such as for plurals , personal names , and transliterated foreign words. ' The goal of our research is to assign a correct part of speech to unknown word as well as identifying it correctly . In this paper , we present a novel statistical model for Japanese unknown words . It consists of a set of word models for each part of speech and word type . We classified Japanese words into nine orthographic types based on the character types that constitute a word . We find that by making different models for each word type , we can better model the length and spelling of unknown words . In the following sections , we first describe the lan guage model used for Japanese word segmentation . We then describe a series of unknown word mod els , from the baseline model to the one we propose . Finally , we prove the effectiveness of the proposed model by experiment . 2.1 Baseline Language Model and Search . Algorithm Let the input Japanese character sequence be C = c1 ... em , and segment it into word sequence W = w1 ... Wn 1 The word segmentation task can be defined as finding the word segmentation W that max imize the joint probability of word sequence given character sequence P ( WIC ) . Since the maximiza tion is carried out with fixed character sequence C , the word segmenter only has to maximize the joint probability of word sequence P ( W ) . W = argmaxP ( WIC ) = argmaxP ( W ) ( 1 ) w w We call P ( W ) the segmentation model . We can use any type of word-based language model for P ( W ) , such as word ngram and class-based ngram . We used the word bigram model in this paper . So , P ( W ) is approximated by the product of word hi gram probabilities P ( wilwi1 ) P ( W ) P ( w1l < bos > ) fr=2 P ( wilwi-l ) P ( < eos > lwn ) ( 2 ) Here , the special symbols < bos > and < eos > indi cate the beginning and the end of a sentence , re spectively . Basically , word bigram probabilities of the word segmentation model is estimated by computing the 1 In this paper , we define a word as a combination of its surface form and part of speech . Two words are considered to be equal only if they have the same surface form and part of speech . 277 `` ( J ) /no/particle < U-noun > '' will appear in the most frequent form of Japanese noun phrases `` A ( J ) B '' , which corresponds to `` B of A '' in English . ( f ) /no/particle < U-noun > < U-verb > \ ... , /shi/inflection < U-number > fil/yen/suffix < U-adjectival-verb > f.t Ina/inflection < U-adjective > It '/i/inflection < U-adverb > c /to/particle frequency 6783 1052 407 405 182 139 words are replaced with their corresponding part of speech-based unknown word tags are very important information source of the contexts where unknown words appears . Word Model 3.1 Baseline Model The simplest unknown word model depends only on relative frequencies of the corresponding events in the word segmented training corpus , with appropri ate smoothing techniques . The maximization search can be efficiently implemented by using the Viterbi like dynamic programming procedure described in ( Nagata , 1994 ) . 2.2 Modification to Handle Unknown . Words To handle unknown words , we made a slight modi fication in the above word segmentation model . We have introduced unknown word tags < U-t > for each part of speech t. For example , < U-noun > and < U verb > represents an unknown noun and an unknown verb , respectively . If Wi is an unknown word whose part of speech is t , the word bigram probability P ( wilwi1 ) is ap proximated as the product of word bigram probabil ity P ( < U-t > lwi_I ) and the probability of Wi given it is an unknown word whose part of speech is t , P ( wii < U-t > ) . P ( wdwi1 ) = P ( < U-t > lwi_I ) P ( wii < U-t > , Wi1 ) P ( < U-t > lwi1 ) P ( wii < U-t > ) ( 3 ) the spelling . We think of an unknown word as a word having a special part of speech < UNK > . Then , the unknown word model is formally defined as the joint probability of the character sequence wi = c1 ... ck if it is an unknown word . Without loss of generality , we decompose it into the product of word length probability and word spelling probability given its length , P ( wii < UNK > ) = P ( c1 ... cki < UNK > ) = P ( ki < UNK > ) P ( c1 ... cklk , < UNK > ) ( 4 ) where k is the length of the character sequence . We call P ( ki < UNK > ) the word length model , and P ( c1 ... Ck Ik , < UNK > ) the word spelling model . In order to estimate the entropy of English , ( Brown et al. , 1992 ) approximated P ( ki < UNK > ) by a Poisson distribution whose parameter is the average word length A in the training corpus , and P ( c1 ... Ck lk , < UNK > ) by the product of character zerogram probabilities . This means all characters in the character set are considered to be selected inde pendently and uniformly . Here , we made an assumption that the spelling P ( c1 .. .ck I < UNK > ) k ! k ( 5 ) of an unknown word solely depends on its part of speech and is independent of the previous word . This is the same assumption made in the hidden Markov model , which is called output independence . Here , a word is represented by a list of surface form , pronunciation , and part of speech , which are delimited by a slash '/ ' . The first 2 Throughout in this paper , we use the term `` infrequent words '' to represent words that appeared only once in the corpus . They are also called `` hapax legomena '' or `` hapax words '' . It is well known that the characteristics of hapax where p is the inverse of the number of characters in the character set . If we assume JIS-X-0208 is used as the Japanese character set , p = 1/6879 . Since the Poisson distribution is a single parame ter distribution with lower bound , it is appropriate to use it as a first order approximation to the word length distribution . But the Brown model has two problems . It assigns a certain amount of probability mass to zero-length words , and it is too simple to express morphology . For Japanese word segmentation and OCR error correction , ( Nagata , 1996 ) proposed a modified ver sion of the Brown model . Nagata also assumed the word length probability obeys the Poisson distribu tion . But he moved the lower bound from zero to one . legomena are similar to those of unknown words ( Baayen and Sproat , 1996 ) . P ( ki < UNK > ) ( A 1 ) k-1 - e- < > .- ) ( 6 ) ( k- 1 ) ! Instead of zerogram , He approximated the word spelling probability P ( c1 .. .ckJk , < UNK > ) by the product of word-based character bigram probabili ties , regardless of word length . P ( cl ckJk , < VNK > ) P ( cd < bow > ) f1 =2 P ( ciJci-l ) P ( < eow > Jck ) ( 7 ) where < bow > and < eow > are special symbols that indicate the beginning and the end of a word . 3.2 Correction of Word Spelling . ... ck in the set of all strings whose length are k , while the righthand side 0.. 0.5 0.45 0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 0 0 Word Length Distribution Probs from Raw Counts ( hapax words ) _.. , _ EsUmates by Poisson ( hapax words ) -+ -- Wor Character represents the probability of the string in the set of all possible strings ( from length zero to infinity ) . Let Pb ( c1 ... Ck I < UNK > ) be the probability of character string c1 ... Ck estimated from the char acter bigram model . d Length Pb ( cl ... cki < UNK > ) = 0.5 UnknoWn Word Length Distribution kanji-+ P ( c1 J < bow > ) f1 =2 P ( ciJci-l ) P ( < eow > Jck ) ( 8 ) Let Pb ( kJ < VNK > ) be the sum of the probabilities of all strings which are generated by the character bigram model and whose length are k. More appro priate estimate for P ( c1 .. .ckJk , < UNK > ) is , Pb ( c1 ... ckJ < UNK > ) P ( cl ckJk , < VNK > ) Pb ( kJ < VNK > ) ( 9 ) But how can we estimate Pb ( kl < VNK > ) ? It is difficult to compute it directly , but we can get a rea sonable estimate by considering the unigram case . If strings are generated by the character unigram model , the sum of the probabilities of all length k strings equals to the probability of the event that the end of word symbol < eow > is selected after a 0.. 0.45 0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 0 0 2 4 6 Word Character Length katakana -+ -- 10 character other than < eow > is selected k - 1 times . Pb ( kl < VNK > ) ( 1- P ( < eow > ) ) k-l P ( < eow > ) ( 10 ) 3.3 Japanese Orthography and Word . When a substring of an unknown word coincides with other word in the dic tionary , it is very likely to be decomposed into the dictionary word and the remaining substring . We find the reason of the decomposition is that the word The empirical and the estimated distributions agree fairly well . But the estimates by Poisson are smaller than empirical probabilities for shorter words ( < = 4 characters ) , and larger for longer words ( > characters ) . This is because we rep It shows that the length of kanji words distributes around 3 char acters , while that of katakana words distributes around 5 characters . The empirical word length dis tribution of In the Japanese writing system , there are at least five different types of characters other than punc tuation marks : kanji , hiragana , katakana , Roman alphabet , and Arabic numeral . Kanji which means 'Chinese character ' is used for both Chinese origin words and Japanese words semantically equivalent to Chinese characters . Hiragana and katakana are syllabaries : The former is used primarily for gram matical function words , such as particles and inflec tional endings , while the latter is used primarily to transliterate Western origin words . Roman alphabet is also used for Western origin words and acronyms . Arabic numeral is used for numbers . Most Japanese words are written in kanji , while more recent loan words are written in katakana . K atakana words are likely to be used for techni cal terms , especially in relatively new fields like computer science . Kanji words are shorter than katakana words because kanji is based on a large ( > 6,000 ) alphabet of ideograms while katakana is based on a small ( < 100 ) alphabet of phonograms . It shows approximately 65 % of words are constituted by a single character type . Among the words that are constituted by more than two character types , only the kanjihiragana and hiraganakanji sequences are morphemes and others are compound words in a strict sense although they are identified as words in the EDR corpus 3 . Therefore , we classified Japanese words into 9 word types based on the character types that consti tute a word : < sym > , < num > , < alpha > , < hira > , < kata > , and < kan > represent a sequence of sym bols , numbers , alphabets , hiraganas , katakanas , and kanjis , respectively . < kanhira > and < hirakan > represent a sequence of kanjis followed by hiraganas and that of hiraganas followed by kanjis , respec tively . The rest are classified as < mise > . The resulting unknown word model is as follows . We first select the word type , then we select the length and spelling . P ( c1 ... cki < UNK > ) = P ( < WT > I < UNK > ) P ( ki < WT > , < UNK > ) P ( c1 ... ckik , < WT > , < UNK > ) ( 11 ) 3.4 Part of Speech and Word . Morphology It is obvious that the beginnings and endings of words play an important role in tagging part of speech . This symbol typically appears at the end of transliterated Western origin words written in katakana . It is natural to make a model for each part of speech . The resulting unknown word model is as follows . P ( c1 ... cki < U-t > ) = P ( ki < U-t > ) P ( c1 ... ckik , < U-t > ) ( 12 ) This results in kanjihiragana sequence . When a Chinese character is too difficult to read , it is transliterated in hiragana . This results in either hiraganakanji or kanji hiragana sequence . type and part of speech information . This is the un known word model we propose in this paper . It first selects the word type given the part of speech , then the word length and spelling . P ( c1 ... cki < U-t > ) = P ( < WT > I < U-t > ) P ( ki < WT > , < U-t > ) P ( c1 ... ckik , < WT > , < U-t > ) ( 13 ) - ) = C ( < WT > , < U-t > ) t > C ( < U-t > ) ( 14 ) Where ad ( c ; , < WT > , < U-t > ) +ad ( c ; ic ; -1 , < WT > , < U-t > ) +a3f ( ci ) + a4f ( cilci1 ) + as ( 1/V ) ( 17 ) Here , C ( ) represents the counts in the corpus . To estimate the probabilities of the combinations of word type and part of speech that did not appeared in the training corpus , we used the Witten-Bell method ( Witten and Bell , 1991 ) to obtain an esti mate for the sum of the probabilities of unobserved events . We then redistributed this evenly among all unobserved events 4 . A < WT > , < U-t > is the average length of words whose word type is < WT > and part of speech is < U-t > . P ( ki < WT > , < U-t > ) = ( . > . < WT > . < U-t > -1 ) k-t e- ( > . < WT > < U-t > -1 ) ( k-1 ) ! ' If the combinations of word type and part of speech that did not appeared in the training corpus , we used the average word length of all words . Basically , they are estimated from the relative fre quency of the character bigrams for each word type and part of speech . f ( c ; ic ; _1 , < WT > , < U-t > ) = C ( < WT > , < U-t > , c ; -t , c ; ) C ( < WT > , < U-t > , c ; -t ) However , if we divide the corpus by the combina tion of word type and part of speech , the amount of each training data becomes very small . Therefore , we linearly interpolated the following five probabili ties ( Jelinek and Mercer , 1980 ) . P ( e ; ic ; -1 , < WT > , < U-t > ) = 4 The Witten-Bell method estimates the probability of ob serving novel events to be r/ ( n+r ) , where n is the total num ber of events seen previously , and r is the number of symbols that are distinct . The probability of the event observed c times is cf ( n + r ) . a1 +a2+a3+a4+as = 1. f ( ci , < WT > , < U-t > ) and f ( c ; lc ; -1 , < WT > , < U-t > ) are the relative frequen cies of the character unigram and bigram for each word type and part of speech . f ( c ; ) and f ( c ; jc ; _1 ) are the relative frequencies of the character unigram and bigram . V is the number of characters ( not to kens but types ) appeared in the corpus . 4.1 Training and Test Data for the . Language Model We used the EDR Japanese Corpus Version 1.0 ( EDR , 1991 ) to train the language model . It is a manually word segmented and tagged corpus of ap proximately 5.1 million words ( 208 thousand sen tences ) . It contains a variety of Japanese sentences taken from newspapers , magazines , dictionaries , en cyclopedias , textbooks , etc.. In this experiment , we randomly selected two sets of 100 thousand sentences . The first 100 thousand sentences are used for training the language model . The second 100 thousand sentences are used for test ing . The remaining 8 thousand sentences are used as a heldout set for smoothing the parameters . For the evaluation of the word segmentation ac curacy , we randomly selected 5 thousand sentences from the test set of 100 thousand sentences . We call the first test set ( 100 thousand sentences ) `` test set-1 '' and the second test set ( 5 thousand sentences ) `` test set-2 '' . There were 94,680 distinct words in the training test . We discarded the words whose frequency was one , and made a dictionary of 45,027 words . Af ter replacing the words whose frequency was one with the corresponding unknown word tags , there were 474,155 distinct word bigrams . We discarded the bigrams with frequency one , and the remaining 175,527 bigrams were used in the word segmentation model . There were 3,120 dis tinct character unigrams and 55,486 distinct char acter bigrams . We discarded the bigram with fre quency one and remaining 20,775 bigrams were used . There were 12,633 distinct character unigrams and 80,058 distinct character bigrams when we classified them for each word type and part of speech . We discarded the bigrams with frequency one and re 0.85 0.7 POS + WT + Poisson + bigram -+ POS + Poisson + bigram -+ -- maining 26,633 bigrams were used in the unknown word model . Average word lengths for each word type and part of speech were also computed from the words with frequency one in the training set . 4.2 Cross Entropy and Perplexity . ( Poisson + zerogram ) . Cross entropy was computed over the words in test set-1 that were not found in the dictionary of the word segmentation model ( 56,121 words ) . Character perplexity is more intu itive than cross entropy because it shows the average number of equally probable characters out of 6,879 characters in JIS-X-0208 . It also shows that by making a separate model for each word type , character per plexity is reduced by an additional 45 % ( 128 -- + 71 ) . This shows that the word type information is useful for modeling the morphology of Japanese words . 4.3 Part of Speech Prediction Accuracy . without Context It shows the accuracies up to the top 10 candidates . The first model is o.s51 -- -:23 _.-J4L -- -- '-5 -- 6.L ... -- -- ' 7 -- -- '-8 -- 9 ' -- J10 Rank The test words are the same 56,121 words used to compute the cross entropy . Since these unknown word models give the prob ability of spelling for each part of speech P ( wit ) , we used the empirical part of speech probability P ( t ) to compute the joint probability P ( w , t ) . The part of speech t that gives the highest joint probability is selected . i = argmp.xP ( w , t ) = P ( t ) P ( wit ) ( 18 ) The part of speech prediction accuracy of the first and the second model was 67.5 % and 74.4 % , respec tively . Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research ( Sproat et al. , 1996 ) . Let the number of words in the manually segmented corpus be Std , the number of words in the output of the word segmenter be Sys , and the number of matched words be M. Recall is defined as M/Std , and precision is defined as M/Sys . Since it is inconvenient to use both recall and precision all the time , we also use the F-measure to indicate the overall performance . It is calculated by F = ( ! 3 2 + 1.0 ) X p X R ( 32 X p + R ( 19 ) where Pis precision , R is recall , and ( 3 is the relative importance given to recall over precision . 0 65 .0 62 .0 61 .5 66 .4 42 . 7 52 .5 48 .3 51 . 4 f3 = 1.0 throughout this experiment . That is , we put equal importance on recall and precision . Com pared to the baseline model ( Poisson+ bigram ) , by using word type and part of speech information , the precision of the proposed model ( POS + WT + Pois son + bigram ) is improved by a modest 0.6 % . The impact of the proposed model is small because the out-of-vocabulary rate of test set-2 is only 3.1 % . To closely investigate the effect of the proposed unknown word model , we computed the word seg mentation accuracy of unknown words . The accuracy of the proposed model ( POS + WT + Poisson + bigram ) is signif icantly higher than the baseline model ( Poisson + bigram ) . Recall is improved from 31.8 % to 42.0 % and precision is improved from 65.0 % to 66.4 % . Here , recall is the percentage of correctly seg mented unknown words in the system output to the all unknown words in the test sentences . Precision is the percentage of correctly segmented unknown words in the system 's output to the all words that system identified as unknown words . Notice that the baseline model ( Poisson + bigram ) can not predict part of speech . To roughly estimate the amount of improvement brought by the proposed model , we applied a simple tagging strat egy to the output of the baseline model . That is , words that include numbers are tagged as numbers , and others are tagged as nouns . Other than the usual recall/precision measures , we defined another precision , which roughly correspond to the tagging accuracy in English where word segmentation is trivial . Prec2 is defined as the percentage of correctly tagged un known words to the correctly segmented unknown words . The tagging accuracy in context ( 96.6 % ) is significantly higher than that without context ( 74.4 % ) . This shows that the word bigrams using unknown word tags for each part of speech are useful to predict the part of speech . Since English uses spaces between words , unknown words can be identified by simple dictionary lookup . So the topic of interest is part of speech estimation . Some statistical model to estimate the part of speech of unknown words from the case of the first letter and the prefix and suffix is proposed ( Weischedel et al. , 1993 ; Brill , 1995 ; Ratnaparkhi , 1996 ; Mikheev , 1997 ) . On the contrary , since Asian languages like Japanese and Chinese do not put spaces between words , previous work on unknown word problem is focused on word segmentation ; there are few studies estimating part of speech of unknown words in Asian languages . The cues used for estimating the part of speech of unknown words for Japanese in this paper are ba sically the same for English , namely , the prefix and suffix of the unknown word as well as the previous and following part of speech . The contribution of this paper is in showing the fact that different char acter sets behave differently in Japanese and a better word model can be made by using this fact . By introducing different length models based on character sets , the number of decomposition errors of unknown words are significantly reduced . In other words , the tendency of over-segmentation is cor rected . This is the main reason of the remaining under-segmented and over-segmented errors . To improve the unknown word model , feature based approach such as the maximum entropy method ( Ratnaparkhi , 1996 ) might be useful , be cause we do n't have to divide the training data into several disjoint sets ( like we did by part of speech and word type ) and we can incorporate more lin guistic and morphological knowledge into the same probabilistic framework . We are thinking of re implementing our unknown word model using the maximum entropy method as the next step of our research. ) r e c pr ec F pr ec 2 Po iss on +b igr a m W T + P o is s o n + b i g r a m P O S + P o is s o n + b i g r a m P O S + W T + P o is s o n + b i g r a m 28 .1 37 .7 37 .5 40 .6 57 .3 51 .5 58 .1 64 .1 37 .7 43 .5 45 .6 49 .7 8 8 . 6 We present a statistical model of Japanese unknown words using word morphology and word context . We find that Japanese words are better modeled by clas sifying words based on the character sets ( kanji , hi ragana , katakana , etc . ) This is because the different character sets behave differ ently in many ways ( historical etymology , ideogram vs. phonogram , etc . ) . Both word segmentation ac curacy and part of speech tagging accuracy are im proved by treating them differently . Text prediction is a form of interactive machine translation that is well suited to skilled translators . In principle it can assist in the production of a target text with minimal disruption to a translator s normal routine . However , recent evaluations of a prototype prediction system showed that it significantly decreased the productivity of most translators who used it . In this paper , we analyze the reasons for this and propose a solution which consists in seeking predictions that maximize the expected benefit to the translator , rather than just trying to anticipate some amount of upcoming text . Using a model of a typical translator constructed from data collected in the evaluations of the prediction prototype , we show that this approach has the potential to turn text prediction into a help rather than a hindrance to a translator . The idea of using text prediction as a tool for translators was first introduced by Church and Hovy as one of many possible applications for crummy machine translation technology ( Church and Hovy , 1993 ) . Text prediction can be seen as a form of interactive MT that is well suited to skilled translators . Compared to the traditional form of IMT based on Kay s original work ( Kay , 1973 ) in which the user s role is to help disambiguate the source text prediction is less obtrusive and more natural , allowing the translator to focus on and directly control the contents of the target text . Predictions can benefit a translator in several ways : by accelerating typing , by suggesting translations , and by serving as an implicit check against errors . The first implementation of a predictive tool for translators was described in ( Foster et al. , 1997 ) , in the form of a simple word-completion system based on statistical models . Various enhancements to this were carried out as part of the TransType project ( Langlais et al. , 2000 ) , including the addition of a realistic user interface , better models , and the capability of predicting multi-word lexical units . In the final TransType prototype for English to French translation , the translator is presented with a short pop- up menu of predictions after each character typed . These may be incorporated into the text with a special command or rejected by continuing to type normally . Although TransType is capable of correctly anticipating over 70 % of the characters in a freely-typed translation ( within the domain of its training corpus ) , this does not mean that users can translate in 70 % less time when using the tool . In fact , in a trial with skilled translators , the users rate of text production declined by an average of 17 % as a result of using TransType ( Langlais et al. , 2002 ) . There are two main reasons for this . First , it takes time to read the system s proposals , so that in cases where they are wrong or too short , the net effect will be to slow the translator down . Second , translators do not always act rationally when confronted with a proposal ; that is , they do not always accept correct proposals and they occasionally accept incorrect ones . Many of the former cases correspond to translators simply ignoring proposals altogether , which is understandable behaviour given the first point . This paper describes a new approach to text prediction intended to address these problems . The main idea is to make predictions that maximize the expected benefit to the user in each context , rather than systematically proposing a fixed amount of text after each character typed . The expected benefit is estimated from two components : a statistical translation model that gives the probability that a candidate prediction will be correct or incorrect , and a user model that determines the benefit to the translator in either case . The user model takes into account the cost of reading a proposal , as well as the random nature of the decision to accept it or not . This approach can be characterized as making fewer but better predictions : in general , predictions will be longer in contexts where the translation model is confident , shorter where it is less so , and absent in contexts where it is very uncertain . Other novel aspects of the work we describe here are the use of a more accurate statistical translation model than has previously been employed for text prediction , and the use of a decoder to generate predictions of arbitrary length , rather than just single words or lexicalized units as in the TransType prototype . The translation model is based on the maximum entropy principle and is designed specifically for this application . To evaluate our approach to prediction , we simulated the actions of a translator over a large corpus of previously-translated text . The result is an increase of over 10 % in translator productivity when using the predictive tool . This is a considerable improvement over the -17 % observed in the TransType trials . In the basic prediction task , the input to the predictor is a source sentence s and a prefix h of its translation ( ie , the target text before the current cursor position ) ; the output is a proposed extension x to h. F Unlike the TransType prototype , which proposes a set of single-word ( or single-unit ) suggestions , we assume that each prediction consists of only a single proposal , but one that may span an arbitrary number of words . As described above , the goal of the predictor is to find the prediction x that maximizes the expected s : Let us return to serious matters . h x t : O n v a r e venir aux ch o ses se rieuses . x : evenir a ` s is the source sentence , h is the part of its translation that has already been typed , x is what the translator wants to type , and x is the prediction . benefit to the user : x = argmax B ( x , h , s ) , ( 1 ) x where B ( x , h , s ) measures typing time saved . This obviously depends on how much of x is correct , and how long it would take to edit it into the desired text . A major simplifying assumption we make is that the user edits only by erasing wrong characters from the end of a proposal . Given a TransType-style interface where acceptance places the cursor at the end of a proposal , this is the most common editing method , and it gives a conservative estimate of the cost attainable by other methods . With this assumption , the key determinant of edit cost is the length of the correct prefix of x , so the expected benefit can be written as : l B ( x , h , s ) = ) p ( k|x , h , s ) B ( x , h , s , k ) , ( 2 ) k=0 where p ( k|x , h , s ) is the probability that exactly k characters from the beginning of x will be correct , l is the length of x , and B ( x , h , s , k ) is the benefit to the user given that the first k characters of x are correct . Equations ( 1 ) and ( 2 ) define three main problems : estimating the prefix probabilities p ( k|x , h , s ) , estimating the user benefit function B ( x , h , s , k ) , and searching for x . The following three sections describe our solutions to these . The correct-prefix probabilities p ( k|x , h , s ) are derived from a word-based statistical translation model . The first step in the derivation is to convert these into a form that deals explicitly with character strings . This is accomplished by noting that p ( k|x , h , s ) is the probability that the first k characters of x are correct and that the k + 1th character ( if there is one ) is incorrect . For k < l : p ( k|x , h , s ) = p ( xk |h , s ) p ( xk+1|h , s ) likelihood that a word w will follow a previous sequence of words h in the translation of s.1 This is the family of distributions we have concentrated on modeling . Our model for p ( w|h , s ) is a log-linear combination of a trigram language model for p ( w|h ) and a maximum-entropy translation model for p ( w|s ) , de1 1 scribed in ( Foster , 2000a ; Foster , 2000b ) . The trans lation component is an analog of the IBM model 2 where xk = x1 . l , p ( k|x , h , s ) = ( Brown et al. , 1993 ) , with parameters that are op p ( x |h , s ) . Also , p ( x0 ) 1. timiz ed for use with the trigr am . The com bine d The next step is to convert string probabilities into word probabilities . To do this , we assume that strings map one-to-one into token sequences , so that : model is shown in ( Foster , 2000a ) to have significantly lower test corpus perplexity than the linear combination of a trigram and IBM 2 used in the TransType experiments ( Langlais et al. , 2002 ) . Both models support O ( mJ V 3 ) Viterbi-style searches for p ( xk |h , s ) p ( v1 , w2 , . , wm 1 , um |h , s ) , the most likely sequence of m words that follows h , where v1 is a possibly-empty word suffix , each wi is a complete word , and um is a possibly empty word prefix . The one-to-one assumption is reasonable given that entries in our lexicon contain neither whitespace nor internal punctuation . To model word-sequence probabilities , we apply the chain rule : where J is the number of tokens in s and V is the size of the target-language vocabulary . Compared to an equivalent noisy-channel combination of the form p ( t ) p ( s|t ) , where t is the target sentence , our model is faster but less accurate . It is faster because the search problem for noisy- channel models is NP-complete ( Knight , 1999 ) , and even the fastest dynamic-programming heuristics used in statistical MT ( Niessen et al. , 1998 ; Till- mann and Ney , 2000 ) , are polynomial in J for in p ( v1 , w2 , . , wm 1 , um|h , s ) = stance O ( mJ 4V 3 ) in ( Tillmann and Ney , 2000 ) . It m 1 p ( v1|h , s ) n p ( wi|h , v1 , wi 1 , s ) i=2 p ( um|h , v1 , wm 1 , s ) . ( 3 ) The probabilities of v1 and um can be expressed in terms of word probabilities as follows . Our model is therefore suitable for making predictions in real time , but not for establishing complete translations unassisted by a human . The most expensive part of the calculation in is the sum in ( 4 ) over all words in the vo p ( v1|h , s ) = p ( w1|ht , s ) / ) p ( w|ht , s ) , cabulary , which according to ( 2 ) must be carried out w : w=u1 v where the sum is over all words that start with u1 . Similarly : p ( um|ht , wm 1 , s ) = ) p ( w|ht , wm 1 , s ) . ( 4 ) 1 1 w : w=um v Thus all factors in ( 3 ) can be calculated from probabilities of the form p ( w|h , s ) which give the for every character position k in a given prediction x . We reduce the cost of this by performing sums only at the end of each sequence of complete tokens in x ( eg , after revenir and revenir aux in the above example ) . At these points , probabilities for all possible prefixes of the next word are calculated in a 1 Here we ignore the distinction between previous words that have been sanctioned by the translator and those that are hypothesized as part of the current prediction . single recursive pass over the vocabulary and stored in a trie for later access . In addition to the exact calculation , we also experimented with establishing exact probabilities via p ( w|h , s ) only at the end of each token in x , and assuming that the probabilities of the intervening characters vary linearly between these points . As a result of this assumption , p ( k|x , h , s ) = p ( xk |h , s ) 1 0.9 0.8 0.7 0.6 0.5 0.4 raw smoothed model 1 |h , s ) is constant for all k between the end of one word and the next , and therefore can be factored out of the sum in between these points . The purpose of the user model is to determine the expected benefit B ( x , h , s , k ) to the translator of a prediction x whose first k characters match the text that the translator wishes to type . This will depend on whether the translator decides to accept or reject the prediction , so the first step in our model is the following expansion : B ( x , h , s , k ) = ) p ( a|x , h , s , k ) B ( x , h , s , k , a ) , a { 0,1 } where p ( a|x , h , s , k ) is the probability that the translator accepts or rejects x , B ( x , h , s , k , a ) is the benefit they derive from doing so , and a is a random variable that takes on the values 1 for acceptance and 0 for rejection . The first two quantities are the main elements in the user model , and are described in following sections . The parameters of both were estimated from data collected during the TransType trial described in ( Langlais et al. , 2002 ) , which involved nine accomplished translators using a prototype prediction tool for approximately half an hour each . In all cases , estimates were made by pooling the data for all nine translators . Ideally , a model for p ( a|x , h , s , k ) would take into account whether the user actually reads the proposal before accepting or rejecting it , eg : p ( a|x , h , s , k ) = ) p ( a|r , x , h , s , k ) p ( r|x , h , s , k ) r { 0,1 } where r is a boolean read variable . factors which influence whether a user is likely to read a proposal such as a record of how many previous predictions have been accepted are not available to the predictor in our formulation . We thus model p ( a|x , h , s , k ) directly . Our model is based on the assumption that the probability of accepting x depends only on what the user stands to gain from it , defined according to the editing scenario given in section 2 as the amount by which the length of the correct prefix of x exceeds the length of the incorrect suffix : p ( a|x , h , s , k ) p ( a|2k l ) , where k ( l k ) = 2k l is called the gain . The strongest part of this assumption is dropping the dependence on h , because there is some evidence from the data that users are more likely to accept at the beginnings of words . However , this does not appear to have a severe effect on the quality of the model . There is a certain amount of noise intrinsic to the estimation procedure , since it is difficult to determine x , and there fore k , reliably from the data in some cases ( when the user is editing the text heavily ) . Nonetheless , it is apparent from the plot that gain is a useful abstrac 4000 3500 3000 raw least squ ares fit 4000 3500 3000 r a w l e a s t s q u a r e s f i t 2500 2500 2000 2000 1500 1500 1000 1000 500 500 0 0 This relatively clean separation supports the basic assumption in section 2 that benefit depends on k. The model probabilities are taken from the curve at integral values . This is straightforward in the case of T ( x , k ) and E ( x , k ) , which are estimated as k and l k + 1 respectively for E ( x , k ) , this corresponds to one keystroke for the command to accept a prediction , and one to erase each wrong character . This is likely to slightly underestimate the true benefit , because it is usually harder to type n characters than to erase them . As in the previous section , read costs are interpreted as expected values with respect to the probability that the user actually does read x , eg , assuming 0 cost for not reading , R0 ( x ) = p ( r = 1|x ) Rt ( x ) , where Rt ( x ) is the unknown true cost of reading The benefit B ( x , h , s , k , a ) is defined as the typing time the translator saves by accepting or rejecting a prediction x whose first k characters are correct . To determine this , we assume that the translator first reads x , then , if he or she decides to accept , uses a special command to place the cursor at the end of x and erases its last l k characters . Assuming independence from h , s as before , our model is : r and rejecting x . To determine Ra ( x ) , we measured the average elapsed time in the TransType data from the point at which a proposal was displayed to the point at which the next user action occurred either an acceptance or some other command signalling a rejection . Times greater than 5 seconds were treated as indicating that the translator was distracted and were filtered out . 2 Here the number of characters read was assumed to include . the whole contents of the TransType menu in the case of rejections , and only the proposal that was ultimately accepted in the case of acceptances . tionship between the number of characters read and the time taken to read them , so we used the least- squares lines shown as our models . Both plots are noisy and would benefit from a more sophisticated psycholinguistic analysis , but they are plausible and empirically-grounded first approximations . To convert reading times to keystrokes for the benefit function we calculated an average time per keystroke ( 304 milliseconds ) based on sections of the trial where translators were rapidly typing and when predictions were not displayed . This gives an upper bound for the per-keystroke cost of reading compare to , for instance , simply dividing the total time required to produce a text by the number of characters in it and therefore results in a conservative estimate of benefit . Combining these with the acceptance probability of .25 gives an overall expected benefit B ( x , h , s , k = 7 ) for this proposal of 0.05 keystrokes . Searching directly through all character strings x in order to find x according to would be very expensive . The fact that B ( x , h , s ) is non- monotonic in the length of x makes it difficult to organize efficient dynamic-programming search techniques or use heuristics to prune partial hypotheses . Because of this , we adopted a fairly radical search strategy that involves first finding the most likely sequence of words of each length , then calculating the benefit of each of these sequences to determine the best proposal . The algorithm is : 1 . For each length m = 1 . M , find the best . word sequence : M average time maximum time 1 2 3 4 5 0.0012 0.0038 0.0097 0.0184 0.0285 0.01 0.23 0.51 0.55 0.57 In all experiments reported below , M was set to a maximum of 5 to allow for convenient testing . Step 1 is carried out using a Viterbi beam search . To speed this up , the search is limited to an active vocabulary of target words likely to appear in translations of s , defined as the set of all words connected by some word-pair feature in our translation model to some word in s. Step 2 is a trivial deterministic procedure that mainly involves deciding whether or not to introduce blanks between adjacent words ( eg yes in the case of la + vie , no in the case of l + an ) . This also removes the prefix u1 from the proposal . Step 3 involves a straightforward evaluation of m strings according to . Times for the linear model are similar . Although the maximum times shown would cause perceptible delays for M > 1 , these occur very rarely , and in practice typing is usually not noticeably impeded when using the TransType interface , even at M = 5 . We evaluated the predictor for English to French translation on a section of the Canadian Hansard corpus , after training the model on a chronologi w m = argmax w1 : ( w1 =u1 v ) , wm p ( wm|ht , s ) , cally earlier section . The test corpus consisted of 5,020 sentence pairs and approximately 100k words where u1 and ht are as defined in section 3 . Convert each w m to a corresponding character . string x m. in each language ; details of the training corpus are given in ( Foster , 2000b ) . To simulate a translator s responses to predictions , we relied on the user model , accepting prob 3 . = argmaxm B ( x m , h , s ) , or the abilistically according to p ( a|x , h , s , k ) , determinempty string if all B ( x m , h , s ) are non positive . ing the associated benefit using B ( x , h , s , k , a ) , and advancing the cursor k characters in the case of an config M 1 2 3 4 5 fixed linear exact corr best -8.50.43.6011.620.8 6.1 9.40 8.8 8.1 7.8 5.3 10.10 10.7 10.0 9.7 5.8 10.7 12.0 12.5 12.6 7.9 17.90 24.5 27.7 29.2 fixed exact best -11.59.315.122.028.2 3.0 4.3 5.0 5.2 5.2 6.2 12.1 15.4 16.7 17.3 Numbers give % reductions in keystrokes . user M 1 2 3 4 5 superman rational real 48.6 53.5 51.8 51.1 50.9 11.7 17.8 17.2 16.4 16.1 5.3 10.10 10.7 10.0 9.7 Numbers give % reductions in keystrokes . Here k was obtained by comparing x to the known x from the test corpus . It may seem artificial to measure performance according to the objective function for the predictor , but this is biased only to the extent that it misrepresents an actual user s characteristics . There are two cases : either the user is a better candidate types more slowly , reacts more quickly and rationally than assumed by the model , or a worse one . The predictor will not be optimized in either case , but the simulation will only overestimate the benefit in the second case . By being conservative in estimating the parameters of the user model , we feel we have minimized the number of translators who would fall into this category , and thus can hope to obtain realistic lower bounds for the average benefit across all translators . The top portion corresponds to the MEMD2B maximum entropy model described in ( Foster , 2000a ) ; the bottom portion corresponds to the linear combination of a trigram and IBM 2 used in the TransType experiments ( Langlais et al. , 2002 ) . Columns give the maximum permitted number of words in predictions . Rows show different predic tor configurations : fixed ignores the user model and makes fixed M -word predictions ; linear uses the linear character-probability estimates described in section 3.1 ; exact uses the exact character-probability calculation ; corr is described below ; and best gives an upper bound on performance by choosing m in step 3 of the search algorithm so as to maximize B ( x , h , s , k ) using the true value of k. For each simulation , the predictor optimized benefits for the corresponding user model . Several conclusions can be drawn from these results . First , it is clear that estimating expected benefit is a much better strategy than making fixed-word- length proposals , since the latter causes an increase in time for all values of M . In general , making exact estimates of string prefix probabilities works better than a linear approximation , but the difference is fairly small . Second , the MEMD2B model significantly outperforms the trigram+IBM2 combination , producing better results for every predictor configuration tested . The figure of -11.5 % in bold corresponds to the TransType configuration , and corroborates the validity of the simulation.3 Third , there are large drops in benefit due to reading times and probabilistic acceptance . The biggest cost is due to reading , which lowers the best possible keystroke reduction by almost 50 % for M = 5 . Probabilistic acceptance causes a further drop of about 15 % for M = 5 . The main disappointment in these results is that performance peaks at M = 3 rather than continuing to improve as the predictor is allowed to consider longer word sequences . Since the predictor knows B ( x , h , s , k ) , the most likely cause for this is that the estimates for p ( w m|h , s ) become worse with increasing m. Significantly , performance lev 3 Although the drop observed with real users was greater at about 20 % ( = 17 % reduction in speed ) , there are many differences between experimental setups that could account for the discrepancy . For instance , part of the corpus used for the TransType trials was drawn from a different domain , which would adversely affect predictor performance . els off at three words , just as the search loses direct contact with h through the trigram . To correct for this , we used modified probabilities of the form m p ( w m|h , s ) , where m is a length-specific correction factor , tuned so as to optimize benefit on a cross-validation corpus . In this case , performance improves with M , reaching a maximum keystroke reduction of 12.6 % at M = 5 . We have described an approach to text prediction for translators that is based on maximizing the benefit to the translator according to an explicit user model whose parameters were set from data collected in user evaluations of an existing text prediction prototype . Using this approach , we demonstrate in simulated results that our current predictor can reduce the time required for an average user to type a text in the domain of our training corpus by over 10 % . We look forward to corroborating this result in tests with real translators . There are many ways to build on the work described here . The statistical models which are the backbone of the predictor could be improved by making them adaptive taking advantage of the user s input and by adding features to capture the alignment relation between h and s in such a way as to preserve the efficient search properties . The user model could also be made adaptive , and it could be enriched in many other ways , for instance so as to capture the propensity of translators to accept at the beginnings of words . We feel that the idea of creating explicit user models to guide the behaviour of interactive systems is likely to have applications in areas of NLP apart from translators tools . For one thing , most of the approach described here carries over more or less directly to monolingual text prediction , which is an important tool for the handicapped ( Carlberger et al. , 1997 ) . Other possibilities include virtually any application where a human and a machine communicate through a language-rich interface . Given the lack of word delimiters in written Japanese , word segmentation is generally consid ered a crucial first step in processing Japanese texts . Typical Japanese segmentation algorithms rely ei ther on a lexicon and grammar or on pre-segmented data . In contrast , we introduce a novel statistical method utilizing unsegmentd training data , with performance on kanji sequences comparable to and sometimes surpassing that of morphological analyz ers over a variety of error metrics . Because Japanese is written without delimiters be tween words , 1 accurate word segmentation to re cover the lexical items is a key step in Japanese text processing . Proposed applications of segmentation technology include extracting new technical terms , indexing documents for information retrieval , and correcting optical character recognition ( OCR ) er rors ( Wu and Tseng , 1993 ; Nagao and Mori , 1994 ; Nagata , 1996a ; Nagata , 1996b ; Sproat et al. , 1996 ; Fung , 1998 ) . Typically , Japanese word segmentation is per formed by morphological analysis based on lexical and grammatical knowledge . This analysis is aided by the fact that there are three types of Japanese characters , kanji , hiragana , and katakana : changes in character type often indicate word boundaries , al though using this heuristic alone achieves less than 60 % accuracy ( Nagata , 1997 ) . Character sequences consisting solely of kanji pose a challenge to morphologically-based seg menters for several reasons . First and most importantly , kanji sequences often contain domain terms and proper nouns : Fung ( 1998 ) notes that5085 % of the terms in various technical dictio 1The analogous situation in English would be if words were written without spaces between them . naries are composed at least partly of kanji . Such words tend to be missing from general-purpose lexicons , causing an unknown word problem for morphological analyzers ; yet , these terms are quite important for information retrieval , information extraction , and text summarization , making correct segmentation of these terms critical . Second , kanji sequences often consist of compound nouns , so grammatical constraints are not applicable . For instance , the sequence shachohjkenjgyoh-mujbu choh ( presidentjandjbusinessjgeneral manager = `` a president as well as a general manager of business '' ) could be incorrectly segmented as : sha chohjkengyohjmujbu-choh ( presidentjsubsidiary business ! Tsutomu [ a name ] jgeneral manager ) ; since both alternatives are four-noun sequences , they can not be distinguished by part-of-speech information alone . Finally , heuristics based on changes in character type obviously do not apply to kanji-only sequences . Since se quences of more than 3 kanji generally consist of more than one word , at least 21.2 % of 1993 Nikkei newswire consists of kanji sequences requiring seg mentation . Thus , accuracy on kanji sequences is an important aspect of the total segmentation process . As an alternative to lexica-grammatical and su pervised approaches , we propose a simple , effi cient segmentation method which learns mostly from very large amounts of unsegmented training data , thus avoiding the costs of building a lexicon or grammar or hand-segmenting large amounts of training data . Some key advantages of this method are : No Japanese-specific rules are employed , en hancing portability to other languages . A very small number of pre-segmented train ing examples ( as few as 5 in our experiments ) are needed for good performance , as long as large amounts of unsegmented data are avail able . For long kanji strings , the method produces re sults rivalling those produced by Juman 3.61 ( Kurohashi and Nagao , 1998 ) and Chasen 1.0 ( Matsumoto et al. , 1997 ) , two morphological analyzers in widespread use . For instance , we achieve 5 % higher word precision and 6 % bet ter morpheme recall . Let I > ( y , z ) be an indicator function that is 1 when y > z , and 0 otherwise.2 In order to compensate for the fact that there are more n-gram questions than ( n -1 ) -gram questions , we calculate the fraction of affirmative answers separately for each n in N : 2 n-1 Our algorithm employs counts of character n-grams Vn ( k ) = 2 ( n 1 L L I > ( # ( 8f ) , # ( tj ) ) i=l j=l in an unsegmented corpus to make segmentation de cisions . Let `` A B C D W X Y Z '' represent an eight-kanji VN ( k ) = 1 INI L Vn ( k ) nEN sequence . To decide whether there should be a word boundary between D and W , we check whether n grams that are adjacent to the proposed boundary , such as the 4-grams s1 = '' A B C D '' and 82 = '' W X Y Z '' , tend to be more frequent than n-grams that straddle it , such as the 4-gram t 1 = '' BCD W '' . If so , we have evidence of a word boundary between D and W , since there seems to be relatively little cohesion between the characters on opposite sides of this gap . The n-gram orders used as evidence in the seg mentation decision are specified by the set N. For instance , if N = { 4 } in our example , then we pose the six questions of the form , `` Is # ( 8i ) > # ( tj ) ? `` , where # ( x ) denotes the number of occurrences of x in the ( unsegmented ) training corpus . If N = { 2,4 } , then two more questions ( Is `` # ( CD ) > # ( D W ) ? '' and `` Is # ( W X ) > # ( D W ) ? '' ) More formally , let 8 and 8 be the non straddling n-grams just to the left and right of lo cation k , respectively , and let tj be the straddling n-gram with j characters to the right of location k. After vN ( k ) is computed for every location , bound aries are placed at all locations f such that either : VN ( f ) > VN ( f- 1 ) and VN ( ) > VN ( f. + 1 ) ( that is , R. is a local maximum ) , or vN ( R. ) t , a threshold parameter . Note that it also controls the granularity of the segmentation : low thresholds encourage shorter segments . Both the count acquisition and the testing phase are efficient . Computing n-gram statistics for all possible values of n simultaneously can be done in 0 ( m log m ) time using suffix arrays , where m is the training corpus size ( Manber and Myers , 1993 ; Nagao and Mori , 1994 ) . However , if the set N of n gram orders is known in advance , conceptually simpler algorithms suffice . Memory allocation for 2Note that we do not take into account the magnitude of the difference between the two frequencies ; see section 5 for discussion . v , ./k ) A BI C DI W XI Y1 Z the word level , a stem and its affixe s are brack eted toget her as a single unit . The X- Y boundary is created by the threshold criterion , the other three by the local maximum condition . count tables can be significantly reduced by omit ting n-grams occurring only once and assuming the count of unseen n-grams to be one . In the applica tion phase , the algorithm is clearly linear in the test corpus size if INI is treated as a constant . Finally , we note that some pre-segmented data is necessary in order to set the parameters N and t. However , as described below , very little such data was required to get good performance ; we therefore deem our algorithm to be `` mostly unsupervised '' . Five 500-sequence held-out subsets were ob tained from this corpus , the rest of the data serv ing as the unsegmented corpus from which to derive character n-gram counts . Each held-out subset was hand-segmented and then split into a 50-sequence parameter-training set and a 450-sequence test set . Finally , any sequences occurring in both a test set and its corresponding parameter-training set were discarded from the parameter-training set , so that these sets were disjoint . ( Typically no more than five sequences were removed . ) Each held-out set contained 500 randomly-extracted kanji sequences at least ten characters long ( about twelve on average ) , lengthy sequences being the most difficult to segment ( Takeda and Fujisaki , 1987 ) . To obtain the gold-standard annotations , we segmented the sequences by hand , using an observa tion of Takeda and Fujisaki ( 1987 ) that many kanji compound words consist of two-character stem words together with one-character prefixes and suf fixes . Using this terminology , our two-level bracket serves as a suffix . Loosely speaking , word-level bracketing demarcates discourse entities , whereas morpheme-level brackets enclose strings that can not be further segmented without loss of meaning.4 For instance , if one segments naga-no in naga-no-shi into naga ( long ) and no ( field ) , the intended mean ing disappears . Here is an example sequence from our datasets : [ ' J ' f'.X : J [ mli'JJ [ [ iiibJ [ J J [ J Three native Japanese speakers participated in the annotation : one segmented all the held-out data based on the above rules , and the other two reviewed 350 sequences in total . The percentage of agree ment with the first person 's bracketing was 98.42 % : only 62 out of 3927 locations were contested by a verifier . Interestingly , all disagreement was at the morpheme level . We evaluated our segmentation method by com paring its performance against Chasen 1.05 ( Mat sumoto et al. , 1997 ) and Juman 3.61,6 ( Kurohashi and Nagao , 1998 ) , two state-of-the-art , publically available , user-extensible morphological analyzers . In both cases , the grammars were used as distributed without modification . The sizes of Chasen 's and Ju man 's default lexicons are approximately 115,000 and 231,000 words , respectively . Comparison issues An important question that arose in designing our experiments was how to en able morphological analyzers to make use of the parameter-training data , since they do not have pa rameters to tune . The only significant way that they can be updated is by changing their grammars or lexicons , which is quite tedious ( for instance , we had to add part-of-speech information to new en tries by hand ) . We took what we felt to be a rea sonable , but not too time-consuming , course of cre ating new lexical entries for all the bracketed words in the parameter-training data . Evidence that this ing annotation may be summarized as follows.3 At 4This level of segmentation is consistent with Wu 's ( 1998 ) 3A complete description of the annotation policy , including the treatment of numeric expressions , may be found in a tech nical report ( Ando and Lee , 1999 ) . The three rightmost groups represent our algorithm with parameters tuned for different optimization criteria . was appropriate comes from the fact that these ad ditions never degraded test set performance , and in deed improved it by one percent in some cases ( only small improvements are to be expected because the parameter-training sets were fairly small ) . It is important to note that in the end , we are com paring algorithms with access to different sources of knowledge . Juman and Chasen use lexicons and grammars developed by human experts . Our al gorithm , not having access to such pre-compiled knowledge bases , must of necessity draw on other information sources ( in this case , a very large un segmented corpus and a few pre-segmented exam ples ) to compensate for this lack . Since we are in terested in whether using simple statistics can match the performance of labor-intensive methods , we do not view these information sources as conveying an unfair advantage , especially since the annotated training sets were small , available to the morpho logical analyzers , and disjoint from the test sets . We report the average results over the five test sets using the optimal parameter settings for the corre sponding training sets ( we tried all nonempty sub sets of { 2 , 3 , 4 , 5 , 6 } for the set of n-gram orders N and all values in { .05 , .1 , .15 , ... , 1 } for the thresh old t ) 7 . In all performance graphs , the `` error bars '' represent one standard deviation . The results for Chasen and Juman reflect the lexicon additions de 7 For simplicity , ties were deterministically broken by pre ferring smaller sizes of N , shorter n-grams in N , and larger threshold values , in that order . Word and morpheme accuracy The standard metrics in word segmentation are word precision and recall . Treating a proposed segmentation as a non-nested bracketing ( e.g. , `` IABICI '' corresponds to the bracketing `` [ AB ] [ C ] '' ) , word precision ( P ) is defined as the percentage of proposed brackets that exactly match word-level brackets in the annotation ; word recall ( R ) is the percentage of word-level an notation brackets that are proposed by the algorithm in question ; and word F combines precision and re call : F = 2PR/ ( P + R ) .. One problem with using word metrics is that morphological analyzers are designed to produce morpheme-level segments . To compensate , we al tered the segmentations produced by Juman and Chasen by concatenating stems and affixes , as iden tified by the part-of-speech information the analyz ers provided . ( We also measured morpheme accu racy , as described below . ) Our algorithm achieves 5.27 % higher preci sion and 0.26 % better F-measure accuracy than Ju man , and does even better ( 8.8 % and 4.22 % , respec tively ) with respect to Chasen . The recall perfor mance falls ( barely ) between that of Juman and that of Chasen . As noted above , Juman and Chasen were de signed to produce morpheme-level segmentations . We therefore also measured morpheme precision , recall , and F measure , all defined analogously to their word counterparts . We see that our algorithm can achieve better recall ( by 6.51 % ) and F-measure ( by 1.38 % ) than Juman , and does better than Chasen by an even wider mar gin ( 11.18 % and 5.39 % , respectively ) . Precision was generally worse than the morphological analyz ers . Compatible Brackets Although word-level accu racy is a standard performance metric , it is clearly very sensitive to the test annotation . Morpheme ac curacy suffers the same problem . Indeed , the au thors of Juman and Chasen may well have con structed their standard dictionaries using different notions of word and morpheme than the definitions we used in annotating the data . BS , I M o r p h e m e a c c u r a c y CHASEN JUMAN opJrrlze rocal oplmlzo F p r e d l i o n Fi g ur e 5 : M or p h e m e ac c ur ac y. segme ntatio ns can be count ed as correc t. W e also use the all comp atibl e brac kets rate , whic h is the fracti on of seque nces for whic h all the prop osed brack ets are comp atible . Intuit ively , this funct ion meas ures the ease with whic h a huma n could corre ct the outpu t of the segm entati on algo rithm : if the all comp atible brack ets rate is high , then the error s are conc entrat ed in relati vely few seque nces ; if it is low , then a huma n doing post proce ssing woul d have to corre ct many sequ ences . Our algor ithm does bet ter on both metri cs ( for insta nce , when F meas ure is opti mize d , by 2.16 % and 1.9 % , respe ctivel y , in rect with respect to any reasonable annotation . Our novel metrics account for two types of er rors . The first , a crossing bracket , is a proposed bracket that overlaps but is not contained within an annotation bracket ( Grishman et al. , 1992 ) . Cross ing brackets can not coexist with annotation brack ets , and it is unlikely that another human would create such brackets . The second type of er ror , a morpheme-dividing bracket , subdivides a morpheme-level annotation bracket ; by definition , such a bracket results in a loss of meaning . We define a compatible bracket as a proposed bracket that is neither crossing nor morpheme dividing . The compatible brackets rate is simply the compatible brackets precision . Note that this met ric accounts for different levels of segmentation si multaneously , which is beneficial because the gran ularity of Chasen and Juman 's segmentation varies from morpheme level to compound word level ( by our definition ) . For instance , well-known university names are treated as single segments by virtue of be ing in the default lexicon , whereas other university names are divided into the name and the word `` uni versity '' . Using the compatible brackets rate , both comparison to Chasen , and by 3.15 % and 4.96 % , respectively , in comparison to Juman ) , regardless of training optimization function ( word precision , re call , or F -we can not directly optimize the com patible brackets rate because `` perfect '' performance is possible simply by making the entire sequence a single segment ) . Compatible and all-compatible bracket.s rates CHASEN JUIAAH cpWnizo Pf '' '' '' '' '' O ! ! Urtlos _brad < JIUIIU Juman5 vs. Juman50 Our50 vs Juman50 Our5 vs. Juman5 Our5 vs. Juman50 precision recall F-measure -1.040.630.84 +5.274.39 +0.26 +6.183.73 +1.14 +5.144.36 +0.30 '' 5 '' and `` 50 '' denote training set size before discarding overlaps with the test sets . Minimal human effort is needed . In contrast to our mostly-unsupervised method , morphological analyzers need a lexicon and grammar rules built using human expertise . The workload in creating dictionaries on the order of hundreds of thousands of words ( the size of Chasen 's and Juman 's de fault lexicons ) is clearly much larger than annotat ing the small parameter-training sets for our algo rithm . We also avoid the need to segment a large amount of parameter-training data because our al gorithm draws almost all its information from an unsegmented corpus . Indeed , the only human effort involved in our algorithm is pre-segmenting the five 50-sequence parameter training sets , which took only 42 minutes . In contrast , previously proposed supervised approaches have used segmented train ing sets ranging from 10005000 sentences ( Kash ioka et al. , 1998 ) to 190,000 sentences ( Nagata , l996a ) . To test how much annotated training data is actu ally necessary , we experimented with using minis cule parameter-training sets : five sets of only five strings each ( from which any sequences repeated in the test data were discarded ) . It took only 4 minutes to perform the hand segmentation in this case . Both the local maximum and threshold condi tions contribute . In our algorithm , a location k is deemed a word boundary if vN ( k ) is either ( 1 ) a local maximum or ( 2 ) at least as big as the thresh old t. It is natural to ask whether we really need two conditions , or whether just one would suffice . We therefore studied whether optimal perfor mance could be achieved using only one of the con ditions . Indeed , in some cases , both are needed to achieve the best perfor mance ; also , each condition when used in isolation yields suboptimal performance with respect to some performance metrics . a c c ur a c y o p ti m i z e p r e c i s i o n o p t i m i z e r e c a l l o p ti m iz eF m ea su re w or d M M & T M m or p h e m e M & T T T Japanese Many previously proposed segmenta tion methods for Japanese text make use of either a pre existing lexicon ( Yamron et al. , 1993 ; Mat sumoto and Nagao , 1994 ; Takeuchi and Matsumoto , 1995 ; Nagata , 1997 ; Fuchi and Takagi , 1998 ) or pre-segmented training data ( Nagata , 1994 ; Papa georgiou , 1994 ; Nagata , 1996a ; Kashioka et al. , 1998 ; Mori and Nagao , 1998 ) . Other approaches bootstrap from an initial segmentation provided by a baseline algorithm such as Juman ( Matsukawa et al. , 1993 ; Yamamoto , 1996 ) . Unsupervised , non-lexicon-based methods for Japanese segmentation do exist , but they often have limited applicability . Both Tomokiyo and Ries ( 1997 ) and Teller and Batchelder ( 1994 ) explicitly avoid working with kanji charactes . Takeda and Fujisaki ( 1987 ) propose the short unit model , a type of Hidden Markov Model with linguistically determined topology , to segment kanji compound words . However , their method does not handle three-character stem words or single-character stem words with affixes , both of which often occur in proper nouns . In our five test datasets , we found that 13.56 % of the kanji sequences contain words that can not be handled by the short unit model.Nagao and Mori ( 1994 ) propose using the heuris tic that high-frequency character n-grams may rep resent ( portions of ) new collocations and terms , but the results are not experimentally evaluated , nor is a general segmentation algorithm proposed . The work of Ito and Kohda ( 1995 ) similarly relies on high-frequency character n-grams , but again , is more concerned with using these frequent n-grams as pseudo-lexicon entries ; a standard segmentation algorithm is then used on the basis of the induced lexicon . Our algorithm , on the hand , is fundamen tally different in that it incorporates no explicit no tion of word , but only `` sees '' locations between characters . Chinese According to Sproat et al . ( 1996 ) , most prior work in Chinese segmentation has exploited lexical knowledge bases ; indeed , the authors assert that they were aware of only one previously pub lished instance ( the mutual-information method of Sproat and Shih ( 1990 ) ) of a purely statistical ap proach . In a later paper , Palmer ( 1997 ) presents a transformation-based algorithm , which requires pre-segmented training data . To our knowledge , the Chinese segmenter most similar to ours is that of Sun et al . They also avoid using a lexicon , determining whether a given location constitutes a word boundary in part by deciding whether the two characters on either side tend to occur together ; also , they use thresholds and several types of local minima and maxima to make segmentation decisions . However , the statis tics they use ( mutual information and t-score ) are more complex than the simple n-gram counts that we employ . Our preliminary reimplementation of their method shows that it does not perform as well as the morphological analyzers on our datasets , al though we do not want to draw definite conclusions because some aspects of Sun et al 's method seem incomparable to ours . We do note , however , that their method incorporates numerical differences between statistics , whereas we only use indicator functions ; for example , once we know that one trigram is more common than another , we do not take into account the difference between the two frequencies . We conjecture that using absolute differences may have an adverse effect on rare sequences . In this paper , we have presented a simple , mostly unsupervised algorithm that segments Japanese se quences into words based on statistics drawn from a large unsegmented corpus . We evaluated per formance on kanji with respect to several metrics , including the novel compatible brackets and all compatible brackets rates , and found that our al gorithm could yield performances rivaling that of lexicon-based morphological analyzers . In future work , we plan to experiment on Japanese sentences with mixtures of character types , possibly in combination with morphologi cal analyzers in order to balance the strengths and weaknesses of the two types of methods . Since our method does not use any Japanese-dependent heuristics , we also hope to test it on Chinese or other languages as well . We thank Minoru Shindoh and Takashi Ando for reviewing the annotations , and the anonymous re viewers for their comments . This material was sup ported in part by a grant from the GE Foundation . Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation We proposed two approaches to improve Chinese word segmentation : a subword-based tagging and a confidence measure approach . We found the former achieved better performance than the existing character-based tagging , and the latter improved segmentation further by combining the former with a dictionary-based segmentation . In addition , the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates . By these techniques we achieved higher F-scores in CITYU , PKU and MSR corpora than the best results from Sighan Bakeoff 2005 . The character-based IOB tagging approach has been widely used in Chinese word segmentation recently ( Xue and Shen , 2003 ; Peng and McCallum , 2004 ; Tseng et al. , 2005 ) . Under the scheme , each character of a word is labeled as B if it is the first character of a multiple-character word , or O if the character functions as an independent word , or I otherwise . For example , ( whole ) ( Beijing city ) is labeled as ( whole ) /O ( north ) /B ( capital ) /I ( city ) /I . We found that so far all the existing implementations were using character-based IOB tagging . In this work we propose a subword-based IOB tagging , which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters . If only Chinese characters are used , the subword-based IOB tagging is downgraded into a character-based one . Taking the same example mentioned above , ( whole ) ( Beijing city ) is labeled as ( whole ) /O ( Beijing ) /B ( city ) /I in the subword-based tagging , where ( Beijing ) /B is labeled as one unit . We will give a detailed description of this approach in Section 2 . Now the second author is affiliated with NTT . In addition , we found a clear weakness with the IOB tagging approach : It yields a very low in-vocabulary ( IV ) rate ( R-iv ) in return for a higher out-of-vocabulary ( OOV ) rate ( R-oov ) . In the results of the closed test in Bakeoff 2005 ( Emerson , 2005 ) , the work of ( Tseng et al. , 2005 ) , using conditional random fields ( CRF ) for the IOB tagging , yielded very high R-oovs in all of the four corpora used , but the R-iv rates were lower . While OOV recognition is very important in word segmentation , a higher IV rate is also desired . In this work we propose a confidence measure approach to lessen the weakness . By this approach we can change R-oovs and R-ivs and find an optimal tradeoff . This approach will be described in Section 2.2 . In the followings , we illustrate our word segmentation process in Section 2 , where the subword-based tagging is implemented by the CRFs method . Section 3 presents our experimental results . Section 4 describes current state- of-the-art methods for Chinese word segmentation , with which our results were compared . Section 5 provides the concluding remarks . Our word segmentation process is illustrated in Fig . It is composed of three parts : a dictionary-based N-gram word segmentation for segmenting IV words , a subword- based tagging by the CRF for recognizing OOVs , and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging . An example exhibiting each step s results is also given in the Since the dictionary-based approach is a well-known method , we skip its technical descriptions . However , keep in mind that the dictionary-based approach can produce a higher R-iv rate . We will use this advantage in the confidence measure approach . 2.1 Subword-based IOB tagging using CRFs . There are several steps to train a subword-based IOB tag- ger . First , we extracted a word list from the training data sorted in decreasing order by their counts in the training 193 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL , pages 193 196 , New York , June 2006 . Qc 2006 Association for Computational Linguistics input +XDQJ < LQJ & KXQ OLYHV LQ % HLMLQJFLW\ Dictionary-based word segmentation +XDQJ < LQJ & KXQ OLYHV LQ % HLMLQJFLW\ Subword-based IOB tagging / % / , / , /2 /2 / % / , +XDQJ/ % < LQJ/ , & KXQ/ , OLYHV/2 LQ/2 % HLMLQJ/ % FLW\/ , Confidence-based segmentation / % / , / , /2 /2 / % / , +XDQJ/ % < LQJ/ , & KXQ/ , OLYHV/2 LQ/2 % HLMLQJ/ % FLW\/ , output +XDQJ < LQJ & KXQ OLYHV LQ % HLMLQJFLW\ : Outline of word segmentation process data . We chose all the single characters and the top multi- character words as a lexicon subset for the IOB tagging . If the subset consists of Chinese characters only , it is a character-based IOB tagger . We regard the words in the subset as the subwords for the IOB tagging . Second , we re-segmented the words in the training data into subwords belonging to the subset , and assigned IOB tags to them . For a character-based IOB tagger , there is only one possibility of re-segmentation . However , there are multiple choices for a subword-based IOB tagger . For example , ( Beijing-city ) can be segmented as ( Beijing-city ) /O , or ( Beijing ) /B ( city ) /I , or ( north ) /B ( capital ) /I ( city ) /I . In this work we used forward maximal match ( FMM ) for disambiguation . Of course , backward maximal match ( BMM ) or other approaches are also applicable . We did not conduct comparative experiments because trivial differences of these approaches may not result in significant consequences to the subword-based ap proach . In the third step , we used the CRFs approach to train the IOB tagger ( Lafferty et al. , 2001 ) on the training data . We downloaded and used the package CRF++ from the site http : //www.chasen.org/ taku/software . According to the CRFs , the probability of an IOB tag sequence , T = t0 t1 tM , given the word sequence , W = w0 w1 wM , is defined by p ( T |W ) = and current observation ti simultaneously ; gk ( ti , W ) , the unigram feature functions because they trigger only current observation ti . k and k are the model parameters corresponding to feature functions fk and gk respectively . The model parameters were trained by maximizing the log-likelihood of the training data using L-BFGS gradient descent optimization method . In order to overcome overfitting , a gaussian prior was imposed in the training . The types of unigram features used in our experiments included the following types : w0 , w 1 , w1 , w 2 , w2 , w0 w 1 , w0 w1 , w 1 w1 , w 2 w 1 , w2 w0 where w stands for word . The subscripts are position indicators . 0 means the current word ; 1 , 2 , the first or second word to the left ; 1 , 2 , the first or second word to the right . For the bigram features , we only used the previous and the current observations , t 1 t0 . As to feature selection , we simply used absolute counts for each feature in the training data . We defined a cutoff value for each feature type and selected the features with occurrence counts over the cutoff . A forward-backward algorithm was used in the training and viterbi algorithm was used in the decoding . Before moving to this step in , we produced two segmentation results : the one by the dictionary-based approach and the one by the IOB tagging . However , neither was perfect . The dictionary-based segmentation produced results with higher R-ivs but lower R-oovs while the IOB tagging yielded the contrary results . In this section we introduce a confidence measure approach to combine the two results . We define a confidence measure , C M ( tiob |w ) , to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation . The confidence measure comes from two sources : IOB tagging and dictionary- based word segmentation . Its calculation is defined as : C M ( tiob |w ) = C Miob ( tiob |w ) + ( 1 ) ( tw , tiob ) ng ( 2 ) where tiob is the word w s IOB tag assigned by the IOB tagging ; tw , a prior IOB tag determined by the results of the dictionary-based segmentation . After the dictionary- based word segmentation , the words are re-segmented into subwords by FMM before being fed to IOB tagging . Each subword is given a prior IOB tag , tw . C Miob ( t|w ) , a M confidence probability derived in the process of IOB tag exp ) ' ) ' k fk ( ti 1 , ti , W ) + ) ' k gk ( ti , W ) /Z , i=1 k k ( 1 ) ging , is defined as Z = ) ' T =t0 t1 tM p ( T |W ) C Miob ( t|w ) = L , T =t0 t1 tM , ti =t P ( T |W , wi ) T =t 0 t1 tM P ( T | W ) where we call fk ( ti 1 , ti , W ) bigram feature functions because the features trigger the previous observation ti 1 where the numerator is a sum of all the observation sequences with word wi labeled as t. ( tw , tiob ) ng denotes the contribution of the dictionary- based segmentation . It is a Kronecker delta function defined as ( tw , tiob ) ng = { 1 if tw = tiob 0 otherwise In Eq . 2 , is a weighting between the IOB tagging and the dictionary-based word segmentation . We found the value 0.7 for , empirically . 2 the results of IOB tagging were reevaluated . A confidence measure threshold , t , was defined for making a decision based on the value . If the value was lower than t , the IOB tag was rejected and the dictionary-based segmentation was used ; otherwise , the IOB tagging segmentation was used . A new OOV was thus created . For the two extreme cases , t = 0 is the case of the IOB tagging while t = 1 is that of the dictionary-based approach . In a real application , a satisfactory tradeoff between R- ivs and R-oovs could find through tuning the confidence threshold . In Section 3.2 we will present the experimental segmentation results of the confidence measure approach . We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections . The data contain four corpora from different sources : Academia Sinica ( AS ) , City University of Hong Kong ( CITYU ) , Peking University ( PKU ) and Microsoft Research in Beijing ( MSR ) . Since this work was to evaluate the proposed subword-based IOB tagging , we carried out the closed test only . Five metrics were used to evaluate segmentation results : recall ( R ) , precision ( P ) , F-score ( F ) , OOV rate ( R-oov ) and IV rate ( R-iv ) . of the corpora and these scores , refer to ( Emerson , 2005 ) . For the dictionary-based approach , we extracted a word list from the training data as the vocabulary . Tri- gram LMs were generated using the SRI LM toolkit for disambiguation . shows the performance of the dictionary-based segmentation . Since there were some single-character words present in the test data but not in the training data , the R-oov rates were not zero in this experiment . In fact , there were no OOV recognition . Hence , this approach produced lower F-scores . However , the R-ivs were very high . 3.1 Effects of the Character-based and the . subword-based tagger The main difference between the character-based and the word-based is the contents of the lexicon subset used for re-segmentation . For the character-based tagging , we used all the Chinese characters . For the subword-based tagging , we added another 2000 most frequent multiple- character words to the lexicons for tagging . The segmentation results of the dictionary-based were re-segmented : Our segmentation results by the dictionary- based approach for the closed test of Bakeoff 2005 , very low R-oov rates due to no OOV recognition applied . R P FR oo vR iv A S 0.9 51 0.9 53 0.9 42 0.9 40 0.9 47 0.9 47 0 . 64 7 0.9 64 0.9 67 CI TY U 0.9 39 0.9 50 0.9 43 0.9 42 0.9 41 0.9 46 0 . 73 6 0.9 58 0.9 67 P K U 0.9 40 0.9 43 0.9 50 0.9 46 0.9 45 0.9 45 0 . 75 4 0.9 49 0.9 55 M S R 0.9 57 0.9 65 0.9 60 0.9 63 0.9 59 0.9 64 0 . 71 6 0.9 64 0.9 72 : Segmentation results by a pure subword-based IOB tagging . The upper numbers are of the character- based and the lower ones , the subword-based . using the FMM , and then labeled with IOB tags by the CRFs . The segmentation results using CRF tagging are shown in , where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based . We found that the proposed subword-based approaches were effective in CITYU and MSR corpora , raising the F-scores from 0.941 to 0.946 for CITYU corpus , 0.959 to 0.964 for MSR corpus . There were no F-score changes for AS and PKU corpora , but the recall rates were improved . Comparing , we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach . However , the R-iv rates were getting worse in return for higher R-oov rates . We will tackle this problem by the confidence measure approach . 3.2 Effect of the confidence measure . In section 2.2 , we proposed a confidence measure approach to reevaluate the results of IOB tagging by combinations of the results of the dictionary-based segmentation . The effect of the confidence measure is shown in , where we used = 0.7 and confidence threshold t = 0.8 . In each slot , the numbers on the top were of the character-based approach while the numbers on the bottom were the subword-based . We found the results in were better than those in and , which prove that using confidence measure approach achieved the best performance over the dictionary-based segmentation and the IOB tagging approach . The act of confidence measure made a tradeoff between R-ivs and R- oovs , yielding higher R-oovs than and higher R R P FR oo vR iv A S 0.9 53 0.9 56 0.9 44 0.9 47 0.9 48 0.9 51 0 . 64 9 0.9 69 0.9 69 CI TY U 0.9 43 0.9 52 0.9 48 0.9 49 0.9 46 0.9 51 0 . 74 1 0.9 64 0.9 69 P K U 0.9 42 0.9 47 0.9 57 0.9 55 0.9 49 0.9 51 0 . 74 8 0.9 52 0.9 59 M S R 0.9 60 0.9 72 0.9 66 0.9 69 0.9 63 0.9 71 0 . 71 2 0.9 67 0.9 76 : Effects of combination using the confidence measure . The upper numbers and the lower numbers are of the character-based and the subword-based , respectively A S CI T Y U M SR P K U Ba ke off be st 0 . 95 0 O u r s 0 . 95 1 : Comparison our results with the best ones from Sighan Bakeoff 2005 in terms of F-score ivs than . Even with the use of confidence measure , the word- based IOB tagging still outperformed the character-based IOB tagging . It proves the proposed word-based IOB tagging was very effective . The IOB tagging approach adopted in this work is not a new idea . It was first used in Chinese word segmentation by ( Xue and Shen , 2003 ) , where maximum entropy methods were used . Later , this approach was implemented by the CRF-based method ( Peng and McCallum , 2004 ) , which was proved to achieve better results than the maximum entropy approach because it can solve the label bias problem ( Lafferty et al. , 2001 ) . Our main contribution is to extend the IOB tagging approach from being a character-based to a subword-based . We proved the new approach enhanced the word segmentation significantly . Our results are listed together with the best results from Bakeoff 2005 in in terms of F-scores . We achieved the highest F-scores in CITYU , PKU and MSR corpora . We think our proposed subword- based tagging played an important role for the good results . Since it was a closed test , some information such as Arabic and Chinese number and alphabetical letters can not be used . We could yield a better results than those shown in using such information . For example , inconsistent errors of foreign names can be fixed if alphabetical characters are known . For AS corpus , Adam Smith are two words in the training but become a one- word in the test , AdamSmith . Our approaches produced wrong segmentations for labeling inconsistency . Another advantage of the word-based IOB tagging over the character-based is its speed . The subword-based approach is faster because fewer words than characters were labeled . We found a speed up both in training and test . The idea of using the confidence measure has appeared in ( Peng and McCallum , 2004 ) , where it was used to recognize the OOVs . In this work we used it more delicately . By way of the confidence measure we combined results from the dictionary-based and the IOB-tagging-based and as a result , we could achieve the optimal performance . In this work , we proposed a subword-based IOB tagging method for Chinese word segmentation . Using the CRFs approaches , we prove that it outperformed the character- based method using the CRF approaches . We also successfully employed the confidence measure to make a confidence-dependent word segmentation . This approach is effective for performing desired segmentation based on users requirements to R-oov and R-iv . The authors appreciate the reviewers effort and good advice for improving the paper . We propose the first joint model for word segmentation , POS tagging , and dependency parsing for Chinese . Based on an extension of the incremental joint model for POS tagging and dependency parsing ( Hatori et al. , 2011 ) , we propose an efficient character-based decoding method that can combine features from state-of-the-art segmentation , POS tagging , and dependency parsing models . We also describe our method to align comparable states in the beam , and how we can combine features of different characteristics in our incremental framework . In experiments using the Chinese Treebank ( CTB ) , we show that the accuracies of the three tasks can be improved significantly over the baseline models , particularly by 0.6 % for POS tagging and 2.4 % for dependency parsing . We also perform comparison experiments with the partially joint models . In processing natural languages that do not include delimiters ( e.g . spaces ) between words , word segmentation is the crucial first step that is necessary to perform virtually all NLP tasks . Furthermore , the word-level information is often augmented with the POS tags , which , along with segmentation , form the basic foundation of statistical NLP . Because the tasks of word segmentation and POS tagging have strong interactions , many studies have been devoted to the task of joint word segmentation and POS tagging for languages such as Chinese ( e.g . This is because some of the segmentation ambiguities can not be resolved without considering the surrounding grammatical constructions encoded in a sequence of POS tags . The joint approach to word segmentation and POS tagging has been reported to improve word segmentation and POS tagging accuracies by more than 1 % in Chinese ( Zhang and Clark , 2008 ) . In addition , some researchers recently proposed a joint approach to Chinese POS tagging and dependency parsing ( Li et al. , 2011 ; Hatori et al. , 2011 ) ; particularly , Ha- tori et al . ( 2011 ) proposed an incremental approach to this joint task , and showed that the joint approach improves the accuracies of these two tasks . In this context , it is natural to consider further a question regarding the joint framework : how strongly do the tasks of word segmentation and dependency parsing interact ? In the following Chinese sentences : current peace-prize and peace operation related The current peace prize and peace operations are related . current peace award peace operation related group The current peace is awarded to peace-operation-related groups . the only difference is the existence of the last word ; however , whether or not this word existschanges the whole syntactic structure and segmen tation of the sentence . This is an example in which word segmentation can not be handled properly without considering long-range syntactic information . Syntactic information is also considered beneficial to improve the segmentation of out of- vocabulary ( OOV ) words . Unlike languages such as Japanese that use a distinct character set ( i.e . katakana ) for foreign words , the transliterated words in Chinese , many of which are OOV words , frequently include characters that are also used as common or function words . In the current systems , the existence of these characters causes numerous over- segmentation errors for OOV words . Based on these observations , we aim at building a joint model that simultaneously processes word segmentation , POS tagging , and dependency parsing , trying to capture global interaction among 1045 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics , pages 1045 1053 , Jeju , Republic of Korea , 814 July 2012 . Qc 2012 Association for Computational Linguistics these three tasks . To handle the increased computational complexity , we adopt the incremental parsing framework with dynamic programming ( Huang and Sagae , 2010 ) , and propose an efficient method of character-based decoding over candidate structures . Two major challenges exist in formalizing the joint segmentation and dependency parsing task in the character-based incremental framework . First , we must address the problem of how to align comparable states effectively in the beam . Because the number of dependency arcs varies depending on how words are segmented , we devise a step alignment scheme using the number of character-based arcs , which enables effective joint decoding for the three tasks . Second , although the feature set is fundamentally a combination of those used in previous works ( Zhang and Clark , 2010 ; Huang and Sagae , 2010 ) , to integrate them in a single incremental framework is not straightforward . Because we must perform decisions of three kinds ( segmentation , tagging , and parsing ) in an incremental framework , we must adjust which features are to be activated when , and how they are combined with which action labels . We have also found that we must balance the learning rate between features for segmentation and tagging decisions , and those for dependency parsing . We perform experiments using the Chinese Tree- bank ( CTB ) corpora , demonstrating that the accuracies of the three tasks can be improved significantly over the pipeline combination of the state-of-the-art joint segmentation and POS tagging model , and the dependency parser . We also perform comparison experiments with partially joint models , and investigate the tradeoff between the running speed and the model performance . In Chinese , Luo ( 2003 ) proposed a joint constituency parser that performs segmentation , POS tagging , and parsing within a single character-based framework . They reported that the POS tags contribute to segmentation accuracies by more than 1 % , but the syntactic information has no substantial effect on the segmentation accuracies . In contrast , we built a joint model based on a dependency-based framework , with a rich set of structural features . Using it , we show the first positive result in Chinese that the segmentation accuracies can be improved using the syntactic information . Another line of work exists on lattice-based parsing for Semitic languages ( Cohen and Smith , 2007 ; Goldberg and Tsarfaty , 2008 ) . These methods first convert an input sentence into a lattice encoding the morphological ambiguities , and then conduct joint morphological segmentation and PCFG parsing . However , the segmentation possibilities considered in those studies are limited to those output by an existing morphological analyzer . In addition , the lattice does not include word segmentation ambiguities crossing boundaries of space-delimited tokens . In contrast , because the Chinese language does not have spaces between words , we fundamentally need to consider the lattice structure of the whole sentence . Therefore , we place no restriction on the segmentation possibilities to consider , and we assess the full potential of the joint segmentation and dependency parsing model . Among the many recent works on joint segmentation and POS tagging for Chinese , the linear-time incremental models by Zhang and Clark ( 2008 ) and Zhang and Clark ( 2010 ) largely inspired our model . Zhang and Clark ( 2008 ) proposed an incremental joint segmentation and POS tagging model , with an effective feature set for Chinese . However , it requires to computationally expensive multiple beams to compare words of different lengths using beam search . More recently , Zhang and Clark ( 2010 ) proposed an efficient character-based decoder for their word-based model . In their new model , a single beam suffices for decoding ; hence , they reported that their model is practically ten times as fast as their original model . To incorporate the word-level features into the character-based decoder , the features are decomposed into substring-level features , which are effective for incomplete words to have comparable scores to complete words in the beam . Because we found that even an incremental approach with beam search is intractable if we perform the word- based decoding , we take a character-based approach to produce our joint model . The incremental framework of our model is based on the joint POS tagging and dependency parsing model for Chinese ( Hatori et al. , 2011 ) , which is an extension of the shift-reduce dependency parser with dynamic programming ( Huang and Sagae , 2010 ) . They specifically modified the shift action so that it assigns the POS tag when a word is shifted onto the stack . However , because they regarded word segmentation as given , their model did not consider the interaction between segmentation and POS tagging . 3.1 Incremental Joint Segmentation , POS . Tagging , and Dependency Parsing Based on the joint POS tagging and dependency parsing model by Hatori et al . ( 2011 ) , we build our joint model to solve word segmentation , POS tagging , and dependency parsing within a single framework . Particularly , we change the role of the shift action and additionally use the append action , inspired by the character-based actions used in the joint segmentation and POS tagging model by Zhang and Clark ( 2010 ) . The list of actions used is the following : A : append the first character in the queue to the word on top of the stack . SH ( t ) : shift the first character in the input queue as a new word onto the stack , with POS tag t. RL/RR : reduce the top two trees on the stack , ( s0 , s1 ) , into a subtree s0s1 / s s1 , respectively . We can first think of using the number of shifted characters as the step index , as Zhang and Clark ( 2010 ) does . However , because RL/RR actions can be performed without incrementing the step index , the decoder tends to prefer states with more dependency arcs , resulting more likely in premature choice of reduce actions or oversegmentation of words . Alternatively , we can consider using the number of actions that have been applied as the step index , as Hatori et al . However , this results in inconsistent numbers of actions to reach the terminal states : some states that segment words into larger chunks reach a terminal state earlier than other states with smaller chunks . For these reasons , we have found that both approaches yield poor models that are not at all competitive with the baseline ( pipeline ) models1 . To address this issue , we propose an indexing scheme using the number of character-based arcs . We presume that in addition to the word-to-word dependency arcs , each word ( of length M ) implicitly has M 1 inter-character arcs , as in : A B C , 0 0 Although SH ( t ) is similar to the one used in Hatori A B C , and A BC ( each rectangle de et al . ( 2011 ) , now it shifts the first character in the queue as a new word , instead of shifting a word . Following Zhang and Clark ( 2010 ) , the POS tag is assigned to the word when its first character is shifted , and the word tag pairs observed in the training data and the closed-set tags ( Xia , 2000 ) are used to prune unlikely derivations . Because 33 tags are defined in the CTB tag set ( Xia , 2000 ) , our model exploits a total of 36 actions . To train the model , we use the averaged perceptron with the early update ( Collins and Roark , 2004 ) . In our joint model , the early update is invoked by mistakes in any of word segmentation , POS tagging , or dependency parsing . When dependency parsing is integrated into the task of joint word segmentation and POS tagging , it is not straightforward to define a scheme to align ( synchronize ) the states in the beam . In beam search , we use the step index that is associated with each state : the parser states in process are aligned according to the index , and the beam search pruning is applied to those states with the same index . Consequently , for the beam search to function effectively , all states with the same index must be comparable , and all terminal states should have the same step index . Then we can define the step index as the sum of the number of shifted characters and the total number of ( inter-word and intra-word ) dependency arcs , which thereby meets all the following conditions : ( 1 ) All subtrees spanning M consecutive characters have the same index 2M 1 . ( 2 ) All terminal states have the same step index 2N ( including the root arc ) , where N is the number of characters in the sentence . ( 3 ) Every action increases the index . Note that the number of shifted characters is also necessary to meet condition ( 3 ) . Otherwise , it allows an unlimited number of SH ( t ) actions without incrementing the step index . In our framework , because an action increases the step index by 1 ( for SH ( t ) or RL/RR ) or 2 ( for A ) , we need to use two beams to store new states at each step . The computational complexity of the entire process is O ( B ( T + 3 ) 2N ) , where B is the beam 1 For example , in our preliminary experiment on CTB5 , the step indexing according to the number of actions underperforms the baseline model by 0.2 0.3 % in segmentation accuracy . step 1 step 2 1 1 1 step 3 1 1 1 3 step 5 denote the reduce actions that determine the word boundary2 , whereas RL1/RR1 denote those reduce actions that are applied when the word boundary has already been fixed . In addition , to capture the shared step 6 step 7 step 8 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 nature of boundary determination actions ( SH ( t ) , 3 1 1 1 3 1 1 1 1 3 1 1 1 1 1 RL0/RR0 ) , we use a generalized action label SH to 3 3 3 3 1 5 1 5 1 1 7 3 3 1 1 5 1 1 1 5 3 7 1 represent any of them when combined with W01 W21 . size , T is the number of POS tags ( = 33 ) , and N is the number of characters in the sentence . Theoretically , the computational time is greater than that with the character-based joint segmentation and tagging model by Zhang and Clark ( 2010 ) by a factor of T +3 2N T +1 N , .. 2.1 , when the same beam size is used . The feature set of our model is fundamentally a combination of the features used in the state-of-the-art joint segmentation and POS tagging model ( Zhang and Clark , 2010 ) and dependency parser ( Huang and Sagae , 2010 ) , both of which are used as baseline models in our experiment . However , we must carefully adjust which features are to be activated and when , and how they are combined with which action labels , depending on the type of the features because we intend to perform three tasks in a single incremental framework . Note that not all features are always considered : each feature is only considered if the action to be performed is included in the list of actions in the When to apply column . Because S01 S05 are used to represent the likelihood score of substring sequences , they are only used for A and SH ( t ) without being combined with any action label . Because T01 T05 are used to determine the POS tag of the word being shifted , they are only applied for SH ( t ) . Because W01 W21 are used to determine whether to segment at the current position or not , they are only used for those actions involved in boundary determination decisions ( A , SH ( t ) , RL0 , and RR0 ) . The action labels RL0/RR0 are used to level and substring-level scores . Regarding the parsing features P01 P28 , because we found that P01 P17 are also useful for segmentation decisions , these features are applied to all actions including A , with an explicit distinction of action labels RL0/RR0 from RL1/RR1 . On the other hand , P18 P28 are only used when one of the parser actions ( SH ( t ) , RL , or RR ) is applied . Note that P07 P09 and P18 P21 ( look-ahead features ) require the look-ahead information of the next word form and POS tags , which can not be incorporated straightforwardly in an incremental framework . Although we have found that these features can be incorporated using the delayed features proposed by Hatori et al . ( 2011 ) , we did not use them in our current model because it results in the significant increase of computational time . 3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation ( Sproat et al. , 1996 ) , the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting . Therefore , we optionally use four features D01 D04 associated with external dictionaries . These features distinguish each dictionary source , reflecting the fact that different dictionaries have different characteristics . These features will also be used in our reimplementation of the model by Zhang and Clark ( 2010 ) . 3.4 Adjusting the Learning Rate of Features . In formulating the three tasks in the incremental framework , we found that adjusting the update rate depending on the type of the features ( segmentation/tagging vs. parsing ) crucially impacts the final performance of the model . To investigate this point , we define the feature vector and score of the 2 A reduce action has an additional effect of fixing the boundary of the top word on the stack if the last action was A or SH ( t ) . Then , if we set p to a number smaller than 1 , perceptron updates for the parsing features will be kept small at the early stage of training because the update is proportional to the values of the W21 q 1 .t c cat ( q 1 .e ) A , SH A , SH ( t ) , RR/RL0 ( W20 , W21 : c q 1 .w\e ) feature vector . q.w and q.t respectively denote the ( root ) word form and POS tag of a subtree ( word ) q , and q.b and q.e the beginning and ending characters of q.w . c0 and c1 are the first and second characters in the queue . q.w\e denotes the set of characters excluding the ending character of q.w . len ( ) denotes the length of the word , capped at 16 if longer . cat ( ) de notes the category of the character , which is the set of POS tags observed in the training data . Di is a dictionary , a set of words . The action label means that the feature is not combined with any label ; as-is denotes the use of the default action set A , SH ( t ) , and RR/RL as is . the global weights for the parsing features will in crease as needed and compensate for the small p as the training proceeds . In this way , we can control the contribution of syntactic dependencies at the early stage of training . Section 4.3 shows that the best setting we found is p = 0.5 : this result suggests that we probably should resolve remaining errors by preferentially using the local n-gram based features at the early stage of training . Otherwise , the premature incorporation of the non-local syntactic dependencies might engender overfitting to the training data . Settings We use the Chinese Penn Treebank ver . 5.1 , 6.0 , and 7.0 ( hereinafter CTB5 , CTB6 , and CTB7 ) for evaluation . These corpora are split into training , development , and test sets , according to previous works . For CTB5 , we refer to the split by Duan et al . ( 2007 ) as CTB 5d , and to the split by Jiang et al . ( 2008 ) as CTB5j . We also prepare a dataset for cross validation : the dataset CTB5c consists of sentences from CTB5 excluding the development and test sets of CTB5d and CTB5j . We split CTB 5c into five sets ( CTB5c-n ) , and alternatively use four of these as the training set and the rest as the test set . CTB6 is split according to the official split described in the documentation , and CTB7 is split according to Wang et al . As external dic94 tionaries , we use the HowNet Word List3 , consist ing of 91,015 words , and page names from the Chi- 92 nese Wikipedia4 as of Oct 26 , 2011 , consisting of 709,352 words . These dictionaries only consist of 90 word forms with no frequency or POS information . 88We use standard measures of word-level preci 76 74 72 70 68 66 Seg ( _p=0.1 ) Seg ( _p=0.2 ) Seg ( _p=0.5 ) 64 Seg ( _p=1.0 ) Tag ( _p=0.1 ) Tag ( _p=0.2 ) 62 Tag ( _p=0.5 ) Dep ( _p=0.1 ) Dep ( _p=0.2 ) Dep ( _p=0.5 ) sion , recall , and F1 score , for evaluating each task . The output of dependencies can not be correct unless Tag ( _p=1.0 ) 86 60 0 10 20 30 40 50 60 70 80 Dep ( _p=1.0 ) 0 10 20 30 40 50 60 70 80 the syntactic head and dependent of the dependency relation are both segmented correctly . Following the standard setting in dependency parsing works , we evaluate the task of dependency parsing with the unlabeled attachment scores excluding punctuations . Statistical significance is tested by McNemar s test ( : p < 0.05 , : p < 0.01 ) . 4.2 Baseline and Proposed Models . We use the following baseline and proposed models for evaluation . SegTag : our reimplementation of the joint segmentation and POS tagging model by Zhang and Clark ( 2010 ) . We used the beam of 16 , which they reported to achieve the best accuracies . Dep : the state-of-the-art dependency parser by Huang and Sagae ( 2010 ) . We used our reimplementation , which is used in Hatori et al . Dep : Dep without look-ahead features . TagDep : the joint POS tagging and dependency parsing model ( Hatori et al. , 2011 ) , where the look-ahead features are omitted.5 SegTag+Dep/SegTag+Dep : a pipeline combination of SegTag and Dep or Dep . SegTag+TagDep : a pipeline combination of Seg- Tag and TagDep , where only the segmentation output of SegTag is used as input to TagDep ; the output tags of TagDep are used for evaluation . SegTagDep : the proposed full joint model . All of the models described above except Dep are based on the same feature sets for segmentation and 3 http : //www.keenage.com/html/e index.html 4 http : //zh.wikipedia.org/wiki 5 We used the original implementation used in Hatori et al . tagging ( Zhang and Clark , 2008 ; Zhang and Clark , 2010 ) and dependency parsing ( Huang and Sagae , 2010 ) . Therefore , we can investigate the contribution of the joint approach through comparison with the pipeline and joint models . We have some parameters to tune : parsing feature weight p , beam size , and training epoch . All these parameters are set based on experiments on CTB5c . For experiments on CTB5j , CTB6 , and CTB7 , the training epoch is set using the development set . In this experiment , the external dictionaries are not used , and the beam size of 32 is used . Interestingly , if we simply set p to 1 , the accuracies seem to converge at lower levels . The p = 0.2 setting seems to reach almost identical segmentation and tagging accuracies as the best setting p = 0.5 , but the convergence occurs more slowly . Based on this experiment , we set p to 0.5 throughout the experiments in this paper . Although even the beam size of 32 results in competitive accuracies for word segmentation and POS tagging , the dependency accuracy is affected most by the increase of the beam size . of SegTagDep on CTB5c1 w.r.t . iments in this paper , unless otherwise noted . 97 4.4 Main Results 96 . In this section , we present experimentally obtained 95results using the proposed and baseline models . Irrespective of the existence of the dictionary features , the joint model SegTagDep largely increases the POS tagging and dependency parsing accuracies ( by 0.56 0.63 % and 2.34 2.44 % ) ; the improvements in parsing accuracies are still 94 SegTag ( Seg ) . SegTagDep ( Seg ) SegTag ( Tag ) SegTag+TagDep ( Tag ) SegTagDep ( Tag ) 92 91 90 0.05 0.1 0.2 0.5 1 2 73 72 71 70 SegTag+Dep ( Dep ) SegTag+TagDep ( Dep ) SegTagDep ( Dep ) . 69 0.05 0.1 0.2 0.5 1 2 significant even compared with SegTag+Dep ( the pipeline model with the look-ahead features ) . However , when the external dictionaries are not used ( wo/dict ) , no substantial improvements for segmentation accuracies were observed . In contrast , when the dictionaries are used ( w/dict ) , the segmentation accuracies are now improved over the baseline model SegTag consistently ( on every trial ) . Although the overall improvement in segmentation is only around 0.1 % , more than 1 % improvement is observed if we specifically examine OOV6 words . The difference between wo/dict and w/dict results suggests that the syntactic dependencies might work as a noise when the segmentation model is insufficiently stable , but the model does improve when it is stable , not receiving negative effects from the syntactic dependencies . The partially joint model SegTag+TagDep is shown to perform reasonably well in dependency parsing : with dictionaries , it achieved the 2.02 % improvement over SegTag+Dep , which is only 0.32 % lower than SegTagDep . However , whereas Seg- Tag+TagDep showed no substantial improvement in tagging accuracies over SegTag ( when the dictionaries are used ) , SegTagDep achieved consistent improvements of 0.46 % and 0.58 % ( without/with dic 6 We define the OOV words as the words that have not seen in the training data , even when the external dictionaries are used . Each point corresponds to the beam size of 4 , 8 , 16 , 32 , ( 64 ) . The beam size of 16 is used for SegTag in SegTag+Dep and SegTag+TagDep . tionaries ) ; these differences can be attributed to the combination of the relieved error propagation and the incorporation of the syntactic dependencies . In addition , SegTag+TagDep has OOV tagging accuracies consistently lower than SegTag , suggesting that the syntactic dependency has a negative effect on the POS tagging accuracy of OOV words7 . In contrast , this negative effect is not observed for SegTagDep : both the overall tagging accuracy and the OOV accuracy are improved , demonstrating the effectiveness of the proposed model . Although SegTagDep takes a few times longer to achieve accuracies comparable to those of SegTag+Dep/TagDep , it seems to present potential 7 This is consistent with Hatori et al . ( 2011 ) s observation that although the joint POS tagging and dependency parsing improves the accuracy of syntactically influential POS tags , it has a slight side effect of increasing the confusion between general and proper nouns ( NN vs. NR ) . Model Segmentation POS Tagging Dependency wo /di ct S e g T a g + D e p S e g T a g + D e p S e g T a g + T a g D e p S e g T a g D e p 9 6 . 94 ( +0 .36 ) 74 . 60 ( +2 .02 ) 74 . 92 ( +2 .34 ) 91 . 86 ( +0 .12 ) 58 . 89 ( 0.9 3 ) 92 . 30 ( +0 .56 ) 61 . 03 ( +1 .21 ) 96 . 19 ( 0.0 3 ) 72.24 ( +0.00 ) w/ dic t S e g T a g + D e p S e g T a g + D e p S e g T a g + T a g D e p S e g T a g D e p 9 6 . 90 ( +0 .37 ) 75 . 45 ( +1 .92 ) 75 . 97 ( +2 .44 ) 92 . 35 ( +0 .01 ) 63 . 20 ( 2.2 4 ) 92 . 97 ( +0 .63 ) 67 . 40 ( +1 .96 ) 96 . 90 ( +0 .08 ) 79 . 38 ( +1 .06 ) Figures in parentheses show the differences over SegTag+Dep ( : p < 0.01 ) . for greater improvement , especially for tagging and Model CTB6 Test Se CTB7 Test Se parsing accuracies , when a larger beam can be used . 4.5 Comparison with Other Systems . Kruengkrai+ 09 is a lattice-based model by Kruengkrai et al . Zhang 10 is the incremental model by Zhang and Clark ( 2010 ) . These two systems use no external resources other than the CTB corpora . Sun+ 11 is a CRF-based model ( Sun , 2011 ) that uses a combination of several models , with a dictionary of idioms . Wang+ 11 is a semi-supervised model by Wang et al . ( 2011 ) , which additionally uses the Chinese Gigaword Corpus . Our models with dictionaries ( those marked with ( d ) ) have competitive accuracies to other state-of- the-art systems , and SegTagDep ( d ) achieved the best reported segmentation and POS tagging accuracies , using no additional corpora other than the dictionaries . Particularly , the POS tagging accuracy is more than 0.4 % higher than the previous best system thanks to the contribution of syntactic dependencies . These results also suggest that the use of readily available dictionaries can be more effective than semi-supervised approaches . In this paper , we proposed the first joint model for word segmentation , POS tagging , and dependency parsing in Chinese . The model demonstrated substantial improvements on the three tasks over the pipeline combination of the state-of-the-art joint segmentation and POS tagging model , and dependency parser . Particularly , results showed that the g Tag Dep g Tag Dep Kruengkrai 09 95.50 90.5095.40 89.86 Wang 11 95.79 91.1295.65 90.46SegTag+Dep 95.46 90.64 72.57 95.49 90.11 71.25 SegTagDep 95.45 91.27 74.88 95.42 90.62 73.58 ( diff . ) -0.01 +0.63 +2.31 -0.07 +0.51 +2.33 SegTag+Dep ( d ) 96.13 91.38 73.62 95.98 90.68 72.06 SegTagDep ( d ) 96.18 91.95 75.76 96.07 91.28 74.58 ( diff . ) +0.05 +0.57 +2.14 +0.09 +0.60 +2.52 For word segmentation , although the overall improvement was only around 0.1 % , greater than 1 % improvements was observed for OOV words . We conducted some comparison experiments of the partially joint and full joint models . Compared to SegTagDep , SegTag+TagDep performs reasonably well in terms of dependency parsing accuracy , whereas the POS tagging accuracies are more than 0.5 % lower . In future work , probabilistic pruning techniques such as the one based on a maximum entropy model are expected to improve the efficiency of the joint model further because the accuracies are apparently still improved if a larger beam can be used . More efficient decoding would also allow the use of the look-ahead features ( Hatori et al. , 2011 ) and richer parsing features ( Zhang and Nivre , 2011 ) . Acknowledgement We are grateful to the anonymous reviewers for their comments and suggestions , and to Xianchao Wu , Kun Yu , Pontus Stenetorp , and Shin- suke Mori for their helpful feedback . Arabic presents an interesting challenge to natural language processing , being a highly inflected and agglutinative language . In particular , this paper presents an in-depth investigation of the entity detection and recognition ( EDR ) task for Arabic . We start by highlighting why segmentation is a necessary prerequisite for EDR , continue by presenting a finite-state statistical segmenter , and then examine how the resulting segments can be better included into a mention detection system and an entity recognition system ; both systems are statistical , build around the maximum entropy principle . Experiments on a clearly stated partition of the ACE 2004 data show that stem-based features can significantly improve the performance of the EDT system by 2 absolute F-measure points . The system presented here had a competitive performance in the ACE 2004 evaluation . Information extraction is a crucial step toward understanding and processing language . One goal of information extraction tasks is to identify important conceptual information in a discourse . These tasks have applications in summarization , information retrieval ( one can get all hits for Washington/person and not the ones for Washington/state or Washington/city ) , data mining , question answering , language understanding , etc . In this paper we focus on the Entity Detection and Recognition task ( EDR ) for Arabic as described in ACE 2004 framework ( ACE , 2004 ) . The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the fo cus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC6 , MUC7 , and the CoNLL 02 and CoNLL 03 shared tasks . Usually , in computational linguistics literature , a named entity is an instance of a location , a person , or an organization , and the NER task consists of identifying each of these occurrences . Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( NIST , 2004 ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g . John Mayor ) , nominal ( the president ) or pronominal ( she , it ) . An entity is the aggregate of all the mentions ( of any level ) which refer to one conceptual entity . For instance , in the sentence President John Smith said he has no comments there are two mentions ( named and pronomial ) but only one entity , formed by the set { John Smith , he } . We separate the EDR task into two parts : a mention detection step , which identifies and classifies all the mentions in a text and a coreference resolution step , which combinines the detected mentions into groups that refer to the same object . In its entirety , the EDR task is arguably harder than traditional named entity recognition , because of the additional complexity involved in extracting non-named mentions ( nominal and pronominal ) and the requirement of grouping mentions into entities . This is particularly true for Arabic where nominals and pronouns are also attached to the word they modify . In fact , most Arabic words are morphologically derived from a list of base forms or stems , to which prefixes and suffixes can be attached to form Arabic surface forms ( blank-delimited words ) . In addition to the different forms of the Arabic word that result from the 63 Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages , pages 63 70 , Ann Arbor , June 2005 . Qc 2005 Association for Computational Linguistics derivational and inflectional process , most prepositions , conjunctions , pronouns , and possessive forms are attached to the Arabic surface word . It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( Xu et al. , 2001 ; Xu et al. , 2002 ) . Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ) . Both systems are built around from the maximum-entropy technique ( Berger et al. , 1996 ) . We formulate the mention detection task as a sequence classification problem . While this approach is language independent , it must be modified to accomodate the particulars of the Arabic language . The Arabic words may be composed of zero or more prefixes , followed by a stem and zero or more suffixes . We begin with a segmentation of the written text before starting the classification . The Arabic alphabet consists of 28 letters that can be extended to ninety by additional shapes , marks , and vowels ( Tayli and AlSalamah , 1990 ) . Unlike Latin-based alphabets , the orientation of writing in Arabic is from right to left . In written Arabic , short vowels are often omitted . Also , because variety in expression is appreciated as part of a good writing style , the synonyms are widespread . Arabic nouns encode information about gender , number , and grammatical cases . There are two genders ( masculine and feminine ) , three numbers ( singular , dual , and plural ) , and three grammatical cases ( nominative , genitive , and accusative ) . A noun has a nominative case when it is a subject , accusative case when it is the object of a verb , and genitive case when it is the object of a preposition . The form of an Arabic noun is consequently determined by its gender , number , and grammatical case . The definitive nouns are formed by attaching the Arabic article J to the immediate front of the This segmentation process consists of separating the nouns , such as in the word A s ' _ '- ( the company ) .normal whitespace delimited words into ( hypothe Also , prepositions such as ( by ) , and J ( to ) can be sized ) prefixes , stems , and suffixes , which become the attached as a prefix as in A s ' _ '-l ( to the company ) .subject of analysis ( tokens ) . The resulting granular ity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label A noun may carry a possessive pronoun as a suffix , such as in tt s ' _ ' '' ( their company ) . For the EDR task , in this previous example , the Arabic blank-delimited ( for instance , in the case of nominal and pronominal word tt s ' _ ' '' should be split into two tokens : A s ' _ ' '' and mentions ) . Additionally , because the prefixes and t ... The first token A s ' _ ' '' is a mention that refers tosuffixes are quite frequent , directly processing unseg mented words results in significant data sparseness . We present in Section 2 the relevant particularities of the Arabic language for natural language processing , especially for the EDR task . We then describe the segmentation system we employed for this task in Section 3 . Section 4 briefly describes our mention detection system , explaining the different feature types we use . We focus in particular on the stem n-gram , prefix n-gram , and suffix n-gram features that are an organization , whereas the second token t.. is also a mention , but one that may refer to a person . Also , the prepositions ( i.e. , and J ) not be considered a part of the mention . Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; Chen and Gey , 2002 ) . The formation of broken plurals is common , more complex and often irregular . As an example , the plural form of the noun J < _ ( man ) is J < _ ( men ) , which is formed by inserting the infix s ' ( book ) is __ s ' specific to a morphologically rich language such as . The plural form of the noun Arabic . We describe in Section 5 our coreferenceresolution system where we also describe the advan ( books ) , which is formed by deleting the infix . Theplural form and the sing , ular form may also be com tage of using stem based features . Section 6 shows pletely different ( e.g . _. , for woman , but - `` for and discusses the different experimental results and Section 7 concludes the paper . The sound plurals are formed by adding plural suffixes to singular nouns ( e.g. , __ ' > , meaning researcher ) : the plural suffix is for feminine nouns in grammatical cases ( e.g. , ' > , ) , 0 _ for masculine Extraction difficult ? nouns in the nominative case ( e.g. , 0 _ ' > , ) , and u , The Arabic language , which is the mother tongue of for masculine nouns in the genitive and accusative cases ( e.g. , u ' ' > , ) . The dual suffix is 0 for the nom more than 300 million people ( Center , 2000 ) , present inative case ' > ( e.g. , 0 , ) , and u , for the genitive orsignificant challenges to many natural language pro cessing applications . Arabic is a highly inflected and accusative ( e.g. , u ' ' > , ) . In Arabic morphology , most morphemes are comprised of a basic word form ( the root or stem ) , to which many affixes can be attached toBecause we consider pronouns and nominals as men tions , it is essential to segment Arabic words into these subword tokens . We also believe that the in formation denoted by these affixes can help with the coreference resolution task1 . Arabic verbs have perfect and imperfect tenses ( Abbou and McCarus , 1983 ) . Perfect tense denotes completed actions , while imperfect denotes ongoing actions . Arabic verbs in the perfect tense consist of a stem followed by a subject marker , denoted as a suf fix . The subject marker indicates the person , gender , known words based upon a character unigram model , although this model is dominated by an empirically chosen unknown word penalty . Using 0.5M words from the combined Arabic Treebanks 1V2 , 2V2 and 3V1 , the dictionary based segmenter achieves a exact word match 97.8 % correct segmentation . and number of the subject . As an example , the verb J , ( to meet ) has a perfect tense __l , for the thirdperson feminine singular , and _ , for the third per /epsilon a/epsilon a/A # epsilon/ # b/AB # b/epsilon b/B UNK/epsilon c/C epsilon/epsilon c/BC d/epsilon e/+E epsilon/+ e/+DE l son masculine plural . We notice also that a verb with a/epsilon b/A # B # b/epsilon c/epsilon d/BCD d/epsilon e/+D+E a subject marker and a pronoun suffix can be by itself a complete sentence , such us in the word tt l , : it has a third-person feminine singular subject-marker ( she ) and a pronoun suffix t.. ( them ) . It is also a complete sentence meaning she met them . The subject markers are often suffixes , but we may find a subject marker as a combination of a prefix and a suffix as in t+l , A. , ( she meets them ) . In this example , the EDR system should be able to separate t+l , A. , , to create two mentions ( and t.. ) . Because the two mentions belong to different entities , the EDR system should not chain them together . An Arabic word can potentially have a large number of variants , and some of the variants can be quite complex . As an example , consider the word t ' > _ ( and to her researchers ) which contains two prefixes and one suffix ( .. + u '' > , + J + _ ) . ( 2003 ) demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation . A trigram language model was used to score and select among hypothesized segmentations determined by a set of prefix and suffix expansion rules . In our latest implementation of this algorithm , we have recast this segmentation strategy as the composition of three distinct finite state machines . The second machine is a dictionary that accepts characters and produces identifiers corresponding to dictionary entries . The final machine is a trigram language model , specifically a KneserNey ( Chen and Goodman , 1998 ) based back- off language model . Differing from ( Lee et al. , 2003 ) , we have also introduced an explicit model for un 1 As an example , we do not chain mentions with different gender , number , etc . In addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation ( Sproat et al. , 1996 ) . For these models , both arabic characters and spaces , and the inserted prefix and suffix markers appear on the arcs of the finite state machine . Here , the language model is conditioned to insert prefix and suffix markers based upon the frequency of their appearance in n-gram character contexts that appear in the training data . The character based model alone achieves a 94.5 % exact match segmentation accuracy , considerably less accurate then the dictionary based model . However , an analysis of the errors indicated that the character based model is more effective at segmenting words that do not appear in the training data . We seeked to exploit this ability to generalize to improve the dictionary based model . As in ( Lee et al. , 2003 ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems . In our case , the character n-gram model is used to segment a portion of the Arabic Giga- word corpus . From this , we create a vocabulary of stems and affixes by requiring that tokens appear more than twice in the supervised training data or more than ten times in the unsupervised , segmented corpus . The resulting vocabulary , predominately of word stems , is 53K words , or about six times the vocabulary observed in the supervised training data . This represents about only 18 % of the total number of unique tokens observed in the aggregate training data . With the addition of the automatically acquired vocabulary , the segmentation accuracy achieves 98.1 % exact match . 3.2 Preprocessing of Arabic Treebank Data . Because the Arabic treebank and the gigaword corpora are based upon news data , we apply some small amount of regular expression based preprocessing . Arabic specific processing include removal ofthe characters tatweel ( - ) , and vowels . Also , the fol lowing characters are treated as an equivalence class during all lookups and processing : ( 1 ) ... , ... , and , _ . We define a token and introduce whites- . pace boundaries between every span of one or more alphabetic or numeric characters . Each punctuation symbol is considered a separate token . Character classes , such as punctuation , are defined according to the Unicode Standard ( Aliprand et al. , 2004 ) . The mention detection task we investigate identifies , for each mention , four pieces of information : 1. the mention type : person ( PER ) , organization ( ORG ) , location ( LOC ) , geopolitical entity ( GPE ) , facility ( FAC ) , vehicle ( VEH ) , and weapon ( WEA ) 2. the mention level ( named , nominal , pronominal , or premodifier ) 3. the mention class ( generic , specific , negatively quantified , etc . ) 4. the mention sub-type , which is a sub-category of the mention type ( ACE , 2004 ) ( e.g . OrgGovernmental , FacilityPath , etc . ) . We formulate the mention detection problem as a classification problem , which takes as input segmented Arabic text . We assign to each token in the text a label indicating whether it starts a specific mention , is inside a specific mention , or is outside any mentions . We use a maximum entropy Markov model ( MEMM ) classifier . The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty different second-stage classifiers to predict the sub- type , the mention level , and the mention class . After the first stage , when the boundary ( starting , inside , or outside a mention ) has been determined , the other classifiers can use this information to analyze a larger context , capturing the patterns around the entire mentions , rather than words . As an example , the token sequence that refers to a mention will become a single recognized unit and , consequently , lexical and syntactic features occuring inside or outside of the entire mention span can be used in prediction . In the first stage ( entity type detection and classification ) , Arabic blank-delimited words , after segmenting , become a series of tokens representing prefixes , stems , and suffixes ( cf . We allow any contiguous sequence of tokens can represent a mention . Thus , prefixes and suffixes can be , and often are , labeled with a different mention type than the stem of the word that contains them as constituents . We use a large set of features to improve the prediction of mentions . This set can be partitioned into 4 categories : lexical , syntactic , gazetteer-based , and those obtained by running other named-entity classifiers ( with different tag sets ) . We use features such as the shallow parsing information associated with the tokens in a window of 3 tokens , POS , etc . The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( Florian et al. , 2004 ) . We denote these features as backward token tri-grams and forward token tri-grams for the previous and next context of ti respectively . For a token ti , the backward token n-gram feature will contains the previous n 1 tokens in the history ( ti n+1 , . ti 1 ) and the forward token n-gram feature will contains the next n 1 tokens ( ti+1 , . Because we are segmenting arabic words into multiple tokens , there is some concern that tri- gram contexts will no longer convey as much contextual information . Consider the following sentence extracted from the development set : of the model ) ( Berger et al. , 1996 ) . One big advan _ l u ' '' __ l_A . J ' .. ( transla tage of this approach is that it can combine arbitrary and diverse types of information in making a classification decision . Our mention detection system predicts the four labels types associated with a mention through a cascade approach . It first predicts the boundary and the main entity type for each mention . Then , it uses the information regarding the type and boundary in tion This represents the location for Political Party Office ) . The Political Party Office is tagged as an organization and , as a word-for-word translation , is expressed as to the Office of the political to the party . It is clear in this example that the word _A.. ( location for ) contains crucial information in distinguishing between a location and an organization when tagging the token __ . After segmentation , the sentence becomes : where mk is one mention in entity e , and the basic + __ . + J + J + _A.. + J + J ' . model building block P ( L = 1|e , mk , m ) is an ex _ > + J + J + u ' '' + J ponential or maximum entropy model ( Berger et al. , 1996 ) . When predicting if the token __ . ( office ) is the For the start model , we use the following approxima beginning of an organization or not , backward and forward token n-gram features contain only J + J ( for the ) and u ' '' + J ( the political ) . This is most likely not enough context , and addressing the tion : PS ( S = 1|e1 , e2 , , et , m ) 1 max PL ( L = 1|ei , m ) ( 2 ) 1 i t problem by increasing the size of the n-gram context quickly leads to a data sparseness problem . We propose in this paper the stem n-gram features as additional features to the lexical set . If the current token ti is a stem , the backward stem n-gram feature contains the previous n 1 stems and the forward stem n-gram feature will contain the following n 1 stems . We proceed similarly for prefixes and suffixes : if ti is a prefix ( or suffix , respectively ) we take the previous and following prefixes ( or suffixes ) 2 . In the sentence shown above , when the system is predict ing if the token __ . ( office ) is the beginning of an organization or not , the backward and forward stem n-gram features contain _A.. J ' . ( represent location of ) and _ > u ' '' ( political office ) . The stem features contain enough information in this example to make a decision that __ . ( office ) is the beginning of an organization . In our experiments , n is 3 , therefore we use stem trigram features . Coreference resolution ( or entity recognition ) is defined as grouping together mentions referring to the same object or entity . For example , in the following text , ( I ) John believes Mary to be the best student three mentions John , Mary , student are underlined . Mary and student are in the same entity since both refer to the same person . The coreference system system is similar to the Bell tree algorithm as described by ( Luo et al. , 2004 ) . In our implementation , the link model between a candidate entity e and the current mention m is comThe start model ( cf . The maximum-entropy model provides us with a flexible framework to encode features into the the system . Our Arabic entity recognition system uses many language-indepedent features such as strict and partial string match , and distance features ( Luo et al. , 2004 ) . In this paper , however , we focus on the addition of Arabic stem-based features . 5.1 Arabic Stem Match Feature . Features using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( Luo et al. , 2004 ) . For Arabic , since words are morphologically derived from a list of roots ( stems ) , we expected that a feature based on the right and left stems would lead to improvement in system accuracy . Let m1 and m2 be two candidate mentions where a mention is a string of tokens ( prefixes , stems , and suffixes ) extracted from the segmented text . In order to make a decision in either linking the two mentions or not we use additional features such as : do the stems in m1 and m2 match , do stems in m1 match all stems in m2 , do stems in m1 partially match stems in m2 . We proceed similarly for prefixes and suffixes . Since prefixes and suffixes can belong to different mention types , we build a parse tree on the segmented text and we can explore features dealing with the gender and number of the token . In the following example , between parentheses we make a word-for-word translations in order to better explain our stemming feature . Let us puted astake the two mentions _ l u ' '' __ l PL ( L = 1|e , m ) max P ( L = 1|e , mk , m ) , ( 1 ) ( to-the-office the-politic to-the-party ) and mk e u ' _ '' '' __ . ( office the party s ) segmented as 2 Thus , the difference to token n-grams is that the tokens of different type are removed from the streams , be _ > + J + J + u ' '' + J + __ . + J + J fore the features are created . and ... + _ > + J + __ . In our development corpus , these two mentions are chained to the same entity . The stemming match feature in this case will contain information such us all stems of m2 match , which is a strong indicator that these mentions should be chained together . Features based on the words alone would not help this specific example , because the two strings m1 and m2 do not match . The system is trained on the Arabic ACE 2003 and part of the 2004 data . We introduce here a clearly defined and replicable split of the ACE 2004 data , so that future investigations can accurately and correctly compare against the results presented here . There are 689 Arabic documents in LDC s 2004 release ( version 1.4 ) of ACE data from three sources : the Arabic Treebank , a subset of the broadcast ( bnews ) and newswire ( nwire ) TDT4 documents . The 178-document devtest is created by taking the last ( in chronological order ) 25 % of documents in each of three sources : 38 Arabic tree- bank documents dating from 20000715 ( i.e. , July 15 , 2000 ) to 20000815 , 76 bnews documents from 20001205.1100.0489 ( i.e. , Dec. 05 of 2000 from 11:00pm to 04:89am ) to 20001230.1100.1216 , and 64 nwire documents from 20001206.1000.0050 to 20001230.0700.0061 . The time span of the test set is intentionally non-overlapping with that of the training set within each data source , as this models how the system will perform in the real world . We want to investigate the usefulness of stem n- gram features in the mention detection system . As stated before , the experiments are run in the ACE 04 framework ( NIST , 2004 ) where the system will identify mentions and will label them ( cf . Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) . Detecting the mention boundaries ( set of consecutive tokens ) and their main type is one of the important steps of our mention detection system . The score that the ACE community uses ( ACE value ) attributes a higher importance ( outlined by its weight ) to the main type compared to other sub- tasks , such as the mention level and the class . Hence , to build our mention detection system we spent a lot of effort in improving the first step : detecting the mention boundary and their main type . In this paper , we report the results in terms of precision , recall , and F-measure3 . Lexical features Pr ec isi on ( % ) Re cal l ( % ) F m ea su re ( % ) To tal 7 3 . 7 FA C G P E L O C O R G P E R V E H W E A 7 6 . 2 Lexical features + Stem Pr ec isi on ( % ) Re cal l ( % ) F m ea su re ( % ) To tal 7 3 . 8 F A C G P E L O C O R G P E R V E H W E A 7 2 . 2 To assess the impact of stemming n-gram features on the system under different conditions , we consider two cases : one where the system only has access to lexical features ( the tokens and direct derivatives including standard n-gram features ) , and one where the system has access to a richer set of information , including lexical features , POS tags , text chunks , parse tree , and gazetteer information . The former framework has the advantage of being fast ( making it more appropriate for deployment in commercial systems ) . The number of parameters to optimize in the MaxEnt framework we use when only lexical features are explored is around 280K parameters . This number increases to 443K approximately when all information is used except the stemming feature . The number of parameters introduced by the use of stemming is around 130K parameters . It is important to notice the stemming n-gram features improved the performance of each category of the main type . In the second case , the systems have access to a large amount of feature types , including lexical , syntactic , gazetteer , and those obtained by running other 3 The ACE value is an important factor for us , but its relative complexity , due to different weights associated with the subparts , makes for a hard comparison , while the F-measure is relatively easy to interpret . interesting improvement in terms of ACE value to the hole EDR system as showed in section 6.3 . Pr ec isi on ( % ) Re cal l ( % ) F m ea su re ( % ) All Fe atu res All Fe atu res +S te m 6 4 . 7 Le xic al Le xic al+ St em 6 4 . 6 Features are also extracted from the shallow parsing information associated with the tokens in window of 3 , POS , etc . The All-features system incorporates all the features except for the stem n- grams . This is true for all types . It is interesting to note that the increase in performance in both cases ( Tables 1 and 2 ) is obtained from increased recall , with little change in precision . When the prefix and suffix n-gram features are removed from the feature set , we notice in both cases ( Tables 1 and 2 ) a insignificant decrease of the overall performance , which is expected : what should a feature of preceeding ( or following ) prepositions or finite articles captures ? As stated in Section 4.1 , the mention detection system uses a cascade approach . However , we were curious to see if the gain we obtained at the first level was successfully transfered into the overall performance of the mention detection system . Despite the fact that the improvement was small in terms of F-measure ( 59.4 vs. 59.7 ) , the stemming n-gram features gave 4 The difference in performance is not statistically significant 6.3 Coreference Resolution . In this section , we present the coreference results on the devtest defined earlier . First , to see the effect of stem matching features , we compare two coreference systems : one with the stem features , the other without . We test the two systems on both true and system mentions of the devtest set . True mentions mean that input to the coreference system are mentions marked by human , while system mentions are output from the mention detection system . We report results with two metrics : ECM-F and ACE- Value . ECM-F is an entity-constrained mention F- measure ( cf . ( Luo et al. , 2004 ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric . On true mention , the stem matching features improve ECM-F from 77.7 % to 80.0 % , and ACE-value from 86.9 % to 88.2 % . The similar improvement is also observed on system mentions.The overall ECM- F improves from 62.3 % to 64.2 % and the ACE value improves from 61.9 to 63.1 % . Note that the increase on the ACE value is smaller than ECM-F . This is because ACE-value is a weighted metric which emphasizes on NAME mentions and heavily discounts PRONOUN mentions . Overall the stem features give rise to consistent gain to the coreference system . In this paper , we present a fully fledged Entity Detection and Tracking system for Arabic . At its base , the system fundamentally depends on a finite state segmenter and makes good use of the relationships that occur between word stems , by introducing features which take into account the type of each segment . In mention detection , the features are represented as stem n-grams , while in coreference resolution they are captured through stem-tailored match features . B a s e B a s e + S t e m ECM F ACEVal ECM F ACEVal Tr ut h Sy ste m 7 7 . 2 63.1 The row marked with Truth represents the results with true mentions while the row marked with System represents that mentions are detected by the system . Numbers under ECM- F are Entity-Constrained-Mention F-measure and numbers under ACE-Val are ACE-values . These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the ACE 2004 Arabic data . The experiments are performed on a clearly specified partition of the data , so comparisons against the presented work can be correctly and accurately made in the future . In addition , we also report results on the official test data . The presented system has obtained competitive results in the ACE 2004 evaluation , being ranked amongst the top competitors . This work was partially supported by the Defense Advanced Research Pro jects Agency and monitored by SPAWAR under contract No . The views and findings contained in this material are those of the authors and do not necessarily reflect the position of policy of the U.S. government and no official endorsement should be inferred . We present a Parameterized Action Representation ( PAR ) that provides a conceptual representation ofdifferent types of actions used to animate virtual human agents in a simulated 3D environment . These actions involve changes of state , changes of location ( kinematic ) and exertion of force ( dynamic ) . PARSare hierarchical , parameterized structures that facilitate both visual and verbal expressions . In order to support the animation of the actions , PARShave to make explicit many details that are often underspecified in the language . This detailed level ofrepresentation also provides a suitable pivot representation for generation in other natural languages , i.e. , a form of interlingua . We show examples of how certain divergences in machine translation can be solved by our approach focusing specifically on how verb-framed and satellite-framed languages can use our representation . In this paper , we describe a Parameterized Ac tion Representation ( PAR ) ( Badler et al. , 1999 ) that provides a conceptual representation of differ ent types of actions used to animate virtual humanagents in a simulated 3D environment . These ac tions involve changes of state , changes of location ( kinematic ) and exertion of force ( dynamic ) . PARSare hierarchical , parameterized structures that fa cilitate both visual and verbal expressions ( Badler et al. , 2000 ) . In order to support the animation ofthe actions , PARS have to make explicit many de tails that are often underspecified in the language . This detailed level of representation is well suitedfor an interlingua for machine translation applications , since the animations of actions and therefore the PARS that control them will be equivalent for the same actions described in different lan guages . These representations can be incorporated into a system which uses PAR-based animations asa workbench for creating accurate conceptual representations , which can map to seeral different lan guages as well as produce faithful animations . The verb classes we are currently considering in this light involve explicit physical actions such asthose expressed in the motion verb class and contact verb class ( Levin , 1993 ) . Since we are employ ing PAR as an interlingual representation , we willshow examples of how it can handle certain diver gences in machine translation , focusing specifically on how verb-framed and satellite-framed languages ( Talmy , 1991 ) can yield equivalent actions in this representation . We use parameterized action representations to ani mate the actions of virtual human agents . The PAR for an action includes the action & apos ; s participants ( its agent and objects ) , & apos ; as well as kinematic properties such as its path , manner and duration , and dynamic properties , such as its speed and force ( see Fig . 1 ) .The representation also allows for traditional statespace properties of actions , such as applicability conditions and preparatory actions that have to be satisfied before the action can be executed , and termina tion conditions and post assertions which determine when an action is concluded and what changes it makes to the environment state . We created a hierarchy of actions , exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs , such as motion verbs or verbs of contact , to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share ( Dang et al. , 1998 ) . The highest nodes in the hierarchy areoccupied by generalized PAR schemas which represent the basic predicate-argument structures for en tire groups of subordinate actions . The lower nodes are occupied by progressively more specific schemas that inherit information from the generalized PARS , and can be instantiated with arguments from natu ral language to represent a specific action such asJohn hit the ball with his bat . The example in is a generalized PAR schema for contact ac & apos ; Objects and agents are stored in a hierarchy and have a number of properties associated with them . Properties of the objects may include their location and status . Agents have capabilities , such as the ability to walk or swim , and properties such as their strength and height . 12 contact/ ( par : contact ) hit/ ( manner : forcefully ) touch/ ( manner : gently ) kick/ ( OBJ2 : foot ) hammer/ ( OBJ2 : hammer ) : A lexical/semantic hierarchy for actions of contact CONTACT PAR activity : [ ACTION ] participants : agent : AGENT objects : OBJ1 , OBJ2 applic cond : reachable ( OBJ1 ) have ( AGENT , OBJ2 ) preparatory spec : [ get ( AGENT , OBJ2 ) ] termination cond : [ contact ( OBJ1 , OBJ2 ) ] post assertions : [ contact ( OBJ1 , OBJ2 ) ] path , duration , motion , force manner : [ MANNER ] : A PAR schema for actions of contact tions between two objects . This schema specifiesthat the ` contact & apos ; action has an agent and two ob jects , and that the action is concluded when the twoobjects come together.2 The preparatory specification of getting the second object is tested and car ried out if the object is not possessed . In order to describe a specific action , say hammer , we wouldcombine all of its ancestor representations in the ac tion hierarchy , as shown in , and add the information specific to that action . Since hammer inherits from the PAR for hit , and ultimately from the PAR for contact , its representation would usethe generalized ` contact & apos ; PAR , with a forceful man ner , and a hammer as the instrument . The action hit does not specify any instrument , but inherits the forceful manner and generalized contact PAR from its ancestors , and the action contact leaves both the 2In this example , the second object is the instrument with which the action is performed.instrument and the manner unspecified , and is asso ciated only with the generalized contact PAR.The PAR is intended to provide slots for information that is typically conveyed in modifiers or ad juncts in addition to internal verb arguments . Assuch , it is often the case that several different syn tactic realizations can all map to the same PAR schema . For example , John hit the ball , John hit the ball with a bat and John swung mightily and his bat hit the ball with a resounding crack would all map to the same schema.3 The main components of our animation system are : a natural language interface , a planner and a graphi cal animation ( see ) . The PARS are used as intermediate representations of the actions between components.An instruction in natural language starts the process . We use a Synchronous Tree Adjoining Gram mar ( Shieber and Schabes , 1990 ; Shieber , 1994 ) forparsing natural language instructions into deriva tions containing predicate-argument dependencies ( Schuler , 1999 ) . The synchronous parser extractsthese predicate-argument structures by first asso ciating each word in an input sentence with one or more elementary trees , which are combined intoa single derivation tree for the entire input sentence using the constrained operations of substitu tion and adjunction in the Tree Adjoining Grammar formalism ( Joshi , 1985 ; Joshi , 1987 ) . As the parser assembles these elementary tree predicates into apredicate-argument structure , it simultaneously se lects and assembles the corresponding schemas . It fills in the participants and modifiers , and outputs the PAR schema for the instruction . These schemas may be underspecified for actions such as ` enter & apos ; or ` put & apos ; and thus not provide enough information for the animation to be produced directly.3The relationship between PARS and alternations may be come much more complicated when we consider other verb classes such as change of state verbs . 13 Natural Language General architecture of the animation system The planner uses information from the general schema , such as preconditions and post-assertions , as well as information derived from the agents & apos ; ca pabilities and the objects properties to fill in these gaps in several ways : to select the way ( activity ) in which the instruction is performed ( enter by walking , by swim ming , etc . ) ; to determine the prepartory actions that must be completed before the instruction is carried out , ( for example , in order for an agent to open the door , the door has to be reachable and that may involve a locomotion process ) ; to decompose the action into smaller units ( put the glass on the table , involves getting the glass , planning a route to the table , etc . ) The output of the planner for the input instructionis a complete description of the actions involved , including participants , preparatory specifications , termination conditions , manner , duration , etc . Partic ipants bring with them a list of inherent propertiesof the agent ( e.g . agent capabilities ) or physical objects ( e.g. , object configurations ) and other charac teristics , such as & apos ; how to open & apos ; for an object such as a door . This complete description refers to a setof animation PARS which can be immediately ani mated . In this way , a PAR schema for the action enter may actually translate into an animation PAR forwalking into a certain area . One way to differenti ate between action PAR schemas and instantiated animation PARs is to consider what it is possible to motion capture & apos ; ( by attaching sensors to a moving human figure ) . For example , the enter action and the put action are quite general and underspecifiedand could not be motion captured . However , char acteristic activities such as walking and swimming could be . For further details about the animation PARS and the animation system see ( Badler et al , 1999 ) and ( Bindiganavale at al. , 2000 ) . The PAR representation for an action can be seen as a general template . PAR schemas include , as part of the basic sub-categorization frame , properties of 4There are several other ways to generate motions , forexample , through inverse kinematics , dynamics and key framing . the action that can occur linguistically either as the main verb or as adjuncts to the main verb phrase . This captures problems of divergences , such as theones described by Talmy ( Talmy , 1991 ) , for verb framed versus satellite-framed languages . New information may come from a sentence in natural language that modifies the action & apos ; s inherent properties , such as in John hit the ball slowly , where & apos ; slowly & apos ; is not part of the initial representation of the action & apos ; hit & apos ; . This new information is added to the PAR schema . Verb- versus Satellite-framed languages Verb-Framed Languages ( VFL ) map the motion ( path or path + ground location ) onto the verb , and the manner either onto a satellite or an ad junct , while Satellite-Framed Languages ( SFL ) map the motion into the satellite , and the manner onto the main verb.English and other Germanic languages are consid ered satellite-framed languages , expressing the pathin the satellite ; Spanish , among other Romance lan guages , is a verb-framed language and expresses the path in the main verb . The pairs of sentences ( 1 ) and ( 2 ) from Talmy ( 1991 ) show examples of thesedivergences . In ( I ) , in English , the exit of the bot tle is expressed by the preposition out , in Spanish the same concept is incorporated in the main verb salir ( to exit ) . In ( 2 ) , the concept of blowing out the candle is represented differently in English and Spanish . ( 1 ) The bottle floated out La botella solid flotando ( the bottle exited floating ) ( 2 ) I blew out the candle Apague la vela sopldndola ( I extinguish the candle blowing ) 4.1 Motion . In order to capture generalizations about motion actions , we have a generalized PAR schema for mo tion , and our hierarchy includes different types of motion actions such as inherently directed motion and manner of motion actions that inherit from the more general schema , as shown in . Directed motion actions , such as enter and exit , don & apos ; t bring with them the manner by which the action is carried out but they have a inherent termination condition.For example , & apos ; enter a room & apos ; may be done by walking , crawling or flying depending on the agents & apos ; ca 14 motion/ ( par : motion ) directed_motion manner_motion enterAterm : in ( 03J ) ) exit/ ( term : out ( OM ) crawl/ ( act : craul ) float/ ( act : float ) : PAR schema hierarchy for motion actions pabilities , but it should end when the agent is in the room . In contrast , manner of motion verbs expressthe action explicitly and don & apos ; t have an intrinsic ter mination condition . Motion is a type of framing event where the path is in the main verb for VFLs and in the satellite for SFLs . In ( 3 ) , we see the English sentence expressing the & apos ; enter & apos ; idea in the preposition into whereas the Spanish sentence expresses it in the main verb entrar ( to enter ) . ( 3 ) The bottle floated into the cave La botella entrci flotando a la cueva ( the bottle entered floating the cave ) The PAR schemas don & apos ; t distinguish the representation for these sentences , because there is a sin gle schema which includes both the manner and thepath without specifying how they are reabized lin guistically . Mappings from the lexical items to the schemas or to constraints in the schemas can be seenin .5 Independent of which is the source language , the PAR schema selected is motion , the activity field , which determines how the action is per formed ( in this case , by floating ) , is filled by float ( the main verb in English , or the adjunct in Span ish ) . The termination condition , which says that action ends when the agent is in the object , is added from the preposition in English and is part of the semantics of the main verb to enter in Spanish . EN float/rparmotion , activity : float ] into/ [ term : in ( AG , OBJ ) ] SP entrarAparmotion , terrn : in ( AG,0111 ) ] flotarga.ctivity : floati : Entries for the example sentences in ( 3 ) Because all of the necessary elements for a trans lation are specified in this representation , it is up 8A lexical item may have several mappings to reflect its semantics . For instance , float in English can be used also in the non-motion sense , in which case there will be two entries to capture that distinction . MOTION PAR - activity : float [ agent : bottle I object : cave _ termination_cond in ( bottle , cave ) : A ( simplified ) PAR schema for the sen tences in ( 3 ) to the language specific component to transform itinto a surface structure that satisfies the grammati cal principles of the destination language . Comparison with other work Our approach now diverges considerably from the approach outlined in Palmer et al . ( 1998 ) which discusses the use of Feature-Based Tree Adjoining Grammars , ( Joshi , 1985 ; VijayShanker and Joshi,1991 ) to capture generalizations about manner-ofmotion verbs . They do not propose an interlin gua but use a transfer-based mechanism expressedin Synchronous Tree Adjoining Grammars to cap ture divergences of VFL and SF % through the useof semantic features and links between the grammars . The problem of whether or not a preposi tional . phrase constitutes an argument to a verb or an adjunct ( described by Palmer et al . ) does not constitute a problem in our representation , since all the information is recovered in the same template for the action to be animated . The PAR approach is much more similar to the Lexical Conceptual Structures ( LOS ) approach , ( Jacicendoff , 1972 ; Jackendoff , 1990 ) , used as an in terlingua representation ( Dorr , 1993 ) . Based on the assumption that motion and manner of motion are conflated in a matrix verb like swim , the use of LCSallows separation of the concepts of motion , direc tion , and manner of motion in the sentence John swam across the lake . Each one of these concepts is participants : 15represented separately in the interlingua represen tation , as GO , PATH and MANNER , respectively . Our approach allows for a similar representation and the end result is the same , namely that the event ofswimming across the lake is characterized by sepa rate semantic components , which can be expressedby the main schema and by the activity field . In ad dition , our representation also incorporates details about the action such as applicability conditions , preparatory specifications , termination conditions , and adverbial modifiers . It is not clear to us how the LCS approach could be used to effect the same commonality of representation . The importance of the additional information such as the termination conditions can be more clearly illustrated with a different set of examples . Another class of actions that presents interesting divergences involves instruments where the instrument is used as the main verb or as an adjunct depending on thelanguage . The sentence pair in ( 4 ) shows this divergence for English and Portuguese . Because Por tuguese does not have a verb for to spoon , it uses a more general verb colocar ( to put ) as the main verb and expresses the instrument in a prepositional phrase . Unlike directed motion actions , a put withhand-held instrument action ( e.g. , spoon , scoop , la dle , etc . ) leaves the activity field unspecified in both languages . The specific action is generated by taking the instrument into account . A simplified schema is shown in . ( 4 ) Mary spoons chocolate over the ice cream Mary coloca chocolate sabre a sorvete corn a collier ( Mary puts chocolate over the ice cream with a spoon ) PUT3 PAR activity : participants : agent : Mary objects : chocolate , icecream , spoon preparatory_spec : get ( Mary , spoon ) termination_cond over ( chocolate , icecream ) : Representation of the sentences in ( 4 ) Notice that the only connection between to spoonand its Portuguese translation would be the termination condition where the object of the verb , choco late , has a new location which is over the ice cream . We have discussed a parameterized representation of actions grounded by the needs of animation of instructions in a simulated environment . In order to support the animation of these instructions , our representation makes explicit many details that are often underspecified in the language , such as start and end states and changes in the environment that happen as a result of the action . Sometimes the start and end state information provides critical information for accurate translation but it is not always necessary . Machine translationcan often simply preserve ambiguities in the transla tion without resolving them . In our application we can not afford this luxury . An interesting question to pursue for future work will be whether or not we can determine which PAR slots are not needed for machine translation purposes . Generalizations based on action classes provide the basis for an interlingua approach that captures the semantics of actions without committing to any language-dependent specification . This framework offers a strong foundation for handling the range of phenomena presented by the machine translation task.The structure of our PAR schenaas incorpo rate into a single template the kind of divergencepresented in verb-framed and satellite-framed lan guages . Although not shown in this paper , thisrepresentation can also capture idioms and non compositional constructions since the animations of actions and therefore the PARS that control them must be equivalent for the same actions described in different languages . Currently , we are also investigating the possibilityof building these action representations from a class based verb lexicon which has explicit syntactic and semantic information ( Kipper et at , , 2000 ) . The authors would like to thank the Actionarygroup , Hoa Trang Dang , and the anonymous reviewers for their valuable comments . This work was pax tially supported by NSF Grant 9900297 . At present , adapting an Information Extraction system to new topics is an expensive and slow process , requiring some knowledge engineering for each new topic . We propose a new paradigm of Information Extraction which operates 'on demand ' in response to a user 's query . On-demand Information Extraction ( ODIE ) aims to completely eliminate the customization effort . Given a user s query , the system will automatically create patterns to extract salient relations in the text of the topic , and build tables from the extracted information using paraphrase discovery technology . It relies on recent advances in pattern discovery , paraphrase discovery , and extended named entity tagging . We report on experimental results in which the system created useful tables for many topics , demonstrating the feasibility of this approach . Most of the world s information is recorded , passed down , and transmitted between people in text form . Implicit in most types of text are regularities of information structure - events which are reported many times , about different individuals , in different forms , such as layoffs or mergers and acquisitions in news articles . The goal of information extraction ( IE ) is to extract such information : to make these regular structures explicit , in forms such as tabular databases . Once the information structures are explicit , they can be processed in many ways : to mine information , to search for specific information , to generate graphical displays and other summaries . However , at present , a great deal of knowledge for automatic Information Extraction must be coded by hand to move a system to a new topic . For example , at the later MUC evaluations , system developers spent one month for the knowledge engineering to customize the system to the given test topic . Research over the last decade has shown how some of this knowledge can be obtained from annotated corpora , but this still requires a large amount of annotation in preparation for a new task . Improving portability - being able to adapt to a new topic with minimal effort is necessary to make Information Extraction technology useful for real users and , we be lieve , lead to a breakthrough for the application of the technology . We propose On-demand information extraction ( ODIE ) : a system which automatically identifies the most salient structures and extracts the information on the topic the user demands . This new IE paradigm becomes feasible due to recent developments in machine learning for NLP , in particular unsupervised learning methods , and it is created on top of a range of basic language analysis tools , including POS taggers , dependency analyzers , and extended Named Entity taggers . The basic functionality of the system is the following . The user types a query / topic description in keywords ( for example , merge or merger ) . Then tables will be created automatically in several minutes , rather than in a month of human labor . These tables are expected to show information about the salient relations for the topic . There are six major compo nents in the system . We will briefly describe each component and how the data is processed ; then , in the next section , four important components will be described in more detail . 731 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions , pages 731 738 , Sydney , July 2006 . Qc 2006 Association for Computational Linguistics Description of task ( query ) 1 ) Relevant documents IR system 6 ) Extended NE tagger 5 ) Language Analyzer Patterns 2 ) Pattern discovery 3 ) Paraphrase discovery Pattern sets 4 ) Table construction P a r a p h r a s e Know ledge base Table System overview 1 ) IR system : Based on the query given by the user , it retrieves relevant documents from the document database . We used a simple TF/IDF IR system we developed . 2 ) Pattern discovery : First , the texts in the retrieved documents are analyzed using a POS tagger , a dependency analyzer and an Extended NE ( Named Entity ) tagger , which will be described later . Then this component extracts sub-trees of dependency trees which are relatively frequent in the retrieved documents compared to the entire corpus . It counts the frequencies in the retrieved texts of all sub- trees with more than a certain number of nodes and uses TF/IDF methods to score them . The top-ranking sub-trees which contain NEs will be called patterns , which are expected to indicate salient relationships of the topic and will be used in the later components . 3 ) Paraphrase discovery : In order to find semantic relationships between patterns , i.e . to find patterns which should be used to build the same table , we use paraphrase discovery techniques . The paraphrase discovery was conducted off- line and created a paraphrase knowledge base . 4 ) Table construction : In this component , the patterns created in ( 2 ) are linked based on the paraphrase knowledge base created by ( 3 ) , producing sets of patterns which are semantically equivalent . Once the sets of patterns are created , these patterns are applied to the documents retrieved by the IR system ( 1 ) . The matched patterns pull out the entity instances and these entities are aligned to build the final tables . 5 ) Language analyzers : We use a POS tagger and a dependency analyzer to analyze the text . The analyzed texts are used in pattern discovery and paraphrase discovery . 6 ) Extended NE tagger : Most of the participants in events are likely to be Named Entities . However , the traditional NE categories are not sufficient to cover most participants of various events . For example , the standard MUC s 7 NE categories ( i.e . person , location , organization , percent , money , time and date ) miss product names ( e.g . Windows XP , Boeing 747 ) , event names ( Olympics , World War II ) , nu merical expressions other than monetary expressions , etc . We used the Extended NE categories with 140 categories and a tagger based on the categories . In this section , four important components will be described in detail . Prior work related to each component is explained and the techniques used in our system are presented . The pattern discovery component is responsible for discovering salient patterns for the topic . The patterns will be extracted from the documents relevant to the topic which are gathered by an IR system . Several unsupervised pattern discovery techniques have been proposed , e.g . ( Riloff 96 ) , ( Agichtein and Gravano 00 ) and ( Yangarber et al . Most recently we ( Sudo et al . 03 ) proposed a method which is triggered by a user query to discover important patterns fully automatically . In this work , three different representation models for IE patterns were compared , and the sub-tree model was found more effective compared to the predicate-argument model and the chain model . In the sub-tree model , any connected part of a dependency tree for a sentence can be considered as a pattern . As it counts all possible sub-trees from all sentences in the retrieved documents , the computation is very expensive . This problem was solved by requiring that the sub-trees contain a predicate ( verb ) and restricting the number of nodes . It was implemented using the sub-tree counting algorithm proposed by ( Abe et al . The patterns are scored based on the relative frequency of the pattern in the retrieved documents ( fr ) and in the entire corpus ( fall ) . The formula uses the TF/IDF idea ( Formula 1 ) . The system ignores very frequent patterns , as those patterns are so common that they are not likely to be important to any particular topic , and also very rare patterns , as most of those patterns are noise . ( COM means company and MNY means money ) < COM1 > < agree to buy > < COM2 > < for MNY > < COM1 > < will acquire > < COM2 > < for MNY > < a MNY merger > < of COM1 > < and COM2 > Pattern examples 3.2 Paraphrase Discovery . The role of the paraphrase discovery component is to link the patterns which mean the same thing for the task . Recently there has been a growing amount of research on automatic paraphrase discovery . For example , ( Barzilay 01 ) proposed a method to extract paraphrases from parallel translations derived from one original document . We proposed to find paraphrases from multiple newspapers reporting the same event , using shared Named Entities to align the phrases ( Shinyama et al . We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus ( Sekine 05 ) . The phrases connecting two NEs are grouped based on two types of evidence . One is the identity of the NE instance pairs , as multiple instances of the same NE pair ( e.g . and Overture ) are likely to refer to the same relationship ( e.g . The other type of evidence is the keywords in the phrase . If we gather a lot of phrases connecting NE 's of the same two NE types ( e.g . company and company ) , we can cluster score ( t : subtree ) = f r ( t ) log ( f all ( t ) + c ) ( 1 ) these phrases and find some typical expressions ( e.g . merge , acquisition , buy ) . The phrases are clustered based on these two types of evidence The scoring function sorts all patterns which contain at least one extended NE and the top 100 patterns are selected for later processing . Chunks are shown in brackets and extended NEs are shown in upper and sets of paraphrases are created . Basically , we used the paraphrases found by the approach mentioned above . Note that there is an alternative method of paraphrase discovery , using a hand crafted synonym dictionary like WordNet ( WordNet Home page ) . However , we found that the coverage of WordNet for a particular topic is not sufficient . Furthermore , even if these words are found as synonyms , there is the additional task of linking expressions . For example , if one of the expressions is reject the merger , it shouldn t be a paraphrase of acquire . Named Entities ( NE ) were first introduced by the MUC evaluations ( Grishman and Sundheim 96 ) . As the MUCs concentrated on business and mili set using discovered paraphrase knowledge . Once the pattern sets are built , a table is created for each pattern set . We gather all NE instances matched by one of the patterns in the set . These instances are put in the same column of the table for the pattern set . When creating tables , we impose some restrictions in order to reduce the number of meaningless tables and to gather the same relations in one table . We require columns to have at least three filled instances and delete tables with fewer than three rows . These thresholds are empirically determined using training data . Newspaper Pattern Set ANrtiecwle1s Paper * COM1 agree to buy tary topics , the important entity types were limited to a few classes of names and numerical expressions . However , along with the development of Information Extraction and Question Answering technologies , people realized that there should be more and finer categories for NE . We proposed one of those extended NE sets ( Sekine 02 ) . It includes 140 hierarchical categories . For example , ABC agreed to buy CDE for $ 1M . Article 2 a $ 20M merger of FGH and IJK Constructed table COM2 for MNY * COM1 will acquire COM2 for MNY * a MNY merger of COM1 and COM2 the categories include Company , Company group , Military , Government , Political party , and International Organization as subcategories of Organization . Also , new categories are introduced such as Vehicle , Food , Award , Religion , Language , Offense , Art and so on as subcategories of Product , as well as Event , Natural Object , Vocation , Unit , Weight , Temperature , Number of people and so on . We used a rule-based tagger developed to tag the 140 categories for this experiment . Note that , in the proposed method , the slots of the final table will be filled in only with instances of these extended Named Entities . Most common nouns , verbs or sentences can t be entries in the table . This is obviously a limitation of the proposed method ; however , as the categories are designed to provide good coverage for a factoid type QA system , most interesting types of entities are covered by the categories . Basically the table construction is done by applying the discovered patterns to the original corpus . The discovered patterns are grouped into pattern Article Company Money 1 ABC , CDE $ 1M . 2 FGH , IJK $ 20M . Table Construction 4.1 Data and Processing . We conducted the experiments using the 1995 New York Times as the corpus . The queries used for system development and threshold tuning were created by the authors , while queries based on the set of event types in the ACE extraction evaluations were used for testing . A total of 31 test queries were used ; we discarded several queries which were ambiguous or uncertain . The test queries were derived from the example sentences for each event type in the ACE guidelines . Examples of queries are shown in the Appendix . At the moment , the whole process takes about 15 minutes on average for each query on a Pentium 2.80GHz processor running Linux . The corpus was analyzed in advance by a POS tagger , NE tagger and dependency analyzer . The processing and counting of sub-trees takes the majority ( more than 90 % ) of the time . We believe we can easily make it faster by programming techniques , for example , using distributed computing . Out of 31 queries , the system is unable to build any tables for 11 queries . The major reason is that the IR component can t find enough newspaper articles on the topic . It retrieved only a few articles for topics like born , divorce or injure from The New York Times . For the moment , we will focus on the 20 queries for which tables were built . The Appendix shows some examples of queries and the generated tables . In total , 127 tables are created for the 20 topics , with one to thirteen tables for each topic . The number of columns in a table ranges from 2 to 10 , including the document ID column , and the average number of columns is 3.0 . The number of rows in a table range from 3 to 125 , and the average number of rows is 16.9 . The created tables are usually not fully filled ; the average rate is 20.0 % . In order to measure the potential and the usefulness of the proposed method , we evaluate the result based on three measures : usefulness , argument role coverage , and correctness . For the usefulness evaluation , we manually reviewed the tables to determine whether a useful table is included or not . This is inevitably subjective , as the user does not specify in advance what table rows and columns are expected . We asked a subject to judge usefulness in three grades ; A ) very useful for the query , many people might want to use this table for the further investigation of the topic , B ) useful at least , for some purpose , some people might want to use this table for further investigation and C ) not useful no one will be interested in using this table for further investigation . The argument role coverage measures the percentage of the roles specified for each ACE event type which appeared as a column in one or more of the created tables for that event type . The correctness was measured based on whether a row of a table reflects the correct information . As it is impossible to evaluate all the data , the evaluation data are selected randomly . Out of 20 topics , two topics are judged very useful and twelve are judged useful . The very useful top ics are fine ( Q4 in the appendix ) and acquit ( not shown in the appendix ) . Compared to the results in the useful category , the tables for these two topics have more slots filled and the NE types of the fillers have fewer mistakes . The topics in the not useful category are appeal , execute , fired , pardon , release and trial . These are again topics with very few relevant articles . By increasing the corpus size or improving the IR component , we may be able to improve the performance for these topics . The majority category , useful , has 12 topics . Five of them can be found in the appendix ( all those besides Q4 ) . For these topics , the number of relevant articles in the corpus is relatively high and interesting relations are found . The examples in the appendix are selected from larger tables with many columns . Although there are columns that can not be filled for every event instance , we found that the more columns that are filled in , the more useful and interesting the information is . Usefulness evaluation result Ev al ua tio n N u m b e r o f t o p i c s Ve ry us ef ul 2 Us ef ul 1 2 N ot us ef ul 6 For the 14 very useful and useful topics , the role coverage was measured . Some of the roles in the ACE task can be filled by different types of Named Entities , for example , the defendant of a sentence event can be a Person , Organization or GPE . However , the system creates tables based on NE types ; e.g . for the sentence event , a Person column is created , in which most of the fillers are defendants . In such cases , we regard the column as covering the role . Out of 63 roles for the 14 event types , 38 are found in the created tables , for a role coverage of 60.3 % . Note that , by lowering the thresholds , the coverage can be increased to as much as 90 % ( some roles can t be found because of Extended NE limitations or the rare appearance of roles ) but with some sacrifice of precision . We randomly select 100 table rows among the topics which were judged very useful or useful , and determine the correctness of the information by reading the newspaper articles the information was extracted from . Out of 100 rows , 84 rows have correct information in all slots . 4 rows have some incorrect information in some of the columns , and 12 contain wrong information . Most errors are due to NE tagging errors ( 11 NE errors out of 16 errors ) . These errors include instances of people which are tagged as other categories , and so on . Also , by looking at the actual articles , we found that co-reference resolution could help to fill in more information . Because the important information is repeatedly mentioned in newspaper articles , referential expressions are often used . For example , in a sentence In 1968 he was elected mayor of Indianapolis . , we could not extract he at the moment . We plan to add coreference resolution in the near future . Other sources of error include : The role of the entity is confused , i.e . victim and murderer Different kinds of events are found in one table , e.g. , the victory of Jack Nicklaus was found in first proposed by ( Aone and RamosSantacruz 00 ) and the ACE evaluations of event detection follow this line ( ACE Home Page ) . An unsupervised learning method has been applied to a more restricted IE task , Relation Discovery . 2004 ) used large corpora and an Extended Named Entity tagger to find novel relations and their participants . However , the results are limited to a pair of participants and because of the nature of the procedure , the discovered relations are static relations like a country and its presidents rather than events . Topic-oriented summarization , currently pursued by the DUC evaluations ( DUC Home Page ) , is also closely related . The systems are trying to create summaries based on the specified topic for a manually prepared set of documents . In this case , if the result is suitable to present in table format , it can be handled by ODIE . Our previous study ( Se the political election use terms like win ) query ( as both of them kine and Nobata 03 ) found that about one third of randomly constructed similar newspaper article An unrelated but often collocate entity was included . For example , Year period expressions are found in fine events , as there are many expressions like He was sentenced 3 years and fined $ 1,000 . Correctness evaluation result Ev al ua tio n N u m b e r o f r o w s Co rre ct 8 4 Pa rti all y co rre ct 4 In co rre ct 1 2 As far as the authors know , there is no system similar to ODIE . Several methods have been proposed to produce IE patterns automatically to facilitate IE knowledge creation , as is described in Section 3.1 . But those are not targeting the fully automatic creation of a complete IE system for a new topic . There exists another strategy to extend the range of IE systems . It involves trying to cover a wide variety of topics with a large inventory of relations and events . It is not certain if there are only a limited number of topics in the world , but there are a limited number of high-interest topics , so this may be a reasonable solution from an engineering point of view . This line of research was clusters are well-suited to be presented in table format , and another one third of the clusters can be acceptably expressed in table format . This suggests there is a big potential where an ODIE-type system can be beneficial . We demonstrated a new paradigm of Information Extraction technology and showed the potential of this method . However , there are problems to be solved to advance the technology . One of them is the coverage of the extracted information . Although we have created useful tables for some topics , there are event instances which are not found . This problem is mostly due to the inadequate performance of the language analyzers ( information retrieval component , dependency analyzer or Extended NE tagger ) and the lack of a coreference analyzer . Even though there are possible applications with limited coverage , it will be essential to enhance these components and add coreference in order to increase coverage . Also , there are basic domain limitations . We made the system on-demand for any topic , but currently only within regular news domains . As configured , the system would not work on other domains such as a medical , legal , or patent domain , mainly due to the design of the extended NE hierarchy . While specific hierarchies could be incorporated for new domains , it will also be desirable to integrate bootstrapping techniques for rapid incremental additions to the hierarchy . Also at the moment , table column labels are simply Extended NE categories , and do not indicate the role . We would like to investigate this problem in the future . In this paper , we proposed On-demand Information Extraction ( ODIE ) . It is a system which automatically identifies the most salient structures and extracts the information on whatever topic the user demands . It relies on recent advances in NLP technologies ; unsupervised learning and several advanced NLP analyzers . Although it is at a preliminary stage , we developed a prototype system which has created useful tables for many topics and demonstrates the feasibility of this approach . This research was supported in part by the Defense Advanced Research Projects Agency under Contract HR001106-C-0023 and by the National Science Foundation under Grant IIS0325657 . This paper does not necessarily reflect the position of the U.S. Government . We would like to thank Prof. Ralph Grishman , Dr. Kiyoshi Sudo , Dr. Chikashi Nobata , Mr. Takaaki Hasegawa , Mr. Koji Murakami and Mr. Yusuke Shinyama for useful comments , discussion . The paper aims at a deeper understanding of several well-known algorithms and proposes ways to optimize them . It describes and discusses factors and strategies of factor interaction used in the algorithms . The factors used in the algorithms and the algorithms themselves are evaluated on a German corpus annotated with syntactic and coreference information ( Negra ) ( Skut et al. , 1997 ) . A common format for pronoun resolution algorithms with several open parameters is proposed , and the parameter settings optimal on the evaluation data are given . In recent years , a variety of approaches to pronoun resolution have been proposed . Some of them are based on centering theory ( Strube , 1998 ; Strube and Hahn , 1999 ; Tetreault , 2001 ) , others on Machine Learning ( Aone and Bennett , 1995 ; Ge et al. , 1998 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Yang et al. , 2003 ) . They supplement older heuristic approaches ( Hobbs , 1978 ; Lappin and Leass , 1994 ) . Unfortunately , most of these approaches were evaluated on different corpora making different assumptions so that direct comparison is not possible . Appreciation of the new insights is quite hard . Evaluation differs not only with regard to size and genre of corpora but also along the following lines . Scope of application : Some approaches only deal with personal and possessive pronouns ( centering and heuristic ) , while others consider coreference links in general ( Soon et al. , 2001 ; Ng and Cardie , 2002 ; Yang et al. , 2003 ) . A drawback of this latter view is that it mixes problems on different lev els of difficulty . It remains unclear how much of the success is due to the virtues of the approach and how much is due to the distribution of hard and easy problems in the corpus . In this paper , we will only deal with coreferential pronouns ( i.e . possessive , demonstrative , and third person pronouns ) . My thanks go to Melvin Wurster for help in annotation and to Ciprian Gerstenberger for discussion . Quality of linguistic input : Some proposals were evaluated on hand annotated ( Strube and Hahn , 1999 ) or tree bank input ( Ge et al. , 1998 ; Tetreault , 2001 ) . Other proposals provide a more realistic picture in that they work as a backend to a parser ( Lappin and Leass , 1994 ) or noun chunker ( Mitkov , 1998 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ) ) . In evaluation of applications presupposing parsing , it is helpful to separate errors due to parsing from intrinsic errors . On the other hand , one would also like to gauge the end-to-end performance of a system . Thus we will provide performance figures for both ideal ( hand-annotated ) input and realistic ( automatically generated ) input . Language : Most approaches were evaluated on English where large resources are available , both in terms of pre-annotated data ( MUC6 and MUC7 data ) and lexical information ( WordNet ) . This paper deals with German . Arguably , the free word-order of German arguably leads to a clearer distinction between grammatical function , surface order , and information status ( Strube and Hahn , 1999 ) . The paper is organized as follows . Section 2 describes the evaluation corpus . Section 3 describes several factors relevant to pronoun resolution . It assesses these factors against the corpus , measuring their precision and restrictiveness . Section 4 describes and evaluates six algorithms on the basis of these factors . It also captures the algorithms as para- metric systems and proposes parameter settings optimal on the evaluation data . We chose as an evaluation base the NEGRA tree bank , which contains about 350,000 tokens of German newspaper text . The same corpus was also processed with a finite-state parser , performing at 80 % dependency f-score ( Schiehlen , 2003 ) . All personal pronouns ( PPER ) , possessive pronouns ( PPOSAT ) , and demonstrative pronouns ( PDS ) in Negra were annotated in a format geared to the MUC7 guidelines ( MUC7 , 1997 ) . Proper names were annotated automatically by a named entity recognizer . In a small portion of the corpus ( 6.7 % ) , all coreference links were annotated . Thus the size of the annotated data ( 3,115 personal pronouns1 , 2,198 possessive pronouns , 928 demonstrative pronouns ) compares favourably with the size of evaluation data in other proposals ( 619 German pronouns in ( Strube and Hahn , 1999 ) , 2,477 English pronouns in ( Ge et al. , 1998 ) , about 5,400 English coreferential expressions in ( Ng and Cardie , 2002 ) ) . In the experiments , systems only looked for single NP antecedents . Hence , propositional or predicative antecedents ( 8.4 % of the pronouns annotated ) and split antecedents ( 0.2 % ) were inaccessible , which reduced optimal success rate to 91.4 % . Pronoun resolution is conditioned by a wide range of factors . Two questions arise : Which factors are the most effective ? How is interaction of the factors modelled ? The present section deals with the first question , while the second question is postponed to section 4 . Many approaches distinguish two classes of resolution factors : filters and preferences . Filters express linguistic rules , while preferences are merely tendencies in interpretation . Logically , filters are monotonic inferences that select a certain subset of possible antecedents , while preferences are non- monotonic inferences that partition the set of antecedents and impose an order on the cells . In the sequel , factors proposed in the literature are discussed and their value is appraised on evaluation data . Every factor narrows the set of antecedents and potentially discards correct antecedents . Figures are also given for parsed input . Preferences are evaluated on filtered sets of antecedents . An important filter comes from morphology : Agreement in gender and number is generally regarded as a prerequisite for coreference . Exceptions are existant but few ( 2.5 % ) : abstract pronouns ( such as that in English ) referring to non- neuter or plural NPs , plural pronouns co-referring with singular collective NPs ( Ge et al. , 1998 ) , antecedent and anaphor matching in natural gender 1 Here , we only count anaphoric pronouns , i.e . third person pronouns not used expletively . All in all , a maximal performance of 88.9 % is maintained . The filter is very restrictive , and cuts the set of possible antecedents in half . Binding constraints have been in the focus of linguistic research for more than thirty years . They provide restrictions on coindexation of pronouns with clause siblings , and therefore can only be applied with systems that determine clause boundaries , i.e . parsers ( Mitkov , 1998 ) . Empirically , binding constraints are rules without exceptions , hence they do not lead to any loss in achievable performance . The downside is that their restrictive power is quite bad as well ( 0.3 % in our corpus , cf . More controversial are sortal constraints . Intuitively , they also provide a hard filter : The correct antecedent must fit into the environment of the pronoun ( Carbonell and Brown , 1988 ) . In general , however , the required knowledge sources are lacking , so they must be hand-coded and can only be applied in restricted domains ( Strube and Hahn , 1999 ) . Selectional restrictions can also be modelled by collocational data extracted by a parser , which have , however , only a very small impact on overall performance ( Kehler et al. , 2004 ) . We will neglect sortal constraints in this paper . Preferences can be classified according to their requirements on linguistic processing . Sentence Re- cency and Surface Order can be read directly off the surface . NP Form presupposes at least tagging . A range of preferences ( Grammatical Roles , Role Parallelism , Depth of Embedding , Common Path ) , as well as all filters , presuppose full syntactic analysis . Mention Count and Information Status are based on previous decisions of the anaphora resolution module . Sentence Recency ( SR ) . The most important criterion in pronoun resolution ( Lappin and Leass , 1994 ) is the textual distance between anaphor and antecedent measured in sentences . Lappin and Le ass ( 1994 ) motivate this preference as a dynamic expression of the attentional state of the human hearer : Memory capability for storage of discourse refer ents degrades rapidly . Perhaps most obvious is the strategy implicit in Lappin and Leass ( 1994 ) s algorithm : The antecedent issearched in a sentence that is as recent as possi ble , beginning with the already uttered part of the current sentence , continuing in the last sentence , in the one but last sentence , and so forth . In case no Co nst rai nt Uppe r Boun d n u m b e r o f a nt e c. P a r s e r tot al PP E R PP O S A T P D S U pp er B an tec . no V P 91 .6 9 8 . 0 48 .5 1 2 3 . 4 no spl it 91 .4 9 8 . 0 47 .8 1 2 3 . 2 ag re e m en t 88 .9 9 6 . 5 37 .6 5 3 . 8 bi nd in g 88 .9 5 2 . 4 se nt en ce re ce nc y SR 78 .8 8 4 . 7 gr a m m ati cal rol e GR 74 .0 82 .3 2 8 7 . 9 13 .0 1 4 . 0 rol e pa ral lel is m RP 64 .3 7 7 . 4 20 .0 1 2 . 3 su rfa ce or de r LR 53 .5 6 2 . 6 15 .3 1 4 2 . 6 1 su rfa ce or de r RL 45 .9 4 5 . 7 22 .7 1 3 5 . 2 1 de pt h of e m be dd in g DE 51 .6 5 1 . 0 co m m on pa th CP 51 .7 5 2 . 3 eq ui va le nc e cla ss es EQ 63 .6 6 7 . 5 m en tio n co un t MC 32 .9 4 0 . 1 inf or m ati on sta tus IS 65 .3 7 1 . 4 16 .7 1 6 . 3 N P for m NF 42 .4 4 9 . 3 N P for m ( pr on ou n ) NP 73 .7 8 2 . 8 30 .2 2 9 . 6 10000 all PPER age only 2.4 such antecedents . However , the benefit also comes at a cost : The upper ceiling of performance is lowered to 82.0 % in our corpus : In many cases an incorrect antecedent is found in a more recent sentence.Similarly , we can assess other strategies of sen 1000 100 10 1 P P O S A T P D S -2-1 0 1 2 3 4 5 6 7 8 9 12 19 Fi gu re 1 : Se nte nc e Re ce nc y tence orderi ng that have been propo sed in the litera ture . Hard core center ing appro aches only deal with the last senten ce ( Bren nan et al. , 1987 ) . In Negra , these appro aches can conse quentl y have at most a succes s rate of 44.2 % . Perfor mance is partic ularly low with posses sive prono uns which often only have antece dents in the curren t senten ce . Strube and Hahn ( 1999 ) extend the contex t to more than the last senten ce , but switch prefer ence order betwe en the last and the curren t senten ce so that an antece dent is deter mined in the last senten ce , whene ver possi In Negra , 55.3 % of all pronominal anaphora can be resolved intrasententially , and 97.6 % within the last three sentences . Since only 1.6 % of all pronouns are cataphoric , it seems reasonable to neglect cat- aphora , as is mostly done ( Strube and Hahn , 1999 ; Hobbs , 1978 ) . In Negra , this ordering imposes an upper limit of 51.2 % . Grammatical Roles ( GR ) . Another important factor in pronoun resolution is the grammatical role of the antecedent . The role hierarchy used in centering ( Brennan et al. , 1987 ; Grosz et al. , 1995 ) ranks subjects over direct objects over indirect objects over others . Lappin and Leass ( 1994 ) provide a more elaborate model which ranks NP complements and NP adjuncts lowest . Two other distinctions in their model express a preference of rhematic2 over thematic arguments : Existential subjects , which follow the verb , rank very high , between subjects and direct objects . Topic adjuncts in pre-subject position separated by a comma rank very low , between adjuncts and NP complements . Both positions are not clearly demarcated in German . When the Lap- pin & Leass hierarchy is adopted to German without changes , a small drop in performance results as compared with the obliqueness hierarchy used in centering . So we will use the centering hierarchy . The factor is both less restrictive and less precise than sentence recency . The definition of a grammatical role hierarchy is more involved in case of automatically derived input , as the parser can not always decide on the grammatical role ( determining grammatical roles in German may require world knowledge ) . It proposes a syntactically preferred role , however , which we will adopt . Role Parallelism ( RP ) . Carbonell and Brown ( 1988 ) argue that pronouns prefer antecedents in the same grammatical roles . Lappin and Leass ( 1994 ) also adopt such a principle . The factor is , however , not applicable to possessive pronouns . Again , role ambiguities make this factor slightly problematic . Several approaches are conceivable : Antecedent and pronoun are required to have a common role in one reading ( weak match ) . Antecedent and pronoun are required to have the same role in the reading preferred by surface order ( strong match ) . Antecedent and pronoun must display the same role ambiguity ( strongest match ) . Weak match restricted performance to 49.9 % with 12.1 antecedents on average . Strong match gave an upper limit of 47.0 % but with only 10.3 antecedents on average . Strongest match lowered the upper limit to 43.1 % but yielded only 9.3 antecedents . In interaction , strong match performed best , so we adopt it . Surface Order ( LR , RL ) . Surface Order is usually used to bring down the number of available antecedents to one , since it is the only factor that produces a unique discourse referent . There is less consensus on the preference order : ( sentence-wise ) left-to-right ( Hobbs , 1978 ; Strube , 1998 ; Strube and Hahn , 1999 ; Tetreault , 1999 ) or right-to-left ( recency ) ( Lappin and Leass , 1994 ) . Furthermore , something has to be said about antecedents which embed other antecedents ( e.g . conjoined NPs and their conjuncts ) . We registered performance gains 2 Carbonell and Brown ( 1988 ) also argue that clefted or fronted arguments should be preferred . ( of up to 3 % ) by ranking embedding antecedents higher than embedded ones ( Tetreault , 2001 ) . Left-to-right order is often used as a surrogate for grammatical role hierarchy in English . The most notable exception to this equivalence are fronting constructions , where grammatical roles outperform surface order ( Tetreault , 2001 ) . Left-to-right order performs better ( upper limit 56.8 % ) than right-to-left order ( upper limit 49.2 % ) . The gain is largely due to personal pronouns ; demonstrative pronouns are better modelled by right-to-left order . It is well-known that German demonstrative pronouns contrast with personal pronouns in that they function as topic-shifting devices . Another effect of this phenomenon is the poor performance of the role preferences in connection with demonstrative pronouns . Depth of Embedding ( DE ) . A prominent factor in Hobbs ( 1978 ) s algorithm is the level of phrasal embedding : Hobbs s algorithm performs a breadth- first search , so antecedents at higher levels of embedding are preferred . Common Path ( CP ) . The syntactic version of Hobbs ( 1978 ) s algorithm also assumes maximization of the common path between antecedents and anaphors as measured in NP and S nodes . Accordingly , intrasentential antecedents that are syntactically nearer to the pronoun are preferred . The factor only applies to intrasentential anaphora . The anaphora resolution module itself generates potentially useful information when processing a text . Arguably , discourse entities that have been often referred to in the previous context are topical and more likely to serve as antecedents again . This principle can be captured in different ways . Equivalence Classes ( EQ ) . Lappin and Leass ( 1994 ) make use of a mechanism based on equivalence classes of discourse referents which manages the attentional properties of the individual entities referred to . The mechanism stores and provides information on how recently and in which grammatical role the entities were realized in the discourse . The net effect of the storage mechanism is that discourse entities are preferred as antecedents if they recently came up in the discourse . But the mechanism also integrates the preferences Role Hierarchy and Role Parallelism . Hence , it is one of the best- performing factors on our data . Since the equivalence class scheme is tightly integrated in the parser , the problem of ideal anaphora resolution data does not arise . Mention Count ( MC ) . ( 1998 ) try to factorize the same principle by counting the number of times a discourse entities has been mentioned in the discourse already . However , they do not only train but also test on the manually annotated counts , and hence presuppose an optimal anaphora resolution system . In our implementation , we did not bother with intrasentential mention count , which depends on the exact traversal . Rather , mention count was computed only from previous sentences . Information Status ( IS ) . Strube ( 1998 ) and Strube and Hahn ( 1999 ) argue that the information status of an antecedent is more important than the grammatical role in which it occurs . They distinguish three levels of information status : entities known to the hearer ( as expressed by coreferential NPs , unmodified proper names , appositions , relative pronouns , and NPs in titles ) , entities related to such hearer-old entities ( either overtly via modifiers or by bridging ) , and entities new to the hearer . Like ( Ge et al. , 1998 ) , Strube ( 1998 ) evaluates on ideal hand annotated data . NP Form ( NF , NP ) . A cheap way to model information status is to consider the form of an antecedent ( Tetreault , 2001 ; Soon et al. , 2001 ; Strube and M ller , 2003 ) . Personal and demonstrative pronouns are necessarily context-dependent , and proper nouns are nearly always known to the hearer . Definite NPs may be coreferential or interpreted by bridging , while indefinite NPs are in their vast majority new to the hearer . We considered two proposals for orderings of form : preferring pronouns and proper names over other NPs over indefinite NPs ( Tetreault , 2001 ) ( NF ) or preferring pronouns over all other NPs ( Tetreault , 2001 ) ( NP ) . In this section , we consider the individual approaches in more detail , in particular we will look at their choice of factors and their strategy to model factor interaction . According to interaction potential , we distinguish three classes of approaches : Serialization , Weighting , and Machine Learning . We re-implemented some of the algorithms described in the literature and evaluated them on syntactically ideal and realistic German3 input . With the ideal treebank input , we also assumed ideal input for the factors dependent on previous 3 A reviewer points out that most of the algorithms were proposed for English , where they most likely perform better . However , the algorithms also incorporate a theory of saliency , which should be language-independent . With realistic parsed input , we fed the results of the actual system back into the computation of such factors . Algorithmical approaches first apply filters unconditionally ; possible exceptions are deemed non- existant or negligible . With regard to interaction of preferences , many algorithms ( Hobbs , 1978 ; Strube , 1998 ; Tetreault , 2001 ) subscribe to a scheme , which , though completely rigid , performs surprisingly well : The chosen preferences are applied one after the other in a certain predefined order . Application of a preference consists in selecting those of the antecedents still available that are ranked highest in the preference order . Hobbs ( 1978 ) s algorithm essentially is a concatenation of the preferences Sentence Recency ( without cataphora ) , Common Path , Depth of Embedding , and left-to-right Surface Order . It also implements the binding constraints by disallowing sibling to the anaphor in a clause or NP as antecedents . Like Lappin and Leass ( 1994 ) , we replaced this implementation by our own mechanism to check binding constraints , which raised the success rate . The Left-Right Centering algorithm of Tetreault ( 1999 ) is similar to Hobbs s algorithm , and is composed of the preferences Sentence Recency ( without cataphora ) , Depth of Embedding , and left-to-right Surface Order . Since it is a centering approach , it only inspects the current and last sentence . Strube ( 1998 ) s S-list algorithm is also restricted to the current and last sentence . Predicative complements and NPs in direct speech are excluded as antecedents . The primary ordering criterion is Information Status , followed by Sentence Recency ( without cataphora ) and left-to-right Surface Order . Since serialization provides a quite rigid frame , we conducted an experiment to find the best performing combination of pronoun resolution factors on the treebank and the best combination on the parsed input . For this purpose , we checked all permutations of preferences and subtracted preferences from the best-performing combinations until performance degraded ( greedy descent ) . The completely annotated 6.7 % of the corpus were used as development set , the rest as test set . 0 17 .4 17 .2 12 .0 22 .7 10 .6 4 5 . 6 ( L ap pi n an d Le as s , 19 94 ) E Q S R R L 65 .4 7 1 . 0 16 .6 5 0 . 8 ( G e et al . , 19 98 ) ( S oo n et al . , 20 01 ) op ti m al al go r. ( C 4 . 5 ) H ob bs +M C ( S R+ N P ) R L ( S R/ R L+ G R+ N F/ I S ) R L 43 .4 24 .8 71 .1 4 5 . 7 In the actual realization , however , the weights of factors lie so much apart that in the majority of cases interaction boils down to serialization . The weighting scheme includes Sentence Recency , Grammatical Roles , Role Parallelism , on the basis of the equivalence class approach described in section 3.2 . Final choice of antecedents is relegated to right-to-left Surface Order . Interestingly , the Lappin & Leass algorithm outperforms even the best serialization algorithm on parsed input . Machine Learning approaches ( Ge et al. , 1998 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ) do not distinguish between filters and preferences . They submit all factors as features to the learner . For every combination of feature values the learner has the freedom to choose different factors and to assign different strength to them . Thus the main problem is not choice and interaction of factors , but rather the formulation of anaphora resolution as a classification problem . Two proposals emerge from the literature . ( 1 ) Given an anaphor and an antecedent , decide if the antecedent is the correct one ( Ge et al. , 1998 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ) . ( 2 ) Given an anaphor and two antecedents , decide which antecedent is more likely to be the correct one ( Yang et al. , 2003 ) . In case ( 1 ) , the lopsidedness of the distribution is problematic : There are much more negative than positive training examples . Machine Learning tools have to surpass a very high baseline : The strategy of never proposing an antecedent typically already yields an f-score of over 90 % . In case ( 2 ) , many more correct decisions have to be made before a correct antecedent is found . Thus it is important in this scenario , that the set of antecedents is subjected to a strict filtering process in advance so that the system only has to choose among the best candidates and errors are less dangerous . ( 1998 ) s probabilistic approach combines three factors ( aside from the agreement filter ) : the result of the Hobbs algorithm , Mention Count dependent on the position of the sentence in the article , and the probability of the antecedent occurring in the local context of the pronoun . In our re-implementation , we neglected the last factor ( see section 3.1 ) . Evaluation was performed using 10- fold cross validation . Other Machine Learning approaches ( Soon et al. , 2001 ; Ng and Cardie , 2002 ; Yang et al. , 2003 ) make use of decision tree learning4 ; we used C4.5 ( Quinlan , 1993 ) . To construct the training set , Soon et al . ( 2001 ) take the nearest correct antecedent in the previous context as a positive example , while all possible antecedents between this antecedent and the pronoun serve as negative examples . For testing , potential antecedents are presented to the classifier in Right-to-Left order ; the first one classified positive is chosen . Apart from agreement , only two of Soon et al . ( 2001 ) s features apply to pronominal anaphora : Sentence Recency , and NP Form ( with personal pronouns only ) . We used every 10th sentence in Negra for testing , all other sentences for training . On parsed input , a very simple decision tree is generated : For every personal and possessive pronoun , the nearest agreeing pronoun is chosen as antecedent ; demonstrative pronouns never get an antecedent . This tree performs better than the more complicated tree generated from treebank input , where also non-pronouns in previous sentences can serve as antecedents to a personal pronoun . ( 2001 ) s algorithm performs below its potential . We modified it somewhat to get better results . For one , we used every possible antecedent 4 On our data , Maximum Entropy ( Kehler et al. , 2004 ) had problems with the high baseline , i.e . in the training set , which improved performance on the treebank set ( by 1.8 % ) but degraded performance on the parsed data ( by 2 % ) . Furthermore , we used additional features , viz . the grammatical role of antecedent and pronoun , the NP form of the antecedent , and its information status . The latter two features were combined to a single feature with very many values , so that they were always chosen first in the decision tree . We also used fractional numbers to express intrasentential word distance in addition to Soon et al . ( 2001 ) s sentential distance . Role Parallelism ( Ng and Cardie , 2002 ) degraded performance ( by 0.3 % F-value ) . Introducing agreement as a feature had no effect , since the learner always determined that mismatches in agreement preclude coreference . Mention Count , Depth of Embedding , and Common Path did not affect performance either . The paper has presented a survey of pronoun resolution factors and algorithms . Two questions were investigated : Which factors should be chosen , and how should they interact ? Two types of factors , filters and preferences , were discussed in detail . In particular , their restrictive potential and effect on success rate were assessed on the evaluation corpus . To address the second question , several well-known algorithms were grouped into three classes according to their solution to factor interaction : Serialization , Weighting , and Machine Learning . Six algorithms were evaluated against a common evaluation set so as to facilitate direct comparison . Different algorithms have different strengths , in particular as regards their robustness to parsing errors . Two of the interaction strategies ( Serialization and Machine Learning ) allow data-driven optimization . Optimal algorithms could be proposed for these strategies . Better Arabic Parsing : Baselines , Evaluations , and Analysis In this paper , we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena , annotation choices , and model design . First , we identify sources of syntactic ambiguity understudied in the existing parsing literature . Second , we show that although the Penn Arabic Treebank is similar to other tree- banks in gross statistical terms , annotation consistency remains problematic . Third , we develop a human interpretable grammar that is competitive with a latent variable PCFG . Fourth , we show how to build better models for three different parsers . Finally , we show that in application settings , the absence of gold segmentation lowers parsing performance by 2–5 % F1 . It is well-known that constituency parsing models designed for English often do not generalize easily to other languages and treebanks.1 Explanations for this phenomenon have included the relative informativeness of lexicalization ( Dubey and Keller , 2003 ; Arun and Keller , 2005 ) , insensitivity to morphology ( Cowan and Collins , 2005 ; Tsarfaty and Sima ’ an , 2008 ) , and the effect of variable word order ( Collins et al. , 1999 ) . Certainly these linguistic factors increase the difficulty of syntactic disambiguation . Less frequently studied is the interplay among language , annotation choices , and parsing model design ( Levy and Manning , 2003 ; Ku¨ bler , 2005 ) . 1 The apparent difficulty of adapting constituency models to non-configurational languages has been one motivation for dependency representations ( Hajicˇ and Zema´nek , 2004 ; Habash and Roth , 2009 ) . To investigate the influence of these factors , we analyze Modern Standard Arabic ( henceforth MSA , or simply “ Arabic ” ) because of the unusual opportunity it presents for comparison to English parsing results . The Penn Arabic Treebank ( ATB ) syntactic guidelines ( Maamouri et al. , 2004 ) were purposefully borrowed without major modification from English ( Marcus et al. , 1993 ) . Further , Maamouri and Bies ( 2004 ) argued that the English guidelines generalize well to other languages . But Arabic contains a variety of linguistic phenomena unseen in English . Crucially , the conventional orthographic form of MSA text is unvocalized , a property that results in a deficient graphical representation . For humans , this characteristic can impede the acquisition of literacy . How do additional ambiguities caused by devocalization affect statistical learning ? How should the absence of vowels and syntactic markers influence annotation choices and grammar development ? Motivated by these questions , we significantly raise baselines for three existing parsing models through better grammar engineering . Our analysis begins with a description of syntactic ambiguity in unvocalized MSA text ( §2 ) . Next we show that the ATB is similar to other tree- banks in gross statistical terms , but that annotation consistency remains low relative to English ( §3 ) . We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic ( §4 ) . To facilitate comparison with previous work , we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed ( §5 ) . Finally , we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing ( §6 ) . We quantify error categories in both evaluation settings . To our knowledge , ours is the first analysis of this kind for Arabic parsing . Arabic is a morphologically rich language with a root-and-pattern system similar to other Semitic languages . The basic word order is VSO , but SVO , VOS , and VO configurations are also possible.2 Nouns and verbs are created by selecting a consonantal root ( usually triliteral or quadriliteral ) , which bears the semantic core , and adding affixes and diacritics . Word Head Of Complement POS 1 '01 inna “ Indeed , truly ” VP Noun VBP 2 '01 anna “ That ” SBAR Noun IN 3 01 in “ If ” SBAR Verb IN 4 01 an “ to ” SBAR Verb IN Diacritics can also be used to specify grammatical relations such as case and gender . But diacritics are not present in unvocalized text , which is the standard form of , e.g. , news media documents.3 VBD she added VP PUNC S VP VBP NP ... VBD she added VP PUNC “ SBAR IN NP 0 NN . Let us consider an example of ambiguity caused by devocalization . “ 0 Indeed NN Indeed Saddam Whereas Arabic linguistic theory as Saddam ( a ) Reference ( b ) Stanford signs ( 1 ) and ( 2 ) to the class of pseudo verbs 01 +i J > 1� inna and her sisters since they can beinflected , the ATB conventions treat ( 2 ) as a com plementizer , which means that it must be the head of SBAR . Because these two words have identical complements , syntax rules are typically unhelpful for distinguishing between them . Even with vocalization , there are linguistic categories that are difficult to identify without semantic clues . Two common cases are the attribu tive adjective and the process nominal _ ; maSdar , which can have a verbal reading.4 At tributive adjectives are hard because they are or- thographically identical to nominals ; they are inflected for gender , number , case , and definiteness . Moreover , they are used as substantives much 2 Unlike machine translation , constituency parsing is not significantly affected by variable word order . However , when grammatical relations like subject and object are evaluated , parsing performance drops considerably ( Green et al. , 2009 ) . In particular , the decision to represent arguments in verb- initial clauses as VP internal makes VSO and VOS configurations difficult to distinguish . Topicalization of NP subjects in SVO configurations causes confusion with VO ( pro-drop ) . 3 Techniques for automatic vocalization have been studied ( Zitouni et al. , 2006 ; Habash and Rambow , 2007 ) . However , the data sparsity induced by vocalization makes it difficult to train statistical models on corpora of the size of the ATB , so vocalizing and then parsing may well not help performance . 4 Traditional Arabic linguistic theory treats both of these types as subcategories of noun � '.i . more frequently than is done in English . Process nominals name the action of the transitive or ditransitive verb from which they derive . The verbal reading arises when the maSdar has an NP argument which , in vocalized text , is marked in the accusative case . When the maSdar lacks a determiner , the constituent as a whole resem bles the ubiquitous annexation construct � ? f iDafa . Gabbard and Kulick ( 2008 ) show that there is significant attachment ambiguity associated with iDafa , which occurs in 84.3 % of the trees in our development set . All three models evaluated in this paper incorrectly analyze the constituent as iDafa ; none of the models attach the attributive adjectives properly . For parsing , the most challenging form of ambiguity occurs at the discourse level . A defining characteristic of MSA is the prevalence of discourse markers to connect and subordinate words and phrases ( Ryding , 2005 ) . Instead of offsetting new topics with punctuation , writers of MSA in sert connectives such as � wa and � fa to link new elements to both preceding clauses and the text as a whole . Length English ( WSJ ) Arabic ( ATB ) ≤ 20 41.9 % 33.7 % ≤ 40 92.4 % 73.2 % ≤ 63 99.7 % 92.6 % ≤ 70 99.9 % 94.9 % English parsing evaluations usually report results on sentences up to length 40 . Arabic sentences of up to length 63 would need to be . evaluated to account for the same fraction of the data . We propose a limit of 70 words for Arabic parsing evaluations . ATB CTB6 Negra WSJ Trees 23449 28278 20602 43948 Word Typess 40972 45245 51272 46348 Tokens 738654 782541 355096 1046829 Tags 32 34 499 45 Phrasal Cats 22 26 325 27 Test OOV 16.8 % 22.2 % 30.5 % 13.2 % Per Sentence Test set OOV rate is computed using the following splits : ATB ( Chiang et al. , 2006 ) ; CTB6 ( Huang and Harper , 2009 ) ; Negra ( Dubey and Keller , 2003 ) ; English , sections 221 ( train ) and section 23 ( test ) . The ATB gives several different analyses to these words to indicate different types of coordination . A better approach would be to distin guish between these cases , possibly by drawing on the vast linguistic work on Arabic connectives ( AlBatal , 1990 ) . Linguistic intuitions like those in the previous section inform language-specific annotation choices . The resulting structural differences between tree- banks can account for relative differences in parsing performance . The ATB is disadvantaged by having fewer trees with longer average 5 LDC A-E catalog numbers : LDC2008E61 ( ATBp1v4 ) , LDC2008E62 ( ATBp2v3 ) , and LDC2008E22 ( ATBp3v3.1 ) . We map the ATB morphological analyses to the shortened “ Bies ” tags for all experiments . yields.6 But to its great advantage , it has a high ratio of non-terminals/terminals ( μ Constituents / μ Length ) . Evalb , the standard parsing metric , is biased toward such corpora ( Sampson and Babarczy , 2003 ) . Also surprising is the low test set OOV rate given the possibility of morphological variation in Arabic . In general , several gross corpus statistics favor the ATB , so other factors must contribute to parsing underperformance . Annotation consistency is important in any supervised learning task . In the initial release of the ATB , inter-annotator agreement was inferior to other LDC treebanks ( Maamouri et al. , 2008 ) . To improve agreement during the revision process , a dual-blind evaluation was performed in which 10 % of the data was annotated by independent teams . ( 2008 ) reported agreement between the teams ( measured with Evalb ) at 93.8 % F1 , the level of the CTB . But Rehbein and van Genabith ( 2007 ) showed that Evalb should not be used as an indication of real difference— or similarity—between treebanks . Instead , we extend the variation n-gram method of Dickinson ( 2005 ) to compare annotation error rates in the WSJ and ATB . For a corpus C , let M be the set of tuples ∗n , l ) , where n is an n-gram with bracketing label l. If any n appears 6 Generative parsing performance is known to deteriorate with sentence length . As a result , Habash et al . ( 2006 ) developed a technique for splitting and chunking long sentences . In application settings , this may be a profitable strategy . NN � .e NP NNP NP DTNNP NN � .e NP NP NNP NP The samples from each corpus were independently evaluated . The ATB has a much higher fraction of nuclei per tree , and a higher type-level error rate . summit Sharm ( a ) Al-Sheikh summit Sharm ( b ) DTNNP Al-Sheikh in a corpus position without a bracketing label , then we also add ∗n , NIL ) to M. We call the set of unique n-grams with multiple labels in M the variation nuclei of C. Bracketing variation can result from either annotation errors or linguistic ambiguity . Human evaluation is one way to distinguish between the two cases . Following Dickinson ( 2005 ) , we randomly sampled 100 variation nuclei from each corpus and evaluated each sample for the presence of an annotation error . The human evaluators were a non-native , fluent Arabic speaker ( the first author ) for the ATB and a native English speaker for the WSJ.7 The 95 % confidence intervals for type-level errors are ( 5580 , 9440 ) for the ATB and ( 1400 , 4610 ) for the WSJ . The results clearly indicate increased variation in the ATB relative to the WSJ , but care should be taken in assessing the magnitude of the difference . On the one hand , the type-level error rate is not calibrated for the number of n-grams in the sample . At the same time , the n-gram error rate is sensitive to samples with extreme n-gram counts . For example , one of the ATB samples was the determiner - '' '' ; dhalik “ that. ” The sample occurred in 1507 corpus po sitions , and we found that the annotations were consistent . If we remove this sample from the evaluation , then the ATB type-level error rises to only 37.4 % while the n-gram error rate increases to 6.24 % . The number of ATB n-grams also falls below the WSJ sample size as the largest WSJ sample appeared in only 162 corpus positions . 7 Unlike Dickinson ( 2005 ) , we strip traces and only con- . We can use the preceding linguistic and annotation insights to build a manually annotated Arabic grammar in the manner of Klein and Manning ( 2003 ) . Manual annotation results in human in- terpretable grammars that can inform future tree- bank annotation decisions . In our grammar , features are realized as annotations to basic category labels . We start with noun features since written Arabic contains a very high proportion of NPs . genitiveMark indicates recursive NPs with a indefinite nominal left daughter and an NP right daughter . This is the form of recursive levels in iDafa constructs . We also add an annotation for one-level iDafa ( oneLevelIdafa ) constructs since they make up more than 75 % of the iDafa NPs in the ATB ( Gabbard and Kulick , 2008 ) . For all other recursive NPs , we add a common annotation to the POS tag of the head ( recursiveNPHead ) . Base NPs are the other significant category of nominal phrases . markBaseNP indicates these non-recursive nominal phrases . This feature includes named entities , which the ATB marks with a flat NP node dominating an arbitrary number of NNP pre-terminal daughters ( Figure 2 ) . For verbs we add two features . First we mark any node that dominates ( at any level ) a verb sider POS tags when pre-terminals are the only intervening nodes between the nucleus and its bracketing ( e.g. , unaries , base NPs ) . Since our objective is to compare distributions of bracketing discrepancies , we do not use heuristics to prune the set of nuclei . 8 We use head-finding rules specified by a native speaker . This PCFG is incorporated into the Stanford Parser , a factored model that chooses a 1-best parse from the product of constituency and dependency parses . termined by the category of the word that follows it . Because conjunctions are elevated in the parse trees when they separate recursive constituents , we choose the right sister instead of the category of the next word . We create equivalence classes for verb , noun , and adjective POS categories . This feature has a linguistic justification . Historically , Arabic grammar has identified two sentences types : those that begin with a nominal ( � '.i �u _.. ) , and thosethat begin with a verb ( � ub..i �u _.. But for eign learners are often surprised by the verbless predications that are frequently used in Arabic . Although these are technically nominal , they have become known as “ equational ” sentences . mark- ContainsVerb is especially effective for distinguishing root S nodes of equational sentences . We also mark all nodes that dominate an SVO configuration ( containsSVO ) . In MSA , SVO usually appears in non-matrix clauses . Lexicalizing several POS tags improves performance . splitIN captures the verb/preposition idioms that are widespread in Arabic . Although this feature helps , we encounter one consequence of variable word order . Unlike the WSJ corpus which has a high frequency of rules like VP →VB PP , Arabic verb phrases usually have lexi calized intervening nodes ( e.g. , NP subjects and direct objects ) . For example , we might have VP → VB NP PP , where the NP is the subject . This annotation choice weakens splitIN . We compare the manually annotated grammar , which we incorporate into the Stanford parser , to both the Berkeley ( Petrov et al. , 2006 ) and Bikel ( Bikel , 2004 ) parsers . All experiments use ATB parts 1–3 divided according to the canonical split suggested by Chiang et al . Preprocessing the raw trees improves parsing performance considerably.9 We first discard all trees dominated by X , which indicates errors and non-linguistic text . At the phrasal level , we remove all function tags and traces . We also collapse unary chains withidentical basic categories like NP → NP . The pre terminal morphological analyses are mapped to the shortened “ Bies ” tags provided with the tree- bank . Finally , we add “ DT ” to the tags for definite nouns and adjectives ( Kulick et al. , 2006 ) . The orthographic normalization strategy we use is simple.10 In addition to removing all diacritics , we strip instances of taTweel J=J4.i , collapse variants of alif to bare alif,11 and map Ara bic punctuation characters to their Latin equivalents . We retain segmentation markers—which are consistent only in the vocalized section of the treebank—to differentiate between e.g . � “ they ” and � + “ their. ” Because we use the vocalized section , we must remove null pronoun markers . Evalb is a Java re-implementation of the standard labeled precision/recall metric.12 The ATB gives all punctuation a single tag . For parsing , this is a mistake , especially in the case of interrogatives . splitPUNC restores the convention of the WSJ . We also mark all tags that dominate a word with the feminine ending : : taa mar buuTa ( markFeminine ) . The intuition here is that the role of a discourse marker can usually be de 9 Both the corpus split and pre-processing code are avail- . able at http : //nlp.stanford.edu/projects/arabic.shtml . 10 Other orthographic normalization schemes have been suggested for Arabic ( Habash and Sadat , 2006 ) , but we observe negligible parsing performance differences between these and the simple scheme used in this evaluation . 11 taTweel ( - ) is an elongation character used in Arabic script to justify text . It has no syntactic function . Variants of alif are inconsistently used in Arabic texts . For alif with hamza , normalization can be seen as another level of devocalization . 12 For English , our Evalb implementation is identical to the most recent reference ( EVALB20080701 ) . For Arabic we M o d e l S y s t e m L e n g t h L e a f A n c e s t o r Co rpu s Sent Exact E v a l b L P LR F1 T a g % B a s e l i n e 7 0 St an for d ( v 1 . 3 ) all G o l d P O S 7 0 0.7 91 0.825 358 0.7 73 0.818 358 0.8 02 0.836 452 80 . 95 B a s e li n e ( S e lf t a g ) 70 a l l B i k e l ( v 1 . 2 ) B a s e l i n e ( P r e t a g ) 7 0 a l l G o l d P O S 70 0.7 70 0.801 278 0.7 52 0.794 278 0.7 71 0.804 295 0.7 52 0.796 295 0.7 75 0.808 309 77 . 60 ( P e tr o v , 2 0 0 9 ) all B e r k e l e y ( S e p . 0 9 ) B a s e l i n e 7 0 a l l G o l d P O S 70 — — — 0 . 8 0 9 0.839 335 0 . 8 3 1 0.859 496 76 . 87 F1 85 Berkeley 80 Stanford . Bikel 75 training trees 5000 10000 15000 The Leaf Ancestor metric measures the cost of transforming guess trees to the reference ( Sampson and Babarczy , 2003 ) . It was developed in response to the non-terminal/terminal bias of Evalb , but Clegg and Shepherd ( 2005 ) showed that it is also a valuable diagnostic tool for trees with complex deep structures such as those found in the ATB . For each terminal , the Leaf Ancestor metric extracts the shortest path to the root . It then computes a normalized Levenshtein edit distance between the extracted chain and the reference . The range of the score is between 0 and 1 ( higher is better ) . We report micro-averaged ( whole corpus ) and macro-averaged ( per sentence ) scores along add a constraint on the removal of punctuation , which has a single tag ( PUNC ) in the ATB . Tokens tagged as PUNC are not discarded unless they consist entirely of punctuation . with the number of exactly matching guess trees . The Stanford parser includes both the manually annotated grammar ( §4 ) and an Arabic unknown word model with the following lexical features : 1 . Presence of the determiner J Al . Ends with the feminine affix : : p. 4 . Various verbal ( e.g. , � , . : : ) and adjectival . suffixes ( e.g. , �= ) Other notable parameters are second order vertical Markovization and marking of unary rules . Modifying the Berkeley parser for Arabic is straightforward . After adding a ROOT node to all trees , we train a grammar using six split-and- merge cycles and no Markovization . We use the default inference parameters . Because the Bikel parser has been parameter- ized for Arabic by the LDC , we do not change the default model settings . However , when we pre- tag the input—as is recommended for English— we notice a 0.57 % F1 improvement . We use the log-linear tagger of Toutanova et al . ( 2003 ) , which gives 96.8 % accuracy on the test set . The Berkeley parser gives state-of-the-art performance for all metrics . Our baseline for all sentence lengths is 5.23 % F1 higher than the best previous result . The difference is due to more careful S-NOM NP NP NP VP VBG : : b NP restoring NP ADJP NN : : b NP NN NP NP ADJP DTJJ ADJP DTJJ NN : : b NP NP NP ADJP ADJP DTJJ J ..i NN : : b NP NP NP ADJP ADJP DTJJ NN _ ; � NP PRP DTJJ DTJJ J ..i _ ; � PRP J ..i NN _ ; � NP PRP DTJJ NN _ ; � NP PRP DTJJ J ..i role its constructive effective ( b ) Stanford ( c ) Berkeley ( d ) Bik el ( a ) Reference Figure 4 : The constituent Restoring of its constructive and effective role parsed by the three different models ( gold segmentation ) . The ATB annotation distinguishes between verbal and nominal readings of maSdar process nominals . Like verbs , maSdar takes arguments and assigns case to its objects , whereas it also demonstrates nominal characteristics by , e.g. , taking determiners and heading iDafa ( Fassi Fehri , 1993 ) . In the ATB , : : b asta ’ adah is tagged 48 times as a noun and 9 times as verbal noun . Consequently , all three parsers prefer the nominal reading . None of the models attach the attributive adjectives correctly . Moreover , the Stanford parser achieves the most exact Leaf Ancestor matches and tagging accuracy that is only 0.1 % below the Bikel model , which uses pre-tagged input . The errors shown are from the Berkeley parser output , but they are representative of the other two parsing models . 6 Joint Segmentation and Parsing . Although the segmentation requirements for Arabic are not as extreme as those for Chinese , Arabic is written with certain cliticized prepositions , pronouns , and connectives connected to adjacent words . Since these are distinct syntactic units , they are typically segmented . The ATB segmentation scheme is one of many alternatives . Until now , all evaluations of Arabic parsing—including the experiments in the previous section—have assumed gold segmentation . But gold segmentation is not available in application settings , so a segmenter and parser are arranged in a pipeline . Segmentation errors cascade into the parsing phase , placing an artificial limit on parsing performance . Lattice parsing ( Chappelier et al. , 1999 ) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart . Recently , lattices have been used successfully in the parsing of Hebrew ( Tsarfaty , 2006 ; Cohen and Smith , 2007 ) , a Semitic language with similar properties to Arabic . We extend the Stanford parser to accept pre-generated lattices , where each word is represented as a finite state automaton . To combat the proliferation of parsing edges , we prune the lattices according to a hand-constructed lexicon of 31 clitics listed in the ATB annotation guidelines ( Maamouri et al. , 2009a ) . Formally , for a lexicon L and segments I ∈ L , O ∈/ L , each word automaton accepts the language I∗ ( O + I ) I∗ . Aside from adding a simple rule to correct alif deletion caused by the preposition J , no other language-specific processing is performed . Our evaluation includes both weighted and un- weighted lattices . We weight edges using a unigram language model estimated with Good- Turing smoothing . Despite their simplicity , uni- gram weights have been shown as an effective feature in segmentation models ( Dyer , 2009 ) .13 The joint parser/segmenter is compared to a pipeline that uses MADA ( v3.0 ) , a state-of-the-art Arabic segmenter , configured to replicate ATB segmentation ( Habash and Rambow , 2005 ) . MADA uses an ensemble of SVMs to first re-rank the output of a deterministic morphological analyzer . For each 13 Of course , this weighting makes the PCFG an improper distribution . However , in practice , unknown word models also make the distribution improper . Parent Head Modif er Dir # gold F1 Label # gold F1 NP NP TAG R 946 0.54 ADJP 1216 59.45 S S S R 708 0.57 SBAR 2918 69.81 NP NP ADJ P R 803 0.64 FRAG 254 72.87 NP NP N P R 2907 0.66 VP 5507 78.83 NP NP SBA R R 1035 0.67 S 6579 78.91 NP NP P P R 2713 0.67 PP 7516 80.93 VP TAG P P R 3230 0.80 NP 34025 84.95 NP NP TAG L 805 0.85 ADVP 1093 90.64 VP TAG SBA R R 772 0.86 WHN P 787 96.00 S VP N P L 961 0.87 ( a ) Major phrasal categories ( b ) Major POS categories ( c ) Ten lowest scoring ( Collins , 2003 ) -style dependencies occurring more than 700 times input token , the segmentation is then performed deterministically given the 1-best analysis . Since guess and gold trees may now have different yields , the question of evaluation is complex . Cohen and Smith ( 2007 ) chose a metric like SParseval ( Roark et al. , 2006 ) that first aligns the trees and then penalizes segmentation errors with an edit-distance metric . But we follow the more direct adaptation of Evalb suggested by Tsarfaty ( 2006 ) , who viewed exact segmentation as the ultimate goal . Therefore , we only score guess/gold pairs with identical character yields , a condition that allows us to measure parsing , tagging , and segmentation accuracy by ignoring whitespace . However , MADA is language-specific and relies on manually constructed dictionaries . Conversely , the lattice parser requires no linguistic resources and produces segmentations of comparable quality . Nonetheless , parse quality is much lower in the joint model because a lattice is effectively a long sentence . A cell in the bottom row of the parse chart is required for each potential whitespace boundary . As we have said , parse quality decreases with sentence length . Finally , we note that simple weighting gives nearly a 2 % F1 improvement , whereas Goldberg and Tsarfaty ( 2008 ) found that unweighted lattices were more effective for Hebrew . Coverage indicates the fraction of hypotheses in which the character yield exactly matched the reference . Each model was able to produce hypotheses for all input sentences . By establishing significantly higher parsing baselines , we have shown that Arabic parsing performance is not as poor as previously thought , but remains much lower than English . We have described grammar state splits that significantly improve parsing performance , catalogued parsing errors , and quantified the effect of segmentation errors . With a human evaluation we also showed that ATB inter-annotator agreement remains low relative to the WSJ corpus . Our results suggest that current parsing models would benefit from better annotation consistency and enriched annotation in certain syntactic configurations . Acknowledgments We thank Steven Bethard , Evan Rosen , and Karen Shiells for material contributions to this work . We are also grateful to Markus Dickinson , Ali Farghaly , Nizar Habash , Seth Kulick , David McCloskey , Claude Reichard , Ryan Roth , and Reut Tsarfaty for constructive discussions . The first author is supported by a National Defense Science and Engineering Graduate ( NDSEG ) fellowship . This paper is based on work supported in part by DARPA through IBM . The content does not necessarily reflect the views of the U.S. Government , and no official endorsement should be inferred . This article presents empirical evaluations of aspects of annotation for the linguistic property of animacy in Swedish , ranging from manual human annotation , automatic classification and , finally , an external evaluation in the task of syntactic parsing . We show that a treatment of animacy as a lexical semantic property of noun types enables generalization over distributional properties of these nouns which proves beneficial in automatic classification and furthermore gives significant improvements in terms of parsing accuracy for Swedish , compared to a state-of-the- art baseline parser with gold standard ani- macy information . The property of animacy influences linguistic phenomena in a range of different languages , such as case marking ( Aissen , 2003 ) and argument realization ( Bresnan et al. , 2005 ; de Swart et al. , 2008 ) , and has been shown to constitute an important factor in the production and comprehension of syntactic structure ( Branigan et al. , 2008 ; Weckerly and Kutas , 1999 ) .1 In computational linguistic work , animacy has been shown to provide important information in anaphora resolution ( Ora san and Evans , 2007 ) , argument disambiguation ( Dell Orletta et al. , 2005 ) and syntactic parsing in general ( vrelid and Nivre , 2007 ) .The dimension of animacy roughly distin guishes between entities which are alive and entities which are not , however , other distinctions 1 Parts of the research reported in this paper has been supported by the Deutsche Forschungsgemeinschaft ( DFG , Son- derforschungsbereich 632 , project D4 ) . are also relevant and the animacy dimension is often viewed as a continuum ranging from humans to inanimate objects . Following Silverstein ( 1976 ) several animacy hierarchies have been proposed in typological studies , focusing on the linguistic category of animacy , i.e. , the distinctions which are relevant for linguistic phenomena . An example of an animacy hierarchy , taken from ( Aissen , 2003 ) , is provided in ( 1 ) : ( 1 ) Human > Animate > Inanimate Clearly , nonhuman animates , like animals , are not less animate than humans in a biological sense , however , humans and animals show differing linguistic behaviour.Empirical studies of animacy require human an notation efforts , and , in particular , a well-defined annotation task . However , annotation studies of animacy differ distinctly in their treatment of ani- macy as a type or token-level phenomenon , as well as in terms of granularity of categories . The use of the annotated data as a computational resource furthermore poses requirements on the annotation which do not necessarily agree with more theoretical considerations . Methods for the induction of animacy information for use in practical applications require the resolution of issues of level of representation , as well as granularity.This article addresses these issues through em pirical and experimental evaluation . We present an in-depth study of a manually annotated data set which indicates that animacy may be treated as a lexical semantic property at the type level . We then evaluate this proposal through supervised machine learning of animacy information and focus on an in-depth error analysis of the resulting classifier , addressing issues of granularity of the animacy dimension . Finally , the automatically an Proceedings of the 12th Conference of the European Chapter of the ACL , pages 630 638 , Athens , Greece , 30 March 3 April 2009 . Qc 2009 Association for Computational Linguistics notated data set is employed in order to train a syntactic parser and we investigate the effect of the an- imacy information and contrast the automatically acquired features with gold standard ones . The rest of the article is structured as follows . In section 2 , we briefly discuss annotation schemes for animacy , the annotation strategies and categories proposed there . We go on to describe annotation for the binary distinction of human reference found in a Swedish dependency treebank in section 3 and we perform an evaluation of the consistency of the human annotation in terms of linguistic level . In section 4 , we present experiments in lexical acquisition of animacy based on morphosyntactic features extracted from a considerably larger corpus . Section 5 presents experiments with the acquired animacy information applied in the data-driven dependency parsing of Swedish . Finally , section 6 concludes the article and provides some suggestions for future research . Annotation for animacy is not a common component of corpora or treebanks . However , following from the theoretical interest in the property of an- imacy , there have been some initiatives directed at the animacy of their referent in the particular context . Animacy is thus treated as a token level property , however , has also been proposed as a lexical semantic property of nouns ( Yamamoto , 1999 ) . The indirect encoding of animacy in lexical resources , such as WordNet ( Fellbaum , 1998 ) can also be seen as treating animacy as a type- level property . We may thus distinguish between a purely type level annotation strategy and a purely token level one . Type level properties hold for lexemes and are context-independent , i.e. , independent of the particular linguistic context , whereas token-level properties are determined in context and hold for referring expressions , rather than lexemes . Talbanken05 is a Swedish treebank which was created in the 1970 s and which has recently been converted to dependency format ( Nivre et al. , 2006b ) and made freely available . The written sections of the treebank consist of professional prose and student essays and amount to 197,123 running tokens , spread over 11,431 sentences . animacy annotation of corpus data . Corpus studies of animacy ( Yamamoto , 1999 ; ( 2 ) Samma same erfarenhet experience gjorde made engelsma nnen englishmen-D E FDahl and Fraurud , 1996 ) have made use of an notated data , however they differ in the extent to which the annotation has been explicitly formulated as an annotation scheme . The annotation The same experience , the Englishmen had DT OO ROOT SS study presented in Zaenen et . The main class distinction for animacy is three-way , distinguishing Human , Other animate and Inanimate , with sub- classes under two of the main classes . The Other animate class further distinguishes Organizations and Animals . Within the group of inanimates , further distinctions are made between concrete and non-concrete inanimate , as well as time and place nominals.2 The annotation scheme described in Zaenen et . ( 2004 ) annotates the markables according to 2 The fact that the study focuses on genitival modification has clearly influenced the categories distinguished , as these are all distinctions which have been claimed to influence the choice of genitive construction . For instance , temporal nouns are frequent in genitive constructions , unlike the other inanimate nouns . In addition to information on part-of-speech , dependency head and relation , and various morphosyntactic properties such as definiteness , the annotation expresses a distinction for nominal elements between reference to human and nonhuman . The human/nonhuman contrast forms the central distinction in the animacy dimension and , in this respect , the annotation schemes do not conflict . If we compare the annotation found in Talbanken05 with the annotation proposed in Zaenen et . ( 2004 ) , we find that the schemes differ primarily in the granularity of classes distinguished . The main source of variation in class distinctions consists in the annotation of collective nouns , including organizations , as well as animals . tic contexts , we may group the nouns which were assigned to both classes , into the following categories : that H H is the tag for Abstract nouns Nouns with underspecified or vague type level properties with respect to ani- macy , such as quantifying nouns , e.g . ha lft half , miljon million , as well as nouns which may be employed with varying animacy , e.g . element element , part party , as in ( 3 ) and ( 4 ) : 3.1 Level of annotation . also den the andra other partenH H party-D E F sta r stands utanfo r outside We distinguished above between type and token . also the other party is left outside level annotation strategies , where a type level annotation strategy entails that an element consistently be assigned to only one class . A token level ( 4 ) I in lika ett a fo rha llande relationship starka a r are aldrig never ba gge both parter parties same strongstrategy , in contrast , does not impose this restric tion on the annotation and class assignment may vary depending on the specific context . al ( 2004 ) propose a token level annotation strategy and state that when coding for animacy [ . ] we are not considering the nominal per se ( e.g. , the word church ) , but rather the entity that is the referent of that nominal ( e.g . some particular thing in the real world ) . This indicates that for all possible markables , a referent should be determinable . The brief instruction with respect to annotation for human reference in the annotation manual for Talbanken05 ( Teleman , 1974 , 223 ) gives leeway for interpretation in the annotation and does not clearly state that it should be based on token level reference in context . It may thus be interesting In a relationship , both parties are never equally strong We also find that nouns which denote abstract concepts regarding humans show variable annotation , e.g . individ individual , adressat addressee , medlem member , kandidat candidate , representant representative , auktoritet authority Reference shifting contexts These are nouns whose type level animacy is clear but which are employed in a specific context which shifts their reference . Examples include metonymic usage of nouns , as in ( 5 ) and nouns occurring in dereferencing constructions , such as predicative constructions ( 6 ) , titles ( 7 ) and idioms ( 8 ) : to examine the extent to which this manual an ( 5 ) . kindergarten-D E F.G E N otillra ckliga inadequate resurser resources notation is consistent across lexemes or whether we observe variation . the kindergarten s inadequate resources intersection of the two classes of noun lemmas ( 6 ) . for att to bli become en bra a good soldat soldier in the written sections of Talbanken , i.e. , the set of nouns which have been assigned both classes . in order to become a good soldier by the annotators . It contains 82 noun lemmas , ( 7 ) . thinks biskop bishop Hellsten Hellstenwhich corresponds to only 1.1 % of the total number of noun lemmas in the treebank ( 7554 lem thinks bishop Hellsten mas all together ) . After a manual inspection of the intersective elements along with their linguis ( 8 ) ta take studenten student-D E F graduate from highschool ( lit . take the student ) It is interesting to note that the main variation in annotation stems precisely from difficulties in determining reference , either due to bleak type level properties such as for the abstract nouns , or due to properties of the context , as in the reference shifting constructions . The small amount of variation in the human annotation for animacy clearly supports a type-level approach to animacy , however , underline the influence of the linguistic context on the conception of animacy , as noted in the literature ( Zaenen et al. , 2004 ; Rosenbach , 2008 ) . Even though knowledge about the animacy of a noun clearly has some interesting implications , little work has been done within the field of lexical acquisition in order to automatically acquire ani- macy information . Ora san and Evans ( 2007 ) make use of hyponym-relations taken from the Word- Net resource in order to classify animate referents . However , such a method is clearly restricted to languages for which large scale lexical resources , such as the WordNet , are available . The task of animacy classification bears some resemblance to the task of named entity recognition ( NER ) which usually makes reference to a person class . However , whereas most NER systems make extensive use of orthographic , morphological or contextual clues ( titles , suffixes ) and gazetteers , animacy for nouns is not signaled overtly in the same way . Following a strategy in line with work on verb classification ( Merlo and Stevenson , 2001 ; Stevenson and Joanis , 2003 ) , we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus . This is thus equivalent to treatment of animacy as a lexical semantic property and the classification strategy is based on generalization of morphosyntactic behaviour of common nouns over large quantities of data . Due to the small size of the Talbanken05 treebank and the small amount of variation , this strategy was pursued for the acquisition of animacy information . In the animacy classification of common nouns we exploit well-documented correlations between morphosyntactic realization and semantic properties of nouns . For instance , animate nouns tend to be realized as agentive subjects , inanimate nouns do not ( Dahl and Fraurud , 1996 ) . Animate nouns make good possessors , whereas inanimate nouns are more likely possessees ( Rosenbach , 2008 ) . for common nouns in Talbanken05 . It is clear that the data is highly skewed towards the nonhuman class , which accounts for 91.5 % of the type instances . For classification we organize the data into accumulated frequency bins , which include all nouns with frequencies above a certain threshold . We here approximate the class of animate to human and the class of inanimate to nonhuman . Intersective elements , see section 3.1 , are assigned to their majority class.3 4.1 Features for animacy classification . We define a feature space , which makes use of distributional data regarding the general syntactic properties of a noun , as well as various morphological properties . It is clear that in order for a syntactic environment to be relevant for animacy classification it must be , at least potentially , nominal . We define the nominal potential of a dependency relation as the frequency with which it is realized by a nominal element ( noun or pronoun ) and determine empirically a threshold of .10 . The syntactic and morphological features in the feature space are presented below : Syntactic features A feature for each dependency relation with nominal potential : ( transitive ) subject ( SU B J ) , object ( O B J ) , prepositional complement ( PA ) , root ( RO OT ) 4 , apposition ( A PP ) , conjunct ( C C ) , determiner ( D ET ) , predicative ( PR D ) , complement of comparative subjunction ( U K ) . We also include a feature for the head of a genitive modifier , the so-called possessee , ( G EN H D ) . Morphological features A feature for each morphological distinction relevant for a noun 3 When there is no majority class , i.e . in the case of ties , the noun is removed from the data set . 12 lemmas were consequently removed . 4 Nominal elements may be assigned the root relation of the dependency graph in sentence fragments which do not contain a finite verb . in Swedish : gender ( N EU /U TR ) , number ( SIN/PLU ) , definiteness ( D EF/IN D ) , case ( N O M/G EN ) . Also , the part-of-speech tags distinguish dates ( DAT ) and quantifying nouns ( SET ) , e.g . del , rad part , row , so these are also included as features . For extraction of distributional data for the set of Swedish nouns we make use of the Swedish Parole corpus of 21.5M tokens.5 To facilitate feature extraction , we part-of-speech tag the corpus and parse it with MaltParser6 , which assigns a dependency analysis.7 4.2 Experimental methodology . For machine learning , we make use of the Tilburg Memory-Based Learner ( TiMBL ) ( Daelemans et al. , 2004 ) .8 Memory-based learning is a supervised machine learning method characterized by a lazy learning algorithm which postpones learning until classification time , using the k-nearest neighbor algorithm for the classification of unseen instances . For animacy classification , the TiMBL parameters are optimized on a subset of the full data set.9 For training and testing of the classifiers , we make use of leave-one-out cross-validation . The baseline represents assignment of the majority class ( inanimate ) to all nouns in the data set . Due to the skewed distribution of classes , as noted above , the baseline accuracy is very high , usually around 90 % .Clearly , however , the class-based measures of precision and recall , as well as the combined F-score measure are more informative for these results . The baseline F-score for the animate class is thus 0 , and a main goal is to improve on the rate of true positives for animates , while limiting the trade-off in terms of performance for 6 http : //www.maltparser.org 7 For part-of-speech tagging , we employ the MaltTagger . a HMM part-of-speech tagger for Swedish ( Hall , 2003 ) . For parsing , we employ MaltParser ( Nivre et al. , 2006a ) , a language-independent system for data-driven dependency parsing , with the pretrained model for Swedish , which has been trained on the tags output by the tagger . 8 http : //ilk.uvt.nl/software.html 9 For parameter optimization we employ the paramsearch tool , supplied with TiMBL , see http : //ilk.uvt.nl/software.html . Paramsearch implements a hill climbing search for the optimal settings on iteratively larger parts of the supplied data . We performed parameter optimization on 20 % of the total data set , where we balanced the data with respect to frequency . The resulting settings are k = 11 , GainRatio feature weighting and Inverse Linear ( IL ) class voting weights . Bin Instances Baseline MBL SVM > 1000 291 89.3 97.3 95.2 > 500 597 88.9 97.3 97.1 > 100 1668 90.5 96.8 96.9 > 50 2278 90.6 96.1 96.0 > 10 3786 90.8 95.4 95.1 > 0 5481 91.3 93.9 93.7 the majority class of inanimates , which start out with F-scores approaching 100 . For calculation of the statistical significance of differences in the performance of classifiers tested on the same data set , McNemar s test ( Dietterich , 1998 ) is employed . We observe a clear improvement on all data sets ( p < .0001 ) , compared to the respective baselines . As we recall , the data sets are successively larger , hence it seems fair to conclude that the size of the data set partially counteracts the lower frequency of the test nouns . It is not surprising , however , that a method based on dis- tributional features suffers when the absolute frequencies approach 1 . We obtain results for ani- macy classification , ranging from 97.3 % accuracy to 93.9 % depending on the sparsity of the data . We find that classification of the inanimate class is quite stable throughout the experiments , whereas the classification of the minority class of animate nouns suffers from sparse data . It is an important point , however , that it is largely recall for the animate class which goes down with increased sparseness , whereas precision remains quite stable . All of these properties are clearly advantageous in the application to realistic data sets , where a more conservative classifier is to be preferred . The human reference annotation of the Talbanken05 nouns distinguishes only the classes corresponding to human and inanimate along the A n i m a t e Inanimate Prec isio n Rec all Fsc ore Prec isio n Rec all Fsco re > 10 00 > 50 0 > 10 0 > 50 > 10 > 0 8 9 . 7 Table 3 : Precision , recall and F-scores for the two classes in MBL-experiments with a general feature space . An interesting question is whether the errors show evidence of the gradience in categories discussed earlier and explicitly expressed in the annotation scheme by Zaenen et.al . If so , we would expect erroneously classified inanimate nouns to contain nouns of intermediate animacy , such as animals and organizations . The error analysis examines the performance of the MBLclassifier employing all features on the > 10 data set in order to abstract away from the most serious effects of data sparseness . If we examine the errors for the inanimate class we indeed find evidence of gradience within this category . The errors contain a group of nouns referring to animals and other living beings ( bacteria , algae ) , as listed in ( 9 ) , as well as one noun referring to an intelligent machine , included in the intermediate animacy category in Zaenen et al . Collective nouns with human reference and organizations are also found among the errors , listed in ( 11 ) . We also find some nouns among the errors with human denotation , listed in ( 12 ) . These are nouns which typically occur in dereferencing contexts , such as titles , e.g . herr mister , biskop bishop and which were annotated as nonhuman referring by the human an- notators.10 Finally , a group of abstract , human 10In fact , both of these showed variable annotation in the treebank and were assigned their majority class inanimate denoting nouns are also found among the errors , as listed in ( 13 ) . In summary , we find that nouns with gradient animacy properties account for 53.1 % of the errors for the inanimate class . ( 9 ) Animals/living beings : alg algae , apa monkey , bakterie bacteria , bjo rn bear , djur animal , fa gel bird , fladdermo ss bat , myra ant , ma s seagull , parasit parasite ( 10 ) Intelligent machines : robot robot ( 11 ) Collective nouns , organizations : myndighet authority , nation nation , fo retagsledning corporate-board , personal personell , stiftelse foundation , idrottsklubb sport-club ( 12 ) Human-denoting nouns : biskop bishop , herr mister , nationalist nationalist , tolk interpreter ( 13 ) Abstract , human nouns : fo rlorare loser , huvudpart main-party , konkurrent competitor , majoritet majority , va rd host It is interesting to note that both the human and automatic annotation showed difficulties in ascertaining class for a group of abstract , human-denoting nouns , like individ individual , motsta ndare opponent , kandidat candidate , representant representative . These were all assigned to the animate majority class during extraction , but were misclassified as inanimate during classification . In order to evaluate whether the classification method generalizes to a different machine learning algorithm , we design an identical set of experiments to the ones presented above , but where classification is performed with Support Vector Machines ( SVMs ) instead of MBL . We use the LIB- SVM package ( Chang and Lin , 2001 ) with a RBF kernel ( C = 8.0 , = 0.5 ) .11 in the extraction of training data . 11 As in the MBL-experiment , parameter optimization , i.e. , choice of kernel function , C and values , is performed on 20 % of the total data set with the easy.py tool , supplied with LIBSVM . 5 Parsing with animacy information . As an external evaluation of our animacy classifier , we apply the induced information to the task of syntactic parsing . Seeing that we have a tree- bank with gold standard syntactic information and gold standard as well as induced animacy information , it should be possible to study the direct effect of the added animacy information in the assignment of syntactic structure . We use the freely available MaltParser system , which is a language-independent system for data- driven dependency parsing ( Nivre , 2006 ; Nivre et al. , 2006c ) . A set of parsers are trained on Talbanken05 , both with and without additional an- imacy information , the origin of which is either the manual annotation described in section 3 or the automatic animacy classifier described in section 4.24.4 ( MBL ) . The common nouns in the treebank are classified for animacy using leave- one-out training and testing . This ensures that the training and test instances are disjoint at all times . Moreover , the fact that the distributional data is taken from a separate data set ensures non- circularity since we are not basing the classification on gold standard parses . All parsing experiments are performed using 10-fold cross-validation for training and testing on the entire written part of Talbanken05 . Overall parsing accuracy will be reported using the standard metrics of labeled attachment score ( LAS ) and unlabeled attachment score ( UAS ) .13 Statistical significance is checked using Dan Bikel s randomized parsing evaluation comparator.14 As our baseline , we use the settings optimized for Swedish in the CoNLL-X shared task ( Buchholz 12The SVMclassifiers generally show slightly lower results , however , only performance on the > 1000 data set is significantly lower ( p < .05 ) . 13LAS and UAS report the percentage of tokens that are assigned the correct head with ( labeled ) or without ( unlabeled ) the correct dependency label . and Marsi , 2006 ) , where this parser was the best performing parser for Swedish . The addition of automatically assigned animacy information for common nouns ( Anim ) causes a small , but significant improvement in overall results ( p < .04 ) compared to the baseline , as well as the corresponding gold standard experiment ( p < .04 ) . In the gold standard experiment , the results are not significantly better than the baseline and the main , overall , improvement from the gold standard animacy information reported in vrelid and Nivre ( 2007 ) and vrelid ( 2008 ) stems largely from the animacy annotation of pronouns.15 This indicates that the animacy information for common nouns , which has been automatically acquired from a considerably larger corpus , captures distributional distinctions which are important for the general effect of animacy and furthermore that the differences from the gold standard annotation prove beneficial for the results . A closer error analysis shows that the performance of the two parsers employing gold and automatic animacy information is very similar with respect to dependency relations and we observe an improved analysis for subjects , ( direct and indirect ) objects and subject predicatives with only minor variations . This in itself is remarkable , since the covered set of animate instances is notably smaller in the automatically annotated data set . We furthermore find that the main difference between the gold standard and automatic Anim-experiments 15 Recall that the Talbanken05 treebank contains animacy information for all nominal elements pronouns , proper and common nouns . When the totality of this information is added the overall parse results are significantly improved ( p < .0002 ) ( vrelid and Nivre , 2007 ; vrelid , 2008 ) . does not reside in the analysis of syntactic arguments , but rather of non-arguments . One relation for which performance deteriorates with the added information in the gold Anim-experiment is the nominal postmodifier relation ( ET ) which is employed for relative clauses and nominal PP- attachment . With the automatically assigned feature , in contrast , we observe an improvement in the performance for the ET relation , compared to the gold standard experiment , from a F-score in the latter of 76.14 to 76.40 in the former . Since this is a quite common relation , with a frequency of 5 % in the treebank as a whole , the improvement has a clear effect on the results.The parser s analysis of postnominal modifica tion is influenced by the differences in the added animacy annotation for the nominal head , as well as the internal dependent . If we examine the corrected errors in the automatic experiment , compared to the gold standard experiment , we find elements with differing annotation . Preferences with respect to the animacy of prepositional complements vary . In ( 14 ) , the automatic annotation of the noun djur animal as animate results in correct assignment of the ET relation to the preposition hos among , as well as correct nominal , as opposed to verbal , attachment . This preposition is one of the few with a preference for animate complements in the treebank . In contrast , the example in ( 15 ) illustrates an error where the automatic classification of barn children as inanimate causes a correct analysis of the head preposition om about .16 This article has dealt with an empirical evaluation of animacy annotation in Swedish , where the main focus has been on the use of such annotation for computational purposes.We have seen that human annotation for ani macy shows little variation at the type-level for a binary animacy distinction . Following from this observation , we have shown how a type- level induction strategy based on morphosyntac- tic distributional features enables automatic ani- macy classification for noun lemmas which furthermore generalizes to different machine learning algorithms ( MBL , SVM ) . We obtain results for an- imacy classification , ranging from 97.3 % accuracy to 93.9 % depending on the sparsity of the data . With an absolute frequency threshold of 10 , we obtain an accuracy of 95.4 % , which constitutes a50 % reduction of error rate . A detailed error anal ysis revealed some interesting results and we saw that more than half of the errors performed by the animacy classifier for the large class of inanimate nouns actually included elements which have been assigned an intermediate animacy status in theoretical work , such as animals and collective nouns . The application of animacy annotation in the task of syntactic parsing provided a test bed for the applicability of the annotation , where we could contrast the manually assigned classes with the automatically acquired ones . The results showed that the automatically acquired information gives a slight , but significant improvement of overall parse results where the gold standard annotation ( 14 ) . societies hos among olika different djur animals does not , despite a considerably lower coverage . This is a suprising result which highlights impor . social organizations among different animals tant properties of the annotation . First of all , the ( 15 ) Fo ra ldrar parents har have va rdnaden custody-D E F om sina of their barn children automatic annotation is completely consistent at the type level . Second , the automatic animacy Parents have the custody of their children A more thorough analysis of the different factors involved in PP-attachment is a complex task which is clearly beyond the scope of the present study . We may note , however , that the distinctions induced by the animacy classifier based purely on linguistic evidence proves useful for the analysis of both arguments and non-arguments . 16Recall that the classification is based purely on linguistic evidence and in this respect children largely pattern with the inanimate nouns . A child is probably more like a physical object in the sense that it is something one possesses and otherwise reacts to , rather than being an agent that acts upon its surroundings . classifier captures important distributional properties of the nouns , exemplified by the case of nominal postmodifiers in PP-attachment . The automatic annotation thus captures a purely linguistic notion of animacy and abstracts over contextual influence in particular instances . Animacy has been shown to be an important property in a range of languages , hence animacy classification of other languages constitutes an interesting line of work for the future , where empirical evaluations may point to similarities and differences in the linguistic expression of animacy . A new hybrid approach to the coreference resolution problem is presented . The COR , UDISsystem ( COreference R , Ules with Disambiguation Statistics ) combines syntactico-semantic rules with statistics derived from an annotated corpus . First , the rules and corpus annotationsare described and exemplified . Then , the coreference resolution algorithm and the involved statistics are explained . Finally , the proposed method is evaluated against a baseline modeland some directions for further research are indicated . Coreference resolution is a central problem innatural language understanding since coreference links play an important role for text coher ence. & apos ; In sentence ( 1 ) for instance , one wants to know what the German personal pronouns sic and ihr refer to . Both can refer to Madchen or Zeitung because grammatical gender agreement in German can be overruled by natural gender agreement in certain cases . ( 1 ) [ Das Mddehenji hest [ die The girl+NEUT reads the Zeitungli ; danach geht newspaper+FEM ; afterwards goes sie mit ihTi fins she+FEM with her+FEM in the Bitralk . & apos ; The girl reads the newspaper ; afterwards she goes to the office with it. & apos ; 11 would like to thank Hermann Helbig , Rainer Osswald , and the anonymous reviewers for their helpful com ments and suggestions . The task in this paper is similar to theMUC coreference task ( Hirschman and Chin chor , 1997 ) 2 : only identity coreference is treated ( and not part-whole or other complex semantic relationships ) ; only noun phrases ( NPs ) are considered asmarkables for coreference ( and not situa tions expressed by clauses etc . ) .This kind of coreference is an equivalence rela tion so that coreference resolution comes down to finding the correct partition3 of markables . If there exists a genuine ambiguity for humanreaders ( and not just a spurious one for com puters ) , several partitions of markables wouldbe the correct answer to the coreference prob lem . But since such ambiguities are rare the disambiguation method described in this paper always delivers only one partition . In this paper , the full MUC coreference taskis tackled with a new hybrid approach combin ing syntactico-semantic rules with rule statisticsderived from an annotated corpus . Two ques tion might arise . Why not a purely statisticalapproach : first , because why throw away tradi tional linguistic knowledge , and second , becausestatistics on rules reduce the sparse data prob lem since the applicability of one rule classifies combinations of many relevant features into onefeature value . Why not a purely rule-based approach : because it would leave too many alter natives and would not indicate which to choose . 2Some problems of this task definition are discussed by van Deemter and Kibble ( 2000 ) . 3A partition of a set S is a set of pairwise disjoint subsets of S ( the partition elements ) that cover S. Two kinds of data are required for the corefer ence resolution method described in section 3 : handcrafted rules defining whether two mark ables can corefer or not and a corpus annotated with coreference information . The rules licensepossible coreferences ; the corpus is used for scoring alternative coreference partitions with esti mated probabilities . The coreference rules are designed to licensepossible coreference relations among two mark ables . Some rules are language-dependent , some are universal ; in this paper , the rules ( and the corpus ) are for German , but the approach suits other languages as well . Each rule consists of a unique name , a premise , and a conclusion . For development and maintenance reasons , arule is accompanied by a description , some pos itive example texts , and some negative example texts . A positive example shows that the rule premise is satisfied and the conclusion that the two markables at hand are coreferential would be correct , whereas a negative example shows that the rule premise is not satisfied and theconclusion would indeed be incorrect for the ex ample . The rule premise is a conjunction of ( possibly negated ) constraints ; these can be constituent constraints ( c-constraints ) referring to featurevalues of one markable and interconstituent con straints ( ic-constraints ) referring to feature values of both markables that are to be tested for coreference . Both types of constraints can be attribute-value equations . The features used in coreference rules are listed in ; the feature values for markables stem from a parser using a semantically oriented lexicon currently containing 14000 German lexemes ( HaGenLex ) .A feature value can be a single type or a disjunc tion of types . Furthermore , one can constructconstraints with predicates . The most important predicates are given in : they realize concepts from Dependency Grammar ( de pend/2 ) and Government and Binding Theory ( c-command/2 ) or define simple relationshipsbetween constituents ( e. g. compatible-gend-n gend/2 ) .The conclusion of a rule expresses a corefer ence relation with a semantic network ( basedon the MultiNet formalism defined by Hel big ( 2001 ) which has been applied in several other projects , see ( Hartrumpf , 1999 ; Knoll etal. , 1998 ) ) . For identity coreference , a rela tion named EQU ( equivalence ) leading from the anaphor ( called c2 in rules ) 4 to the antecedent ( called c1 in rules ) suffices.Seven rules from the eighteen rules cur rently used are given in . The rule ident.gend_conflict would license a link between das Mddchen and sic in sentence ( 1 ) . The premise and conclusion can also be viewed as one attribute value matrix employing structure sharing for expressing ic-constraints . A corpus ( a collection of German newspaper articles from the Sitddeutsche Zeitung ) is anno tated for coreference according to the guidelinesfor the MUC coreference task adapted from En glish to German . The annotations are inserted as SGML tags into the corpus , which is already marked up according to the Corpus Encoding Standard ( Ide et al. , 1996 ) . To resolve coreference ambiguities , one must find the partition of markables that corresponds to the correct coreference equivalence relation . The search space is huge since the number of different partitions for n markables is equal to the Bell number B ( n ) . These numbers are also called Exponential numbers , see ( Bell , 1934 ) ; some example values are : B ( 1 ) = 1 , B ( 2 ) = 2 , B ( 3 ) = 5 , B ( 4 ) = 15 , B ( 5 ) = 52 , B ( 10 ) = 4R.ules for cataphora are also among the coreference rules . In such rules , cl corresponds to the cataphor and c2 to the postcedent . feature name use* descriptionCAT syntactic category en ( noun ) , perspro ( personal pronoun ) , possdet ( pos sessive determiner ) , reflpro ( reflexive pronoun ) , etc . ) ENTITY ic semantic classification comprising the semantic sort ( feature SORT ) andsemantic Boolean features ( currently 16 , all defined for the MultiNet ( mul tilayered extended semantic network ) formalism by Helbig ( 2001 ) ) ETYPE ic extension type ( 0 ( an individual ) , 1 ( a set ) , 2 ( a set of sets ) , etc . ) , part of the complex feature LAY containing other extensional and intensional layer features like CARD ( cardinality ) GEND ic gender ( syntactic ; masculine , feminine , and neuter in German ) NUM c , ic number ( syntactic ; singular and plural in German ) PERS c , ic person ( only tested jointly with the other agreement features GEND and NUM ) PROPER proper noun ( Boolean feature ) REFER c , ic reference type ( determinate , indeterminate ; based on article choice ) SENTENCE-ID ic sentence number in text SORT semantic sort ( 45 hierarchically ordered values , 15 of them for nominal concepts ) *c means : feature is used in c-constraints ; ic means : feature is used in ic-constraints . : Features in coreference rules predicate name/arity description =/2 c-command/2 compatible-gend-n-gend/2 The values are unifiable . The first argument ( a constituent ) c-commands the second.The grammatical gender value at the first argument position is compatible with the natural gender value at the second argument posi tion . The first argument ( a possessive determiner ) can refer to the second argument ( a constituent ) . The arguments ( two constituents ) are related by a copula . The first argument ( a constituent ) depends on the second . Numerical difference between two feature values is greater than a third value . Two constituents containing ( possibly complex ) names match . The argument ( a feature value ) is maximal , i. e. , a leaf node in the type hierarchy . One argument ( a constituent ) is a compound suffix of the other argument ( a constituent ) or both arguments have the same nominal head . compatible-possdet/2 copula-related/2 depend/2 difference & gt ; /3 matching-names/2 maximal/1 similar-nouns/2 : Predicates in coreference rules 115975 , B ( 15 ) 1.38x 109 , B ( 20 ) 5.17x 1013 , B ( 25 ) 4.64 x 1018.The evaluated algorithm for coreference resolution is implemented as the COR , UDIS sys tem ( COreference R , Ules with Disambiguation Statistics ) and works as follows : 1 . The markables in a given text are iden- . For this task and for gaining thesyntactico-semantic feature values to be ac cessed by rules in step 2 , each sentence inthe text is parsed independently . If a sentence parse fails , a chunk parse is gener desc . ( el CAT ) n ( e2 cm ) perspro ( = ( el Num ) ( e2 NUM ) ) ( = ( C1 PERS ) ( C2 PERS ) ) ( = ( C1 GEND ) ( C2 GEND ) ) ( = ( C1 ENTITY ) ( C2 ENTITY ) ) ( not ( c-command el e2 ) ) ( not ( c-command ( 2 el ) ) desc . same gender - anaphoric exam . Per Mann liest [ das Buch ] i lir versteht [ es ] nicht . ( Ci CAT ) perspro ( e2 eAT ) n ( = ( el Num ) ( e2 NUM ) ) ( = ( C1 PERS ) ( C2 PERS ) ) ( = ( C1 GEND ) ( C2 GEND ) ) ( = ( C1 ENTITY ) ( C2 ENTITY ) ) ( not ( c-command cl c2 ) ) ( not ( c-command c2 Ci ) ) ( difference & gt ; ( C1 SENTENCETD ) ( C2 SENTENCE-ID ) 0 ) desc . personal pronoun - cataphoric exam . [ Sie ] i will die Welt andern ; und Mie Wissenschaftlerin ] i macht sich frisch ans Werk . ( Ci CAT ) perspro ( C2 CAT ) perspro ( = ( el Num ) ( e2 NUM ) ) ( = ( Cl PERS ) ( C2 PERS ) ) ( = ( C1 GEND ) ( C2 GEND ) ) ( = ( C1 ENTITY ) ( C2 ENTITY ) ) ( not ( c-command cl c2 ) ) ( not ( c-command c2 Ci ) ) desc . same gender - anaphoric exam . [ Sie ] i schreiben viel . Und [ sie ] lesen viel . ( Ci CAT ) n ( c2 CAT ) perspro ( = ( el Num ) ( e2 NUM ) ) ( = ( C1 PERS ) ( C2 PERS ) ) ( not ( = ( C1 GEND ) ( C2 GEND ) ) ) ( compatible-gencl-n-gencl ( C2 GEND ) ( C1 N-GEND ) ) ( not ( c-command cl c2 ) ) ( not ( c-command c2 Ci ) ) desc . A personal pronoun refers to an NY with a nominal head and conflicting grammatical gender . [ Das Madchen ] i lacht . [ Sie ] i war stets so . ident.nuna_conflict ( el CAT ) n ( Cl PROPER ) noproper ( C2 CAT ) n ( e2 PROPER ) noproper ( not ( = ( el Nem ) ( e2 Nem ) ) ) ( = ( C1 =TYE ) ( C2 ETYPE ) ) ( = ( C1 ENTITY ) ( C2 ENTITY ) ) ( not ( c-command el e2 ) ) ( not ( c-command C2 el ) ) different number values ( one aggregate and one nonaggre gate but equal etype values ) Per Vorstand ] i entschied riber die Entlassungen . [ these Manner ] i hatten keine Skrupel . ident.sinailar_sem ( Ci CAT ) n ( Ci soul ] ) co ( c2 CAT ) n ( c2 Ithrhit ) det ( e2 PROPER ) noproper ( = ( el Num ) ( e2 Nutt ) ) ( similar-nouns cl c2 ) ( difference & gt ; ( C1 SENTENCE-ID ) ( C2 SENTENCE-ID ) 0 ) two semantically similar NPs . Cases contained in similar nouns : compound and base noun ; synonyms . Mer Buchautor ] i ... [ der Autor ] i Mie Grol3stadte ] i Mie Stadte ] i [ Krankenhaus ] i [ Klinik ] i ident.compatible_sem ( Ci CAT ) n ( Ci soul ] ) co ( C1 PROPER ) noproper ( e2 CAT ) n ( e2 Ithrhit ) det ( e2 PROPER ) noproper ( = ( el NUM ) ( C2 NUM ) ) ( = ( C1 =TYE ) ( C2 ETYPE ) ) ( = ( C1 ENTITY ) ( C2 ENTITY ) ) ( not ( similar-nouns cl c2 ) ) ( maximal ( C1 ENTITY ) ) ( maximal ( c2 ENTITY ) ) ( difference & gt ; ( C1 SENTENCE-ID ) ( C2 SENTENCE-ID ) 0 ) two semantically compatible NPs . Mie Tater ] Mie Manner ] i [ einer hollandischen Fanailie ] i [ die entfiihrte Deutsche ] j id pre . : Example coreference rules ated . ( In such cases , constraints in rule premises that involve predicates requiring full parses ( e. g. c-command ) are ignored instep 2 . ) For details on the parser , see ( Hel big and Hartrumpf , 1997 ) . All possible coreference rule activations . that link an anaphor to an antecedent candidate are collected . This is done by test ing rule premises on all markable pairs ( constituent c1 must precede constituent c2 ) . For two markables , one rule ( at most ) is activated since the rules have disjoint premises for real text purposes . For each anaphor , one antecedent candi- . This decision is based on rule statistics gained from the annotatedtraining corpus . The sparse data prob lem is alleviated by backed-off estimation ( see for example ( Katz , 1987 ; Collins and Brooks , 1995 ) ) .The algorithm deals with three sets of ob jects : first , the possible anaphors ( all identified markables ) ; second , the candidate antecedentsfor each possible anaphor ( all preceding markables and the artificial nonreferable mark able explained below ) ; third , the coreference rules . The nonreferable markable is used asthe artificial anaphor of a nonreferring markable in order to represent all alternative references for a possible anaphor as a pair . For first mentions , the disambiguation algorithm shouldselect a coreference with the nonreferable mark able as antecedent . Currently , one rule licenses the nonreferable markable as antecedent . But it might be useful to apply more finely grained rules and not just one rough licensing rule , asindicated by promising research results for definite descriptions referring to discourse-new en tities ( see ( Vieira and Poesio , 2000 ) ) . antecedent candidates Step 3 of the algorithm given in section 3.1 isthe most interesting one and needs some expla nation . Leaving the issue of search algorithms aside for a moment , all possible and licensed partitions of identified markables are generated , filtered , and finally scored using estimated prob abilities . The partitions are generated incrementallystarting with the first possible anaphor in a sin gleton partition element . For each antecedent candidate licensed by a coreference rule instep 2 , an extended partition with this an tecedent in the same partition element as the anaphor in question is introduced . This process is iterated until all possible anaphors have been investigated . Partitions are filtered out if they violate one of the following distance and compatibility constraints : sentence distance The distance between the anaphor and the antecedent measured in sentences must be below the limit for the linking coreference rule . These limits have been learned from the training corpus . paragraph distance The distance betweenthe linked markables measured in para graphs must be below the limit learned for the licensing coreference rule . Typically , pronominal anaphoras can span only two paragraphs , while for example coreferences between named entities can span arbitrary distances . semantic compatibility All markables in apartition element must bear compatible se mantics ( unifiable ENTITY and LAY feature values , see ) .Because of the huge search space ( see sec tion 3.1 ) , the generation of partitions and thefiltering is intertwined in a heuristic search al gorithm so that impossible alternatives in thesearch tree are pruned early . Also the scor ing described below is done during the search so that alternatives with low ( bad ) scores can be delayed and possibly discarded early by the search algorithm . The score for a partition is constructed as the sum of estimated probabilities for addingthe possible anaphor in currently under inves tigation to one of the antecedent candidates C = Kei , e2 , , ek ) . The candidates are orderedby distance ; each ci is a feature structure rep resenting the parse result from algorithm step 1for the corresponding markable . Each coreference between in and ci is licensed by a coreference rule ri so that this coreference alterna tive can be represented as the triple ( m , In order to generalize from the token-based representation ( rn , ci , ri ) and to make usefulstatistics from an annotated corpus , an ab straction function a is applied that abstracts from the given anaphor , antecedent candidate , and linking coreference rule to a type-based representation . The abstraction function inequation ( 3 ) turned out to be a good compro mise between limited sparseness of statistical matrices and distinctiveness for disambiguation purposes : It reduces a coreference alternative ( m , ci , ri ) to the candidate antecedent position i and the licensing coreference rule a ( m , ci , ri ) : = ( i , ri ) ( 3 ) Let ai be the abstracted coreference alternative a ( m , c , ri ) and A be the list ( al , a2 , , ak ) of abstracted coreference alter natives for the possible anaphor in . Then , the probability that ai corresponds to the closestcorrect antecedent for in is estimated as the rel ative frequency rf ( i , A ) : rf ( i , A ) : = kf ( i , A ) ( 4 ) f ( 1 , A ) 1=1 The uses the statistical values f ( i , A ) , which count how many times in the annotatedtraining corpus the abstracted coreference alter native ai wins as the one with the closest correctantecedent in the context of abstracted corefer ence alternatives A . Further experiments have shown that looking at more than 5 antecedent candidates does not improve disambiguation results . Therefore , k is reduced to 5 if necessary . Backed-off estimation can alleviate sparse data problems . The basic idea is that if for a context A no statistical values are known , they are estimated by looking at increasingly smaller parts of A until statistical values are found . Onemight call such a backed-off estimation backed off estimation over alternatives . Backed-off estimation as defined by equations ( 5 ) to ( 7 ) is applied in the coreference resolution method when all counts f ( i , A ) are zero and f3 ( i , A ) is calculated for j = 1 . The parameter j is increased by one until one of the f3 ( i , A ) becomes positive ( then , the rP ( i , A ) are used as scores for the antecedent candidates ) or j reaches k 1 ( in this case , allcandidates receive equal scores ) . If the backoff process stops at j = b , the relative frequencies rfb ( i , A ) are used as estimates for the con ditional probabilities P ( i1C ) that ci is the closest correct antecedent given antecedent candidates C : P ( ir ) rfb ( i , A ) ( 8 ) One could add other scores to those based on estimated probabilities . In the literature , syntactic parallelism between anaphor and an tecedent ( based on syntactic case ) , semanticparallelism ( based on semantic roles ) , and max imality of antecedent NPs are proposed among others . In several experiments , such additional scores have been applied for certain rules ( e. g. rules involving pronouns ) . Small improvements have been achieved , but this topic has not been investigated completely yet . Evaluation results from 12-fold cross-validation for 502 anaphors5 are listed in . The standard definitions for recall and precision used in information retrieval are as follows : # true positives true positives # false negatives # true positives ( 10 ) true positives # false positives For coreference resolution , true positives are correct coreference links found , false negatives are correct coreference links not reported , andfalse positives are incorrect coreference links re ported . ( 1995 ) illustrate that thesedefinitions sometimes yield counter-intuitive re sults for coreference evaluations and proposemodel-theoretic definitions of recall and preci sion . The values in are calculated with these modified definitions . There are three different evaluation results . The first is the full coreference task . The secondone could be called markable-relative evalu ation since the numbers are calculated only forthe markables that have been successfully iden tified ( in some sense , this concentrates on the coreference relation aspect of the coreference task ) . And the final evaluation result comesfrom a baseline model : & amp ; quot ; always select the clos est antecedent candidate that is licensed by a rule and fulfills the distance and compatibility constraints from section 3.2 & amp ; quot ; . There are many recent approaches to this prob lem , e. g. syntax-based approaches ( Lappin and Leass , 1994 ) , cooccurrence-based approaches ( Dagan and Itai , 1990 ) , machine-learning approaches ( Connolly et al. , 1994 ; Aone and Ben nett , 1996 ; Soon et al. , 1999 ) , uncertainty reasoning approaches ( Mitkov , 1995 ; Mitkov , 1997 ) , and robust knowledge-poor approaches ( Kennedy and Boguarev , 1996 ; Baldwin , 1997 ; 5The number of markables that are coreferential with some other markable ranges from 28 to 63 for the foldsbecause the texts in the evaluation corpus were not bro ken up for cross-validation in order to yield statistical data about whole texts . Therefore the training corpussize varied between 439 and 474 and the test corpus be tween 28 and 63 during the cross-validation . fo ( i , A ) f ( i , A ) f3 ( i , A ) : = fi- & apos ; ajeA/CA,011=k j for 1 & lt ; j & lt ; k 1 rf ( i , A ) : = P ( i , A ) 1=i for 0 & lt ; j & lt ; k-1 : = P : = ( 9 ) method evaluation results in percentage coreference ( incl . markable identification ) markable-relative coreference evaluation baseline : always closest candidate recall precision F-measure 55 82 66 76 82 79 46 42 44 F-measure is calculated with equal weight to recall r and precision p as 2r : Coreference resolution results r p Mitkov , 1998b ; Mitkov , 1999 ) .6 The following two systems tackle the MUC coreference task and bear some similarities to COR , UDIS . The system described by Cardie and Wagstaff ( 1999 ) resembles the presented system in that it views coreference resolution in a text as partitioning ( or clustering ) . The difference in terms of clustering is that the first system usesgreedy clustering while COR , UDIS optimizes us ing global scores . The fundamental difference isthat the first system partitions based on a simi larity function over markable representations as attribute value pairs , while COR , UDIS applies linguistic rules to license possible coreference links and applies corpus statistics to choose one link because typically alternatives exist.The SWIZZLE system ( Harabagiu and Maiorano , 2000 ) applies heuristics and heuristic or dering by bootstrapping to pick one antecedent per anaphor ; in the COR , UDIS system , rules license alternatives and one is selected based on a learned statistical model . COR , UDIS usessentence parsing , SWIZZLE as an intention ally knowledge-poor approach only approximate phrasal parsing . I have presented a disambiguation methodwhich combines traditional linguistically moti vated rules and a backed-off statistical model derived form an annotated corpus in a powerfulway . Comparison to other approaches is diffi cult since evaluation results for German are not available for the MUC coreference task . But theresults presented seem to be competitive corn 6 The cited works deal only with pronominal . anaphors , except the approaches by Aone and Bennett ( 1996 ) , Baldwin ( 1997 ) , Connolly et al . ( 1994 ) , and Soon et al . pared to the 60 % F-measure results for English in MUC7 . Additional filtering conditions , additionalscores ( preferences ) , and features from Center ing Theory ( Grosz et al. , 1995 ) might improve the results reported in this paper significantly . The use of a large lexical-semantic network likeGermaNet would solve some problematic coref erence cases . More sophisticated evaluationscentered around different error types as recom mended by Mitkov ( 1998a ) and larger data sets are planned for the future . Lightweight semantic annotation of textcalls for a simple representation , ideally without requiring a semantic lexicon to achieve good coverage in the language and domain.In this paper , we repurpose WordNet s super- sense tags for annotation , developing specificguidelines for nominal expressions and applying them to Arabic Wikipedia articles in four topical domains . The resulting corpus has high coverage and was completed quickly with reasonable inter-annotator agreement . The goal of lightweight semantic annotation of text , particularly in scenarios with limited resources and expertise , presents several requirements for arepresentation : simplicity ; adaptability to new lan guages , topics , and genres ; and coverage . This paper describes coarse lexical semantic annotationof Arabic Wikipedia articles subject to these con straints . Traditional lexical semantic representations are either narrow in scope , like named entities,1 or make reference to a full-fledged lexicon/ontology , which may insufficiently cover the language/domainof interest or require prohibitive expertise and ef fort to apply.2 We therefore turn to supersense tags ( SSTs ) , 40 coarse lexical semantic classes ( 25 fornouns , 15 for verbs ) originating in WordNet . Previ ously these served as groupings of English lexicon 1Some ontologies like those in Sekine et al . ( 2002 ) and BBN Identifinder ( Bikel et al. , 1999 ) include a large selection of classes , which tend to be especially relevant to proper names . 2E.g. , a WordNet ( Fellbaum , 1998 ) sense annotation effortreported by Passonneau et al . ( 2010 ) found considerable inter annotator variability for some lexemes ; FrameNet ( Baker etal. , 1998 ) is limited in coverage , even for English ; and Prop Bank ( Kingsbury and Palmer , 2002 ) does not capture semanticrelationships across lexemes . We note that the Omega ontology ( Philpot et al. , 2003 ) has been used for fine-grained cross lingual annotation ( Hovy et al. , 2006 ; Dorr et al. , 2010 ) . AD ACT TIME The Guinness Book of World Records considers the University of AlKaraouine in Fez , Morocco , established in the year 859 AD , the oldest university in the world . The Arabic is shown left-to-right . entries , but here we have repurposed them as target labels for direct human annotation . Part of the earliest versions of WordNet , the supersense categories ( originally , lexicographer classes ) were intended to partition all English noun and verb senses into broad groupings , or semanticfields ( Miller , 1990 ; Fellbaum , 1990 ) . SSTs both refine and relate lexical items : they capture lexical polysemy on the one hand e.g.,3Note that work in supersense tagging used text with fine grained sense annotations that were then coarsened to SSTs . 4The noun/verb distinction might prove problematic in some languages . Q .J K considers ~ JJ k. Guinness H. A~J book C J AJ @ A ; P @ that for-records the-standard Ag . university @ Q @ AlKaraouine Ai in Fez H. Q @ Morocco Ag . oldest university Y ~ @ in ARTIFACT LOCATION A @ the-world A D J A K established J ~ in year IJ ~ k was where LOCATION 253 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics , pages 253 258 , Jeju , Republic of Korea , 814 July 2012. c 2012 Association for Computational Linguistics Crusades Damascus Ibn Tolun Mosque Imam Hussein Shrine Islamic Golden Age Islamic History Ummayad Mosque 434s 16,185t 5,859m Atom Enrico Fermi Light Nuclear power Periodic Table Physics Muhammad alRazi 777s 18,559t 6,477m 2004 Summer Olympics Christiano Ronaldo Football FIFA World Cup Portugal football team Ra ul Gonz ales Real Madrid 390s 13,716t 5,149m . Computer Computer Software Internet Linux Richard Stallman Solaris X Window System 618s 16,992t 5,754m The 7 article titles ( translated ) in each domain , with total counts of sentences , tokens , and supersense mentions . Overall , there are 2,219 sentences with 65,452 tokens and 23,239 mentions ( 1.3 tokens/mention on average ) . Counts exclude sentences marked as problematic and mentions marked ? . POSSESSION for the noun principal and generalize across lexemes on the other e.g. , principal , teacher , and student can all be PERSONs . This lumping property might be expected to give too much latitude to annotators ; yetwe find that in practice , it is possible to elicit reason able inter-annotator agreement , even for a languageother than English . We encapsulate our interpreta tion of the tags in a set of brief guidelines that aims to be usable by anyone who can read and understand a text in the target language ; our annotators had no prior expertise in linguistics or linguistic annotation . Finally , we note that ad hoc categorization schemes not unlike SSTs have been developed for purposes ranging from question answering ( Li and Roth , 2002 ) to animacy hierarchy representation for corpus linguistics ( Zaenen et al. , 2004 ) . We believe the interpretation of the SSTs adopted here can serveas a single starting point for diverse resource en gineering efforts and applications , especially when fine-grained sense annotation is not feasible . WordNet s definitions of the supersenses are terse , and we could find little explicit discussion of the specific rationales behind each category . English examples are given , but the guidelines are intended to be language-neutral . A more systematic breakdown , formulated as a 43-rule decision list , is included with the corpus.5 In developing these guidelines we consulted English WordNet ( Fellbaum , 1998 ) and SemCor ( Miller et al. , 1993 ) for examples and synset definitions , occasionally making simplifying decisions where we found distinctions that seemed esoteric or internally inconsistent . Special cases ( e.g. , multiword expressions , anaphora , figurative 5For example , one rule states that all man-made structures ( buildings , rooms , bridges , etc . ) are to be tagged as ARTIFACTs . language ) are addressed with additional rules . The annotation in this work was on top of a smallcorpus of Arabic Wikipedia articles that had al ready been annotated for named entities ( Mohit et al. , 2012 ) . Here we use two different annotators , both native speakers of Arabic attending a university with English as the language of instruction . Data & amp ; procedure . The annotation task was to identify and categorize mentions , i.e. , occurrences of terms belonging tonoun supersenses . Working in a custom , browser based interface , annotators were to tag each relevanttoken with a supersense category by selecting the to ken and typing a tag symbol . Any token could be marked as continuing a multiword unit by typing & lt ; . If the annotator was ambivalent about a token they were to mark it with the ? Sentences werepre-tagged with suggestions where possible.6 Anno tators noted obvious errors in sentence splitting and grammar so ill-formed sentences could be excluded.Training . Over several months , annotators alternately annotated sentences from 2 designated arti cles of each domain , and reviewed the annotationsfor consistency . All tagging conventions were deve loped collaboratively by the author ( s ) and annotators during this period , informed by points of confusionand disagreement . WordNet and SemCor were con sulted as part of developing the guidelines , but not during annotation itself so as to avoid complicating the annotation process or overfitting to WordNet sidiosyncracies . The training phase ended once inter annotator mention FI had reached 75 % .6Suggestions came from the previous named entity annota tion of PERSONs , organizations ( GROUP ) , and LOCATIONs , as well as heuristic lookup in lexical resources Arabic WordNet entries ( Elkateb et al. , 2006 ) mapped to English WordNet , and named entities in OntoNotes ( Hovy et al. , 2006 ) . 254 O NATURAL OBJECT natural feature or nonliving object in nature barrier reef nest neutron star planet sky fishpond metamorphic rock Mediterranean cave stepping stone boulder Orion ember universe A ARTIFACT man-made structures and objects bridge restaurant bedroom stage cabinet toaster antidote aspirin L LOCATION any name of a geopolitical entity , as well as other nouns functioning as locations or regions Cote d Ivoire New York City downtown stage left India Newark interior airspace P PERSON humans or personified beings ; names of socialgroups ( ethnic , political , etc . ) kernel , version , distribution , environment ) . A connection is a RE LATION ; project , support , and a configuration are tagged as COGNITION ; development and collaboration are ACTs . Arabic conventions Masdar constructions ( verbal nouns ) are treated as nouns . Some examples and longer descriptions have been omitted due to space constraints . Below : A few domain- and language-specific elaborations of the general guidelines . 255 ( Counts are of the union of the annotators choices , even when they disagree . ) tag num tag num ACT ( ! ) 3473 LOCATION ( G ) 1583 COMMUNICATION ( C ) 3007 GROUP ( L ) 1501 PERSON ( P ) 2650 TIME ( T ) 1407 ARTIFACT ( A ) 2164 SUBSTANCE ( $ ) 1291 COGNITION ( ) 1672 QUANTITY ( Q ) 1022Main annotation . After training , the two annota tors proceeded on a per-document basis : first they worked together to annotate several sentences fromthe beginning of the article , then each was independently assigned about half of the remaining sentences ( typically with 5 10 shared to measure agreement ) . Throughout the process , annotators were en couraged to discuss points of confusion with each other , but each sentence was annotated in its entiretyand never revisited . Annotation of 28 articles re quired approximately 100 annotator-hours . Articles used in pilot rounds were re-annotated from scratch . Some of the most concrete tags BODY , ANIMAL , PLANT , NATURAL OBJECT , and FOOD were barely present , but would likely be frequent in life sciences domains . Others , such as MOTIVE , POSSESSION , and SHAPE , are limited in scope.To measure inter-annotator agreement , 87 sentences ( 2,774 tokens ) distributed across 19 of the ar ticles ( not including those used in pilot rounds ) wereannotated independently by each annotator . Interannotator mention Fl ( counting agreement over en tire mentions and their labels ) was 70 % . Excluding the 1,397 tokens left blank by both annotators , the token-level agreement rate was 71 % , with Cohen s n = 0.69 , and token-level Fl was 83 % .7We also measured agreement on a tag-by-tag basis . For 8 of the 10 most frequent SSTs ( fig ure 3 ) , inter-annotator mention Fl ranged from 73 % to 80 % . The two exceptions were QUANTITY at63 % , and COGNITION ( probably the most heterogeneous category ) at 49 % . An examination of the confusion matrix reveals four pairs of supersense cate gories that tended to provoke the most disagreement : COMMUNICATION/COGNITION , ACT/COGNITION , ACT/PROCESS , and ARTIFACT/COMMUNICATION . 7Token-level measures consider both the supersense label and whether it begins or continues the mention . Also in that sentence , an notators disagreed on the second use of university ( ARTIFACT vs. GROUP ) . As with any sense anno tation effort , some disagreements due to legitimate ambiguity and different interpretations of the tags especially the broadest ones are unavoidable . A soft agreement measure ( counting as matches any two mentions with the same label and at leastone token in common ) gives an Fl of 79 % , showing that boundary decisions account for a major portion of the disagreement . Further examples include the technical term thin client , for which one annotator omitted the adjective ; and World Cup Football Championship , where one an notator tagged the entire phrase as an EVENT while the other tagged football as a separate ACT . We have codified supersense tags as a simple an notation scheme for coarse lexical semantics , andhave shown that supersense annotation of Ara bic Wikipedia can be rapid , reliable , and robust ( about half the tokens in our data are coveredby a nominal supersense ) . Our tagging guide lines and corpus are available for download at http : //www.ark.cs.cmu.edu/ArabicSST/ . We thank Nourhen Feki and Sarah Mustafa for assistance with annotation , as well as Emad Mohamed , CMU ARK members , and anonymous reviewers for their comments.This publication was made possible by grant NPRP08 4851-083 from the Qatar National Research Fund ( a member of the Qatar Foundation ) . The statements made herein are solely the responsibility of the authors . Most previous research on verb clustering has focussed on acquiring flat classifications from corpus data , although many manually built classifications are taxonomic in nature . Also Natural Language Processing ( NLP ) applications benefit from taxonomic classifications because they vary in terms of the granularity they require from a classification . We introduce a new clustering method called Hierarchical Graph Factorization Clustering ( HGFC ) and extend it so that it is optimal for the task . Our results show that HGFC outperforms the frequently used agglomerative clustering on a hierarchical test set extracted from VerbNet , and that it yields state-of-the-art performance also on a flat test set . We demonstrate how the method can be used to acquire novel classifications as well as to extend existing ones on the basis of some prior knowledge about the classification . A variety of verb classifications have been built to support NLP tasks . These include syntactic and semantic classifications , as well as ones which integrate aspects of both ( Grishman et al. , 1994 ; Miller , 1995 ; Baker et al. , 1998 ; Palmer et al. , 2005 ; Kipper , 2005 ; Hovy et al. , 2006 ) . Classifications which integrate a wide range of linguistic properties can be particularly useful for tasks suffering from data sparseness . One such classification is the taxonomy of English verbs proposed by Levin ( 1993 ) which is based on shared ( morpho- ) syntactic 1023 and semantic properties of verbs . Levin s taxonomy or its extended version in VerbNet ( Kipper , 2005 ) has proved helpful for various NLP application tasks , including e.g . parsing , word sense disambiguation , semantic role labeling , information extraction , question-answering , and machine translation ( Swier and Stevenson , 2004 ; Dang , 2004 ; Shi and Mihalcea , 2005 ; Zapirain et al. , 2008 ) . Because verbs change their meaning and be- haviour across domains , it is important to be able to tune existing classifications as well to build novel ones in a cost-effective manner , when required . In recent years , a variety of approaches have been proposed for automatic induction of Levin style classes from corpus data which could be used for this purpose ( Schulte im Walde , 2006 ; Joanis et al. , 2008 ; Sun et al. , 2008 ; Li and Brew , 2008 ; Korhonen et al. , 2008 ; O Se aghdha and Copestake , 2008 ; Vlachos et al. , 2009 ) . The best of such approaches have yielded promising results . However , they have mostly focussed on acquiring and evaluating flat classifications . Levin s classification is not flat , but taxonomic in nature , which is practical for NLP purposes since applications differ in terms of the granularity they require from a classification . In this paper , we experiment with hierarchical Levin-style clustering . We adopt as our baseline method a well-known hierarchical method agglomerative clustering ( AGG ) which has been previously used to acquire flat Levin-style classifications ( Stevenson and Joanis , 2003 ) as well as hierarchical verb classifications not based on Levin ( Fer- rer , 2004 ; Schulte im Walde , 2008 ) . The method has also been popular in the related task of noun clus Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing , pages 1023 1033 , Edinburgh , Scotland , UK , July 27 31 , 2011 . Qc 2011 Association for Computational Linguistics tering ( Ushioda , 1996 ; Matsuo et al. , 2006 ; Bassiou and Kotropoulos , 2011 ) . We introduce then a new method called Hierarchical Graph Factorization Clustering ( HGFC ) ( Yu et al. , 2006 ) . This graph-based , probabilistic clustering algorithm has some clear advantages over AGG ( e.g . it delays the decision on a verb s cluster membership at any level until a full graph is available , minimising the problem of error propagation ) and it has been shown to perform better than several other hierarchical clustering methods in recent comparisons ( Yu et al. , 2006 ) . The method has been applied to the identification of social network communities ( Lin et al. , 2008 ) , but has not been used ( to the best of our knowledge ) in NLP before . We modify HGFC with a new tree extraction algorithm which ensures a more consistent result , and we propose two novel extensions to it . The first is a method for automatically determining the tree structure ( i.e . number of clusters to be produced for each level of the hierarchy ) . This avoids the need to predetermine the number of clusters manually . The second is addition of soft constraints to guide the clustering performance ( Vlachos et al. , 2009 ) . This is useful for situations where a partial ( e.g . a flat ) verb classification is available and the goal is to extend it . Adopting a set of lexical and syntactic features which have performed well in previous works , we compare the performance of the two methods on test sets extracted from Levin and VerbNet . When evaluated on a flat clustering task , HGFC outperforms AGG and performs very similarly with the best flat clustering method reported on the same test set ( Sun and Korhonen , 2009 ) . When evaluated on a hierarchical task , HGFC performs considerably better than AGG at all levels of gold standard classification . The constrained version of HGFC performs the best , as expected , demonstrating the usefulness of soft constraints for extending partial classifications . Our qualitative analysis shows that HGFC is capable of detecting novel information not included in our gold standards . The unconstrained version can be used to acquire novel classifications from scratch while the constrained version can be used to extend existing ones with additional class members , classes and levels of hierarchy . The taxonomy of Levin ( 1993 ) groups English verbs ( e.g . break , fracture , rip ) into classes ( e.g . 45.1 Break verbs ) on the basis of their shared meaning components and ( morpho- ) syntactic behaviour , defined in terms of diathesis alternations ( e.g . the causative/inchoative alternation , where an NP frame alternates with an intransitive frame : Tony broke the window The window broke ) . It classifies over 3000 verbs in 57 top level classes , some of which divide further into subclasses . The extended version of the taxonomy in VerbNet ( Kipper , 2005 ) classifies 5757 verbs . Its 5 level taxonomy includes 101 top level and 369 subclasses . We used three gold standards ( and corresponding test sets ) extracted from these resources in our experiments : T1 : The first gold standard is a flat gold standard which includes 13 classes appearing in Levin s original taxonomy ( Stevenson and Joanis , 2003 ) . We included this small gold standard in our experiments so that we could compare the flat version of our method against previously published methods . Following Stevenson and Joanis ( 2003 ) , we selected 20 verbs from each class which occur at least 100 times in our corpus . This gave us 260 verbs in total . T2 : The second gold standard is a large , hierarchical gold standard which we extracted from VerbNet as follows : 1 ) We removed all the verbs that have less than 1000 occurrences in our corpus . 2 ) In order to minimise the problem of pol- ysemy , we assigned each verb to the class which , according to VerbNet , corresponds to its predominant sense in WordNet ( Miller , 1995 ) . 3 ) In order to minimise the sparse data problem with very fine- grained classes , we converted the resulting classification into a 3-level representation so that the classes at the 4th and 5th level were combined . For example , the sub-classes of Declare verbs ( numbered as 29.4.1.1 . { 1,2,3 } ) were combined into 29.4.1 . 4 ) Theclasses that have fewer than 5 members were dis carded . The total number of verb senses in the resulting gold standard is 1750 , which is 33.2 % of the verbs in VerbNet . T2 has 51 top level , 117 second level , and 133 third level classes . T3 : The third gold standard is a subset of T2 where singular classes ( top level classes which do not divide into subclasses ) are removed . This gold standard was constructed to enable proper evaluation of the constrained version of HGFC ( introduced in the following section ) where we want to compare the impact of constraints across several levels of classification . T3 provides classification of 357 verbs into 11 top level , 14 second level , and 32 third level classes . For each verb appearing in T1T3 , we extracted all the occurrences ( up to 10,000 ) from the British National Corpus ( Leech , 1992 ) and North American News Text Corpus ( Graff , 1995 ) . 3.1 Features and feature extraction . Previous works on Levin style verb classification have investigated optimal features for this task ( Stevenson and Joanis , 2003 ; Li and Brew , 2008 ; Sun and Korhonen , 2009 ) ) . We adopt for our experiments a set of features which have performed well in recent verb clustering works : A : Subcategorization frames ( SCFs ) and their relative frequencies with individual verbs . B : A with SCFs parameterized for prepositions . C : B with SCFs parameterized for subjects appearing in grammatical relations associated with the verb in parsed data . D : B with SCFs parameterized for objects appearing in grammatical relations associated with the verb in parsed data . These features are purely syntactic . Although semantic features verb selectional preferences proved the best ( when used in combination with syntactic features ) in the recent work of Sun and Korhonen ( 2009 ) , we left such features for future work because we noticed that different levels of classification are likely to require semantic features at different granularities . We extracted the syntactic features using the system of Preiss et al . The system tags , lemma- tizes and parses corpus data using the RASP ( Robust Accurate Statistical Parsing toolkit ( Briscoe et al. , 2006 ) ) , and on the basis of the resulting grammatical relations , assigns each occurrence of a verb as a member of one of the 168 verbal SCFs . We pa- rameterized the SCFs as described above using the information provided by the system . We introduce the agglomerative clustering ( AGG ) and Hierarchical Graph Factorization Clustering ( HGFC ) methods in the following two subsections , respectively . The subsequent two subsections present our extensions to HGFC : ( i ) automatically determining the cluster structure and ( ii ) adding soft constraints to guide clustering performance . 3.2.1 Agglomerative clustering AGG is a method which treats each verb as a singleton cluster and then successively merges two closest clusters until all the clusters have been merged into one . We used the SciPy s implementation ( Oliphant , 2007 ) of the algorithm . The cluster distance is measured using linkage criteria . We experimented with four commonly used linkage criteria : Single , Average , Complete and Ward s ( Ward Jr. , 1963 ) . Ward s criterion performed the best and was used in all the experiments in this paper . It measures the increase in variance after two clusters are merged . The output of AGG tends to have excessive number of levels . Cut-based methods ( Wu and Leahy , 1993 ; Shi and Malik , 2000 ) are frequently applied to extract a simplified view . We followed previous verb clustering works and cut the AGG hierarchy manually . AGG suffers from two problems . The first is error propagation . When a verb is misclassified at a lower level , the error propagates to all the upper levels . The second is local pairwise merging , i.e . the fact that only two clusters can be combined at any level . For example , in order to group clusters representing Levin classes 9.1 , 9.2 and 9.3 into a single cluster representing class 9 , the method has to produce intermediate clusters , e.g . { 1,2 } and 9.3.Such clusters do not always have a semantic inter pretation . Although they can be removed using a cut-based method , this requires a predefined cutoff value which is difficult to set ( Stevenson and Joanis , 2003 ) . In addition , a significant amount of information is lost in pairwise clustering . In the above example , only the clusters 9 . { 1,2 } and 9.3 are consid ered , while alternative clusters 9 . { 1,3 } and 9.2 are ignored . Ideally , information about all the possible intermediate clusters should be aggregated , but this is intractable in practice . 3.2.2 Hierarchical Graph Factorization We use the divergence distance : ( X , Y ) = xij Clustering ij ( xij log yij xij + yij ) . ( 2006 ) showed Our new method HGFC derives a probabilistic bipartite graph from the similarity matrix ( Yu et al. , 2006 ) . The local and global clustering structures are that this cost function is non-increasing under the update rule : wi j learned via the random walk properties of the graph.The method does not suffer from the above prob h ip hip j ( H H T ) ij p hjp s.t . h ip = 1 ( 2 ) i lems with AGG . Firstly , there is no error propagation p p wij ( H H T ) ij hip hjp s.t . p = wij ( 3 ) because the decision on a verb s membership at any j p ij level is delayed until the full bipartite graph is available and until a tree structure can be extracted from it by aggregating probabilistic information from all the levels . Secondly , the bipartite graph enables the wij can be interpreted as the probability of the direct transition between vi and vj : wij = p ( vi , vj ) , when L ; ij wij = 1. bip can be interpreted as : construction of a hierarchical structure without any intermediate classes . For example , we can group classes 9 . { 1,2,3 } directly into class 9 . We use HGFC with the distributional similarity bip biq di pq m ( 4 ) measure JensenShannon Divergence ( djs ( v , v ) ) . D = diag ( d1 , ... , dn ) where di = bip Given a set of verbs , V = { vn } N , we p=0 compute a similarity matrix W where Wij =exp ( djs ( v1 , v2 ) ) . The graph G and the cluster structure can be represented by a bipartite graph K ( V , U ) . V are the p ( up , uq ) is the similarity between the clusters . It takes into account of a weighted average of contributions from all the data . This is different from the linkage method where only the data from two clusters are considered.Given the cluster similarity p ( up , uq ) , we can con vertices on G. The cluster algorithm can be applied clusters . The matrix B denotes the n m adjacency matrix , with bip being the connection weight between the vertex vi and the cluster up . Thus , B represents the connections between clusters at an upper and lower level of clustering . A flat clustering algorithm can be induced by computing B . This process can go on itera tively , leading to a hierarchical graph . Algorithm 1 HGFC algorithm ( Yu et al. , 2006 ) Require : N verbs V , number of clusters ml for L levels Compute the similarity matrix W0 from V Build the graph G0 from W0 , and m0 n for l = 1 , 2 to L do Factorize Gl 1 to obtain bipartite graph Kl with the ( W ) between vi and vj : w L ; m p=1 bip bjp p adjacency matrix Bl ( eq . 1 , 2 and 3 ) Build a graph Gl with similarity matrix Wl = ( B 1BT ) ij where = diag ( 1 , ... , m ) . There fore , B can be found by approximating the similarity matrix W of G using W derived from K . Given a distance function between two similarity matrices , B approximates W by minimizing the cost function ( W , B 1BT ) . ( 2006 ) performs the extraction via a propagation of probabilities from the bottom level clusters . This prevented us from extracting a Levin style hierarchical classification in our initial experiments . For example , where two verbs were grouped together at a lower level , they could belong to separate clusters at an upper level . We therefore propose a new tree extraction algorithm ( Algorithm 2 ) . The new algorithm starts from the top level bipartite graph , and generates consistent labels for each level by taking into account of the tree constraints set at upper levels . However , this information is not always available ( e.g . when the goal is to actually learn this information automatically ) . We therefore propose a method for inferring the cluster structure from data . A walker can also go to other vertices via multi-hop transitions . According to the chain rule of the Markov process , the multi-hop transitions indicate a decaying similarity function on the graph ( Yu et al. , 2006 ) . After t transitions , the similarity matrix ( Wt ) becomes : Wt = Wt 1 D0 W0Yu et al . ( 2006 ) proved the correspondence be tween the HGFC levels ( l ) and the random walk time : A lgorithm 2 Tree extraction algorithm for HGFC Require : Given N , ( Bl , ml ) on each level for L levels On the top level L , collect the labels T L ( eq . 5 ) Define C to be a ( mL 1 mL ) zero matrix , Cij 1 , where i , j = arg maxi , j { BL } t = 2l 1 . So the vertices at level l induce a similarity matrix of verbs after t-hop transitions . The decaying similarity function captures the different scales of clustering structure in the data ( Azran and Ghahramani , 2006b ) . The upper levels would have for l = L 1 to 1 do i = 1 to N do a smalle r numb er of cluster s which repres ent a more for Compute p ( vl |vi ) for each cluster p ( eq . After several levels , all the verbs l l are expected to be grouped into one cluster . The ti = argmaxp { p ( vp |vi ) |p = 1 ... ml , Cptl+1 = 0 } end for Redefine C to be a ( ml 1 ml ) zero matrix , Cij 1 , where i , j = arg maxi , j { Bl } number of levels and clusters at each level can thus be learned automatically.We therefore propose a method that uses the de end for return Tree consistent labels T L , T L 1 ... T 1 caying similarity function to learn the hierarchical clustering structure . One simple modification to algorithm 1 is to set the number of clusters at level l ( ml ) to be ml 1 1. m is denoted as the number of clusters that have at least one member according to eq . We start by treating each verb as a cluster at the bottom level . The algorithm stops when all the data points are merged into one cluster . The increasingly decaying similarity causes many clusters to have 0 members especially at lower levels , which are pruned in the tree extraction . 3.2.4 Adding constraints to HGFCThe basic version of HGFC makes no prior as where nij is the size of the intersection between class i and cluster j . We used normalized mutual information ( NMI ) and F-Score ( F ) to evaluate hierarchical clustering results on T2 and T3 . NMI measures the amount of statistical information shared by two random variables representing the clustering result and the gold- standard labels . Given random variables A and B : NMI ( A , B ) = I ( A ; B ) [ H ( A ) + H ( B ) ] /2 sumptions about the classification . It is useful I ( A , B ) = | ( vk cj | log N |vk cj | for learning novel verb classifications from scratch . However , when wishing to extend an existing classification ( e.g . VerbNet ) it may be desirable to guide the clustering performance on the basis of information that is already known . We propose a constrained version of HGFC which makes uses of labels at the bottom level to learn upper level classifications . We do this by adding soft constraints to clustering , following Vlachos et al . We modify the similarity matrix W as follows : If two verbs have different labels ( li = lj ) , the similarity between them is decreased by a factor a , and a < 1 . We set a to 0.5 in the experiments . The re k j N |vk ||cj | where |vk cj | is the number of shared membership between cluster vk and gold-standard class cj . The normalized variant of mutual information ( MI ) enables the comparison of clustering with different cluster numbers ( Manning et al. , 2008 ) . F is the harmonic mean of precision ( P ) and recall ( R ) . P is calculated using modified purity a global measure which evaluates the mean precision of clusters . Each cluster is associated with its prevalent class . The number of verbs in a cluster K that take this class is denoted by nprevalent ( K ) . sulting tree is generally consistent with the original L ; nprevalent ( ki ) > 2 nprevalent ( ki ) classification . The influence of the underlying data mPUR = number of verbs ( domain or features ) is reduced according to a . We applied the clustering methods introduced in section 3 to the test sets described in section 2 and evaluated them both quantitatively and qualitatively , as described in the subsequent sections . We used class based accuracy ( ACC ) and adjusted rand index ( Radj ) to evaluate the results on the flat test set T1 ( see section 2 for details of T1T3 ) . ACC is the proportion of members of dominant clusters DOMCLUSTi within all classes ci . ACC = i=1 verbs in DOMCLUSTi number of verbs The formula of Radj is ( Hubert and Arabie , 1985 ) : R is calculated using ACC . F = 2 mPUR ACC mPUR + ACC F is not suitable for comparing results with different cluster numbers ( Rosenberg and Hirschberg , 2007 ) . Therefore , we only report NMI when the number of classes in clustering and gold-standard is substantially different . Finally , we supplemented quantitative evaluation with qualitative evaluation of clusters produced by different methods . We first evaluated AGG and the basic ( unconstrained ) HGFC on the small flat test set T1 . The main purpose of this evaluation was to compare the results of our methods against previously published results on the same test set . The number of clus L ; ( nij L ; ( ni L ; ( n j / ( n ters ( K ) and levels ( L ) were inferred automatically Radj = i , j 2 i 2 j 2 2 2 [ L ; i ( 2 + L ; j ( 2 ] L ; i ( 2 L ; j ( 2 / ( 2 for HGFC as described in section 3.2.3 . However , to 1 ni n j ni n j n make the results comparable with previously published ones , we cut the resulting hierarchy at the level of closest match ( 12 clusters ) to the K ( 13 ) in the gold-standard . For AGG , we cut the hierarchy at 13 clusters . In this experiment , we used the same feature set as Stevenson and Joanis ( 2003 ) ( set B , see section 3.1 ) and were therefore able to reproduce their AGG result with a difference smaller than 2 % . When using this simple feature set , HGFC outperforms the best performing AGG clearly : 8.5 % in ACC and 7.3 % in Radj . We also compared HGFC against the best reported clustering method on T1 to date that of spectral clustering by Sun and Korhonen ( 2009 ) . We used the feature sets C and D which are similar to the features ( SCF parameterized by lexical prefences ) in their experiments . HGFC obtains F of 49.93 % on T1 which is 5 % lower than the result of Sun and Korhonen ( 2009 ) . The difference comes from the tree consistency requirement . When the HGFC is forced to produce a flat clustering ( a one level tree only ) , it achieves the F of 52.55 % which is very close to the performance of spectral clustering . We then evaluated our methods on the hierarchical test sets T2 and T3 . In the first set of experiments , we predefined the tree structure for HGFC by setting L to 3 and K at each level to be the K in the hierarchical gold standard . The hierarchy produced by AGG was cut into 3 levels according to K s in the gold standard . This enabled direct evaluation of the results against the 3 level gold standards using both NMI and F. The results are reported in . In these tables , Nc is the number of clusters in HGFC clustering while Nl is the number of classes in the gold standard ( the two do not always correspond perfectly because a few clusters have zero members ) . As with T1 , HGFC outperforms AGG clearly . The benefit can now be seen at 3 different levels of hierarchy . On average , the HGFC outperforms AGG 3.5 % in NMI and 4.8 % in F. The difference between the methods becomes clearer when moving towards the upper levels of the hierarchy . The results are generally generally better on this test set than on T2 which is to be expected since T3 is a refined subset of T21 . Recall that the constrained version of HGFC learns the upper levels of classification on the basis of soft constraints set at the bottom level , as described earlier in section 3.2.4 . As a consequence , NMI and F are both greater than 90 % at the bottom level and the results at the top level are notably lower because the impact of the constraints degrades the further away one moves from the bottom level . Yet , the relatively high result across all levels shows that the constrained version of HGFC can be employed a useful method to extend the hierarchical structure of known classifications . 1 NMI is higher on T2 , however , because NMI has a higher baseline for larger number of clusters ( Vinh et al. , 2009 ) . NMI is not ideal for comparing the results of T2 and T3 . T 2 T 3 Nc Nl H GF C Nc Nl H GF C 14 8 13 3 53 .2 6 64 32 54 .9 1 97 11 7 49 .8 5 35 32 50 .8 3 46 51 33 .5 5 20 14 44 .0 2 19 51 25 .8 0 10 14 34 .4 1 9 51 19 .1 7 6 11 32 .2 7 3 51 13 .0 6 6 levels are learned for T2 and 5 for T3 . The number of clusters produced ranges from 3 to 148 for T2 and from 6 to 64 for T3 . We can see that the automatically detected cluster numbers distribute evenly across different levels . The scale of the clustering structure is more complete here than in the gold standards . This evaluation is not fully reliable because the match between the gold standard and the clustering is poor at some levels of hierarchy . However , it is encouraging to see that the results do not drop dramatically until the match between the two is really poor . To gain a better insight into the performance of HGFC , we conducted further qualitative analysis of the clusters the two versions of this method produced for T3 . We focussed on the top level of 11 clusters ( in the evaluation against the hierarchical gold standard , As expected , the constrained HGFC kept many individual verbs belonging to same Verbnet subclass together ( e.g . verbs enjoy , hate , disdain , regret , love , despise , detest , dislike , fear for the class 31.2.1 ) so that most clusters simply group lower level classes and their members together . Three nearly clean clusters were produced which only include sub-classes of the same class ( e.g . 31.2.0 and 31.2.1 which both belong to 31.2 Admire verbs ) . However , the remaining 8 clusters group together sub-classes ( and their members ) belonging to unrelated parent classes . Interestingly , 6 of these make both syntactic and semantic sense . For example , several such 37.7 Say verbs and 29.5 Conjencture verbs are found together which share the meaning of communication and which take similar sentential complements . In contrast , none of the clusters produced by the unconstrained HGFC represent a single VerbNet class . The majority represent a high number of classes and fewer members per class . Yet many of the clusters make syntactic and semantic sense . A good example is a cluster which includes member verbs from 9.7 Spray/Load verbs , 21.2 Carve verbs , 51.3.1 Roll verbs , and 10.4 Wipe verbs . The verbs included in this cluster share the meaning of specific type of motion and show similar syntactic behaviour . Thorough Levin style investigation of especially the unconstrained method would require looking at shared diathesis alternations between cluster members . We left this for future work . However , the analysis we conducted confirmed that the constrained method could indeed be used for extending known classifications , while the unconstrained method is more suitable for acquiring novel classifications from scratch . The errors in clusters produced by both methods were mostly due to syntactic idiosyncracy and the lack of semantic information in clustering . We plan to address the latter problem in our future work . We have introduced a new graph-based method HGFC to hierarchical verb clustering which avoids some of the problems ( e.g . error propagation , pairwise cluster merging ) reported with the frequently used AGG method . We modified HGFC so that it can be used to automatically determine the tree structure for clustering , and proposed two extensions to it which make it even more suitable for our task . The first involves automatically determining the number of clusters to be produced , which is useful when this is not known in advance . The second involves adding soft constraints to guide the clustering performance , which is useful when aiming to extend existing classification . The results reported in the previous section are promising . On a flat test set ( T1 ) , the unconstrained version of HGFC outperforms AGG and performs very similarly with the best current flat clustering method ( spectral clustering ) evaluated on the same dataset . On the hierarchical test sets ( T2 and T3 ) , the unconstrained and constrained versions of HGFC outperform AGG clearly at all levels of classification . The constrained version of HGFC detects the missing hierarchy from the existing gold standards with high accuracy . When the number of clusters and levels is learned automatically , the unconstrained method produces a multi-level hierarchy . Our evaluation against a 3-level gold standard shows that such a hierarchy is fairly accurate . Finally , the results from our qualitative evaluation show that both constrained and unconstrained versions of HGFC are capable of learning valuable novel information not included in the gold standards . The previous work on Levin style verb classification has mostly focussed on flat classifications using methods suitable for flat clustering ( Schulte im Walde , 2006 ; Joanis et al. , 2008 ; Sun et al. , 2008 ; Li and Brew , 2008 ; Korhonen et al. , 2008 ; O Se aghdha and Copestake , 2008 ; Vlachos et al. , 2009 ) . However , some works have employed hierarchical clustering as a method to infer flat clustering . For example , Schulte im Walde and Brew ( 2002 ) employed AGG to initialize the KMeans clustering for German verbs . This gave better results than random initialization . Stevenson and Joanis ( 2003 ) used AGG for flat clustering on T1 . They cut the hierarchy at the number of classes in the gold standard and found that it is difficult to automatically determine a good cutoff . Our evaluation in the previous section shows that HGFC outperforms their implementation of AGG . AGG was also used by Ferrer ( 2004 ) who performed hierarchical clustering of 514 Spanish verbs . The results were evaluated against a hierarchical gold standard resembling that of Levin s classification in English ( Va zquez et al. , 2000 ) . Radj of 0.07 was reported for a 15-way classification which is comparable to the result of Stevenson and Joanis ( 2003 ) . Hierarchical clustering has also been performed for the related task of semantic verb classification . For example , Basili et al . ( 1993 ) identified the prob lems of AGG , and applied a conceptual clustering algorithm ( Fisher , 1987 ) to Italian verbs . They used semi-automatically acquired semantic roles and the concept types as features . No quantitative results were reported . The qualitative evaluation shows that the resulting clusters are very fine-grained . Schulte im Walde ( 2008 ) performed hierarchical clustering of German verbs using human verb association as features and AGG as a method . They focussed on two small collections of 56 and 104 verbs and evaluated the result against flat gold standard extracted from GermaNet ( Kunze and Lemnitzer , 2002 ) and German FrameNet ( Erk et al. , 2003 ) , respectively . They reported F of 62.69 % for the 56 verbs , and F of 34.68 % for the 104 verbs . In the future , we plan to extend this research line in several directions . First , we will try to determine optimal features for different levels of clustering . For example , the general syntactic features ( e.g . SCF ) may perform the best at top levels of a hierarchy while more specific or refined features ( e.g . SCF+pp ) may be optimal at lower levels . We also plan to investigate incorporating semantic features , like verb selectional preferences , in our feature set . It is likely that different levels of clustering require more or less specific selectional preferences . One way to obtain the latter is hierarchical clustering of relevant noun data . In addition , we plan to apply the unconstrained HGFC to specific domains to investigate its capability to learn novel , previously unknown classifications . As for the constrained version of HGFC , we will conduct a larger scale experiment on the Verb- Net data to investigate what kind of upper level hierarchy it can propose for this resource ( which currently has over 100 top level classes ) . Finally , we plan to compare HGFC to other hierarchical clustering methods that are relatively new to NLP but have proved promising in other fields , including Bayesian Hierarchical Clustering ( Heller and Ghahramani , 2005 ; Teh et al. , 2008 ) and the method of Azran and Ghahramani ( 2006a ) based on spectral clustering . Our work was funded by the Royal Society University Research Fellowship ( AK ) , the Dorothy Hodgkin Postgraduate Award ( LS ) , the EPSRC grants EP/F030061/1 and EP/G051070/1 ( UK ) and the EU FP7 project PANACEA . This paper describes a Chinese word segmentation system that is based on majority voting among three models : a forward maximum matching model , a conditional random field ( CRF ) model using maximum subword-based tagging , and a CRF model using minimum subword- based tagging . In addition , it contains a post-processing component to deal with inconsistencies . Testing on the closed track of CityU , MSRA and UPUC corpora problem . In the next step , the solutions from these three methods are combined via the hanzi- level majority voting algorithm . Then , a post- processing procedure is applied in order to to get the final output . This procedure merges adjoining words to match the dictionary entries and then splits words which are inconsistent with entries in the training corpus . Input Sentence in the third SIGHAN Chinese Word Segmentation Bakeoff , the system achieves a F-score of 0.961 , 0.953 and 0.919 , respectively . Tokenizing input text into words is the first step of any text analysis task . In Chinese , a sentence is written as a string of characters , to which we shall refer by their traditional name of hanzi , without separations between words . As a result , before any text analysis on Chinese , word segmentation task Forward Maximum Matching CRF with Maximum Subword based Tagging Majority Voting Post processing Result CRF with Minimum Subword based Tagging has to be completed so that each word is isolated by the word-boundary information . Participating in the third SIGHAN Chinese Word Segmentation Bakeoff in 2006 , our system is tested on the closed track of CityU , MSRA and UPUC corpora . The sections below provide a detailed description of the system and our experimental results . The maximum matching algorithm is a greedy segmentation approach . It proceeds through the sentence , mapping the longest word at each point with an entry in the dictionary . In our system , the well-known forward maximum matching algorithm ( Chen and Liu , 1992 ) is implemented . The maximum matching approach is simple and efficient , and it results in high in-vocabulary accuracy ; However , the small size of the dictionary , which is obtained only from the training data , is a major bottleneck for this approach to be applied by itself . 126 Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing , pages 126 129 , Sydney , July 2006 . Qc 2006 Association for Computational Linguistics 2.2 CRF Model with Maximum . Subword-based Tagging Conditional random fields ( CRF ) , a statistical sequence modeling approach ( Lafferty et al. , 2001 ) , has been widely applied in various sequence learning tasks including Chinese word segmentation . In this approach , most existing methods use the character-based IOB tagging . For example , ; g ( all ) 3 ' : :KJ ! ! ( extremely important ) is labeled as ; g ( all ) /O 3 ' ( until ) /B : :K ( close ) /I J ! ! ( heavy ) /I ( demand ) /I . Recently ( Zhang et al. , 2006 ) proposed a maximum subword-based IOB tagger for Chinese word segmentation , and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions . In this method , all single-hanzi words and the top frequently occurring multihanzi words are extracted from the training corpus to form the lexicon subset . Then , each word in the training corpus is segmented for IOB tagging , with the forward maximum matching algorithm , using the formed lexicon subset as the dictionary . In the above example , the tagging labels become ; g ( all ) /O3 ' ( until ) /B : :K ( close ) /I J ! ! ( important ) /I , assuming that J ! ! ( important ) is the longest sub word in this word , and it is one of the top frequently occurring words in the training corpus . After tagging the training corpus , we use the package CRF++1 to train the CRF model . Suppose w0 represents the current word , w 1 is the first word to the left , w 2 is the second word to the left , w1 is the first word to the right , and w2 is the second word to the right , then in our experiments , the types of unigram features used include w0 , w 1 , w1 , w 2 , w2 , w0w 1 , w0w1 , w 1w1 , w 2w 1 , and w2w0 . In addition , only combinations of previous observation and current observation are exploited as bigram features . 2.3 CRF Model with Minimum . Subword-based Tagging In our third model , we applies a similar approach as in the previous section . However , instead of finding the maximum subwords , we explore the minimum subwords . At the beginning , we build the dictionary using the whole training corpus . Then , for each word in the training data , a forward shortest matching is used to get the sequence of minimum-length subwords , and this sequence is 1 available from http : //www/chasen.org/ taku/software tagged in the same IOB format as before . Suppose a , ac , de and acde are the only entries in the dictionary . Then , for the word acde , the sequence of subwords is a , c and de , and the tags assigned to acde are a/B c/I de/I . After tagging the training data set , CRF++ package is executed again to train this type of model , using the identical unigram and bigram feature sets that are used in the previous model . Meanwhile , the unsegmented test data is segmented by the forward shortest matching algorithm . After this initial segmentation process , the result is fed into the trained CRF model for re- segmentation by assigning IOB tags . Having the segmentation results from the above three models in hand , in this next step , we adopt the hanzi-level majority voting algorithm . First , for each hanzi in a segmented sentence , we tag it either as B if it is the first hanzi of a word or a single-hanzi word , or as I otherwise . Then , for a given hanzi in the results from those three models , if at least two of the models provide the identical tag , it will be assigned that tag . For instance , suppose a c de is the segmentation result via forward maximum matching , and it is also the result from CRF model with maximum subword- based tagging , and ac d e is the result from the third model . Then , for a , since all of them assign B to it , a is given the B tag ; for c , because two of segmentations tag it as B , c is given the B tag as well . Similarly , the tag for each remaining hanzi is determined by this majority voting process , and we get a c de as the result for this example . To test the performance of each of the three models and that of the majority voting , we divide the MSRA corpus into training set and held- out set . Throughout all the experiments we conducted , we discover that those two CRF models perform much better than the pure hanzi-based CRF method , and that the voting process improves the performance further . While analyzing errors with the segmentation result from the held-out set , we find two inconsistency problems : First , the inconsistency between the dictionary and the result : that is , certain words that appear in the dictionary are separated into consecutive words in the test result ; Second , the inconsistency among words in the dictionary ; For instance , both t ' '' ltlil'Jl ( scientific research ) and t ' '' lt ( science ) lil'Jl ( research ) appear in the training corpus . To deal with the first phenomena , for the segmented result , we try to merge adjoining words to match the dictionary entries . Suppose a b c de are the original voting result , and ab , abc and cd form the dictionary . Then , we merge a , b and c together to get the longest match with the dictionary . Therefore , the output is abc de . For the second problem , we introduce the split procedure . In our system , we only consider two consecutive words . First , all bigrams are extracted from the training corpus , and their frequencies are counted . After that , for example , if a b appears more often than ab , then whenever in the test result we encounter ab , we split it into a b . The post-processing steps detailed above attempt to maximize the value of known words in the training data as well as attempting to deal with the word segmentation inconsistencies in the training data . The third International Chinese Language Processing Bakeoff includes four different corpora , Academia Sinica ( CKIP ) , City University of Hong Kong ( CityU ) , Microsoft Research ( MSRA ) , and University of Pennsylvania and University of Colorado , Boulder ( UPUC ) , for the word segmentation task . In this bakeoff , we test our system in CityU , MSRA and UPUC corpora , and follow the closed track . That is , we only use training material from the training data for the particular corpus we are testing on . No other material or any type of external knowledge is used , including part-of-speech information , externally generated word-frequency counts , Arabic and Chinese numbers , feature characters for place names and common Chinese surnames . 3.1 Results on SIGHAN Bakeoff 2006 . To observe the result of majority voting and the contribution of the post-processing step , the experiment is ran for each corpus by first producing the outcome of majority voting and then producing the output from the post-processing . In each experiment , the precision ( P ) , recall ( R ) , F-measure ( F ) , Out-of-Vocabulary rate ( OOV ) , OOV recall rate ( ROOV ) , and In-Vocabulary rate ( RI V ) are recorded . , , show the scores for the CityU corpus , for the MSRA corpus , and for the UPUC corpus , respectively . In addition , the post- processing step indeed helps to improve the performance . The errors that occur in our system are mainly due to the following three factors : First , there is inconsistency between the gold segmentation and the training corpus . Although the inconsistency problem within the training corpus is intended to be tackled in the post-processing step , we can not conclude that the segmentation for certain words in the gold test set always follows the convention in the training data set . For example , in the MSRA training corpus , 00 ll & JM ( Chinese government ) is usually considered as a single word ; while in the gold test set , it is separated as two words 00 ( Chinese ) and ll & JM ( government ) . This inconsistency issue lowers the system performance . This problem , of course , affects all competing systems . Second , we don t have specific steps to deal with words with postfixes such as ( person ) . Compared to our system , ( Zhang , 2005 ) proposed a segmentation system that contains morphologically derived word recognition post-processing component to solve this problem . Lacking of such a step prevents us from identifying certain types of words such as } J ( worker ) to be a single word . In addition , the unknown words are still troublesome because of the limited size of the training corpora . In the class of unknown words , we encounter person names , numbers , dates , organization names and words translated from languages other than Chinese . For example , in the produced CityU test result , the translated person name * ft1l ( Mihajlovic ) is incorrectly separatedas * ft 1 and l . Moreover , in cer tain cases , person names can also create ambiguity . Take the name Of d /J ( Qiu , Beifang ) in UPUC test set for example , without understanding the meaning of the whole sentence , it is difficult even for human to determine whether it is a person name or it represents Of ( autumn ) , d /J ( north ) , with the meaning of the autumn in the north . In designing the voting procedure , we also attempt to develop and use a segmentation lattice , which proceeds using a similar underlying principle as the one applied in ( Xu et al. , 2005 ) . In our approach , for an input sentence , the segmentation result using each of our three models is transformed into an individual lattice . Also , each edge in the lattice is assigned a particular weight , according to certain features such as whether or not the output word from that edge is in the dictionary . After building the three lattices , one for each model , we merge them together . Then , the shortest path , referring to the path that has the minimum weight , is extracted from the merged lattice , and therefore , the segmentation result is determined by this shortest path . However , in the time we had to run our experiments on the test data , we were unable to optimize the edge weights to obtain high accuracy on some held-out set from the training corpora . So instead , we tried a simple method for finding edge weights by uniformly distributing the weight for each feature ; Nevertheless , by testing on the shared task data from the 2005 SIGHAN bakeoff , the performance is not competitive , compared to our simple majority voting method described above . As a result , we decide to abandon this approach for this year s SIGHAN bakeoff . Our Chinese word segmentation system is based on majority voting among the initial outputs from forward maximum matching , from a CRF model with maximum subword-based tagging , and from a CRF model with minimum subword-based tagging . In addition , we experimented with various steps in post-processing which effectively boosted the overall performance . In future research , we shall explore more sophisticated ways of voting , including the continuing investigation on the segmentation lattice approach . Also , more powerful methods on how to accurately deal with unknown words , including person and place names , without external knowledge , will be studied as well . Text Segmentation Using Reiteration and Collocation A method is presented for segmenting text into subtopic areas . The proportion of related pairwise words is calculated between adjacent windows of text to determine their lexical similarity . The lexical cohesion relations of reiteration and collocation are used to identify related words . These relations are automatically located using a combination of three linguistic features : word repetition , collocation and relation weights . This method is shown to successfully detect known subject changes in text and corresponds well to the segmentations placed by test subjects . Many examples of heterogeneous data can be found in daily life . The Wall Street Journal archives , for example , consist of a series of articles about different subject areas . Segmenting such data into distinct topics is useful for information retrieval , where only those segments relevant to a user 's query can be retrieved . Text segmentation could also be used as a pre-processing step in automatic summarisation . Each segment could be summarised individually and then combined to provide an abstract for a document . Previous work on text segmentation has used term matching to identify clusters of related text . Salton and Buckley ( 1992 ) and later , Hearst ( 1994 ) extracted related text pmtions by matching high frequency terms . Yaari ( 1997 ) segmented text into a hierarchical structure , identifying sub-segments of larger segments . Ponte and Croft ( 1997 ) used word co-occurrences to expand the number of terms for matching . Reynar ( 1994 ) compared all Lindsay J. Evett Department of Computing Nottingham Trent University Nottingham NGI 4BU , UK lje @ doc.ntu.ac.uk words across a text rather than the more usual nearest neighbours . A problem with using word repetition is that inappropriate matches can be made because of the lack of contextual information ( Salton et al. , 1994 ) . Another approach to text segmentation is the detection of semantically related words . Hearst ( 1993 ) incorporated semantic information derived from WordNet but in later work reported that this information actually degraded word repetition results ( Hearst , 1994 ) . Related words have been located using spreading activation on a semantic network ( Kozima , 1993 ) , although only one text was segmented . Another approach extracted semantic information from Roget 's Thesaurus ( RT ) . Lexical cohesion relations ( Halliday and Hasan , 1976 ) between words were identified in RT and used to construct lexical chains of related words in five texts ( Morris and Hirst , 1991 ) . It was reported that the lexical chains closely correlated to the intentional structure ( Grosz and Sidner , 1986 ) of the texts , where the start and end of chains coincided with the intention ranges . However , RT does not capture all types of lexical cohesion relations . In previous work , it was found that collocation ( a lexical cohesion relation ) was under-represented in the thesaurus . Furthermore , this process was not automated and relied on subjective decision making . Following Morris and Hirst 's work , a segmentation algorithm was developed based on identifying lexical cohesion relations across a text . The proposed algorithm is fully automated , and a quantitative measure of the association between words is calculated . This algorithm utilises linguistic features additional to those captured in the thesaurus to identify the other types of lexical cohesion relations that can exist in text . 1 Background Theory : Lexical Cohesion . Cohesion concerns how words in a text are related . The major work on cohesion in English was conducted by Halliday and Hasan ( 1976 ) . An instance of cohesion between a pair of elements is referred to as a tie . Ties can be anaphoric or cataphoric , and located at both the sentential and suprasentential level . Halliday and Hasan classified cohesion under two types : grammatical and lexical . Grammatical cohesion is expressed through the grammatical relations in text such as ellipsis and conjunction . Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words . Identifying semantic relations in a text can be a useful indicator of its conceptual structure . Lexical cohesion is divided into three classes : general noun , reiteration and collocation . General noun 's cohesive function is both grammatical and lexical , although Halliday and Hasan 's analysis showed that this class plays a minor cohesive role . Consequently , it was not further considered . Reiteration is subdivided into four cohesive effects : word repetition ( e.g . ascent and ascent ) , synonym ( e.g . ascent and climb ) which includes near-synonym and hyponym , superordinate ( e.g . ascent and task ) and general word ( e.g . The effect of general word is difficult to automatically identify because no common referent exists between the general word and the word to which it refers . A collocation is a predisposed combination of words , typically pairwise words , that tend to regularly co-occur ( e.g . All semantic relations not classified under the class of reiteration are attributed to the class of collocation . To automatically detect lexical cohesion tics between pairwise words , three linguistic features were considered : word repetition , collocation and relation weights . The first two methods represent lexical cohesion relations . Word repetition is a component of the lexical cohesion class of reiteration , and collocation is a lexical cohesion class in its entirety . The remaining types of lexical cohesion considered , include synonym and superordinate ( the cohesive effect of general word was not included ) . These types can be identified using relation weights ( Jobbins and Evett , 1998 ) . Word repetition : Word repetition ties in lexical cohesion are identified by same word matches and matches on inflections derived from the same stem . An inflected word was reduced to its stem by look­ up in a lexicon ( Keenan and Evett , 1989 ) comprising inflection and stem word pair records ( e.g . `` orange oranges '' ) . Relation Weights : Relation weights quantify the amount of semantic relation between words based on the lexical organisation of RT ( Jobbins and Evett , 1995 ) . A thesaurus is a collection of synonym groups , indicating that synonym relations are captured , and the hierarchical structure of RT implies that superordinate relations are also captured . An alphabetically-ordered index of RT was generated , referred to as the Thesaurus Lexicon ( TLex ) . Relation weights for pairwise words are calculated based on the satisfaction of one or more of four possible connections in TLex . The proposed segmentation algorithm compares adjacent windows of sentences and determines their lexical similarity . A window size of three sentences was found to produce the best results . Multiple sentences were compared because calculating lexical similarity between words is too fine ( Rotondo , 1984 ) and between individual sentences is unreliable ( Salton and Buckley , 1991 ) . Lexical similarity is calculated for each window comparison based on the proportion of related words , and is given as a normalised score . Word repetitions are identified between identical words and words derived from the same stem . troughs placed subject change linguistic feature points located average std . ( out of 42 poss . ) word repetition 7.1 3.16 41 collocation ( 97.6 % ) word repetition 7.3 5.22 41 relation weights ( 97.6 % ) 41 Collocations are located by looking up word pairs in the collocation lexicon . Relation weights are word repetition 8.5 3.62 ( 97.6 % ) calculated between pairwise words according to their location in RT . The lexical similarity score indicates the amount of lexical cohesion demonstrated by two windows . Scores plotted on a graph show a series of peaks ( high scores ) and troughs ( low scores ) . Low scores indicate a weak collocation 5.8 3.70 40 relation weights ( 95.2 % ) word repetition 40 collocation 6.4 4.72 ( 95.2 % ) relation weights 39 level of cohesion . Hence , a trough signals a potential subject change and texts can be relation weights 7 4.23 ( 92.9 % ) segmented at these points . An investigation was conducted to determine whether the segmentation algorithm could reliably locate subject change in text . Method : Seven topical articles of between 250 to 450 words in length were extracted from the World Wide Web . A total of 42 texts for test data were generated by concatenating pairs of these articles . Hence , each generated text consisted of two articles . The transition from the first article to the second represented a known subject change point . Previous work has identified the breaks between concatenated texts to evaluate the performance of text segmentation algorithms ( Reynar , 1994 ; Stairmand , 1997 ) . For each text , the troughs placed by the segmentation algorithm were compared to the location of the known subject change point in that text . An error margin of one sentence either side of this point , determined by empirical analysis , was allowed . collocation 6.3 3.83 35 ( 83.3 % ) /S > Discussion : The segmentation algorithm using the linguistic features word repetition and collocation in combination achieved the best result . A total of 41 out of a possible 42 known subject change points were identified from the least number of troughs placed per text ( 7.I ) . For the text where the known subject change point went undetected , a total of three troughs were placed at sentences 6 , 11 and 18 . The subject change point occurred at sentence 13 , just two sentences after a predicted subject change at sentence 11 . In this investigation , word repetition alone achieved better results than using either collocation or relation weights individually . The combination of word repetition with another linguistic feature improved on its individual result , where less troughs were placed per text . ECTION > The objective of the current investigation was to determine whether all troughs coincide with a subject change . The troughs placed by the algorithm were compared to the segmentations identified by test subjects for the same texts . Method : Twenty texts were randomly selected for test data each consisting of approximately 500 words . These texts were presented to seven test subjects who were instructed to identify the sentences at which a new subject area commenced . No restriction was placed on the number of subject changes that could be identified . Segmentation points , indicating a change of subject , were determined by the agreement of three or more test subjects ( Litman ami Passonneau , 1996 ) . Adjacent segmentation points were treated as one point because it is likely that they refer to the same subject change . The troughs placed by the segmentation algorithm were compared to the segmentation points identified by the test subjects . In Experiment 1 , the top five approaches investigated identified at least 40 out of 42 known subject change points . Due to that success , these five approaches were applied in this experiment . To evaluate the results , the information retrieval metrics precision and recall were used . These metrics have tended to be adopted for the assessment of text segmentation algorithms , but they do not provide a scale of correctness ( Beeferman et al. , 1997 ) . The degree to which a segmentation point was missed by a trough , for instance , is not considered . Allowing an error margin provides some degree of flexibility . An error margin of two sentences either side of a segmentation point was used by Hearst ( 1993 ) and Reynar ( 1994 ) allowed three sentences . In this investigation , an error margin of two sentences was considered . Discussion : The segmentation algorithm usmg word repetition and relation weights in combination achieved mean precision and recall rates of 0.80 and 0.69 , respectively . For 9 out of the 20 texts segmented , all troughs were relevant . Therefore , many of the troughs placed by the segmentation algorithm represented valid subject Table 2 . Comparison of troughs to segmentation points placed by the test subjects . Both word repetition in combination with collocation and all three features in combination also achieved a precision rate of 0.80 but attained a lower recall rate of 0.62 . These results demonstrate that supplementing word repetition with other linguistic features can improve text segmentation . As an example , a text segmentation algorithm developed by Hearst ( 1994 ) based on word repetition alone attained inferior precision and recall rates of 0.66 and 0.61 . In this investigation , recall rates tended to be lower than precision rates because the algorithm identified fewer segments ( 4.1 per text ) than the test subjects ( 4.5 ) . Each text was only 500 words in length and was related to a specific subject area . These factors limited the degree of subject change that occurred . Consequently , the test subjects tended to identify subject changes that were more subtle than the algorithm could detect . Conclusion The text segmentation algorithm developed used three linguistic features to automatically detect lexical cohesion relations across windows . The combination of features word repetition and relation weights produced the best precision and recall rates of 0.80 and 0.69 . When used in isolation , the performance of each feature was inferior to a combined approach . This fact provides evidence that different lexical relations are detected by each linguistic feature considered . Areas for improving the segmentation algorithm include incorporation of a threshold for troughs . Currently , all troughs indicate a subject change , however , minor fluctuations in scores may be discounted . Future work with this algorithm should include application to longer documents . With trough thresholding the segments identified in longer documents could detect significant subject changes . Having located the related segments in text , a method of determining the subject of each segment could be developed , for example , for information retrieval purposes . Mining New Word Translations from Comparable Corpora New words such as names , technical terms , etc appear frequently . As such , the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations . Comparable corpora such as news documents of the same period from different news agencies are readily available . In this paper , we present a new approach to mining new word translations from comparable corpora , by using context information to complement transliteration information . We evaluated our approach on six months of Chinese and English Gigaword corpora , with encouraging results . New words such as person names , organization names , technical terms , etc . In order for a machine translation system to translate these new words correctly , its bilingual lexicon needs to be constantly updated with new word translations . Much research has been done on using parallel corpora to learn bilingual lexicons ( Melamed , 1997 ; Moore , 2003 ) . But parallel corpora are scarce resources , especially for uncommon lan guage pairs . Comparable corpora refer to texts that are not direct translation but are about the same topic . For example , various news agencies report major world events in different languages , and such news documents form a readily available source of comparable corpora . Being more readily available , comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations , although relatively less research has been done in the past on comparable corpora . Previous research efforts on acquiring translations from comparable corpora include ( Fung and Yee , 1998 ; Rapp , 1995 ; Rapp , 1999 ) . When translating a word w , two sources of information can be used to determine its translation : the word w itself and the surrounding words in the neighborhood ( i.e. , the context ) of w. Most previous research only considers one of the two sources of information , but not both . For example , the work of ( AlOnaizan and Knight , 2002a ; AlOnaizan and Knight , 2002b ; Knight and Graehl , 1998 ) used the pronunciation of w in translation . On the other hand , the work of ( Cao and Li , 2002 ; Fung and Yee , 1998 ; Koehn and Knight , 2002 ; Rapp , 1995 ; Rapp , 1999 ) used the context of w to locate its translation in a second language . In this paper , we propose a new approach for the task of mining new word translations from comparable corpora , by combining both context and transliteration information . Since both sources of information are complementary , the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone . We fully implemented our method and tested it on ChineseEnglish comparable corpora . We translated Chinese words into English . That is , Chinese is the source language and English is the target language . While we have only tested our method on Chinese-English comparable corpora , our method is general and applicable to other language pairs . The work of ( Fung and Yee , 1998 ; Rapp , 1995 ; Rapp , 1999 ) noted that if an English word e is the translation of a Chinese word c , then the contexts of the two words are similar . We could view this as a document retrieval problem . The context ( i.e. , the surrounding words ) of c is viewed as a query . The context of each candidate translation e ' is viewed as a document . Since the context of the correct translation e is similar to e , is considered as a document in IR . If an English word e is the translation of a Chinese word c , they will have similar contexts . So we use the the context of c , we are likely to retrieve the context of e when we use the context of c as query C ( c ) to retrieve a document C ( e* ) that * the query and try to retrieve the most similar best matches the query . The English word e document . We employ the language modeling approach ( Ng , 2000 ; Ponte and Croft , 1998 ) for corresponding to that document translation of c . C ( e* ) is the this retrieval problem . More details are given in Section 3 . On the other hand , when we only look at the word w itself , we can rely on the pronunciation of w to locate its translation . We use a variant of Within IR , there is a new approach to document retrieval called the language modeling approach ( Ponte & Croft , 98 ) . In this approach , a language model is derived from each document D . Then the probability of generating the query the machine transliteration method proposed by Q according to that language model , P ( Q | D ) , ( Knight and Graehl , 1998 ) . More details are is estimated . The document with the highest given in Section 4 . Each of the two individual methods provides a P ( Q | D ) is the one that best matches the query . ranked list of candidate words , associating with each candidate a score estimated by the particular method . If a word e in English is indeed the translation of a word c in Chinese , then we would expect e to be ranked very high in both lists in general . Specifically , our combination method is as follows : we examine the top M The language modeling approach to IR has been shown to give superior retrieval performance ( Ponte & Croft , 98 ; Ng , 2000 ) , compared with traditional vector space model , and we adopt this approach in our current work . To estimate P ( Q | D ) , we use the approach of ( Ng , 2000 ) . We view the document D as a multinomial distribution of terms and assume that words in both lists and finde1 , e2 , ... , ek that ap query Q is generated by this model : pear in top M positions in both lists . rank these words e1 , e2 , ... , ek according to the P ( Q | D ) = ∏ P ( t | D ) c t average of their rank positions in the two lists . t The candidate ei that is ranked the highest according to the average rank is taken to be the cor where t is a term in the corpus , ct is the number rect translation and is output . If no words appear within the top M positions in both lists , then no translation is output . Since we are using comparable corpora , it is possible that the translation of a new word does not exist in the target corpus . In particular , our experiment was conducted on comparable corpora that are not very closely related and as such , most of the Chinese words have no translations of times term t occurs in the query Q , n = ∑t ct is the total number of terms in query Q . For ranking purpose , the first fraction n ! can be omitted as this part depends on the query only and thus is the same for all the documents . in the English target corpus . In our translation problem , C ( c ) is viewed as the query and C ( e ) is viewed as a document . So our task is to compute P ( C ( c ) | C ( e ) ) for each In a typical information retrieval ( IR ) problem , a query is given and a ranked list of documents English word e and find the e that gives the highest P ( C ( c ) | C ( e ) ) , estimated as : most relevant to the query is returned from a document collection . ∏ P ( tc tc∈C ( c ) | T ( C ( e ) ) ) q ( tc ) For our task , the query is C ( c ) , the context Term tc is a Chinese word . q ( tc ) is the number ( i.e. , the surrounding words ) of a Chinese word c . Each C ( e ) , the context of an English word of occurrenc es of tc in C ( c ) . Tc ( C ( e ) ) is the bag of Chinese words obtained by translating the First , each Chinese character in a Chinese English words in C ( e ) , as determined by a bi word c is converted to pinyin form . Then we sum lingual dictionary . If an English word is ambiguous and has K translated Chinese words listed in the bilingual dictionary , then each of the K trans over all the alignments that this pinyin form of c can map to an English word e. For each possible alignment , we calculate the probability by taking lated Chinese words is counted as occurring 1/K times in Tc ( C ( e ) ) for the purpose of probability the product of each mapping . ble of pinyin , api is the ith sylla li is the English letter sequence estimation . We use backoff and linear interpolation for probability estimation : P ( tc | Tc ( C ( e ) ) ) = α ⋅ Pml ( tc | Tc ( C ( e ) ) ) + ( 1 −α ) ⋅ Pml ( tc ) that the ith pinyin syllable maps to in the particular alignment a . Since most Chinese characters have only one pronunciation and hence one pinyin form , we assume that Chinese character-to-pinyin mapping is one-to-one to simplify the problem . We use the Pml ( tc | Tc ( C ( e ) ) ) = dT ( C ( e ) ) ( tc ) ∑dT ( C ( e ) ) ( t ) expect ation maxi mizati on ( EM ) algorit hm to genera te mappi ng proba bilitie s from pinyin syl c t∈Tc ( C ( e ) ) lables to English letter sequences . To reduce the search space , we limit the number of English letters that each pinyin syllable can map to as 0 , where Pml ( • ) are the maximu m likelihood esti 1 , or 2 . we do not allow cross mappin gs . mates , dT ( C ( e ) ) ( tc ) is the number of occurre nces That is , if an English letter sequenc e e1 precede s of the term tc in Tc ( C ( e ) ) , andPml ( tc ) is esti another English letter sequence e2 in an English mated similarly by counting the occurrences of word , then the pinyin syllable mapped to e1 tc in the Chinese translation of the whole English corpus . α is set to 0.6 in our experiments . must precede the pinyin syllable mapped to e2 . Our method differs from ( Knight and Graehl , 1998 ) and ( AlOnaizan and Knight , 2002b ) in that our method does not generate candidates but For the transliteration model , we use a modified only estimatesP ( e | c ) for candidates e appearmodel of ( Knight and Graehl , 1998 ) and ( Al ing in the English corpus . Another difference is Onaizan and Knight , 2002b ) . Knight and Graehl ( 1998 ) proposed a probabilistic model for machine transliteration . In this model , a word in the target language ( i.e. , English in our task ) is written and pronounced . This pronunciation is converted to source language pronunciation and then to source language word that our method estimates stead of P ( c | e ) and P ( e ) . P ( e | c ) directly , in ( i.e. , Chinese in our task ) . AlOnaizan and Knight ( 2002b ) suggested that pronunciation can be skipped and the target language letters can be mapped directly to source language letters . Pinyin is the standard Romanization system of Chinese characters . For transliteration , we estimate P ( e | c ) as follows : P ( e | c ) = P ( e | pinyin ) = ∑ P ( e , a | pinyin ) a For the Chinese corpus , we used the Linguistic Data Consortium ( LDC ) Chinese Gigaword Corpus from Jan 1995 to Dec 1995 . The corpus of the period Jul to Dec 1995 was used to come up with new Chinese words c for translation into English . The corpus of the period Jan to Jun 1995 was just used to determine if a Chinese word c from Jul to Dec 1995 was new , i.e. , not occurring from Jan to Jun 1995 . Chinese Giga- word corpus consists of news from two agencies : = ∑∏ P ( l a a i | pi ) Xinhua News Agency and Central News Agency . As for English corpus , we used the LDC English Gigaword Corpus from Jul to Dec 1995 . The English Gigaword corpus consists of news from four newswire services : Agence France Press English Service , Associated Press Worldstream English Service , New York Times Newswire Service , and Xinhua News Agency English Service . To avoid accidentally using parallel texts , we did not use the texts of Xinhua News Agency them English translation candidate words . For a Chinese source word occurring within a half- month period p , we looked for its English translation candidate words occurring in news documents in the same period p. 5.3 Translation candidates . The size of the English corpus from Jul to Dec The context C ( c ) of a Chinese word c was col 1995 was about 730M bytes , and the size of the Chinese corpus from Jul to Dec 1995 was about 120M bytes . We used a ChineseEnglish dictionary which contained about 10,000 entries for translating the words in the context . For the training of transliteration probability , we required a ChineseEnglish name list . We used a list of 1,580 ChineseEnglish name pairs as training data for the EM algorithm . lected as follows : For each occurrence of c , we set a window of size 50 characters centered at c. We discarded all the Chinese words in the context that were not in the dictionary we used . The contexts of all occurrences of a word c were then concatenated together to form C ( c ) . The context of an English translation candidate word e , C ( e ) , was similarly collected . The window size of English context was 100 words.After all the counts were collected , we esti mated P ( C ( c ) | C ( e ) ) as described in Section 3 , 5.2 Preprocessing . Unlike English , Chinese text is composed of Chinese characters with no demarcation for words . So we first segmented Chinese text with a Chinese word segmenter that was based on maximum entropy modeling ( Ng and Low , 2004 ) . for each pair of Chinese source word and English translation candidate word . For each Chinese source word , we ranked all its English translation candidate words according to the estimated P ( C ( c ) | C ( e ) ) . For each Chinese source word c and an English translation candidate word e , we also calcu We then divided the Chinese corpus from Jul to Dec 1995 into 12 periods , each containing text lated the probability P ( e | c ) ( as described in from a half-month period . Then we determined the new Chinese words in each half-month period p. By new Chinese words , we refer to those words that appeared in this period p but not from Jan to Jun 1995 or any other periods that preceded p. Among all these new words , we selected those occurring at least 5 times . These words made up our test set . We call these words Chinese source words . They were the words that we were supposed to find translations from the English corpus . For the English corpus , we performed sentence segmentation and converted each word to its morphological root form and to lower case . We also divided the English corpus into 12 periods , each containing text from a half-month period . For each period , we selected those English words occurring at least 10 times and were not present in the 10,000-word ChineseEnglish dictionary we used and were not stop words . We considered these English words as potential translations of the Chinese source words . We call Section 4 ) , which was used to rank the English candidate words based on transliteration . Finally , the English candidate word with the smallest average rank position and that appears within the top M positions of both ranked lists is the chosen English translation ( as described in Section 2 ) . If no words appear within the top M positions in both ranked lists , then no translation is output . Note that for many Chinese words , only one English word e appeared within the top M positions for both lists . And among those cases where more than one English words appeared within the top M positions for both lists , many were multiple translations of a Chinese word . This happened for example when a Chinese word was a non-English person name . The name could have multiple translations in English . For example , 米 洛西娜 was a Russian name . Mirochina and Miroshina both appeared in top 10 positions of both lists . We evaluated our method on each of the 12 half- month periods . The correctness of the English translations was manually checked . We attempted to estimate recall by manually finding the English translations for all the Chinese source words for the two periods Dec 01 – Dec 15 and Dec 16 – Dec 31 in the English part of the corpus . During the whole December period , we only managed to find English translations which were present in the English side of the comparable corpora for 43 Chinese words . Precision and recall for different values of M The past research of ( Fung and Yee , 1998 ; Rapp , 1995 ; Rapp , 1999 ) utilized context information alone and was evaluated on different corpora from ours , so it is difficult to directly compare our current results with theirs . Similarly , AlOnaizan and Knight ( 2002a ; 2002b ) only made use of transliteration information alone and so was not directly comparable . To investigate the effect of the two individual sources of information ( context and transliteration ) , we checked how many translations could be found using only one source of information ( i.e. , context alone or transliteration alone ) , on those Chinese words that have translations in the English part of the comparable corpus . As mentioned earlier , for the month of Dec 1995 , there are altogether 43 Chinese words that have their translations in the English part of the corpus . Since our method currently only considers unigram English words , we are not able to find translations for these words . But it is not difficult to extend our method to handle this problem . We can first use a named entity recognizer and noun phrase chunker to extract English names and noun phrases . Our method is not able to find 43 ( 329 + 205 ) × 4499 = 362words in all 12 pe these translations . But this is due to search space riods . And our program finds correct translations for 115 words . So we estimate that recall ( for M = 10 ) is approximately 115 / 362 = 31.8 % . If we are willing to spend more time on searching , then in principle we can find these translations . And using just transliteration information alone , 9 Chinese words have their correct English translations at rank one position . Hence , our method of using both sources of information outperforms using either information source alone . As pointed out earlier , most previous research only considers either transliteration or context information in determining the translation of a source language word w , but not both sources of information . For example , the work of ( AlOnaizan and Knight , 2002a ; AlOnaizan and Knight , 2002b ; Knight and Graehl , 1998 ) used only the pronunciation or spelling of w in translation . On the other hand , the work of ( Cao and Li , 2002 ; Fung and Yee , 1998 ; Rapp , 1995 ; Rapp , 1999 ) used only the context of w to locate its translation in a second language . In contrast , our current work attempts to combine both complementary sources of information , yielding higher accuracy than using either source of information alone . Koehn and Knight ( 2002 ) attempted to combine multiple clues , including similar context and spelling . But their similar spelling clue uses the longest common subsequence ratio and works only for cognates ( words with a very similar spelling ) . The work that is most similar to ours is the recent research of ( Huang et al. , 2004 ) . They attempted to improve named entity translation by combining phonetic and semantic information . Their contextual semantic similarity model is different from our language modeling approach to measuring context similarity . It also made use of part-of-speech tag information , whereas our method is simpler and does not require part-of- speech tagging . They combined the two sources of information by weighting the two individual scores , whereas we made use of the average rank for combination . In this paper , we proposed a new method to mine new word translations from comparable corpora , by combining context and transliteration information , which are complementary sources of information . We evaluated our approach on six months of Chinese and English Gigaword corpora , with encouraging results . We thank Jia Li for implementing the EM algorithm to train transliteration probabilities . This research is partially supported by a research grant R252000-125112 from National University of Singapore Academic Research Fund . 